"""
çµ±åˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚·ã‚¹ãƒ†ãƒ  - Phase 49å®Œäº†

EnsembleModelã€VotingSystemã€ProductionEnsembleæ©Ÿèƒ½ã‚’1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«çµ±åˆã€‚
é‡è¤‡ã‚³ãƒ¼ãƒ‰ã‚’æ’é™¤ã—ã€ä¿å®ˆæ€§ã¨ã‚³ãƒ¼ãƒ‰ã®å¯èª­æ€§ã‚’å‘ä¸Šã€‚

Phase 49å®Œäº†
"""

import time
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union  # noqa: F401

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

from ..core.config import get_threshold
from ..core.exceptions import DataProcessingError
from ..core.logger import get_logger
from .models import BaseMLModel, LGBMModel, RFModel, XGBModel


class VotingMethod(Enum):
    """æŠ•ç¥¨æ–¹å¼ã®åˆ—æŒ™å‹"""

    SOFT = "soft"  # ç¢ºç‡ãƒ™ãƒ¼ã‚¹ã®æŠ•ç¥¨
    HARD = "hard"  # ã‚¯ãƒ©ã‚¹ãƒ™ãƒ¼ã‚¹ã®æŠ•ç¥¨
    WEIGHTED = "weighted"  # é‡ã¿ä»˜ã‘æŠ•ç¥¨


class VotingSystem:
    """
    æŠ•ç¥¨ã‚·ã‚¹ãƒ†ãƒ 

    è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’çµ±åˆã™ã‚‹ãŸã‚ã®æŠ•ç¥¨ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æä¾›ã€‚
    ã‚½ãƒ•ãƒˆæŠ•ç¥¨ã¨ãƒãƒ¼ãƒ‰æŠ•ç¥¨ã®ä¸¡æ–¹ã«å¯¾å¿œã€‚
    """

    def __init__(
        self,
        method: VotingMethod = VotingMethod.SOFT,
        weights: Optional[Dict[str, float]] = None,
    ):
        """
        æŠ•ç¥¨ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–

        Args:
            method: æŠ•ç¥¨æ–¹å¼ï¼ˆSOFT, HARD, WEIGHTEDï¼‰
            weights: ãƒ¢ãƒ‡ãƒ«é‡ã¿è¾æ›¸
        """
        self.method = method
        self.weights = weights or {}
        self.logger = get_logger()

        self.logger.info(f"âœ… VotingSystem initialized with method: {method.value}")

    def vote(
        self,
        predictions: Dict[str, np.ndarray],
        probabilities: Optional[Dict[str, np.ndarray]] = None,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        æŠ•ç¥¨ã®å®Ÿè¡Œ

        Args:
            predictions: ãƒ¢ãƒ‡ãƒ«åˆ¥äºˆæ¸¬ã‚¯ãƒ©ã‚¹è¾æ›¸
            probabilities: ãƒ¢ãƒ‡ãƒ«åˆ¥äºˆæ¸¬ç¢ºç‡è¾æ›¸ï¼ˆã‚½ãƒ•ãƒˆæŠ•ç¥¨ç”¨ï¼‰

        Returns:
            Tuple[np.ndarray, np.ndarray]: (æœ€çµ‚äºˆæ¸¬, ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢)
        """
        try:
            if self.method == VotingMethod.SOFT:
                return self._soft_voting(probabilities or {})
            elif self.method == VotingMethod.HARD:
                return self._hard_voting(predictions)
            elif self.method == VotingMethod.WEIGHTED:
                return self._weighted_voting(predictions, probabilities)
            else:
                raise ValueError(f"Unsupported voting method: {self.method}")

        except Exception as e:
            self.logger.error(f"Voting failed: {e}")
            raise DataProcessingError(f"Voting process failed: {e}")

    def _soft_voting(self, probabilities: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
        """ã‚½ãƒ•ãƒˆæŠ•ç¥¨ï¼ˆç¢ºç‡ãƒ™ãƒ¼ã‚¹ï¼‰"""
        if not probabilities:
            raise ValueError("Probabilities required for soft voting")

        avg_probabilities = self._average_probabilities(probabilities)
        predictions = np.argmax(avg_probabilities, axis=1)
        confidence = np.max(avg_probabilities, axis=1)

        return predictions, confidence

    def _hard_voting(self, predictions: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
        """ãƒãƒ¼ãƒ‰æŠ•ç¥¨ï¼ˆå¤šæ•°æ±ºï¼‰"""
        if not predictions:
            raise ValueError("Predictions required for hard voting")

        prediction_matrix = np.column_stack(list(predictions.values()))
        model_names = list(predictions.keys())

        final_predictions = []
        vote_agreement = []

        for i in range(prediction_matrix.shape[0]):
            votes = prediction_matrix[i, :]

            if self.weights:
                final_pred = self._weighted_hard_vote(votes, model_names)
            else:
                final_pred = self._simple_majority_vote(votes)

            agreement = np.sum(votes == final_pred) / len(votes)

            final_predictions.append(final_pred)
            vote_agreement.append(agreement)

        return np.array(final_predictions), np.array(vote_agreement)

    def _weighted_voting(
        self,
        predictions: Dict[str, np.ndarray],
        probabilities: Optional[Dict[str, np.ndarray]] = None,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """é‡ã¿ä»˜ã‘æŠ•ç¥¨"""
        if probabilities:
            return self._weighted_soft_voting(probabilities)
        else:
            return self._hard_voting(predictions)

    def _weighted_soft_voting(
        self, probabilities: Dict[str, np.ndarray]
    ) -> Tuple[np.ndarray, np.ndarray]:
        """é‡ã¿ä»˜ã‘ã‚½ãƒ•ãƒˆæŠ•ç¥¨"""
        weighted_avg = self._average_probabilities(probabilities, use_weights=True)
        predictions = np.argmax(weighted_avg, axis=1)
        confidence = np.max(weighted_avg, axis=1)

        return predictions, confidence

    def _average_probabilities(
        self, probabilities: Dict[str, np.ndarray], use_weights: bool = False
    ) -> np.ndarray:
        """ç¢ºç‡ã®å¹³å‡åŒ–"""
        if not probabilities:
            raise ValueError("No probabilities provided")

        prob_arrays = list(probabilities.values())

        # å½¢çŠ¶ç¢ºèª
        n_samples, n_classes = prob_arrays[0].shape
        for prob in prob_arrays[1:]:
            if prob.shape != (n_samples, n_classes):
                raise ValueError("All probability arrays must have the same shape")

        if use_weights and self.weights:
            # é‡ã¿ä»˜ã‘å¹³å‡
            weighted_sum = np.zeros((n_samples, n_classes))
            total_weight = 0.0

            for model_name, prob in probabilities.items():
                weight = self.weights.get(model_name, 1.0)
                weighted_sum += weight * prob
                total_weight += weight

            return weighted_sum / total_weight if total_weight > 0 else weighted_sum
        else:
            # å˜ç´”å¹³å‡
            return np.mean(prob_arrays, axis=0)

    def _simple_majority_vote(self, votes: np.ndarray) -> int:
        """å˜ç´”å¤šæ•°æ±º"""
        unique_votes, counts = np.unique(votes, return_counts=True)
        majority_idx = np.argmax(counts)
        return unique_votes[majority_idx]

    def _weighted_hard_vote(self, votes: np.ndarray, model_names: List[str]) -> int:
        """é‡ã¿ä»˜ã‘ãƒãƒ¼ãƒ‰æŠ•ç¥¨"""
        unique_classes = np.unique(votes)
        class_weights = {}

        for cls in unique_classes:
            total_weight = 0.0
            for i, vote in enumerate(votes):
                if vote == cls:
                    model_name = model_names[i]
                    weight = self.weights.get(model_name, 1.0)
                    total_weight += weight
            class_weights[cls] = total_weight

        winning_class = max(class_weights, key=class_weights.get)
        return winning_class


class EnsembleModel:
    """
    ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åˆ†é¡ãƒ¢ãƒ‡ãƒ«

    3ã¤ã®å¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ã¦ãƒ­ãƒã‚¹ãƒˆãªäºˆæ¸¬ã‚’æä¾›ã€‚
    é‡ã¿ä»˜ã‘æŠ•ç¥¨ã¨confidenceé–¾å€¤ã«ã‚ˆã‚‹é«˜ç²¾åº¦äºˆæ¸¬ã€‚
    """

    def __init__(
        self,
        models: Optional[Dict[str, BaseMLModel]] = None,
        weights: Optional[Dict[str, float]] = None,
        confidence_threshold: Optional[float] = None,
    ):
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–

        Args:
            models: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«è¾æ›¸ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ãƒ¢ãƒ‡ãƒ«è‡ªå‹•ä½œæˆï¼‰
            weights: ãƒ¢ãƒ‡ãƒ«é‡ã¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å‡ç­‰é‡ã¿ï¼‰
            confidence_threshold: äºˆæ¸¬ä¿¡é ¼åº¦é–¾å€¤
        """
        # confidence_thresholdã‚’è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
        if confidence_threshold is None:
            self.confidence_threshold = get_threshold("ml.confidence_threshold", 0.3)
        else:
            self.confidence_threshold = confidence_threshold
        self.logger = get_logger()

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
        if models is None:
            self.models = self._create_default_models()
        else:
            self.models = models

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé‡ã¿ã®è¨­å®šï¼ˆå‡ç­‰é‡ã¿ï¼‰
        if weights is None:
            num_models = len(self.models)
            self.weights = {name: 1.0 / num_models for name in self.models.keys()}
        else:
            self.weights = weights

        # é‡ã¿ã®æ­£è¦åŒ–
        self._normalize_weights()

        # å­¦ç¿’çŠ¶æ…‹ã®ç®¡ç†
        self.is_fitted = False
        self.feature_names = None
        self.classes_ = None
        self.model_performance = {}

        self.logger.info(f"âœ… EnsembleModel initialized with {len(self.models)} models")
        self.logger.info(f"Models: {list(self.models.keys())}")
        self.logger.info(f"Weights: {self.weights}")

    def _create_default_models(self) -> Dict[str, BaseMLModel]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®3ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
        try:
            models = {"lgbm": LGBMModel(), "xgb": XGBModel(), "rf": RFModel()}
            self.logger.info("âœ… Default models created successfully")
            return models
        except Exception as e:
            self.logger.error(f"âŒ Failed to create default models: {e}")
            raise DataProcessingError(f"Model creation failed: {e}")

    def _normalize_weights(self) -> None:
        """é‡ã¿ã®æ­£è¦åŒ–ï¼ˆåˆè¨ˆãŒ1ã«ãªã‚‹ã‚ˆã†ã«èª¿æ•´ï¼‰"""
        total_weight = sum(self.weights.values())
        if total_weight > 0:
            self.weights = {name: weight / total_weight for name, weight in self.weights.items()}
        else:
            # å…¨é‡ã¿ãŒ0ã®å ´åˆã¯å‡ç­‰é‡ã¿ã«æˆ»ã™
            num_models = len(self.weights)
            self.weights = {name: 1.0 / num_models for name in self.weights.keys()}

        self.logger.debug(f"Normalized weights: {self.weights}")

    def fit(self, X: pd.DataFrame, y: pd.Series) -> "EnsembleModel":
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’

        Args:
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿

        Returns:
            EnsembleModel: å­¦ç¿’æ¸ˆã¿ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«
        """
        try:
            self.logger.info(f"Starting ensemble training with {len(X)} samples")

            # ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            self._validate_training_data(X, y)

            # ç‰¹å¾´é‡åã¨ã‚¯ãƒ©ã‚¹ã‚’ä¿å­˜
            self.feature_names = X.columns.tolist()
            self.classes_ = np.unique(y)

            start_time = time.time()

            # å„ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«å­¦ç¿’
            for model_name, model in self.models.items():
                model_start = time.time()

                self.logger.info(f"Training {model_name}...")
                model.fit(X, y)

                # å­¦ç¿’æ™‚é–“ã‚’è¨˜éŒ²
                training_time = time.time() - model_start
                self.model_performance[model_name] = {
                    "training_time": training_time,
                    "is_fitted": model.is_fitted,
                }

                self.logger.info(f"âœ… {model_name} training completed in {training_time:.2f}s")

            total_time = time.time() - start_time
            self.is_fitted = True

            self.logger.info(f"ğŸ‰ Ensemble training completed in {total_time:.2f}s")
            return self

        except Exception as e:
            self.logger.error(f"âŒ Ensemble training failed: {e}")
            raise DataProcessingError(f"Ensemble training failed: {e}")

    def predict(self, X: pd.DataFrame, use_confidence: bool = True) -> np.ndarray:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã®å®Ÿè¡Œ

        Args:
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            use_confidence: confidenceé–¾å€¤ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹

        Returns:
            np.ndarray: äºˆæ¸¬ã‚¯ãƒ©ã‚¹
        """
        if not self.is_fitted:
            raise ValueError("EnsembleModel is not fitted. Call fit() first.")

        try:
            # ç¢ºç‡äºˆæ¸¬ã‚’å–å¾—
            probabilities = self.predict_proba(X)

            if use_confidence:
                # confidenceé–¾å€¤ã‚’é©ç”¨
                predictions = self._apply_confidence_threshold(probabilities)
            else:
                # å˜ç´”ã«æœ€å¤§ç¢ºç‡ã®ã‚¯ãƒ©ã‚¹ã‚’é¸æŠ
                predictions = np.argmax(probabilities, axis=1)

            return predictions

        except Exception as e:
            self.logger.error(f"Ensemble prediction failed: {e}")
            raise DataProcessingError(f"Prediction failed: {e}")

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç¢ºç‡äºˆæ¸¬ã®å®Ÿè¡Œ

        Args:
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿

        Returns:
            np.ndarray: äºˆæ¸¬ç¢ºç‡ï¼ˆé‡ã¿ä»˜ã‘å¹³å‡ï¼‰
        """
        if not self.is_fitted:
            raise ValueError("EnsembleModel is not fitted. Call fit() first.")

        try:
            weighted_probabilities = None
            total_weight = 0.0

            # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡ã‚’é‡ã¿ä»˜ã‘ã—ã¦çµ±åˆ
            for model_name, model in self.models.items():
                if not model.is_fitted:
                    self.logger.warning(f"{model_name} is not fitted, skipping...")
                    continue

                # ãƒ¢ãƒ‡ãƒ«ã®ç¢ºç‡äºˆæ¸¬ã‚’å–å¾—
                model_proba = model.predict_proba(X)

                # é‡ã¿ä»˜ã‘ã—ã¦åŠ ç®—
                weight = self.weights.get(model_name, 0.0)
                if weighted_probabilities is None:
                    weighted_probabilities = weight * model_proba
                else:
                    weighted_probabilities += weight * model_proba

                total_weight += weight

            if weighted_probabilities is None or total_weight == 0:
                raise ValueError("No fitted models available for prediction")

            # é‡ã¿ã§æ­£è¦åŒ–
            if total_weight != 1.0:
                weighted_probabilities /= total_weight

            return weighted_probabilities

        except Exception as e:
            self.logger.error(f"Ensemble probability prediction failed: {e}")
            raise DataProcessingError(f"Probability prediction failed: {e}")

    def _apply_confidence_threshold(self, probabilities: np.ndarray) -> np.ndarray:
        """confidenceé–¾å€¤ã‚’é©ç”¨ã—ãŸäºˆæ¸¬"""
        predictions = np.argmax(probabilities, axis=1)
        max_probabilities = np.max(probabilities, axis=1)

        # confidenceé–¾å€¤æœªæº€ã®äºˆæ¸¬ã¯ä¸æ˜ã‚¯ãƒ©ã‚¹ï¼ˆ-1ï¼‰ã¨ã™ã‚‹
        low_confidence_mask = max_probabilities < self.confidence_threshold
        predictions[low_confidence_mask] = -1

        confidence_ratio = 1.0 - (low_confidence_mask.sum() / len(predictions))
        self.logger.debug(f"Confidence ratio: {confidence_ratio:.3f}")

        return predictions

    def evaluate(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡

        Args:
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            y: æ­£è§£ãƒ©ãƒ™ãƒ«

        Returns:
            Dict[str, float]: è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        """
        try:
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
            y_pred = self.predict(X, use_confidence=False)  # é–¾å€¤ãªã—ã§è©•ä¾¡
            y_pred_conf = self.predict(X, use_confidence=True)  # é–¾å€¤ã‚ã‚Šã§è©•ä¾¡

            # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆé–¾å€¤ãªã—ï¼‰
            metrics = {
                "accuracy": accuracy_score(y, y_pred),
                "precision": precision_score(y, y_pred, average="weighted", zero_division=0),
                "recall": recall_score(y, y_pred, average="weighted", zero_division=0),
                "f1_score": f1_score(y, y_pred, average="weighted", zero_division=0),
            }

            # confidenceé–¾å€¤é©ç”¨æ™‚ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            valid_mask = y_pred_conf != -1
            if valid_mask.sum() > 0:
                y_valid = y[valid_mask]
                y_pred_valid = y_pred_conf[valid_mask]

                metrics.update(
                    {
                        "confidence_coverage": valid_mask.sum() / len(y),
                        "confidence_accuracy": accuracy_score(y_valid, y_pred_valid),
                        "confidence_precision": precision_score(
                            y_valid, y_pred_valid, average="weighted", zero_division=0
                        ),
                    }
                )

            self.logger.info("ğŸ“Š Ensemble evaluation completed")
            self.logger.info(f"Accuracy: {metrics['accuracy']:.3f}, F1: {metrics['f1_score']:.3f}")

            return metrics

        except Exception as e:
            self.logger.error(f"Ensemble evaluation failed: {e}")
            return {}

    def save(self, filepath: Union[str, Path]) -> None:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        try:
            path = Path(filepath)
            path.mkdir(parents=True, exist_ok=True)

            # å„ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«ä¿å­˜
            for model_name, model in self.models.items():
                model_path = path / f"{model_name}_model.pkl"
                model.save(model_path)

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šã‚’ä¿å­˜
            import joblib

            ensemble_config = {
                "weights": self.weights,
                "confidence_threshold": self.confidence_threshold,
                "feature_names": self.feature_names,
                "classes_": self.classes_,
                "is_fitted": self.is_fitted,
                "model_performance": self.model_performance,
            }

            config_path = path / "ensemble_config.pkl"
            joblib.dump(ensemble_config, config_path)

            self.logger.info(f"âœ… EnsembleModel saved to {path}")

        except Exception as e:
            self.logger.error(f"Failed to save EnsembleModel: {e}")
            raise DataProcessingError(f"Model save failed: {e}")

    @classmethod
    def load(cls, filepath: Union[str, Path]) -> "EnsembleModel":
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            path = Path(filepath)

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šã‚’èª­ã¿è¾¼ã¿
            import joblib

            config_path = path / "ensemble_config.pkl"
            ensemble_config = joblib.load(config_path)

            # å„ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
            models = {}
            for model_name in ["lgbm", "xgb", "rf"]:
                model_path = path / f"{model_name}_model.pkl"
                if model_path.exists():
                    if model_name == "lgbm":
                        models[model_name] = LGBMModel.load(model_path)
                    elif model_name == "xgb":
                        models[model_name] = XGBModel.load(model_path)
                    elif model_name == "rf":
                        models[model_name] = RFModel.load(model_path)

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’å¾©å…ƒ
            ensemble = cls(
                models=models,
                weights=ensemble_config["weights"],
                confidence_threshold=ensemble_config["confidence_threshold"],
            )

            # çŠ¶æ…‹ã‚’å¾©å…ƒ
            ensemble.feature_names = ensemble_config["feature_names"]
            ensemble.classes_ = ensemble_config["classes_"]
            ensemble.is_fitted = ensemble_config["is_fitted"]
            ensemble.model_performance = ensemble_config["model_performance"]

            logger = get_logger()
            logger.info(f"âœ… EnsembleModel loaded from {path}")

            return ensemble

        except Exception as e:
            logger = get_logger()
            logger.error(f"Failed to load EnsembleModel from {filepath}: {e}")
            raise DataProcessingError(f"Model load failed: {e}")

    def _validate_training_data(self, X: pd.DataFrame, y: pd.Series) -> None:
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        if X.empty or y.empty:
            raise ValueError("Training data is empty")

        if len(X) != len(y):
            raise ValueError(f"Feature and target length mismatch: {len(X)} vs {len(y)}")

        min_samples = get_threshold("ensemble.min_training_samples", 50)
        if len(X) < min_samples:  # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ã¯å¤šã‚ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
            raise ValueError(
                f"Insufficient training data for ensemble: {len(X)} samples (minimum: {min_samples})"
            )

        # ã‚¯ãƒ©ã‚¹æ•°ãƒã‚§ãƒƒã‚¯
        n_classes = len(np.unique(y))
        if n_classes < 2:
            raise ValueError(f"Need at least 2 classes for classification, got {n_classes}")

    def get_model_info(self) -> Dict[str, any]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’å–å¾—"""
        return {
            "ensemble_type": "soft_voting",
            "n_models": len(self.models),
            "model_names": list(self.models.keys()),
            "weights": self.weights,
            "confidence_threshold": self.confidence_threshold,
            "is_fitted": self.is_fitted,
            "n_features": len(self.feature_names) if self.feature_names else 0,
            "feature_names": self.feature_names,
            "classes": (self.classes_.tolist() if self.classes_ is not None else None),
            "model_performance": self.model_performance,
        }


class ProductionEnsemble:
    """
    æœ¬ç•ªç”¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆPhase 38.4æ™‚ç‚¹ã§æœ¬ç•ªä½¿ç”¨ä¸­ï¼‰

    ç¾åœ¨ã®ç›®çš„ï¼š
    - scripts/ml/create_ml_models.pyã§å®Ÿä½¿ç”¨ä¸­ï¼ˆé€±æ¬¡è‡ªå‹•å­¦ç¿’ï¼‰
    - æœ¬ç•ªç’°å¢ƒã§ã®å®‰å®šå‹•ä½œã‚’ä¿è¨¼ï¼ˆml_adapter/ml_loader/trading_cycle_managerï¼‰
    - Phase 50.7: ensemble_level1/2/3.pkl ã¨ã—ã¦ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿

    å°†æ¥ã®çµ±åˆè¨ˆç”»ï¼š
    - æ–°è¨­è¨ˆEnsembleModelã¸ã®æ®µéšçš„ç§»è¡Œã‚’æƒ³å®š
    - Phase 39ä»¥é™ã§çµ±åˆæ¤œè¨ï¼ˆç¾æ™‚ç‚¹ã§ã¯å‰Šé™¤ä¸å¯ï¼‰
    """

    def __init__(self, individual_models: Dict[str, Any]):
        """
        åˆæœŸåŒ–

        Args:
            individual_models: å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«è¾æ›¸
        """
        self.models = individual_models
        self.model_names = list(individual_models.keys())

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé‡ã¿ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ï¼‰
        from ..core.config import get_threshold

        default_weights = get_threshold(
            "ensemble.weights",
            {
                "lightgbm": 0.4,
                "xgboost": 0.4,
                "random_forest": 0.2,
            },
        )
        self.weights = default_weights

        self.is_fitted = True
        # Phase 50.7: ãƒ¬ãƒ™ãƒ«åˆ¥ç‰¹å¾´é‡æ•°å¯¾å¿œ - å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç‰¹å¾´é‡æ•°ã‚’å–å¾—
        from ..core.config.feature_manager import get_feature_count, get_feature_names

        # å®Ÿéš›ã«å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡æ•°ã‚’å–å¾—ï¼ˆãƒ¬ãƒ™ãƒ«åˆ¥å¯¾å¿œï¼‰
        detected_n_features = None
        for model_name, model in self.models.items():
            if hasattr(model, "n_features_in_"):
                detected_n_features = model.n_features_in_
                break
            elif hasattr(model, "_n_features"):
                detected_n_features = model._n_features
                break

        # ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ¤œå‡ºã§ããŸå ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ã€ã§ããªã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if detected_n_features is not None:
            self.n_features_ = detected_n_features
            # ç‰¹å¾´é‡åã‚‚ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦åˆ‡ã‚Šè©°ã‚ã‚‹
            all_feature_names = get_feature_names()
            self.feature_names = all_feature_names[: self.n_features_]
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ç‰¹å¾´é‡ã‚’ä½¿ç”¨ï¼ˆLevel 1ç›¸å½“ï¼‰
            self.n_features_ = get_feature_count()
            self.feature_names = get_feature_names()

        # ãƒ¢ãƒ‡ãƒ«æ•°æ¤œè¨¼
        if len(self.models) == 0:
            raise ValueError("å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã›ã‚“")

    def predict(self, X) -> np.ndarray:
        """äºˆæ¸¬å®Ÿè¡Œï¼ˆé‡ã¿ä»˜ã‘æŠ•ç¥¨ï¼‰

        Phase 51.9-6C: 3ã‚¯ãƒ©ã‚¹åˆ†é¡å¯¾å¿œ
        - predict_proba()ã‚’ä½¿ç”¨ã—ã¦ã‚¯ãƒ©ã‚¹ç¢ºç‡ã‚’å–å¾—
        - argmax()ã§æœ€ã‚‚ç¢ºç‡ã®é«˜ã„ã‚¯ãƒ©ã‚¹ã‚’è¿”ã™
        """
        # predict_proba()ã‚’ä½¿ç”¨ã—ã¦ç¢ºç‡å–å¾—
        probas = self.predict_proba(X)

        # argmax()ã§æœ€ã‚‚ç¢ºç‡ã®é«˜ã„ã‚¯ãƒ©ã‚¹ã‚’è¿”ã™ï¼ˆ2ã‚¯ãƒ©ã‚¹ãƒ»3ã‚¯ãƒ©ã‚¹å…±é€šï¼‰
        return np.argmax(probas, axis=1)

    def predict_proba(self, X) -> np.ndarray:
        """äºˆæ¸¬ç¢ºç‡ï¼ˆé‡ã¿ä»˜ã‘å¹³å‡ï¼‰

        Phase 51.9-6C: 3ã‚¯ãƒ©ã‚¹åˆ†é¡å¯¾å¿œ
        - å„ãƒ¢ãƒ‡ãƒ«ã®å…¨ã‚¯ãƒ©ã‚¹ç¢ºç‡ã‚’ä¿æŒ
        - é‡ã¿ä»˜ã‘å¹³å‡ã§çµ±åˆ
        - 2ã‚¯ãƒ©ã‚¹ãƒ»3ã‚¯ãƒ©ã‚¹ä¸¡å¯¾å¿œ
        """
        if hasattr(X, "values"):
            X_array = X.values
        else:
            X_array = X

        # å…¥åŠ›å½¢çŠ¶æ¤œè¨¼
        if X_array.shape[1] != self.n_features_:
            raise ValueError(f"ç‰¹å¾´é‡æ•°ä¸ä¸€è‡´: {X_array.shape[1]} != {self.n_features_}")

        # sklearnè­¦å‘Šå›é¿ã®ãŸã‚ç‰¹å¾´é‡åä»˜ãDataFrameã‚’ä½œæˆ
        import pandas as pd

        if not isinstance(X, pd.DataFrame):
            X_with_names = pd.DataFrame(X_array, columns=self.feature_names)
        else:
            X_with_names = X

        probabilities = {}
        n_classes = None

        # å„ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç¢ºç‡å–å¾—
        for name, model in self.models.items():
            if hasattr(model, "predict_proba"):
                proba = model.predict_proba(X_with_names)
                # Phase 51.9-6C: å…¨ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡ã‚’ä¿æŒï¼ˆ2ã‚¯ãƒ©ã‚¹ãƒ»3ã‚¯ãƒ©ã‚¹å¯¾å¿œï¼‰
                probabilities[name] = proba
                if n_classes is None:
                    n_classes = proba.shape[1]
            elif hasattr(model, "predict"):
                # predict_probaãŒãªã„å ´åˆã¯predictã‚’ä½¿ç”¨
                pred = model.predict(X_with_names).astype(int)
                # one-hot encoding
                proba = np.zeros((len(pred), n_classes if n_classes else 2))
                proba[np.arange(len(pred)), pred] = 1.0
                probabilities[name] = proba
            else:
                raise ValueError(f"ãƒ¢ãƒ‡ãƒ« {name} ã«äºˆæ¸¬ãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")

        # é‡ã¿ä»˜ã‘å¹³å‡è¨ˆç®—ï¼ˆå…¨ã‚¯ãƒ©ã‚¹ç¢ºç‡ã‚’çµ±åˆï¼‰
        ensemble_proba = np.zeros((len(X_array), n_classes))
        total_weight = 0

        for name, proba in probabilities.items():
            weight = self.weights.get(name, 1.0)
            ensemble_proba += proba * weight
            total_weight += weight

        final_proba = ensemble_proba / total_weight

        # Phase 51.9-6C: å…¨ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡ã‚’è¿”ã™ï¼ˆ2ã‚¯ãƒ©ã‚¹ãƒ»3ã‚¯ãƒ©ã‚¹å¯¾å¿œï¼‰
        # å½¢çŠ¶: (n_samples, n_classes)
        return final_proba

    def get_model_info(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—"""
        return {
            "type": "ProductionEnsemble",
            "individual_models": self.model_names,
            "weights": self.weights.copy(),
            "n_features": self.n_features_,
            "feature_names": self.feature_names.copy(),
            "phase": "Phase 22",
            "status": "production_ready",
        }

    def update_weights(self, new_weights: Dict[str, float]) -> None:
        """é‡ã¿ã®æ›´æ–°"""
        old_weights = self.weights.copy()
        self.weights.update(new_weights)

        # é‡ã¿ã®æ­£è¦åŒ–
        total_weight = sum(self.weights.values())
        if total_weight > 0:
            self.weights = {name: weight / total_weight for name, weight in self.weights.items()}

        print(f"é‡ã¿æ›´æ–°: {old_weights} -> {self.weights}")

    def validate_predictions(self, X, y_true=None) -> Dict[str, Any]:
        """äºˆæ¸¬ç²¾åº¦ã®æ¤œè¨¼"""
        predictions = self.predict(X)
        probabilities = self.predict_proba(X)

        validation_result = {
            "n_samples": len(X),
            "prediction_range": [int(predictions.min()), int(predictions.max())],
            "probability_range": [
                float(probabilities.min()),
                float(probabilities.max()),
            ],
            "buy_ratio": float(predictions.mean()),
            "avg_confidence": float(probabilities.max(axis=1).mean()),
        }

        if y_true is not None:
            from sklearn.metrics import accuracy_score, f1_score

            validation_result.update(
                {
                    "accuracy": float(accuracy_score(y_true, predictions)),
                    "f1_score": float(f1_score(y_true, predictions, average="weighted")),
                }
            )

        return validation_result

    def __repr__(self) -> str:
        """æ–‡å­—åˆ—è¡¨ç¾"""
        return (
            f"ProductionEnsemble("
            f"models={len(self.models)}, "
            f"features={self.n_features_}, "
            f"weights={self.weights})"
        )


class StackingEnsemble(ProductionEnsemble):
    """
    Phase 59.7: Stacking Meta-Learnerã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«

    2æ®µéšäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ :
    1. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBM, XGBoost, RFï¼‰ãŒå„ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡ã‚’äºˆæ¸¬
    2. Meta-Learnerï¼ˆLightGBMï¼‰ãŒãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡ã‚’å…¥åŠ›ã¨ã—ã¦æœ€çµ‚äºˆæ¸¬

    æœŸå¾…åŠ¹æœ:
    - F1ã‚¹ã‚³ã‚¢: 0.41 â†’ 0.50+ï¼ˆ+20%ä»¥ä¸Šï¼‰
    - å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®é•·æ‰€ã‚’æœ€é©ã«çµ„ã¿åˆã‚ã›

    ä½¿ç”¨æ–¹æ³•:
        # è¨“ç·´æ™‚ï¼ˆcreate_ml_models.pyï¼‰
        stacking = StackingEnsemble(base_models, meta_model)

        # æ¨è«–æ™‚
        predictions = stacking.predict(X)
        probabilities = stacking.predict_proba(X)
    """

    def __init__(
        self,
        base_models: Dict[str, Any],
        meta_model: Any,
        stacking_enabled: bool = True,
    ):
        """
        Phase 59.7: StackingEnsembleåˆæœŸåŒ–

        Args:
            base_models: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¾æ›¸ï¼ˆlightgbm, xgboost, random_forestï¼‰
            meta_model: Meta-Learnerï¼ˆLightGBMClassifierï¼‰
            stacking_enabled: Stackingæœ‰åŠ¹åŒ–ãƒ•ãƒ©ã‚°ï¼ˆFalseæ™‚ã¯å¾“æ¥æ–¹å¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        """
        # è¦ªã‚¯ãƒ©ã‚¹ï¼ˆProductionEnsembleï¼‰åˆæœŸåŒ–
        super().__init__(base_models)

        self.meta_model = meta_model
        self.stacking_enabled = stacking_enabled

        # ãƒ¡ã‚¿ç‰¹å¾´é‡åã‚’ç”Ÿæˆï¼ˆãƒ¢ãƒ‡ãƒ«å Ã— ã‚¯ãƒ©ã‚¹æ•°ï¼‰
        self._meta_feature_names = self._generate_meta_feature_names()

        logger = get_logger()
        logger.info(
            f"âœ… Phase 59.7: StackingEnsembleåˆæœŸåŒ–å®Œäº† "
            f"(base_models={len(base_models)}, stacking_enabled={stacking_enabled})"
        )

    def _generate_meta_feature_names(self) -> List[str]:
        """
        ãƒ¡ã‚¿ç‰¹å¾´é‡åã‚’ç”Ÿæˆ

        Returns:
            List[str]: ãƒ¡ã‚¿ç‰¹å¾´é‡åãƒªã‚¹ãƒˆï¼ˆä¾‹: ["lightgbm_class0", "lightgbm_class1", ...]ï¼‰
        """
        feature_names = []
        # n_classesã¯meta_modelã‹ã‚‰æ¨å®š
        if hasattr(self.meta_model, "n_classes_"):
            n_classes = self.meta_model.n_classes_
        elif hasattr(self.meta_model, "classes_"):
            n_classes = len(self.meta_model.classes_)
        else:
            n_classes = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3ã‚¯ãƒ©ã‚¹

        # åŸºæœ¬ãƒ¡ã‚¿ç‰¹å¾´é‡ï¼ˆ9ç‰¹å¾´é‡ï¼‰
        for model_name in self.models.keys():
            for cls in range(n_classes):
                feature_names.append(f"{model_name}_class{cls}")

        # Phase 59.9-B: æ‹¡å¼µãƒ¡ã‚¿ç‰¹å¾´é‡ï¼ˆ6ç‰¹å¾´é‡ï¼‰
        for model_name in self.models.keys():
            feature_names.append(f"{model_name}_max_conf")
        feature_names.extend(["model_agreement", "entropy", "max_prob_gap"])

        # Phase 59.10: è¿½åŠ ãƒ¡ã‚¿ç‰¹å¾´é‡ï¼ˆ6ç‰¹å¾´é‡ï¼‰
        feature_names.extend([
            "prob_std",
            "prob_range",
            "avg_max_conf",
            "conf_std",
            "sell_prob_mean",
            "buy_prob_mean",
        ])

        return feature_names

    def _generate_meta_features(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:
        """
        Phase 59.9 + 59.10: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡ã¨æ‹¡å¼µãƒ¡ã‚¿ç‰¹å¾´é‡ã‚’ç”Ÿæˆ

        æ‹¡å¼µãƒ¡ã‚¿ç‰¹å¾´é‡:
        Phase 59.9ï¼ˆ6ç‰¹å¾´é‡ï¼‰:
        - {model}_max_conf: å„ãƒ¢ãƒ‡ãƒ«ã®æœ€å¤§ç¢ºç‡ï¼ˆÃ—3ï¼‰
        - model_agreement: 3ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ä¸€è‡´åº¦
        - entropy: äºˆæ¸¬ä¸ç¢ºå®Ÿæ€§
        - max_prob_gap: æœ€å¤§ç¢ºç‡ã¨2ç•ªç›®ã®å·®

        Phase 59.10ï¼ˆ6ç‰¹å¾´é‡ï¼‰:
        - prob_std: å…¨ç¢ºç‡ã®æ¨™æº–åå·®
        - prob_range: å…¨ç¢ºç‡ã®ç¯„å›²
        - avg_max_conf: å¹³å‡æœ€å¤§ç¢ºä¿¡åº¦
        - conf_std: ç¢ºä¿¡åº¦ã®æ¨™æº–åå·®
        - sell_prob_mean: SELLç¢ºç‡ã®å¹³å‡
        - buy_prob_mean: BUYç¢ºç‡ã®å¹³å‡

        Args:
            X: å…¥åŠ›ç‰¹å¾´é‡

        Returns:
            np.ndarray: ãƒ¡ã‚¿ç‰¹å¾´é‡ (n_samples, 21) - 9åŸºæœ¬ + 12æ‹¡å¼µ
        """
        base_meta_features = []
        model_probas = []  # æ‹¡å¼µç‰¹å¾´é‡è¨ˆç®—ç”¨

        for name, model in self.models.items():
            if hasattr(model, "predict_proba"):
                proba = model.predict_proba(X)
                base_meta_features.append(proba)
                model_probas.append(proba)
            else:
                # predict_probaãŒãªã„å ´åˆã¯one-hot encoding
                pred = model.predict(X).astype(int)
                n_classes = len(self._meta_feature_names) // len(self.models)
                proba = np.zeros((len(pred), n_classes))
                proba[np.arange(len(pred)), pred] = 1.0
                base_meta_features.append(proba)
                model_probas.append(proba)

        # åŸºæœ¬ãƒ¡ã‚¿ç‰¹å¾´é‡ï¼ˆ9ç‰¹å¾´é‡ï¼‰
        base_features = np.hstack(base_meta_features)

        # Phase 59.9-B + 59.10: æ‹¡å¼µãƒ¡ã‚¿ç‰¹å¾´é‡ï¼ˆ12ç‰¹å¾´é‡ï¼‰
        n_samples = base_features.shape[0]
        n_models = len(self.models)
        n_classes = 3  # SELL, HOLD, BUY
        enhanced_features = []

        for i in range(n_samples):
            row_features = []

            # å„ãƒ¢ãƒ‡ãƒ«ã®æœ€å¤§ç¢ºç‡ã¨äºˆæ¸¬ã‚¯ãƒ©ã‚¹
            model_predictions = []
            model_max_probs = []
            class_probs_by_class = {c: [] for c in range(n_classes)}

            for j in range(n_models):
                probs = model_probas[j][i]
                max_prob = probs.max()
                pred_class = probs.argmax()
                row_features.append(max_prob)  # {model}_max_conf
                model_predictions.append(pred_class)
                model_max_probs.append(max_prob)

                # Phase 59.10: ã‚¯ãƒ©ã‚¹åˆ¥ç¢ºç‡ã‚’è¨˜éŒ²
                for c in range(n_classes):
                    class_probs_by_class[c].append(probs[c])

            # 3ãƒ¢ãƒ‡ãƒ«é–“ã®äºˆæ¸¬ä¸€è‡´åº¦
            unique_preds = len(set(model_predictions))
            agreement = 1.0 if unique_preds == 1 else (0.5 if unique_preds == 2 else 0.0)
            row_features.append(agreement)

            # ç¢ºç‡åˆ†å¸ƒã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            all_probs = base_features[i]
            all_probs_safe = np.clip(all_probs, 1e-10, 1.0)
            entropy = -np.mean(all_probs_safe * np.log(all_probs_safe))
            row_features.append(entropy)

            # æœ€å¤§ç¢ºç‡ã¨2ç•ªç›®ã®ç¢ºç‡ã®å·®
            sorted_probs = np.sort(all_probs)[::-1]
            max_prob_gap = sorted_probs[0] - sorted_probs[1] if len(sorted_probs) > 1 else 0
            row_features.append(max_prob_gap)

            # === Phase 59.10: è¿½åŠ ãƒ¡ã‚¿ç‰¹å¾´é‡ï¼ˆ6ç‰¹å¾´é‡ï¼‰===

            # å…¨ç¢ºç‡ã®æ¨™æº–åå·®
            row_features.append(float(np.std(all_probs)))

            # å…¨ç¢ºç‡ã®ç¯„å›²ï¼ˆmax - minï¼‰
            row_features.append(float(np.max(all_probs) - np.min(all_probs)))

            # å¹³å‡æœ€å¤§ç¢ºä¿¡åº¦
            row_features.append(float(np.mean(model_max_probs)))

            # ç¢ºä¿¡åº¦ã®æ¨™æº–åå·®
            row_features.append(float(np.std(model_max_probs)))

            # SELLç¢ºç‡ã®å¹³å‡ï¼ˆclass 0ï¼‰
            row_features.append(float(np.mean(class_probs_by_class[0])))

            # BUYç¢ºç‡ã®å¹³å‡ï¼ˆclass 2ï¼‰
            row_features.append(float(np.mean(class_probs_by_class[2])))

            enhanced_features.append(row_features)

        enhanced_array = np.array(enhanced_features)

        # åŸºæœ¬ + æ‹¡å¼µã‚’çµåˆï¼ˆ21ç‰¹å¾´é‡ï¼‰
        return np.hstack([base_features, enhanced_array])

    def predict(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:
        """
        Phase 59.7: Stackingäºˆæ¸¬å®Ÿè¡Œ

        stacking_enabled=True: Meta-Learnerã«ã‚ˆã‚‹2æ®µéšäºˆæ¸¬
        stacking_enabled=False: å¾“æ¥ã®é‡ã¿ä»˜ã‘å¹³å‡ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

        Args:
            X: å…¥åŠ›ç‰¹å¾´é‡

        Returns:
            np.ndarray: äºˆæ¸¬ã‚¯ãƒ©ã‚¹
        """
        if not self.stacking_enabled:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥æ–¹å¼
            return super().predict(X)

        # Stackingäºˆæ¸¬
        probas = self.predict_proba(X)
        return np.argmax(probas, axis=1)

    def predict_proba(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:
        """
        Phase 59.7: Stackingç¢ºç‡äºˆæ¸¬

        stacking_enabled=True: Meta-Learnerã«ã‚ˆã‚‹2æ®µéšäºˆæ¸¬
        stacking_enabled=False: å¾“æ¥ã®é‡ã¿ä»˜ã‘å¹³å‡ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

        Args:
            X: å…¥åŠ›ç‰¹å¾´é‡

        Returns:
            np.ndarray: äºˆæ¸¬ç¢ºç‡ (n_samples, n_classes)
        """
        if not self.stacking_enabled:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥æ–¹å¼
            return super().predict_proba(X)

        # å…¥åŠ›æ¤œè¨¼
        if hasattr(X, "values"):
            X_array = X.values
        else:
            X_array = X

        if X_array.shape[1] != self.n_features_:
            raise ValueError(f"ç‰¹å¾´é‡æ•°ä¸ä¸€è‡´: {X_array.shape[1]} != {self.n_features_}")

        # sklearnè­¦å‘Šå›é¿ã®ãŸã‚ç‰¹å¾´é‡åä»˜ãDataFrameã‚’ä½œæˆ
        if not isinstance(X, pd.DataFrame):
            X_with_names = pd.DataFrame(X_array, columns=self.feature_names)
        else:
            X_with_names = X

        # ç¬¬1æ®µéš: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡ã‚’å–å¾—
        meta_features = self._generate_meta_features(X_with_names)

        # ç¬¬2æ®µéš: Meta-Learnerã§æœ€çµ‚äºˆæ¸¬
        return self.meta_model.predict_proba(meta_features)

    def get_model_info(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—"""
        info = super().get_model_info()
        info.update(
            {
                "type": "StackingEnsemble",
                "stacking_enabled": self.stacking_enabled,
                "meta_model_type": type(self.meta_model).__name__,
                "meta_features_count": len(self._meta_feature_names),
                "meta_feature_names": self._meta_feature_names,
                "phase": "Phase 59.7",
            }
        )
        return info

    def __repr__(self) -> str:
        """æ–‡å­—åˆ—è¡¨ç¾"""
        return (
            f"StackingEnsemble("
            f"base_models={len(self.models)}, "
            f"meta_model={type(self.meta_model).__name__}, "
            f"features={self.n_features_}, "
            f"stacking_enabled={self.stacking_enabled})"
        )
