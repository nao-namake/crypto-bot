"""
çµ±åˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚·ã‚¹ãƒ†ãƒ  - Phase 38.4å®Œäº†

EnsembleModelã€VotingSystemã€ProductionEnsembleæ©Ÿèƒ½ã‚’1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«çµ±åˆã€‚
é‡è¤‡ã‚³ãƒ¼ãƒ‰ã‚’æ’é™¤ã—ã€ä¿å®ˆæ€§ã¨ã‚³ãƒ¼ãƒ‰ã®å¯èª­æ€§ã‚’å‘ä¸Šã€‚

Phase 38.4å®Œäº†
"""

import time
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

from ..core.config import get_threshold
from ..core.exceptions import DataProcessingError
from ..core.logger import get_logger
from .models import BaseMLModel, LGBMModel, RFModel, XGBModel


class VotingMethod(Enum):
    """æŠ•ç¥¨æ–¹å¼ã®åˆ—æŒ™å‹"""

    SOFT = "soft"  # ç¢ºç‡ãƒ™ãƒ¼ã‚¹ã®æŠ•ç¥¨
    HARD = "hard"  # ã‚¯ãƒ©ã‚¹ãƒ™ãƒ¼ã‚¹ã®æŠ•ç¥¨
    WEIGHTED = "weighted"  # é‡ã¿ä»˜ã‘æŠ•ç¥¨


class VotingSystem:
    """
    æŠ•ç¥¨ã‚·ã‚¹ãƒ†ãƒ 

    è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’çµ±åˆã™ã‚‹ãŸã‚ã®æŠ•ç¥¨ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æä¾›ã€‚
    ã‚½ãƒ•ãƒˆæŠ•ç¥¨ã¨ãƒãƒ¼ãƒ‰æŠ•ç¥¨ã®ä¸¡æ–¹ã«å¯¾å¿œã€‚
    """

    def __init__(
        self,
        method: VotingMethod = VotingMethod.SOFT,
        weights: Optional[Dict[str, float]] = None,
    ):
        """
        æŠ•ç¥¨ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–

        Args:
            method: æŠ•ç¥¨æ–¹å¼ï¼ˆSOFT, HARD, WEIGHTEDï¼‰
            weights: ãƒ¢ãƒ‡ãƒ«é‡ã¿è¾æ›¸
        """
        self.method = method
        self.weights = weights or {}
        self.logger = get_logger()

        self.logger.info(f"âœ… VotingSystem initialized with method: {method.value}")

    def vote(
        self,
        predictions: Dict[str, np.ndarray],
        probabilities: Optional[Dict[str, np.ndarray]] = None,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        æŠ•ç¥¨ã®å®Ÿè¡Œ

        Args:
            predictions: ãƒ¢ãƒ‡ãƒ«åˆ¥äºˆæ¸¬ã‚¯ãƒ©ã‚¹è¾æ›¸
            probabilities: ãƒ¢ãƒ‡ãƒ«åˆ¥äºˆæ¸¬ç¢ºç‡è¾æ›¸ï¼ˆã‚½ãƒ•ãƒˆæŠ•ç¥¨ç”¨ï¼‰

        Returns:
            Tuple[np.ndarray, np.ndarray]: (æœ€çµ‚äºˆæ¸¬, ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢)
        """
        try:
            if self.method == VotingMethod.SOFT:
                return self._soft_voting(probabilities or {})
            elif self.method == VotingMethod.HARD:
                return self._hard_voting(predictions)
            elif self.method == VotingMethod.WEIGHTED:
                return self._weighted_voting(predictions, probabilities)
            else:
                raise ValueError(f"Unsupported voting method: {self.method}")

        except Exception as e:
            self.logger.error(f"Voting failed: {e}")
            raise DataProcessingError(f"Voting process failed: {e}")

    def _soft_voting(self, probabilities: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
        """ã‚½ãƒ•ãƒˆæŠ•ç¥¨ï¼ˆç¢ºç‡ãƒ™ãƒ¼ã‚¹ï¼‰"""
        if not probabilities:
            raise ValueError("Probabilities required for soft voting")

        avg_probabilities = self._average_probabilities(probabilities)
        predictions = np.argmax(avg_probabilities, axis=1)
        confidence = np.max(avg_probabilities, axis=1)

        return predictions, confidence

    def _hard_voting(self, predictions: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
        """ãƒãƒ¼ãƒ‰æŠ•ç¥¨ï¼ˆå¤šæ•°æ±ºï¼‰"""
        if not predictions:
            raise ValueError("Predictions required for hard voting")

        prediction_matrix = np.column_stack(list(predictions.values()))
        model_names = list(predictions.keys())

        final_predictions = []
        vote_agreement = []

        for i in range(prediction_matrix.shape[0]):
            votes = prediction_matrix[i, :]

            if self.weights:
                final_pred = self._weighted_hard_vote(votes, model_names)
            else:
                final_pred = self._simple_majority_vote(votes)

            agreement = np.sum(votes == final_pred) / len(votes)

            final_predictions.append(final_pred)
            vote_agreement.append(agreement)

        return np.array(final_predictions), np.array(vote_agreement)

    def _weighted_voting(
        self,
        predictions: Dict[str, np.ndarray],
        probabilities: Optional[Dict[str, np.ndarray]] = None,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """é‡ã¿ä»˜ã‘æŠ•ç¥¨"""
        if probabilities:
            return self._weighted_soft_voting(probabilities)
        else:
            return self._hard_voting(predictions)

    def _weighted_soft_voting(
        self, probabilities: Dict[str, np.ndarray]
    ) -> Tuple[np.ndarray, np.ndarray]:
        """é‡ã¿ä»˜ã‘ã‚½ãƒ•ãƒˆæŠ•ç¥¨"""
        weighted_avg = self._average_probabilities(probabilities, use_weights=True)
        predictions = np.argmax(weighted_avg, axis=1)
        confidence = np.max(weighted_avg, axis=1)

        return predictions, confidence

    def _average_probabilities(
        self, probabilities: Dict[str, np.ndarray], use_weights: bool = False
    ) -> np.ndarray:
        """ç¢ºç‡ã®å¹³å‡åŒ–"""
        if not probabilities:
            raise ValueError("No probabilities provided")

        prob_arrays = list(probabilities.values())

        # å½¢çŠ¶ç¢ºèª
        n_samples, n_classes = prob_arrays[0].shape
        for prob in prob_arrays[1:]:
            if prob.shape != (n_samples, n_classes):
                raise ValueError("All probability arrays must have the same shape")

        if use_weights and self.weights:
            # é‡ã¿ä»˜ã‘å¹³å‡
            weighted_sum = np.zeros((n_samples, n_classes))
            total_weight = 0.0

            for model_name, prob in probabilities.items():
                weight = self.weights.get(model_name, 1.0)
                weighted_sum += weight * prob
                total_weight += weight

            return weighted_sum / total_weight if total_weight > 0 else weighted_sum
        else:
            # å˜ç´”å¹³å‡
            return np.mean(prob_arrays, axis=0)

    def _simple_majority_vote(self, votes: np.ndarray) -> int:
        """å˜ç´”å¤šæ•°æ±º"""
        unique_votes, counts = np.unique(votes, return_counts=True)
        majority_idx = np.argmax(counts)
        return unique_votes[majority_idx]

    def _weighted_hard_vote(self, votes: np.ndarray, model_names: List[str]) -> int:
        """é‡ã¿ä»˜ã‘ãƒãƒ¼ãƒ‰æŠ•ç¥¨"""
        unique_classes = np.unique(votes)
        class_weights = {}

        for cls in unique_classes:
            total_weight = 0.0
            for i, vote in enumerate(votes):
                if vote == cls:
                    model_name = model_names[i]
                    weight = self.weights.get(model_name, 1.0)
                    total_weight += weight
            class_weights[cls] = total_weight

        winning_class = max(class_weights, key=class_weights.get)
        return winning_class


class EnsembleModel:
    """
    ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åˆ†é¡ãƒ¢ãƒ‡ãƒ«

    3ã¤ã®å¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ã¦ãƒ­ãƒã‚¹ãƒˆãªäºˆæ¸¬ã‚’æä¾›ã€‚
    é‡ã¿ä»˜ã‘æŠ•ç¥¨ã¨confidenceé–¾å€¤ã«ã‚ˆã‚‹é«˜ç²¾åº¦äºˆæ¸¬ã€‚
    """

    def __init__(
        self,
        models: Optional[Dict[str, BaseMLModel]] = None,
        weights: Optional[Dict[str, float]] = None,
        confidence_threshold: Optional[float] = None,
    ):
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–

        Args:
            models: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«è¾æ›¸ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ãƒ¢ãƒ‡ãƒ«è‡ªå‹•ä½œæˆï¼‰
            weights: ãƒ¢ãƒ‡ãƒ«é‡ã¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å‡ç­‰é‡ã¿ï¼‰
            confidence_threshold: äºˆæ¸¬ä¿¡é ¼åº¦é–¾å€¤
        """
        # confidence_thresholdã‚’è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
        if confidence_threshold is None:
            self.confidence_threshold = get_threshold("ml.confidence_threshold", 0.3)
        else:
            self.confidence_threshold = confidence_threshold
        self.logger = get_logger()

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
        if models is None:
            self.models = self._create_default_models()
        else:
            self.models = models

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé‡ã¿ã®è¨­å®šï¼ˆå‡ç­‰é‡ã¿ï¼‰
        if weights is None:
            num_models = len(self.models)
            self.weights = {name: 1.0 / num_models for name in self.models.keys()}
        else:
            self.weights = weights

        # é‡ã¿ã®æ­£è¦åŒ–
        self._normalize_weights()

        # å­¦ç¿’çŠ¶æ…‹ã®ç®¡ç†
        self.is_fitted = False
        self.feature_names = None
        self.classes_ = None
        self.model_performance = {}

        self.logger.info(f"âœ… EnsembleModel initialized with {len(self.models)} models")
        self.logger.info(f"Models: {list(self.models.keys())}")
        self.logger.info(f"Weights: {self.weights}")

    def _create_default_models(self) -> Dict[str, BaseMLModel]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®3ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
        try:
            models = {"lgbm": LGBMModel(), "xgb": XGBModel(), "rf": RFModel()}
            self.logger.info("âœ… Default models created successfully")
            return models
        except Exception as e:
            self.logger.error(f"âŒ Failed to create default models: {e}")
            raise DataProcessingError(f"Model creation failed: {e}")

    def _normalize_weights(self) -> None:
        """é‡ã¿ã®æ­£è¦åŒ–ï¼ˆåˆè¨ˆãŒ1ã«ãªã‚‹ã‚ˆã†ã«èª¿æ•´ï¼‰"""
        total_weight = sum(self.weights.values())
        if total_weight > 0:
            self.weights = {name: weight / total_weight for name, weight in self.weights.items()}
        else:
            # å…¨é‡ã¿ãŒ0ã®å ´åˆã¯å‡ç­‰é‡ã¿ã«æˆ»ã™
            num_models = len(self.weights)
            self.weights = {name: 1.0 / num_models for name in self.weights.keys()}

        self.logger.debug(f"Normalized weights: {self.weights}")

    def fit(self, X: pd.DataFrame, y: pd.Series) -> "EnsembleModel":
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’

        Args:
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿

        Returns:
            EnsembleModel: å­¦ç¿’æ¸ˆã¿ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«
        """
        try:
            self.logger.info(f"Starting ensemble training with {len(X)} samples")

            # ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            self._validate_training_data(X, y)

            # ç‰¹å¾´é‡åã¨ã‚¯ãƒ©ã‚¹ã‚’ä¿å­˜
            self.feature_names = X.columns.tolist()
            self.classes_ = np.unique(y)

            start_time = time.time()

            # å„ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«å­¦ç¿’
            for model_name, model in self.models.items():
                model_start = time.time()

                self.logger.info(f"Training {model_name}...")
                model.fit(X, y)

                # å­¦ç¿’æ™‚é–“ã‚’è¨˜éŒ²
                training_time = time.time() - model_start
                self.model_performance[model_name] = {
                    "training_time": training_time,
                    "is_fitted": model.is_fitted,
                }

                self.logger.info(f"âœ… {model_name} training completed in {training_time:.2f}s")

            total_time = time.time() - start_time
            self.is_fitted = True

            self.logger.info(f"ğŸ‰ Ensemble training completed in {total_time:.2f}s")
            return self

        except Exception as e:
            self.logger.error(f"âŒ Ensemble training failed: {e}")
            raise DataProcessingError(f"Ensemble training failed: {e}")

    def predict(self, X: pd.DataFrame, use_confidence: bool = True) -> np.ndarray:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã®å®Ÿè¡Œ

        Args:
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            use_confidence: confidenceé–¾å€¤ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹

        Returns:
            np.ndarray: äºˆæ¸¬ã‚¯ãƒ©ã‚¹
        """
        if not self.is_fitted:
            raise ValueError("EnsembleModel is not fitted. Call fit() first.")

        try:
            # ç¢ºç‡äºˆæ¸¬ã‚’å–å¾—
            probabilities = self.predict_proba(X)

            if use_confidence:
                # confidenceé–¾å€¤ã‚’é©ç”¨
                predictions = self._apply_confidence_threshold(probabilities)
            else:
                # å˜ç´”ã«æœ€å¤§ç¢ºç‡ã®ã‚¯ãƒ©ã‚¹ã‚’é¸æŠ
                predictions = np.argmax(probabilities, axis=1)

            return predictions

        except Exception as e:
            self.logger.error(f"Ensemble prediction failed: {e}")
            raise DataProcessingError(f"Prediction failed: {e}")

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç¢ºç‡äºˆæ¸¬ã®å®Ÿè¡Œ

        Args:
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿

        Returns:
            np.ndarray: äºˆæ¸¬ç¢ºç‡ï¼ˆé‡ã¿ä»˜ã‘å¹³å‡ï¼‰
        """
        if not self.is_fitted:
            raise ValueError("EnsembleModel is not fitted. Call fit() first.")

        try:
            weighted_probabilities = None
            total_weight = 0.0

            # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡ã‚’é‡ã¿ä»˜ã‘ã—ã¦çµ±åˆ
            for model_name, model in self.models.items():
                if not model.is_fitted:
                    self.logger.warning(f"{model_name} is not fitted, skipping...")
                    continue

                # ãƒ¢ãƒ‡ãƒ«ã®ç¢ºç‡äºˆæ¸¬ã‚’å–å¾—
                model_proba = model.predict_proba(X)

                # é‡ã¿ä»˜ã‘ã—ã¦åŠ ç®—
                weight = self.weights.get(model_name, 0.0)
                if weighted_probabilities is None:
                    weighted_probabilities = weight * model_proba
                else:
                    weighted_probabilities += weight * model_proba

                total_weight += weight

            if weighted_probabilities is None or total_weight == 0:
                raise ValueError("No fitted models available for prediction")

            # é‡ã¿ã§æ­£è¦åŒ–
            if total_weight != 1.0:
                weighted_probabilities /= total_weight

            return weighted_probabilities

        except Exception as e:
            self.logger.error(f"Ensemble probability prediction failed: {e}")
            raise DataProcessingError(f"Probability prediction failed: {e}")

    def _apply_confidence_threshold(self, probabilities: np.ndarray) -> np.ndarray:
        """confidenceé–¾å€¤ã‚’é©ç”¨ã—ãŸäºˆæ¸¬"""
        predictions = np.argmax(probabilities, axis=1)
        max_probabilities = np.max(probabilities, axis=1)

        # confidenceé–¾å€¤æœªæº€ã®äºˆæ¸¬ã¯ä¸æ˜ã‚¯ãƒ©ã‚¹ï¼ˆ-1ï¼‰ã¨ã™ã‚‹
        low_confidence_mask = max_probabilities < self.confidence_threshold
        predictions[low_confidence_mask] = -1

        confidence_ratio = 1.0 - (low_confidence_mask.sum() / len(predictions))
        self.logger.debug(f"Confidence ratio: {confidence_ratio:.3f}")

        return predictions

    def evaluate(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡

        Args:
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            y: æ­£è§£ãƒ©ãƒ™ãƒ«

        Returns:
            Dict[str, float]: è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        """
        try:
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
            y_pred = self.predict(X, use_confidence=False)  # é–¾å€¤ãªã—ã§è©•ä¾¡
            y_pred_conf = self.predict(X, use_confidence=True)  # é–¾å€¤ã‚ã‚Šã§è©•ä¾¡

            # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆé–¾å€¤ãªã—ï¼‰
            metrics = {
                "accuracy": accuracy_score(y, y_pred),
                "precision": precision_score(y, y_pred, average="weighted", zero_division=0),
                "recall": recall_score(y, y_pred, average="weighted", zero_division=0),
                "f1_score": f1_score(y, y_pred, average="weighted", zero_division=0),
            }

            # confidenceé–¾å€¤é©ç”¨æ™‚ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            valid_mask = y_pred_conf != -1
            if valid_mask.sum() > 0:
                y_valid = y[valid_mask]
                y_pred_valid = y_pred_conf[valid_mask]

                metrics.update(
                    {
                        "confidence_coverage": valid_mask.sum() / len(y),
                        "confidence_accuracy": accuracy_score(y_valid, y_pred_valid),
                        "confidence_precision": precision_score(
                            y_valid, y_pred_valid, average="weighted", zero_division=0
                        ),
                    }
                )

            self.logger.info("ğŸ“Š Ensemble evaluation completed")
            self.logger.info(f"Accuracy: {metrics['accuracy']:.3f}, F1: {metrics['f1_score']:.3f}")

            return metrics

        except Exception as e:
            self.logger.error(f"Ensemble evaluation failed: {e}")
            return {}

    def save(self, filepath: Union[str, Path]) -> None:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        try:
            path = Path(filepath)
            path.mkdir(parents=True, exist_ok=True)

            # å„ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«ä¿å­˜
            for model_name, model in self.models.items():
                model_path = path / f"{model_name}_model.pkl"
                model.save(model_path)

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šã‚’ä¿å­˜
            import joblib

            ensemble_config = {
                "weights": self.weights,
                "confidence_threshold": self.confidence_threshold,
                "feature_names": self.feature_names,
                "classes_": self.classes_,
                "is_fitted": self.is_fitted,
                "model_performance": self.model_performance,
            }

            config_path = path / "ensemble_config.pkl"
            joblib.dump(ensemble_config, config_path)

            self.logger.info(f"âœ… EnsembleModel saved to {path}")

        except Exception as e:
            self.logger.error(f"Failed to save EnsembleModel: {e}")
            raise DataProcessingError(f"Model save failed: {e}")

    @classmethod
    def load(cls, filepath: Union[str, Path]) -> "EnsembleModel":
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            path = Path(filepath)

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šã‚’èª­ã¿è¾¼ã¿
            import joblib

            config_path = path / "ensemble_config.pkl"
            ensemble_config = joblib.load(config_path)

            # å„ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
            models = {}
            for model_name in ["lgbm", "xgb", "rf"]:
                model_path = path / f"{model_name}_model.pkl"
                if model_path.exists():
                    if model_name == "lgbm":
                        models[model_name] = LGBMModel.load(model_path)
                    elif model_name == "xgb":
                        models[model_name] = XGBModel.load(model_path)
                    elif model_name == "rf":
                        models[model_name] = RFModel.load(model_path)

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’å¾©å…ƒ
            ensemble = cls(
                models=models,
                weights=ensemble_config["weights"],
                confidence_threshold=ensemble_config["confidence_threshold"],
            )

            # çŠ¶æ…‹ã‚’å¾©å…ƒ
            ensemble.feature_names = ensemble_config["feature_names"]
            ensemble.classes_ = ensemble_config["classes_"]
            ensemble.is_fitted = ensemble_config["is_fitted"]
            ensemble.model_performance = ensemble_config["model_performance"]

            logger = get_logger()
            logger.info(f"âœ… EnsembleModel loaded from {path}")

            return ensemble

        except Exception as e:
            logger = get_logger()
            logger.error(f"Failed to load EnsembleModel from {filepath}: {e}")
            raise DataProcessingError(f"Model load failed: {e}")

    def _validate_training_data(self, X: pd.DataFrame, y: pd.Series) -> None:
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        if X.empty or y.empty:
            raise ValueError("Training data is empty")

        if len(X) != len(y):
            raise ValueError(f"Feature and target length mismatch: {len(X)} vs {len(y)}")

        min_samples = get_threshold("ensemble.min_training_samples", 50)
        if len(X) < min_samples:  # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ã¯å¤šã‚ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
            raise ValueError(
                f"Insufficient training data for ensemble: {len(X)} samples (minimum: {min_samples})"
            )

        # ã‚¯ãƒ©ã‚¹æ•°ãƒã‚§ãƒƒã‚¯
        n_classes = len(np.unique(y))
        if n_classes < 2:
            raise ValueError(f"Need at least 2 classes for classification, got {n_classes}")

    def get_model_info(self) -> Dict[str, any]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’å–å¾—"""
        return {
            "ensemble_type": "soft_voting",
            "n_models": len(self.models),
            "model_names": list(self.models.keys()),
            "weights": self.weights,
            "confidence_threshold": self.confidence_threshold,
            "is_fitted": self.is_fitted,
            "n_features": len(self.feature_names) if self.feature_names else 0,
            "feature_names": self.feature_names,
            "classes": (self.classes_.tolist() if self.classes_ is not None else None),
            "model_performance": self.model_performance,
        }


class ProductionEnsemble:
    """
    æœ¬ç•ªç”¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆPhase 38.4æ™‚ç‚¹ã§æœ¬ç•ªä½¿ç”¨ä¸­ï¼‰

    ç¾åœ¨ã®ç›®çš„ï¼š
    - scripts/ml/create_ml_models.pyã§å®Ÿä½¿ç”¨ä¸­ï¼ˆé€±æ¬¡è‡ªå‹•å­¦ç¿’ï¼‰
    - æœ¬ç•ªç’°å¢ƒã§ã®å®‰å®šå‹•ä½œã‚’ä¿è¨¼ï¼ˆml_adapter/ml_loader/trading_cycle_managerï¼‰
    - models/production/production_ensemble.pkl ã¨ã—ã¦ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿

    å°†æ¥ã®çµ±åˆè¨ˆç”»ï¼š
    - æ–°è¨­è¨ˆEnsembleModelã¸ã®æ®µéšçš„ç§»è¡Œã‚’æƒ³å®š
    - Phase 39ä»¥é™ã§çµ±åˆæ¤œè¨ï¼ˆç¾æ™‚ç‚¹ã§ã¯å‰Šé™¤ä¸å¯ï¼‰
    """

    def __init__(self, individual_models: Dict[str, Any]):
        """
        åˆæœŸåŒ–

        Args:
            individual_models: å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«è¾æ›¸
        """
        self.models = individual_models
        self.model_names = list(individual_models.keys())

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé‡ã¿ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ï¼‰
        from ..core.config import get_threshold

        default_weights = get_threshold(
            "ensemble.weights",
            {
                "lightgbm": 0.4,
                "xgboost": 0.4,
                "random_forest": 0.2,
            },
        )
        self.weights = default_weights

        self.is_fitted = True
        # Phase 22: ç‰¹å¾´é‡å®šç¾©ä¸€å…ƒåŒ–å¯¾å¿œ
        from ..core.config.feature_manager import get_feature_count, get_feature_names

        self.n_features_ = get_feature_count()
        self.feature_names = get_feature_names()

        # ãƒ¢ãƒ‡ãƒ«æ•°æ¤œè¨¼
        if len(self.models) == 0:
            raise ValueError("å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã›ã‚“")

    def predict(self, X) -> np.ndarray:
        """äºˆæ¸¬å®Ÿè¡Œï¼ˆé‡ã¿ä»˜ã‘æŠ•ç¥¨ï¼‰"""
        if hasattr(X, "values"):
            X_array = X.values
        else:
            X_array = X

        # å…¥åŠ›å½¢çŠ¶æ¤œè¨¼
        if X_array.shape[1] != self.n_features_:
            raise ValueError(f"ç‰¹å¾´é‡æ•°ä¸ä¸€è‡´: {X_array.shape[1]} != {self.n_features_}")

        # sklearnè­¦å‘Šå›é¿ã®ãŸã‚ç‰¹å¾´é‡åä»˜ãDataFrameã‚’ä½œæˆ
        import pandas as pd

        if not isinstance(X, pd.DataFrame):
            X_with_names = pd.DataFrame(X_array, columns=self.feature_names)
        else:
            X_with_names = X

        predictions = {}

        # å„ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰äºˆæ¸¬å–å¾—
        for name, model in self.models.items():
            if hasattr(model, "predict"):
                pred = model.predict(X_with_names)
                predictions[name] = pred
            else:
                raise ValueError(f"ãƒ¢ãƒ‡ãƒ« {name} ã«predictãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")

        # é‡ã¿ä»˜ã‘å¹³å‡è¨ˆç®—
        ensemble_pred = np.zeros(len(X_array))
        total_weight = 0

        for name, pred in predictions.items():
            weight = self.weights.get(name, 1.0)
            ensemble_pred += pred * weight
            total_weight += weight

        # æœ€çµ‚äºˆæ¸¬ï¼ˆé–¾å€¤0.5ï¼‰
        return (ensemble_pred / total_weight > 0.5).astype(int)

    def predict_proba(self, X) -> np.ndarray:
        """äºˆæ¸¬ç¢ºç‡ï¼ˆé‡ã¿ä»˜ã‘å¹³å‡ï¼‰"""
        if hasattr(X, "values"):
            X_array = X.values
        else:
            X_array = X

        # å…¥åŠ›å½¢çŠ¶æ¤œè¨¼
        if X_array.shape[1] != self.n_features_:
            raise ValueError(f"ç‰¹å¾´é‡æ•°ä¸ä¸€è‡´: {X_array.shape[1]} != {self.n_features_}")

        # sklearnè­¦å‘Šå›é¿ã®ãŸã‚ç‰¹å¾´é‡åä»˜ãDataFrameã‚’ä½œæˆ
        import pandas as pd

        if not isinstance(X, pd.DataFrame):
            X_with_names = pd.DataFrame(X_array, columns=self.feature_names)
        else:
            X_with_names = X

        probabilities = {}

        # å„ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç¢ºç‡å–å¾—
        for name, model in self.models.items():
            if hasattr(model, "predict_proba"):
                proba = model.predict_proba(X_with_names)
                probabilities[name] = proba[:, 1] if proba.shape[1] > 1 else proba[:, 0]
            elif hasattr(model, "predict"):
                probabilities[name] = model.predict(X_with_names).astype(float)
            else:
                raise ValueError(f"ãƒ¢ãƒ‡ãƒ« {name} ã«äºˆæ¸¬ãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")

        # é‡ã¿ä»˜ã‘å¹³å‡è¨ˆç®—
        ensemble_proba = np.zeros(len(X))
        total_weight = 0

        for name, proba in probabilities.items():
            weight = self.weights.get(name, 1.0)
            ensemble_proba += proba * weight
            total_weight += weight

        final_proba = ensemble_proba / total_weight

        # [P(class=0), P(class=1)] å½¢å¼ã§è¿”ã™
        return np.column_stack([1 - final_proba, final_proba])

    def get_model_info(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—"""
        return {
            "type": "ProductionEnsemble",
            "individual_models": self.model_names,
            "weights": self.weights.copy(),
            "n_features": self.n_features_,
            "feature_names": self.feature_names.copy(),
            "phase": "Phase 22",
            "status": "production_ready",
        }

    def update_weights(self, new_weights: Dict[str, float]) -> None:
        """é‡ã¿ã®æ›´æ–°"""
        old_weights = self.weights.copy()
        self.weights.update(new_weights)

        # é‡ã¿ã®æ­£è¦åŒ–
        total_weight = sum(self.weights.values())
        if total_weight > 0:
            self.weights = {name: weight / total_weight for name, weight in self.weights.items()}

        print(f"é‡ã¿æ›´æ–°: {old_weights} -> {self.weights}")

    def validate_predictions(self, X, y_true=None) -> Dict[str, Any]:
        """äºˆæ¸¬ç²¾åº¦ã®æ¤œè¨¼"""
        predictions = self.predict(X)
        probabilities = self.predict_proba(X)

        validation_result = {
            "n_samples": len(X),
            "prediction_range": [int(predictions.min()), int(predictions.max())],
            "probability_range": [
                float(probabilities.min()),
                float(probabilities.max()),
            ],
            "buy_ratio": float(predictions.mean()),
            "avg_confidence": float(probabilities.max(axis=1).mean()),
        }

        if y_true is not None:
            from sklearn.metrics import accuracy_score, f1_score

            validation_result.update(
                {
                    "accuracy": float(accuracy_score(y_true, predictions)),
                    "f1_score": float(f1_score(y_true, predictions, average="weighted")),
                }
            )

        return validation_result

    def __repr__(self) -> str:
        """æ–‡å­—åˆ—è¡¨ç¾"""
        return (
            f"ProductionEnsemble("
            f"models={len(self.models)}, "
            f"features={self.n_features_}, "
            f"weights={self.weights})"
        )
