"""
ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«å®Ÿè£… - Phase 12å®Ÿè£…ãƒ»CI/CDãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ãƒ»æ‰‹å‹•å®Ÿè¡Œç›£è¦–ãƒ»æ®µéšçš„ãƒ‡ãƒ—ãƒ­ã‚¤å¯¾å¿œ

3ã¤ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBMã€XGBoostã€RandomForestï¼‰ã‚’çµ±åˆã—ã€
ã‚½ãƒ•ãƒˆæŠ•ç¥¨ã«ã‚ˆã£ã¦äºˆæ¸¬ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã€‚

ä¿å®ˆæ€§ã¨æ€§èƒ½ã®ãƒãƒ©ãƒ³ã‚¹ã‚’é‡è¦–ã—ãŸã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…ã€‚.
"""

import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    precision_score,
    recall_score,
)

from ...core.exceptions import DataProcessingError
from ...core.logger import get_logger
from ..models import BaseMLModel, LGBMModel, RFModel, XGBModel


class EnsembleModel:
    """
    ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åˆ†é¡ãƒ¢ãƒ‡ãƒ«

    3ã¤ã®å¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ã¦ãƒ­ãƒã‚¹ãƒˆãªäºˆæ¸¬ã‚’æä¾›ã€‚
    ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®è¤‡é›‘æ€§ã‚’æ’é™¤ã—ã€ç†è§£ã—ã‚„ã™ã„è¨­è¨ˆã‚’æ¡ç”¨ã€‚.
    """

    def __init__(
        self,
        models: Optional[Dict[str, BaseMLModel]] = None,
        weights: Optional[Dict[str, float]] = None,
        confidence_threshold: float = 0.35,
    ):
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–

        Args:
            models: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«è¾æ›¸ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ãƒ¢ãƒ‡ãƒ«è‡ªå‹•ä½œæˆï¼‰
            weights: ãƒ¢ãƒ‡ãƒ«é‡ã¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å‡ç­‰é‡ã¿ï¼‰
            confidence_threshold: äºˆæ¸¬ä¿¡é ¼åº¦é–¾å€¤.
        """
        self.confidence_threshold = confidence_threshold
        self.logger = get_logger()

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
        if models is None:
            self.models = self._create_default_models()
        else:
            self.models = models

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé‡ã¿ã®è¨­å®šï¼ˆå‡ç­‰é‡ã¿ï¼‰
        if weights is None:
            num_models = len(self.models)
            self.weights = {name: 1.0 / num_models for name in self.models.keys()}
        else:
            self.weights = weights

        # é‡ã¿ã®æ­£è¦åŒ–
        self._normalize_weights()

        # å­¦ç¿’çŠ¶æ…‹ã®ç®¡ç†
        self.is_fitted = False
        self.feature_names = None
        self.classes_ = None
        self.model_performance = {}

        self.logger.info(f"âœ… EnsembleModel initialized with {len(self.models)} models")
        self.logger.info(f"Models: {list(self.models.keys())}")
        self.logger.info(f"Weights: {self.weights}")

    def _create_default_models(self) -> Dict[str, BaseMLModel]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®3ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ."""
        try:
            models = {"lgbm": LGBMModel(), "xgb": XGBModel(), "rf": RFModel()}

            self.logger.info("âœ… Default models created successfully")
            return models

        except Exception as e:
            self.logger.error(f"âŒ Failed to create default models: {e}")
            raise DataProcessingError(f"Model creation failed: {e}")

    def _normalize_weights(self) -> None:
        """é‡ã¿ã®æ­£è¦åŒ–ï¼ˆåˆè¨ˆãŒ1ã«ãªã‚‹ã‚ˆã†ã«èª¿æ•´ï¼‰."""
        total_weight = sum(self.weights.values())
        if total_weight > 0:
            self.weights = {name: weight / total_weight for name, weight in self.weights.items()}
        else:
            # å…¨é‡ã¿ãŒ0ã®å ´åˆã¯å‡ç­‰é‡ã¿ã«æˆ»ã™
            num_models = len(self.weights)
            self.weights = {name: 1.0 / num_models for name in self.weights.keys()}

        self.logger.debug(f"Normalized weights: {self.weights}")

    def fit(self, X: pd.DataFrame, y: pd.Series) -> "EnsembleModel":
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’

        Args:
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿

        Returns:
            EnsembleModel: å­¦ç¿’æ¸ˆã¿ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«.
        """
        try:
            self.logger.info(f"Starting ensemble training with {len(X)} samples")

            # ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            self._validate_training_data(X, y)

            # ç‰¹å¾´é‡åã¨ã‚¯ãƒ©ã‚¹ã‚’ä¿å­˜
            self.feature_names = X.columns.tolist()
            self.classes_ = np.unique(y)

            start_time = time.time()

            # å„ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«å­¦ç¿’
            for model_name, model in self.models.items():
                model_start = time.time()

                self.logger.info(f"Training {model_name}...")
                model.fit(X, y)

                # å­¦ç¿’æ™‚é–“ã‚’è¨˜éŒ²
                training_time = time.time() - model_start
                self.model_performance[model_name] = {
                    "training_time": training_time,
                    "is_fitted": model.is_fitted,
                }

                self.logger.info(f"âœ… {model_name} training completed in {training_time:.2f}s")

            total_time = time.time() - start_time
            self.is_fitted = True

            self.logger.info(f"ğŸ‰ Ensemble training completed in {total_time:.2f}s")
            return self

        except Exception as e:
            self.logger.error(f"âŒ Ensemble training failed: {e}")
            raise DataProcessingError(f"Ensemble training failed: {e}")

    def predict(self, X: pd.DataFrame, use_confidence: bool = True) -> np.ndarray:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã®å®Ÿè¡Œ

        Args:
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            use_confidence: confidenceé–¾å€¤ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹

        Returns:
            np.ndarray: äºˆæ¸¬ã‚¯ãƒ©ã‚¹.
        """
        if not self.is_fitted:
            raise ValueError("EnsembleModel is not fitted. Call fit() first.")

        try:
            # ç¢ºç‡äºˆæ¸¬ã‚’å–å¾—
            probabilities = self.predict_proba(X)

            if use_confidence:
                # confidenceé–¾å€¤ã‚’é©ç”¨
                predictions = self._apply_confidence_threshold(probabilities)
            else:
                # å˜ç´”ã«æœ€å¤§ç¢ºç‡ã®ã‚¯ãƒ©ã‚¹ã‚’é¸æŠ
                predictions = np.argmax(probabilities, axis=1)

            return predictions

        except Exception as e:
            self.logger.error(f"Ensemble prediction failed: {e}")
            raise DataProcessingError(f"Prediction failed: {e}")

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç¢ºç‡äºˆæ¸¬ã®å®Ÿè¡Œ

        Args:
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿

        Returns:
            np.ndarray: äºˆæ¸¬ç¢ºç‡ï¼ˆé‡ã¿ä»˜ã‘å¹³å‡ï¼‰.
        """
        if not self.is_fitted:
            raise ValueError("EnsembleModel is not fitted. Call fit() first.")

        try:
            weighted_probabilities = None
            total_weight = 0.0

            # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡ã‚’é‡ã¿ä»˜ã‘ã—ã¦çµ±åˆ
            for model_name, model in self.models.items():
                if not model.is_fitted:
                    self.logger.warning(f"{model_name} is not fitted, skipping...")
                    continue

                # ãƒ¢ãƒ‡ãƒ«ã®ç¢ºç‡äºˆæ¸¬ã‚’å–å¾—
                model_proba = model.predict_proba(X)

                # é‡ã¿ä»˜ã‘ã—ã¦åŠ ç®—
                weight = self.weights.get(model_name, 0.0)
                if weighted_probabilities is None:
                    weighted_probabilities = weight * model_proba
                else:
                    weighted_probabilities += weight * model_proba

                total_weight += weight

            if weighted_probabilities is None or total_weight == 0:
                raise ValueError("No fitted models available for prediction")

            # é‡ã¿ã§æ­£è¦åŒ–
            if total_weight != 1.0:
                weighted_probabilities /= total_weight

            return weighted_probabilities

        except Exception as e:
            self.logger.error(f"Ensemble probability prediction failed: {e}")
            raise DataProcessingError(f"Probability prediction failed: {e}")

    def _apply_confidence_threshold(self, probabilities: np.ndarray) -> np.ndarray:
        """
        confidenceé–¾å€¤ã‚’é©ç”¨ã—ãŸäºˆæ¸¬

        Args:
            probabilities: äºˆæ¸¬ç¢ºç‡

        Returns:
            np.ndarray: é–¾å€¤é©ç”¨å¾Œã®äºˆæ¸¬ã‚¯ãƒ©ã‚¹.
        """
        predictions = np.argmax(probabilities, axis=1)
        max_probabilities = np.max(probabilities, axis=1)

        # confidenceé–¾å€¤æœªæº€ã®äºˆæ¸¬ã¯ä¸æ˜ã‚¯ãƒ©ã‚¹ï¼ˆ-1ï¼‰ã¨ã™ã‚‹
        low_confidence_mask = max_probabilities < self.confidence_threshold
        predictions[low_confidence_mask] = -1

        confidence_ratio = 1.0 - (low_confidence_mask.sum() / len(predictions))
        self.logger.debug(f"Confidence ratio: {confidence_ratio:.3f}")

        return predictions

    def evaluate(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡

        Args:
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            y: æ­£è§£ãƒ©ãƒ™ãƒ«

        Returns:
            Dict[str, float]: è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹.
        """
        try:
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
            y_pred = self.predict(X, use_confidence=False)  # é–¾å€¤ãªã—ã§è©•ä¾¡
            y_pred_conf = self.predict(X, use_confidence=True)  # é–¾å€¤ã‚ã‚Šã§è©•ä¾¡

            # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆé–¾å€¤ãªã—ï¼‰
            metrics = {
                "accuracy": accuracy_score(y, y_pred),
                "precision": precision_score(y, y_pred, average="weighted", zero_division=0),
                "recall": recall_score(y, y_pred, average="weighted", zero_division=0),
                "f1_score": f1_score(y, y_pred, average="weighted", zero_division=0),
            }

            # confidenceé–¾å€¤é©ç”¨æ™‚ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            valid_mask = y_pred_conf != -1
            if valid_mask.sum() > 0:
                y_valid = y[valid_mask]
                y_pred_valid = y_pred_conf[valid_mask]

                metrics.update(
                    {
                        "confidence_coverage": valid_mask.sum() / len(y),
                        "confidence_accuracy": accuracy_score(y_valid, y_pred_valid),
                        "confidence_precision": precision_score(
                            y_valid,
                            y_pred_valid,
                            average="weighted",
                            zero_division=0,
                        ),
                    }
                )

            self.logger.info(f"ğŸ“Š Ensemble evaluation completed")
            self.logger.info(f"Accuracy: {metrics['accuracy']:.3f}, F1: {metrics['f1_score']:.3f}")

            return metrics

        except Exception as e:
            self.logger.error(f"Ensemble evaluation failed: {e}")
            return {}

    def get_feature_importance(self) -> Optional[pd.DataFrame]:
        """
        çµ±åˆã•ã‚ŒãŸç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—

        Returns:
            pd.DataFrame: é‡ã¿ä»˜ã‘å¹³å‡ã•ã‚ŒãŸç‰¹å¾´é‡é‡è¦åº¦.
        """
        if not self.is_fitted or not self.feature_names:
            return None

        try:
            importance_sum = np.zeros(len(self.feature_names))
            total_weight = 0.0

            # å„ãƒ¢ãƒ‡ãƒ«ã®é‡è¦åº¦ã‚’é‡ã¿ä»˜ã‘ã—ã¦çµ±åˆ
            for model_name, model in self.models.items():
                importance_df = model.get_feature_importance()
                if importance_df is not None:
                    weight = self.weights.get(model_name, 0.0)
                    importance_sum += importance_df["importance"].values * weight
                    total_weight += weight

            if total_weight == 0:
                return None

            # æ­£è¦åŒ–
            importance_sum /= total_weight

            # DataFrameã¨ã—ã¦æ•´ç†
            ensemble_importance = pd.DataFrame(
                {"feature": self.feature_names, "importance": importance_sum}
            ).sort_values("importance", ascending=False)

            return ensemble_importance

        except Exception as e:
            self.logger.error(f"Failed to get ensemble feature importance: {e}")
            return None

    def save(self, filepath: Union[str, Path]) -> None:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜

        Args:
            filepath: ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹.
        """
        try:
            path = Path(filepath)
            path.mkdir(parents=True, exist_ok=True)

            # å„ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«ä¿å­˜
            for model_name, model in self.models.items():
                model_path = path / f"{model_name}_model.pkl"
                model.save(model_path)

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šã‚’ä¿å­˜
            import joblib

            ensemble_config = {
                "weights": self.weights,
                "confidence_threshold": self.confidence_threshold,
                "feature_names": self.feature_names,
                "classes_": self.classes_,
                "is_fitted": self.is_fitted,
                "model_performance": self.model_performance,
            }

            config_path = path / "ensemble_config.pkl"
            joblib.dump(ensemble_config, config_path)

            self.logger.info(f"âœ… EnsembleModel saved to {path}")

        except Exception as e:
            self.logger.error(f"Failed to save EnsembleModel: {e}")
            raise DataProcessingError(f"Model save failed: {e}")

    @classmethod
    def load(cls, filepath: Union[str, Path]) -> "EnsembleModel":
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿

        Args:
            filepath: èª­ã¿è¾¼ã¿å…ƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹

        Returns:
            EnsembleModel: èª­ã¿è¾¼ã‚“ã ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«.
        """
        try:
            path = Path(filepath)

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šã‚’èª­ã¿è¾¼ã¿
            import joblib

            config_path = path / "ensemble_config.pkl"
            ensemble_config = joblib.load(config_path)

            # å„ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
            models = {}
            for model_name in ["lgbm", "xgb", "rf"]:
                model_path = path / f"{model_name}_model.pkl"
                if model_path.exists():
                    if model_name == "lgbm":
                        models[model_name] = LGBMModel.load(model_path)
                    elif model_name == "xgb":
                        models[model_name] = XGBModel.load(model_path)
                    elif model_name == "rf":
                        models[model_name] = RFModel.load(model_path)

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’å¾©å…ƒ
            ensemble = cls(
                models=models,
                weights=ensemble_config["weights"],
                confidence_threshold=ensemble_config["confidence_threshold"],
            )

            # çŠ¶æ…‹ã‚’å¾©å…ƒ
            ensemble.feature_names = ensemble_config["feature_names"]
            ensemble.classes_ = ensemble_config["classes_"]
            ensemble.is_fitted = ensemble_config["is_fitted"]
            ensemble.model_performance = ensemble_config["model_performance"]

            logger = get_logger()
            logger.info(f"âœ… EnsembleModel loaded from {path}")

            return ensemble

        except Exception as e:
            logger = get_logger()
            logger.error(f"Failed to load EnsembleModel from {filepath}: {e}")
            raise DataProcessingError(f"Model load failed: {e}")

    def _validate_training_data(self, X: pd.DataFrame, y: pd.Series) -> None:
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯."""
        if X.empty or y.empty:
            raise ValueError("Training data is empty")

        if len(X) != len(y):
            raise ValueError(f"Feature and target length mismatch: {len(X)} vs {len(y)}")

        if len(X) < 50:  # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ã¯å¤šã‚ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
            raise ValueError(f"Insufficient training data for ensemble: {len(X)} samples")

        # ã‚¯ãƒ©ã‚¹æ•°ãƒã‚§ãƒƒã‚¯
        n_classes = len(np.unique(y))
        if n_classes < 2:
            raise ValueError(f"Need at least 2 classes for classification, got {n_classes}")

    def get_model_info(self) -> Dict[str, any]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’å–å¾—."""
        return {
            "ensemble_type": "soft_voting",
            "n_models": len(self.models),
            "model_names": list(self.models.keys()),
            "weights": self.weights,
            "confidence_threshold": self.confidence_threshold,
            "is_fitted": self.is_fitted,
            "n_features": len(self.feature_names) if self.feature_names else 0,
            "feature_names": self.feature_names,
            "classes": (self.classes_.tolist() if self.classes_ is not None else None),
            "model_performance": self.model_performance,
        }
