"""
ç‰¹å¾´é‡ç”Ÿæˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ  - Phase 21çµ±åˆç‰ˆ

TechnicalIndicatorsã€MarketAnomalyDetectorã€FeatureServiceAdapterã‚’
1ã¤ã®ã‚¯ãƒ©ã‚¹ã«çµ±åˆã—ã€é‡è¤‡ã‚³ãƒ¼ãƒ‰å‰Šé™¤ã¨ä¿å®ˆæ€§å‘ä¸Šã‚’å®Ÿç¾ã€‚

97ç‰¹å¾´é‡ã‹ã‚‰15ç‰¹å¾´é‡ã¸ã®æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ï¼ˆ5æˆ¦ç•¥å¯¾å¿œï¼‰ã€‚

çµ±åˆåŠ¹æœ:
- ãƒ•ã‚¡ã‚¤ãƒ«æ•°å‰Šæ¸›: 3â†’1ï¼ˆ67%å‰Šæ¸›ï¼‰
- ã‚³ãƒ¼ãƒ‰è¡Œæ•°å‰Šæ¸›: 461è¡Œâ†’ç´„250è¡Œï¼ˆ46%å‰Šæ¸›ï¼‰
- é‡è¤‡ã‚³ãƒ¼ãƒ‰å‰Šé™¤: _handle_nan_valuesã€loggeråˆæœŸåŒ–ç­‰
- ç®¡ç†ç°¡ç´ åŒ–: ç‰¹å¾´é‡å‡¦ç†ã®å®Œå…¨ä¸€å…ƒåŒ–

Phase 21çµ±åˆå®Ÿè£…æ—¥: 2025å¹´9æœˆ12æ—¥.
"""

from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd

from ..core.config import get_anomaly_config

# Phase 21: ç‰¹å¾´é‡å®šç¾©ä¸€å…ƒåŒ–ï¼ˆfeature_managerã‹ã‚‰å–å¾—ï¼‰
from ..core.config.feature_manager import get_feature_categories, get_feature_names
from ..core.exceptions import DataProcessingError
from ..core.logger import CryptoBotLogger, get_logger

# ç‰¹å¾´é‡ãƒªã‚¹ãƒˆï¼ˆä¸€å…ƒåŒ–å¯¾å¿œï¼‰
OPTIMIZED_FEATURES = get_feature_names()

# ç‰¹å¾´é‡ã‚«ãƒ†ã‚´ãƒªï¼ˆä¸€å…ƒåŒ–å¯¾å¿œï¼‰
FEATURE_CATEGORIES = get_feature_categories()


class FeatureGenerator:
    """
    çµ±åˆç‰¹å¾´é‡ç”Ÿæˆã‚¯ãƒ©ã‚¹ - Phase 21çµ±åˆç‰ˆ

    ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã€ç•°å¸¸æ¤œçŸ¥ã€ç‰¹å¾´é‡ã‚µãƒ¼ãƒ“ã‚¹æ©Ÿèƒ½ã‚’
    1ã¤ã®ã‚¯ãƒ©ã‚¹ã«çµ±åˆã—ã€15ç‰¹å¾´é‡ç”Ÿæˆã‚’åŠ¹ç‡çš„ã«æä¾›ã€‚

    ä¸»è¦æ©Ÿèƒ½:
    - ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ç”Ÿæˆï¼ˆ9å€‹ï¼‰
    - ç•°å¸¸æ¤œçŸ¥æŒ‡æ¨™ç”Ÿæˆï¼ˆ1å€‹ï¼‰
    - åŸºæœ¬ç‰¹å¾´é‡ç”Ÿæˆï¼ˆ2å€‹ï¼‰
    - ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæŒ‡æ¨™ç”Ÿæˆï¼ˆ3å€‹ï¼‰
    - ãƒ¬ã‚¸ãƒ¼ãƒ æŒ‡æ¨™ç”Ÿæˆï¼ˆ3å€‹ï¼‰
    - çµ±åˆå“è³ªç®¡ç†ã¨15ç‰¹å¾´é‡ç¢ºèª
    """

    def __init__(self, lookback_period: Optional[int] = None) -> None:
        """
        åˆæœŸåŒ–

        Args:
            lookback_period: ç•°å¸¸æ¤œçŸ¥ã®å‚ç…§æœŸé–“
        """
        self.logger = get_logger()
        self.lookback_period = lookback_period or get_anomaly_config(
            "spike_detection.lookback_period", 20
        )
        self.computed_features = set()

    async def generate_features(self, market_data: Dict[str, Any]) -> pd.DataFrame:
        """
        çµ±åˆç‰¹å¾´é‡ç”Ÿæˆå‡¦ç†ï¼ˆ15ç‰¹å¾´é‡ç¢ºèªæ©Ÿèƒ½ä»˜ãï¼‰

        Args:
            market_data: å¸‚å ´ãƒ‡ãƒ¼ã‚¿ï¼ˆDataFrame ã¾ãŸã¯ dictï¼‰

        Returns:
            15ç‰¹å¾´é‡ã‚’å«ã‚€DataFrame
        """
        try:
            # DataFrameã«å¤‰æ›
            result_df = self._convert_to_dataframe(market_data)

            self.logger.info("ç‰¹å¾´é‡ç”Ÿæˆé–‹å§‹ - 15ç‰¹å¾´é‡ã‚·ã‚¹ãƒ†ãƒ ")
            self.computed_features.clear()

            # å¿…è¦åˆ—ãƒã‚§ãƒƒã‚¯
            self._validate_required_columns(result_df)

            # ğŸ”¹ åŸºæœ¬ç‰¹å¾´é‡ã‚’ç”Ÿæˆï¼ˆ3å€‹ï¼‰
            result_df = self._generate_basic_features(result_df)

            # ğŸ”¹ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’ç”Ÿæˆï¼ˆ6å€‹ï¼‰
            result_df = self._generate_technical_indicators(result_df)

            # ğŸ”¹ ç•°å¸¸æ¤œçŸ¥æŒ‡æ¨™ã‚’ç”Ÿæˆï¼ˆ3å€‹ï¼‰
            result_df = self._generate_anomaly_indicators(result_df)

            # ğŸ”¹ NaNå€¤å‡¦ç†ï¼ˆçµ±åˆç‰ˆï¼‰
            result_df = self._handle_nan_values(result_df)

            # ğŸ¯ 15ç‰¹å¾´é‡å®Œå…¨ç¢ºèªãƒ»æ¤œè¨¼
            self._validate_feature_generation(result_df)

            # DataFrameã‚’ãã®ã¾ã¾è¿”ã™ï¼ˆæˆ¦ç•¥ã§ä½¿ç”¨ã™ã‚‹ãŸã‚ï¼‰
            return result_df

        except Exception as e:
            self.logger.error(f"çµ±åˆç‰¹å¾´é‡ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise DataProcessingError(f"ç‰¹å¾´é‡ç”Ÿæˆå¤±æ•—: {e}")

    def _convert_to_dataframe(self, market_data: Dict[str, Any]) -> pd.DataFrame:
        """å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›ï¼ˆã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ è¾æ›¸å¯¾å¿œï¼‰"""
        if isinstance(market_data, pd.DataFrame):
            return market_data.copy()
        elif isinstance(market_data, dict):
            try:
                # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ è¾æ›¸ã®å ´åˆï¼ˆãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ï¼‰
                # è¨­å®šã‹ã‚‰å„ªå…ˆé †ä½ä»˜ãã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—
                from ..core.config import get_data_config

                timeframe_keys = get_data_config("timeframes", ["4h", "15m"])  # è¨­å®šåŒ–å¯¾å¿œ
                for tf in timeframe_keys:
                    if tf in market_data and isinstance(market_data[tf], pd.DataFrame):
                        self.logger.info(f"ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ è¾æ›¸ã‹ã‚‰ãƒ¡ã‚¤ãƒ³æ™‚ç³»åˆ—å–å¾—: {tf}")
                        return market_data[tf].copy()

                # é€šå¸¸ã®è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã®å ´åˆï¼ˆOHLCVå½¢å¼ç­‰ï¼‰
                # å…¨ã¦ã®å€¤ãŒã‚¹ã‚«ãƒ©ãƒ¼ã‹ãƒªã‚¹ãƒˆã‹ã‚’ãƒã‚§ãƒƒã‚¯
                if all(
                    isinstance(v, (int, float, str)) or (isinstance(v, list) and len(v) > 0)
                    for v in market_data.values()
                ):
                    return pd.DataFrame(market_data)

                # ãã®ä»–ã®æ§‹é€ ã®è¾æ›¸
                self.logger.warning(f"è¤‡é›‘ãªè¾æ›¸æ§‹é€ ã‚’æ¤œå‡º: keys={list(market_data.keys())}")
                return pd.DataFrame(market_data)

            except (ValueError, KeyError, TypeError) as e:
                self.logger.error(
                    f"å¸‚å ´ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚¨ãƒ©ãƒ¼ - æ§‹é€ : {type(market_data)}, ã‚­ãƒ¼: {list(market_data.keys()) if hasattr(market_data, 'keys') else 'N/A'}"
                )
                raise DataProcessingError(f"Dictâ†’DataFrameå¤‰æ›ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚¨ãƒ©ãƒ¼: {e}")
            except (MemoryError, OverflowError) as e:
                raise DataProcessingError(f"Dictâ†’DataFrameå¤‰æ›ã‚µã‚¤ã‚ºã‚¨ãƒ©ãƒ¼: {e}")
            except Exception as e:
                raise DataProcessingError(f"Dictâ†’DataFrameå¤‰æ›äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            raise ValueError(f"Unsupported market_data type: {type(market_data)}")

    def _validate_required_columns(self, df: pd.DataFrame) -> None:
        """å¿…è¦åˆ—ã®å­˜åœ¨ç¢ºèª"""
        required_cols = ["open", "high", "low", "close", "volume"]
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise DataProcessingError(f"å¿…è¦åˆ—ãŒä¸è¶³: {missing_cols}")

    def _generate_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åŸºæœ¬ç‰¹å¾´é‡ç”Ÿæˆï¼ˆ3å€‹ï¼‰"""
        result_df = df.copy()

        # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã¯ãã®ã¾ã¾ï¼ˆclose, volumeï¼‰
        basic_features = []
        if "close" in result_df.columns:
            basic_features.append("close")
        if "volume" in result_df.columns:
            basic_features.append("volume")

        self.computed_features.update(basic_features)
        self.logger.debug(f"åŸºæœ¬ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {len(basic_features)}å€‹")
        return result_df

    def _generate_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ç”Ÿæˆï¼ˆ6å€‹ï¼‰"""
        result_df = df.copy()

        # RSI 14æœŸé–“
        result_df["rsi_14"] = self._calculate_rsi(result_df["close"])
        self.computed_features.add("rsi_14")

        # MACDï¼ˆãƒ©ã‚¤ãƒ³ã®ã¿ç”Ÿæˆï¼‰
        macd_line, _ = self._calculate_macd(result_df["close"])
        result_df["macd"] = macd_line
        self.computed_features.add("macd")

        # ATR 14æœŸé–“
        result_df["atr_14"] = self._calculate_atr(result_df)
        self.computed_features.add("atr_14")

        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ä½ç½®
        result_df["bb_position"] = self._calculate_bb_position(result_df["close"])
        self.computed_features.add("bb_position")

        # EMA 20æœŸé–“ãƒ»50æœŸé–“
        result_df["ema_20"] = result_df["close"].ewm(span=20, adjust=False).mean()
        result_df["ema_50"] = result_df["close"].ewm(span=50, adjust=False).mean()
        self.computed_features.update(["ema_20", "ema_50"])

        # Donchian ChannelæŒ‡æ¨™ï¼ˆ3å€‹ï¼‰
        donchian_high, donchian_low, channel_position = self._calculate_donchian_channel(result_df)
        result_df["donchian_high_20"] = donchian_high
        result_df["donchian_low_20"] = donchian_low
        result_df["channel_position"] = channel_position
        self.computed_features.update(["donchian_high_20", "donchian_low_20", "channel_position"])

        # ADXæŒ‡æ¨™ï¼ˆ3å€‹ï¼‰
        adx, plus_di, minus_di = self._calculate_adx_indicators(result_df)
        result_df["adx_14"] = adx
        result_df["plus_di_14"] = plus_di
        result_df["minus_di_14"] = minus_di
        self.computed_features.update(["adx_14", "plus_di_14", "minus_di_14"])

        self.logger.debug("ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ç”Ÿæˆå®Œäº†: 11å€‹")
        return result_df

    def _generate_anomaly_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç•°å¸¸æ¤œçŸ¥æŒ‡æ¨™ç”Ÿæˆï¼ˆ1å€‹ï¼‰"""
        result_df = df.copy()

        # å‡ºæ¥é«˜æ¯”ç‡
        result_df["volume_ratio"] = self._calculate_volume_ratio(result_df["volume"])
        self.computed_features.add("volume_ratio")

        self.logger.debug("ç•°å¸¸æ¤œçŸ¥æŒ‡æ¨™ç”Ÿæˆå®Œäº†: 2å€‹")
        return result_df

    def _calculate_rsi(self, close: pd.Series, period: int = 14) -> pd.Series:
        """RSIè¨ˆç®—"""
        delta = close.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()
        rs = gain / (loss + 1e-8)
        return 100 - (100 / (1 + rs))

    def _calculate_macd(self, close: pd.Series) -> tuple:
        """MACDè¨ˆç®—ï¼ˆMACDãƒ©ã‚¤ãƒ³ã¨ã‚·ã‚°ãƒŠãƒ«ãƒ©ã‚¤ãƒ³ã‚’è¿”ã™ï¼‰"""
        exp1 = close.ewm(span=12, adjust=False).mean()
        exp2 = close.ewm(span=26, adjust=False).mean()
        macd_line = exp1 - exp2
        macd_signal = macd_line.ewm(span=9, adjust=False).mean()
        return macd_line, macd_signal

    def _calculate_atr(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
        """ATRè¨ˆç®—"""
        high_low = df["high"] - df["low"]
        high_close = np.abs(df["high"] - df["close"].shift())
        low_close = np.abs(df["low"] - df["close"].shift())
        true_range = np.maximum(high_low, np.maximum(high_close, low_close))
        return true_range.rolling(window=period, min_periods=1).mean()

    def _calculate_bb_position(self, close: pd.Series, period: int = 20) -> pd.Series:
        """ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ä½ç½®è¨ˆç®—"""
        bb_middle = close.rolling(window=period, min_periods=1).mean()
        bb_std_dev = close.rolling(window=period, min_periods=1).std()
        bb_upper = bb_middle + (bb_std_dev * 2)
        bb_lower = bb_middle - (bb_std_dev * 2)
        return (close - bb_lower) / (bb_upper - bb_lower + 1e-8)

    def _calculate_volume_ratio(self, volume: pd.Series, period: Optional[int] = None) -> pd.Series:
        """å‡ºæ¥é«˜æ¯”ç‡è¨ˆç®—"""
        try:
            if period is None:
                period = get_anomaly_config("volume_ratio.calculation_period", 20)
            volume_avg = volume.rolling(window=period, min_periods=1).mean()
            return volume / (volume_avg + 1e-8)
        except Exception as e:
            self.logger.error(f"å‡ºæ¥é«˜æ¯”ç‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.Series(np.zeros(len(volume)), index=volume.index)

    # Phase 22: market_stressç‰¹å¾´é‡å‰Šé™¤ï¼ˆ15ç‰¹å¾´é‡çµ±ä¸€ï¼‰
    # def _calculate_market_stress(self, df: pd.DataFrame) -> pd.Series:
    #     """å¸‚å ´ã‚¹ãƒˆãƒ¬ã‚¹åº¦æŒ‡æ¨™è¨ˆç®—ï¼ˆçµ±åˆç•°å¸¸æŒ‡æ¨™ï¼‰"""

    def _normalize(self, series: pd.Series) -> pd.Series:
        """0-1ç¯„å›²ã«æ­£è¦åŒ–"""
        try:
            # å¤–ã‚Œå€¤å‡¦ç†ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ï¼‰
            outlier_clip_quantile = get_anomaly_config("normalization.outlier_clip_quantile", 0.95)
            upper_bound = series.quantile(outlier_clip_quantile)
            clipped_series = np.clip(series, 0, upper_bound)

            # 0-1æ­£è¦åŒ–
            min_val = clipped_series.min()
            max_val = clipped_series.max()

            if max_val - min_val == 0:
                return pd.Series(np.zeros(len(series)), index=series.index)

            return (clipped_series - min_val) / (max_val - min_val)

        except Exception:
            return pd.Series(np.zeros(len(series)), index=series.index)

    def _calculate_donchian_channel(self, df: pd.DataFrame, period: int = 20) -> tuple:
        """
        Donchian Channelè¨ˆç®—

        Args:
            df: OHLCV DataFrame
            period: è¨ˆç®—æœŸé–“ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰

        Returns:
            (donchian_high, donchian_low, channel_position)
        """
        try:
            high = df["high"]
            low = df["low"]
            close = df["close"]

            # 20æœŸé–“ã®æœ€é«˜å€¤ãƒ»æœ€å®‰å€¤
            donchian_high = high.rolling(window=period, min_periods=1).max()
            donchian_low = low.rolling(window=period, min_periods=1).min()

            # ãƒãƒ£ãƒãƒ«å†…ä½ç½®è¨ˆç®—ï¼ˆ0-1ï¼‰
            channel_width = donchian_high - donchian_low
            channel_position = (close - donchian_low) / (channel_width + 1e-8)

            # NaNå€¤ã‚’é©åˆ‡ãªå€¤ã§è£œå®Œ
            donchian_high = donchian_high.bfill().fillna(high.iloc[0])
            donchian_low = donchian_low.bfill().fillna(low.iloc[0])
            channel_position = channel_position.fillna(0.5)  # ä¸­å¤®å€¤ã§è£œå®Œ

            return donchian_high, donchian_low, channel_position

        except Exception as e:
            self.logger.error(f"Donchian Channelè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            zeros = pd.Series(np.zeros(len(df)), index=df.index)
            half_ones = pd.Series(np.full(len(df), 0.5), index=df.index)
            return zeros, zeros, half_ones

    def _calculate_adx_indicators(self, df: pd.DataFrame, period: int = 14) -> tuple:
        """
        ADXæŒ‡æ¨™è¨ˆç®—ï¼ˆADXã€+DIã€-DIï¼‰

        Args:
            df: OHLCV DataFrame
            period: è¨ˆç®—æœŸé–“ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 14ï¼‰

        Returns:
            (adx, plus_di, minus_di)
        """
        try:
            high = df["high"]
            low = df["low"]
            close = df["close"]

            # True Rangeè¨ˆç®—
            tr1 = high - low
            tr2 = np.abs(high - close.shift(1))
            tr3 = np.abs(low - close.shift(1))
            tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)

            # Directional Movementè¨ˆç®—
            plus_dm = (high - high.shift(1)).where((high - high.shift(1)) > (low.shift(1) - low), 0)
            minus_dm = (low.shift(1) - low).where((low.shift(1) - low) > (high - high.shift(1)), 0)
            plus_dm = plus_dm.where(plus_dm > 0, 0)
            minus_dm = minus_dm.where(minus_dm > 0, 0)

            # Smoothed True Range ã¨ Directional Movement
            atr = tr.rolling(window=period, min_periods=1).mean()
            plus_dm_smooth = plus_dm.rolling(window=period, min_periods=1).mean()
            minus_dm_smooth = minus_dm.rolling(window=period, min_periods=1).mean()

            # Directional Indicators
            plus_di = 100 * plus_dm_smooth / (atr + 1e-8)
            minus_di = 100 * minus_dm_smooth / (atr + 1e-8)

            # Directional Index
            dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di + 1e-8)

            # ADX (Average Directional Index)
            adx = dx.rolling(window=period, min_periods=1).mean()

            # NaNå€¤è£œå®Œ
            adx = adx.bfill().fillna(0)
            plus_di = plus_di.bfill().fillna(0)
            minus_di = minus_di.bfill().fillna(0)

            return adx, plus_di, minus_di

        except Exception as e:
            self.logger.error(f"ADXæŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            zeros = pd.Series(np.zeros(len(df)), index=df.index)
            return zeros, zeros, zeros

    def _handle_nan_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """NaNå€¤å‡¦ç†ï¼ˆçµ±åˆç‰ˆï¼‰"""
        for feature in self.computed_features:
            if feature in df.columns:
                df[feature] = df[feature].ffill().bfill().fillna(0)
        return df

    def _validate_feature_generation(self, df: pd.DataFrame) -> None:
        """15ç‰¹å¾´é‡å®Œå…¨ç¢ºèªãƒ»æ¤œè¨¼"""
        generated_features = [col for col in OPTIMIZED_FEATURES if col in df.columns]
        missing_features = [col for col in OPTIMIZED_FEATURES if col not in df.columns]

        # ğŸš¨ çµ±åˆãƒ­ã‚°å‡ºåŠ›
        self.logger.info(
            f"ç‰¹å¾´é‡ç”Ÿæˆå®Œäº† - ç·æ•°: {len(generated_features)}/{len(OPTIMIZED_FEATURES)}å€‹",
            extra_data={
                "basic_features": len([f for f in ["close", "volume"] if f in df.columns]),
                "technical_features": len(
                    [
                        f
                        for f in [
                            "rsi_14",
                            "macd",
                            "atr_14",
                            "bb_position",
                            "ema_20",
                            "ema_50",
                        ]
                        if f in df.columns
                    ]
                ),
                "anomaly_features": len([f for f in ["volume_ratio"] if f in df.columns]),
                "generated_features": generated_features,
                "missing_features": missing_features,
                "total_expected": len(OPTIMIZED_FEATURES),
                "success": len(generated_features) == len(OPTIMIZED_FEATURES),
            },
        )

        # âš ï¸ ä¸è¶³ç‰¹å¾´é‡ã®è­¦å‘Š
        if missing_features:
            self.logger.warning(
                f"ğŸš¨ ç‰¹å¾´é‡ä¸è¶³æ¤œå‡º: {missing_features} ({len(missing_features)}å€‹ä¸è¶³)"
            )
        else:
            self.logger.info("âœ… 15ç‰¹å¾´é‡å®Œå…¨ç”ŸæˆæˆåŠŸ")

    def get_feature_info(self) -> Dict[str, Any]:
        """ç‰¹å¾´é‡æƒ…å ±å–å¾—"""
        return {
            "total_features": len(self.computed_features),
            "computed_features": sorted(list(self.computed_features)),
            "feature_categories": FEATURE_CATEGORIES,
            "optimized_features": OPTIMIZED_FEATURES,
            "parameters": {"lookback_period": self.lookback_period},
            "feature_descriptions": {
                "close": "çµ‚å€¤ï¼ˆåŸºæº–ä¾¡æ ¼ï¼‰",
                "volume": "å‡ºæ¥é«˜ï¼ˆå¸‚å ´æ´»å‹•åº¦ï¼‰",
                "rsi_14": "RSIï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒœãƒ¼ãƒˆãƒ»ã‚½ãƒ¼ãƒ«ãƒ‰åˆ¤å®šï¼‰",
                "macd": "MACDï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰è»¢æ›ã‚·ã‚°ãƒŠãƒ«ï¼‰",
                "atr_14": "ATRï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¸¬å®šï¼‰",
                "bb_position": "ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ä½ç½®ï¼ˆä¾¡æ ¼ä½ç½®ï¼‰",
                "ema_20": "EMAçŸ­æœŸï¼ˆçŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰",
                "ema_50": "EMAä¸­æœŸï¼ˆä¸­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰",
                "donchian_high_20": "Donchian Channelä¸Šé™ï¼ˆ20æœŸé–“æœ€é«˜å€¤ï¼‰",
                "donchian_low_20": "Donchian Channelä¸‹é™ï¼ˆ20æœŸé–“æœ€å®‰å€¤ï¼‰",
                "channel_position": "ãƒãƒ£ãƒãƒ«å†…ä½ç½®ï¼ˆ0-1æ­£è¦åŒ–ä½ç½®ï¼‰",
                "adx_14": "ADXï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦æŒ‡æ¨™ï¼‰",
                "plus_di_14": "+DIï¼ˆä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘æ€§ï¼‰",
                "minus_di_14": "-DIï¼ˆä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘æ€§ï¼‰",
                "volume_ratio": "å‡ºæ¥é«˜æ¯”ç‡ï¼ˆå‡ºæ¥é«˜ç•°å¸¸æ¤œçŸ¥ï¼‰",
            },
        }


# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒªã‚¹ãƒˆ
__all__ = [
    "FeatureGenerator",
    "OPTIMIZED_FEATURES",
    "FEATURE_CATEGORIES",
]
