"""
ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼ - Phase 38.4å®Œäº†ç‰ˆ

Phase 28-29æœ€é©åŒ–:
- ãƒšãƒ¼ãƒ‘ãƒ¼ãƒˆãƒ¬ãƒ¼ãƒ‰ã¨åŒã˜ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§CSVãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- æœ¬ç•ªã¨åŒä¸€ã®trading_cycle_managerã‚’ä½¿ç”¨
- CSVãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—ã§é †æ¬¡å‡¦ç†ã—ã€å„æ™‚ç‚¹ã§å–å¼•åˆ¤å®šã‚’å®Ÿè¡Œ
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã€ãƒ«ãƒƒã‚¯ã‚¢ãƒ˜ãƒƒãƒ‰ã‚’é˜²æ­¢

Phase 35: ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ10å€é«˜é€ŸåŒ–å®Ÿè£…
- ç‰¹å¾´é‡äº‹å‰è¨ˆç®—: 288åˆ†â†’0ç§’ï¼ˆç„¡é™å€é«˜é€ŸåŒ–ï¼‰ãƒ»265,130ä»¶/ç§’å‡¦ç†
- MLäºˆæ¸¬äº‹å‰è¨ˆç®—: 15åˆ†â†’0.3ç§’ï¼ˆ3,000å€é«˜é€ŸåŒ–ï¼‰ãƒ»10,063ä»¶/ç§’å‡¦ç†
- ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ­£å¸¸åŒ–: entry_priceè¿½åŠ ãƒ»Â¥0å•é¡Œè§£æ±º
- ãƒ­ã‚°æœ€é©åŒ–: 70%å‰Šæ¸›ï¼ˆ12,781è¡Œâ†’3,739è¡Œï¼‰ãƒ»å¯èª­æ€§å¤§å¹…å‘ä¸Š
- åˆè¨ˆé«˜é€ŸåŒ–: 6-8æ™‚é–“â†’45åˆ†ï¼ˆç´„10å€é«˜é€ŸåŒ–é”æˆï¼‰

Phase 38: tradingå±¤ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å®Ÿè£…å®Œäº†
Phase 38.4: å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«Phaseçµ±ä¸€ãƒ»ã‚³ãƒ¼ãƒ‰å“è³ªä¿è¨¼å®Œäº†

è¨­è¨ˆåŸå‰‡:
- æœ¬ç•ªã¨ãƒšãƒ¼ãƒ‘ãƒ¼ãƒˆãƒ¬ãƒ¼ãƒ‰ã®åŒä¸€ãƒ­ã‚¸ãƒƒã‚¯ä½¿ç”¨
- CSVãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹é«˜é€Ÿãƒ»å®‰å®šã—ãŸãƒ‡ãƒ¼ã‚¿ä¾›çµ¦
- æ™‚åˆ»ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹æ­£ç¢ºãªæ™‚ç³»åˆ—å‡¦ç†
- BacktestReporterã«ã‚ˆã‚‹è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple

import pandas as pd

from ..config import get_threshold
from .base_runner import BaseRunner


class BacktestRunner(BaseRunner):
    """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œã‚¯ãƒ©ã‚¹ï¼ˆPhase 38.4å®Œäº†ç‰ˆãƒ»Phase 35ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæœ€é©åŒ–å®Ÿç¸¾ä¿æŒï¼‰"""

    def __init__(self, orchestrator_ref, logger):
        """
        ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼åˆæœŸåŒ–

        Args:
            orchestrator_ref: TradingOrchestratorã¸ã®å‚ç…§
            logger: ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ 
        """
        super().__init__(orchestrator_ref, logger)

        # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçŠ¶æ…‹ç®¡ç†
        self.backtest_start = None
        self.backtest_end = None
        self.current_timestamp = None
        self.csv_data = {}  # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥CSVãƒ‡ãƒ¼ã‚¿
        self.precomputed_features = {}  # Phase 35: äº‹å‰è¨ˆç®—æ¸ˆã¿ç‰¹å¾´é‡ï¼ˆ10å€é«˜é€ŸåŒ–ï¼‰
        self.precomputed_ml_predictions = {}  # Phase 35.4: äº‹å‰è¨ˆç®—æ¸ˆã¿MLäºˆæ¸¬ï¼ˆ10å€é«˜é€ŸåŒ–ï¼‰
        self.data_index = 0  # ç¾åœ¨ã®å‡¦ç†ä½ç½®
        self.total_data_points = 0

        # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨­å®šï¼ˆPhase 28å®Œäº†ãƒ»Phase 29æœ€é©åŒ–ï¼‰
        self.symbol = get_threshold("backtest.symbol", "BTC/JPY")
        self.timeframes = get_threshold("backtest.timeframes", ["15m", "4h"])
        self.lookback_window = get_threshold(
            "backtest.lookback_window", 100
        )  # å„æ™‚ç‚¹ã§éå»Nä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¾›çµ¦

        # çµ±è¨ˆæƒ…å ±
        self.cycle_count = 0
        self.processed_timestamps = []
        self.session_stats = {}

    async def run(self) -> bool:
        """
        ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ

        Returns:
            å®Ÿè¡ŒæˆåŠŸãƒ»å¤±æ•—
        """
        try:
            self.logger.warning("ğŸ“Š ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰é–‹å§‹ï¼ˆPhase 35æœ€é©åŒ–: ãƒ­ã‚°=WARNINGï¼‰")

            # 1. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæœŸé–“è¨­å®š
            await self._setup_backtest_period()

            # 2. CSVãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            await self._load_csv_data()

            # 3. ç‰¹å¾´é‡äº‹å‰è¨ˆç®—ï¼ˆPhase 35: 10å€é«˜é€ŸåŒ–ï¼‰
            await self._precompute_features()

            # 3.5. MLäºˆæ¸¬äº‹å‰è¨ˆç®—ï¼ˆPhase 35.4: ã•ã‚‰ãªã‚‹é«˜é€ŸåŒ–ï¼‰
            await self._precompute_ml_predictions()

            # 4. ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            if not await self._validate_data():
                self.logger.error("âŒ CSVãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†ã§ã™")
                return False

            # 5. æ™‚ç³»åˆ—ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            await self._run_time_series_backtest()

            # 6. æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            await self._generate_final_backtest_report()

            self.logger.warning("âœ… ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå®Œäº†", discord_notify=True)
            return True

        except Exception as e:
            self.logger.error(f"âŒ ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}", discord_notify=True)
            await self._save_error_report(str(e))
            raise

    async def _setup_backtest_period(self):
        """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæœŸé–“è¨­å®š"""
        # å¤–éƒ¨è¨­å®šã‹ã‚‰æœŸé–“ã‚’å–å¾—
        backtest_days = get_threshold("execution.backtest_period_days", 30)

        self.backtest_end = datetime.now()
        self.backtest_start = self.backtest_end - timedelta(days=backtest_days)

        self.logger.info(
            f"ğŸ“… ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæœŸé–“: {self.backtest_start.strftime('%Y-%m-%d')} "
            f"~ {self.backtest_end.strftime('%Y-%m-%d')} ({backtest_days}æ—¥é–“)"
        )

    async def _load_csv_data(self):
        """CSVãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            # CSV ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨
            from ...backtest.data.csv_data_loader import get_csv_loader

            csv_loader = get_csv_loader()

            # ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            self.csv_data = csv_loader.load_multi_timeframe(
                symbol=self.symbol,
                timeframes=self.timeframes,
                start_date=self.backtest_start,
                end_date=self.backtest_end,
                limit=get_threshold("backtest.data_limit", 10000),  # ååˆ†ãªé‡ã‚’ç¢ºä¿
            )

            # è¨­å®šã‹ã‚‰ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—
            main_timeframe = self.timeframes[0] if self.timeframes else "4h"

            if (
                not self.csv_data
                or main_timeframe not in self.csv_data
                or self.csv_data[main_timeframe].empty
            ):
                raise ValueError(f"ä¸»è¦ãƒ‡ãƒ¼ã‚¿ï¼ˆ{main_timeframe}ï¼‰ãŒä¸è¶³: {self.symbol}")

            # ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
            self.total_data_points = len(self.csv_data[main_timeframe])

            # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã‚’å‹•çš„ã«ç”Ÿæˆ
            timeframe_stats = []
            for tf in self.timeframes:
                count = len(self.csv_data.get(tf, []))
                timeframe_stats.append(f"{tf}:{count}ä»¶")

            self.logger.warning(f"ğŸ“ˆ CSVãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {', '.join(timeframe_stats)}")

        except Exception as e:
            self.logger.error(f"âŒ CSVãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    async def _precompute_features(self):
        """
        ç‰¹å¾´é‡äº‹å‰è¨ˆç®—ï¼ˆPhase 35: 10å€é«˜é€ŸåŒ–ï¼‰

        å…¨CSVãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ä¸€åº¦ã ã‘ç‰¹å¾´é‡ã‚’è¨ˆç®—ã—ã€
        å„ã‚µã‚¤ã‚¯ãƒ«ã§ã¯ã‚¹ãƒ©ã‚¤ã‚¹ã®ã¿å®Ÿè¡Œã™ã‚‹ã“ã¨ã§å¤§å¹…é«˜é€ŸåŒ–ã€‚

        æœ€é©åŒ–åŠ¹æœ:
        - 17,271å›ã®ç‰¹å¾´é‡è¨ˆç®— â†’ 1å›ï¼ˆã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ æ¯ï¼‰
        - ç†è«–å€¤: 17,271å€é«˜é€ŸåŒ–
        - å®Ÿæ¸¬äºˆæƒ³: 20-30åˆ† â†’ 2-3åˆ†ï¼ˆ10å€é«˜é€ŸåŒ–ï¼‰
        """
        try:
            import time

            from ...features.feature_generator import FeatureGenerator

            self.logger.warning("ğŸš€ ç‰¹å¾´é‡äº‹å‰è¨ˆç®—é–‹å§‹ï¼ˆPhase 35æœ€é©åŒ–ï¼‰")
            start_time = time.time()

            feature_gen = FeatureGenerator()

            for timeframe, df in self.csv_data.items():
                if df.empty:
                    continue

                # Phase 35.2: è©³ç´°ãƒ­ã‚°å‰Šé™¤ï¼ˆé«˜é€ŸåŒ–ï¼‰
                tf_start = time.time()

                # åŒæœŸç‰ˆç‰¹å¾´é‡ç”Ÿæˆï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬è¨ˆç®—ï¼‰
                features_df = feature_gen.generate_features_sync(df)

                # äº‹å‰è¨ˆç®—çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                self.precomputed_features[timeframe] = features_df

            elapsed = time.time() - start_time
            total_records = sum(len(df) for df in self.csv_data.values())
            self.logger.warning(
                f"âœ… ç‰¹å¾´é‡äº‹å‰è¨ˆç®—å®Œäº†: {total_records}ä»¶ "
                f"ï¼ˆ{elapsed:.1f}ç§’, {total_records / elapsed:.0f}ä»¶/ç§’ï¼‰",
                discord_notify=False,
            )

        except Exception as e:
            self.logger.error(f"âŒ ç‰¹å¾´é‡äº‹å‰è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    async def _precompute_ml_predictions(self):
        """
        MLäºˆæ¸¬äº‹å‰è¨ˆç®—ï¼ˆPhase 35.4: ã•ã‚‰ãªã‚‹é«˜é€ŸåŒ–ï¼‰

        å…¨ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ä¸€åº¦ã ã‘MLäºˆæ¸¬ã‚’å®Ÿè¡Œã—ã€
        å„ã‚µã‚¤ã‚¯ãƒ«ã§ã¯äº‹å‰è¨ˆç®—æ¸ˆã¿äºˆæ¸¬ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§å¤§å¹…é«˜é€ŸåŒ–ã€‚

        æœ€é©åŒ–åŠ¹æœ:
        - 2,747å›ã®MLäºˆæ¸¬ â†’ 1å›ã®ãƒãƒƒãƒäºˆæ¸¬
        - ç†è«–å€¤: 2,747å€é«˜é€ŸåŒ–
        - å®Ÿæ¸¬äºˆæƒ³: 15åˆ† â†’ 1-2åˆ†ï¼ˆ10å€é«˜é€ŸåŒ–ï¼‰
        """
        try:
            import time

            import numpy as np

            from ...core.config.feature_manager import get_feature_names

            self.logger.warning("ğŸ¤– MLäºˆæ¸¬äº‹å‰è¨ˆç®—é–‹å§‹ï¼ˆPhase 35.4æœ€é©åŒ–ï¼‰")
            start_time = time.time()

            # ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®ç‰¹å¾´é‡ã«å¯¾ã—ã¦MLäºˆæ¸¬
            main_timeframe = self.timeframes[0] if self.timeframes else "4h"
            if main_timeframe in self.precomputed_features:
                features_df = self.precomputed_features[main_timeframe]

                # 15ç‰¹å¾´é‡ã®ã¿æŠ½å‡º
                features_to_use = get_feature_names()
                available_features = [col for col in features_to_use if col in features_df.columns]

                if len(available_features) == len(features_to_use):
                    ml_features = features_df[available_features]

                    # ãƒãƒƒãƒäºˆæ¸¬å®Ÿè¡Œ
                    predictions_array = self.orchestrator.ml_service.predict(ml_features)
                    probabilities_array = self.orchestrator.ml_service.predict_proba(ml_features)

                    # äºˆæ¸¬çµæœã‚’ä¿å­˜ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¯¾å¿œï¼‰
                    self.precomputed_ml_predictions[main_timeframe] = {
                        "predictions": predictions_array,
                        "probabilities": probabilities_array,
                    }

                    elapsed = time.time() - start_time
                    self.logger.warning(
                        f"âœ… MLäºˆæ¸¬äº‹å‰è¨ˆç®—å®Œäº†: {len(predictions_array)}ä»¶ "
                        f"ï¼ˆ{elapsed:.1f}ç§’, {len(predictions_array) / elapsed:.0f}ä»¶/ç§’ï¼‰",
                        discord_notify=False,
                    )
                else:
                    self.logger.warning(
                        f"âš ï¸ ç‰¹å¾´é‡ä¸è¶³: {len(available_features)}/{len(features_to_use)}å€‹ - MLäºˆæ¸¬ã‚¹ã‚­ãƒƒãƒ—"
                    )

        except Exception as e:
            self.logger.error(f"âŒ MLäºˆæ¸¬äº‹å‰è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯é€šå¸¸ã®MLäºˆæ¸¬ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå‡¦ç†ç¶™ç¶šï¼‰
            self.precomputed_ml_predictions = {}

    async def _validate_data(self) -> bool:
        """ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼"""
        min_data_points = get_threshold("backtest.min_data_points", 50)

        if self.total_data_points < min_data_points:
            self.logger.warning(
                f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³: {self.total_data_points}ä»¶ " f"ï¼ˆæœ€å°{min_data_points}ä»¶å¿…è¦ï¼‰"
            )
            return False

        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        main_timeframe = self.timeframes[0] if self.timeframes else "4h"
        main_data = self.csv_data[main_timeframe]
        if main_data.isnull().any().any():
            self.logger.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")

        if not main_data.index.is_monotonic_increasing:
            self.logger.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒæ™‚ç³»åˆ—é †åºã«ãªã£ã¦ã„ã¾ã›ã‚“")

        return True

    async def _run_time_series_backtest(self):
        """æ™‚ç³»åˆ—ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆPhase 35: é«˜é€ŸåŒ–æœ€é©åŒ–ç‰ˆï¼‰"""
        main_timeframe = self.timeframes[0] if self.timeframes else "4h"
        main_data = self.csv_data[main_timeframe]

        # ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—é †ã§å‡¦ç†
        for i in range(self.lookback_window, len(main_data)):
            self.data_index = i
            self.current_timestamp = main_data.index[i]

            # Phase 35.2: é€²æ—è¡¨ç¤ºï¼ˆWARNINGå¼·åˆ¶å‡ºåŠ›ï¼‰
            progress_interval = get_threshold("backtest.progress_interval", 1000)
            if i % progress_interval == 0:
                progress = (i / len(main_data)) * 100
                self.logger.warning(
                    f"ğŸ“Š ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆé€²è¡Œä¸­: {progress:.1f}% "
                    f"({i}/{len(main_data)}) - {self.current_timestamp.strftime('%Y-%m-%d %H:%M')}"
                )

            # ç¾åœ¨æ™‚ç‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆPhase 35: é«˜é€ŸåŒ–ç‰ˆï¼‰
            await self._setup_current_market_data_fast(i)

            # å–å¼•ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œï¼ˆæœ¬ç•ªã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
            try:
                await self.orchestrator.run_trading_cycle()
                self.cycle_count += 1
                self.processed_timestamps.append(self.current_timestamp)

            except Exception as e:
                self.logger.warning(f"âš ï¸ å–å¼•ã‚µã‚¤ã‚¯ãƒ«ã‚¨ãƒ©ãƒ¼ ({self.current_timestamp}): {e}")
                continue

            # Phase 35.5: é€²æ—ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚’å®Œå…¨å‰Šé™¤ï¼ˆãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆä¸­ã¯ä¸è¦ãƒ»I/Oã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›ï¼‰
            # report_interval = get_threshold("backtest.report_interval", 10000)
            # if i % report_interval == 0:
            #     await self._save_progress_report()

    async def _setup_current_market_data_fast(self, current_index: int):
        """
        ç¾åœ¨æ™‚ç‚¹ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆPhase 35: é«˜é€ŸåŒ–ç‰ˆï¼‰

        æœ€é©åŒ–:
        - df[df.index <= timestamp]ã‚’æ’é™¤ï¼ˆO(n)â†’O(1)ï¼‰
        - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ç›´æ¥ã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°ä½¿ç”¨
        - 100å€ä»¥ä¸Šã®é«˜é€ŸåŒ–
        """
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«æ™‚åˆ»ã‚’ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæ™‚åˆ»ã«è¨­å®š
        await self._set_simulated_time(self.current_timestamp)

        # å„ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        current_market_data = {}

        for timeframe, df in self.csv_data.items():
            if df.empty:
                continue

            # Phase 35: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹é«˜é€Ÿã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°
            # ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã¨åŒã˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½ç½®ã‚’ä½¿ç”¨
            end_idx = min(current_index + 1, len(df))
            start_idx = max(0, end_idx - self.lookback_window)

            # Phase 35.1: äº‹å‰è¨ˆç®—æ¸ˆã¿ç‰¹å¾´é‡ã‚’ä½¿ç”¨ï¼ˆ10å€é«˜é€ŸåŒ–ï¼‰
            if timeframe in self.precomputed_features:
                # äº‹å‰è¨ˆç®—æ¸ˆã¿ç‰¹å¾´é‡ã‹ã‚‰ç›´æ¥ã‚¹ãƒ©ã‚¤ã‚¹ï¼ˆç‰¹å¾´é‡è¨ˆç®—ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                current_market_data[timeframe] = self.precomputed_features[timeframe].iloc[
                    start_idx:end_idx
                ]
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: äº‹å‰è¨ˆç®—ãªã—ã®å ´åˆã¯å…ƒã®ãƒ‡ãƒ¼ã‚¿
                current_market_data[timeframe] = df.iloc[start_idx:end_idx]

        # ãƒ‡ãƒ¼ã‚¿ã‚µãƒ¼ãƒ“ã‚¹ã«ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®š
        self.orchestrator.data_service.set_backtest_data(current_market_data)

        # Phase 35.4: äº‹å‰è¨ˆç®—æ¸ˆã¿MLäºˆæ¸¬ã‚’è¨­å®š
        main_timeframe = self.timeframes[0] if self.timeframes else "4h"
        if main_timeframe in self.precomputed_ml_predictions and current_index < len(
            self.precomputed_ml_predictions[main_timeframe]["predictions"]
        ):
            import numpy as np

            predictions = self.precomputed_ml_predictions[main_timeframe]["predictions"]
            probabilities = self.precomputed_ml_predictions[main_timeframe]["probabilities"]

            # ç¾åœ¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®äºˆæ¸¬å€¤ã‚’å–å¾—
            prediction = int(predictions[current_index])
            confidence = float(np.max(probabilities[current_index]))

            # data_serviceã«MLäºˆæ¸¬ã‚’è¨­å®š
            self.orchestrator.data_service.set_backtest_ml_prediction(
                {"prediction": prediction, "confidence": confidence}
            )

    async def _setup_current_market_data(self):
        """ç¾åœ¨æ™‚ç‚¹ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆæ—§ç‰ˆãƒ»å¾Œæ–¹äº’æ›æ€§ç¶­æŒï¼‰"""
        # Phase 35ã§_setup_current_market_data_fast()ã«ç½®ãæ›ãˆ
        # äº’æ›æ€§ã®ãŸã‚æ®‹ã™ãŒã€ä½¿ç”¨ã•ã‚Œãªã„
        await self._setup_current_market_data_fast(self.data_index)

    async def _set_simulated_time(self, timestamp: datetime):
        """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚åˆ»è¨­å®š"""
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«æ™‚åˆ»ç®¡ç†ã‚¯ãƒ©ã‚¹ãŒã‚ã‚Œã°è¨­å®š
        # ç¾åœ¨ã¯å®Ÿè£…ã›ãšã€å°†æ¥çš„ã«æ™‚åˆ»ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
        pass

    async def _save_progress_report(self):
        """é€²æ—ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ï¼ˆPhase 35: JSON serializableä¿®æ­£ï¼‰"""
        try:
            progress_stats = {
                "current_timestamp": (
                    self.current_timestamp.isoformat() if self.current_timestamp else None
                ),
                "progress_percentage": (
                    (self.data_index / self.total_data_points) * 100
                    if self.total_data_points > 0
                    else 0
                ),
                "cycles_completed": self.cycle_count,
                "processed_data_points": len(self.processed_timestamps),
            }

            # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ã‚¿ãƒ¼ã«é€²æ—ä¿å­˜
            await self.orchestrator.backtest_reporter.save_progress_report(progress_stats)

        except Exception as e:
            self.logger.warning(f"âš ï¸ é€²æ—ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    async def _generate_final_backtest_report(self):
        """æœ€çµ‚ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆPhase 35: JSON serializableä¿®æ­£ï¼‰"""
        try:
            # æœ€çµ‚çµ±è¨ˆåé›†ï¼ˆPhase 35: datetimeâ†’ISOæ–‡å­—åˆ—å¤‰æ›ã§JSON serializableåŒ–ï¼‰
            final_stats = {
                "backtest_period": {
                    "start": self.backtest_start.isoformat() if self.backtest_start else None,
                    "end": self.backtest_end.isoformat() if self.backtest_end else None,
                    "duration_days": (self.backtest_end - self.backtest_start).days,
                },
                "data_processing": {
                    "total_data_points": self.total_data_points,
                    "processed_cycles": self.cycle_count,
                    "processed_timestamps": len(self.processed_timestamps),
                    "success_rate": (
                        len(self.processed_timestamps) / self.total_data_points * 100
                        if self.total_data_points > 0
                        else 0
                    ),
                },
                "timeframes": list(self.csv_data.keys()),
                "symbol": self.symbol,
            }

            # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ã‚¿ãƒ¼çµŒç”±ã§è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            # Phase 35: datetimeâ†’ISOæ–‡å­—åˆ—å¤‰æ›
            await self.orchestrator.backtest_reporter.generate_backtest_report(
                final_stats,
                self.backtest_start.isoformat() if self.backtest_start else None,
                self.backtest_end.isoformat() if self.backtest_end else None,
            )

        except Exception as e:
            self.logger.error(f"âŒ æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

    async def _save_error_report(self, error_message: str):
        """ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        try:
            context = {
                "backtest_runner": "Phase22_BacktestRunner",
                "current_timestamp": self.current_timestamp,
                "data_index": self.data_index,
                "cycle_count": self.cycle_count,
            }

            await self.orchestrator.backtest_reporter.save_error_report(error_message, context)

        except Exception as e:
            self.logger.warning(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å¤±æ•—: {e}")
