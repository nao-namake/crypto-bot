"""
ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ - ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å¯¾å¿œ

15åˆ†è¶³ã¨4æ™‚é–“è¶³ã®ãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡çš„ã«å–å¾—ãƒ»ç®¡ç†ã™ã‚‹
ã‚·ãƒ³ãƒ—ãƒ«ã§é«˜é€Ÿãªãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…ã€‚

ä¸»ãªç‰¹å¾´:
- ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å¯¾å¿œï¼ˆ15m, 4hï¼‰2è»¸æ§‹æˆ
- åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤
- ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½

Phase 13æ”¹å–„å®Ÿè£…æ—¥: 2025å¹´8æœˆ24æ—¥.
"""

import asyncio
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path

# å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆå›é¿ã®ãŸã‚ã€å‹ãƒ’ãƒ³ãƒˆã§ã®ã¿ä½¿ç”¨
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union

import pandas as pd

from ..core.exceptions import DataFetchError
from ..core.logger import get_logger

if TYPE_CHECKING:
    from .bitbank_client import BitbankClient, get_bitbank_client


class TimeFrame(Enum):
    """ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ."""

    M15 = "15m"  # 15åˆ†è¶³
    H4 = "4h"  # 4æ™‚é–“è¶³


@dataclass
class DataRequest:
    """ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒªã‚¯ã‚¨ã‚¹ãƒˆ."""

    symbol: str = "BTC/JPY"
    timeframe: TimeFrame = TimeFrame.H4
    limit: int = 1000
    since: Optional[int] = None


class DataPipeline:
    """
    ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ  ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

    åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ãƒ»å“è³ªç®¡ç†ã‚’æä¾›.
    """

    def __init__(self, client: Optional["BitbankClient"] = None) -> None:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–

        Args:
            client: Bitbankã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆNoneã®å ´åˆã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½¿ç”¨ï¼‰.
        """
        self.logger = get_logger()
        if client is None:
            from .bitbank_client import get_bitbank_client

            self.client = get_bitbank_client()
        else:
            self.client = client

        # ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ¡ãƒ¢ãƒªï¼‰
        self._cache: Dict[str, pd.DataFrame] = {}
        self._cache_timestamps: Dict[str, datetime] = {}

        # è¨­å®š
        self.cache_duration_minutes = 5  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æœŸé–“
        self.max_retries = 3  # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
        self.retry_delay = 1.0  # ãƒªãƒˆãƒ©ã‚¤é–“éš”ï¼ˆç§’ï¼‰

        self.logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")

        # Phase 18çµ±åˆ: ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæ©Ÿèƒ½çµ±åˆ
        self.backtest_data_dir = Path(__file__).parent / "historical"
        self.backtest_data_dir.mkdir(parents=True, exist_ok=True)

        # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿å“è³ªè¨­å®š
        self.backtest_quality_thresholds = {
            "min_data_points": 1000,  # æœ€å°ãƒ‡ãƒ¼ã‚¿æ•°
            "max_gap_hours": 2,  # æœ€å¤§ãƒ‡ãƒ¼ã‚¿æ¬ ææ™‚é–“
            "volume_threshold": 1000,  # æœ€å°å‡ºæ¥é«˜
            "price_change_limit": 0.2,  # 20%ä»¥ä¸Šã®ä¾¡æ ¼å¤‰å‹•åˆ¶é™
        }

        # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨é•·æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        self.backtest_cache_duration_hours = 24 * 7  # 1é€±é–“

    def _generate_cache_key(self, request: DataRequest) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ."""
        return f"{request.symbol}_{request.timeframe.value}_{request.limit}"

    def _is_cache_valid(self, cache_key: str) -> bool:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯."""
        if cache_key not in self._cache_timestamps:
            return False

        cache_time = self._cache_timestamps[cache_key]
        now = datetime.now()

        return (now - cache_time).total_seconds() < (self.cache_duration_minutes * 60)

    def _validate_ohlcv_data(self, data: List[List[Union[int, float]]]) -> bool:
        """OHLCV ãƒ‡ãƒ¼ã‚¿ã®å“è³ªãƒã‚§ãƒƒã‚¯."""
        if not data:
            return False

        for row in data:
            if len(row) != 6:  # [timestamp, open, high, low, close, volume]
                return False

            timestamp, open_price, high, low, close, volume = row

            # åŸºæœ¬çš„ãªä¾¡æ ¼æ¤œè¨¼
            if not all(isinstance(x, (int, float)) for x in row):
                return False

            if high < low or high < open_price or high < close:
                return False

            if low > open_price or low > close:
                return False

            if volume < 0:
                return False

        return True

    def _convert_to_dataframe(self, ohlcv_data: List[List[Union[int, float]]]) -> pd.DataFrame:
        """OHLCV ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›."""
        columns = ["timestamp", "open", "high", "low", "close", "volume"]

        df = pd.DataFrame(ohlcv_data, columns=columns)

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ—¥æ™‚ã«å¤‰æ›
        df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
        df.set_index("timestamp", inplace=True)

        # æ•°å€¤å‹ã«å¤‰æ›
        numeric_columns = ["open", "high", "low", "close", "volume"]
        df[numeric_columns] = df[numeric_columns].astype(float)

        # ã‚½ãƒ¼ãƒˆ
        df.sort_index(inplace=True)

        return df

    async def fetch_ohlcv(self, request: DataRequest, use_cache: bool = True) -> pd.DataFrame:
        """
        OHLCV ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Args:
            request: ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            use_cache: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ãƒ•ãƒ©ã‚°

        Returns:
            OHLCV DataFrameOHLCV ãƒ‡ãƒ¼ã‚¿ï¼ˆpandas DataFrameï¼‰.
        """
        cache_key = self._generate_cache_key(request)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if use_cache and self._is_cache_valid(cache_key):
            self.logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—: {cache_key}")
            return self._cache[cache_key].copy()

        # ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰
        for attempt in range(self.max_retries):
            try:
                self.logger.debug(
                    f"ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: {request.symbol} {request.timeframe.value} "
                    f"(è©¦è¡Œ: {attempt + 1}/{self.max_retries})"
                )

                # APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
                ohlcv_data = await self.client.fetch_ohlcv(
                    symbol=request.symbol,
                    timeframe=request.timeframe.value,
                    since=request.since,
                    limit=request.limit,
                )

                # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
                if not self._validate_ohlcv_data(ohlcv_data):
                    raise DataFetchError(
                        f"ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯å¤±æ•—: {request.symbol} {request.timeframe.value}",
                        context={"attempt": attempt + 1},
                    )

                # DataFrameã«å¤‰æ›
                df = self._convert_to_dataframe(ohlcv_data)

                # å‹å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ - DataFrameã®ä¿è¨¼
                if not isinstance(df, pd.DataFrame):
                    self.logger.error(
                        f"ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚¨ãƒ©ãƒ¼: æœŸå¾…ã•ã‚ŒãŸå‹ã¯DataFrameã€å®Ÿéš›ã®å‹ã¯{type(df)}"
                    )
                    return pd.DataFrame()  # ç©ºã®DataFrameã‚’è¿”ã—ã¦å‹å®‰å…¨æ€§ã‚’ä¿è¨¼

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                self._cache[cache_key] = df.copy()
                self._cache_timestamps[cache_key] = datetime.now()

                self.logger.info(
                    f"ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {request.symbol} {request.timeframe.value}",
                    extra_data={
                        "rows": len(df),
                        "latest_timestamp": (df.index[-1].isoformat() if len(df) > 0 else None),
                        "attempt": attempt + 1,
                        "type_safe": isinstance(df, pd.DataFrame),
                    },
                )

                return df

            except Exception as e:
                self.logger.warning(f"ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•— (è©¦è¡Œ {attempt + 1}/{self.max_retries}): {e}")

                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay)
                else:
                    raise DataFetchError(
                        f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {request.symbol} {request.timeframe.value}",
                        context={"max_retries_exceeded": True},
                    )

    async def fetch_multi_timeframe(
        self, symbol: str = "BTC/JPY", limit: int = 1000
    ) -> Dict[str, pd.DataFrame]:
        """
        ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ  ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬å–å¾—ï¼ˆå‹å®‰å…¨æ€§å¼·åŒ–ï¼‰

        Args:
            symbol: é€šè²¨ãƒšã‚¢
            limit: å„ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®å–å¾—ä»¶æ•°

        Returns:
            ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥ãƒ‡ãƒ¼ã‚¿è¾æ›¸ï¼ˆã™ã¹ã¦DataFrameå‹ä¿è¨¼ï¼‰.
        """
        results = {}

        for timeframe in TimeFrame:
            request = DataRequest(symbol=symbol, timeframe=timeframe, limit=limit)

            try:
                df = await self.fetch_ohlcv(request)

                # ğŸš¨ CRITICAL FIX: å³å¯†ãªè¿”ã‚Šå€¤ãƒã‚§ãƒƒã‚¯
                if df is None:
                    raise ValueError(f"fetch_ohlcvãŒNoneã‚’è¿”ã—ã¾ã—ãŸ: {timeframe.value}")
                # å‹å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ - DataFrameã®ä¿è¨¼
                if isinstance(df, pd.DataFrame):
                    results[timeframe.value] = df
                elif isinstance(df, dict):
                    # è¾æ›¸å‹ã®å ´åˆã¯DataFrameã«å¤‰æ›ã‚’è©¦è¡Œ
                    try:
                        results[timeframe.value] = pd.DataFrame(df)
                        self.logger.warning(f"è¾æ›¸ã‹ã‚‰DataFrameã«å¤‰æ›: {timeframe.value}")
                    except Exception:
                        results[timeframe.value] = pd.DataFrame()
                else:
                    self.logger.warning(
                        f"äºˆæœŸã—ãªã„å‹ãŒè¿”å´ã•ã‚Œã¾ã—ãŸ: {type(df)}, ç©ºDataFrameã§ä»£æ›¿"
                    )
                    results[timeframe.value] = pd.DataFrame()

            except asyncio.CancelledError:
                # ğŸš¨ CRITICAL FIX: éåŒæœŸã‚­ãƒ£ãƒ³ã‚»ãƒ«ã¯å†ç™ºç”Ÿã•ã›ã‚‹
                self.logger.info(f"éåŒæœŸå‡¦ç†ã‚­ãƒ£ãƒ³ã‚»ãƒ«: {timeframe.value}")
                raise
            except asyncio.TimeoutError as e:
                self.logger.error(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeframe.value} - {e}")
                results[timeframe.value] = pd.DataFrame()
            except Exception as e:
                error_msg = (
                    f"ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—å¤±æ•—: {timeframe.value} - " f"{type(e).__name__}: {e}"
                )
                self.logger.error(error_msg)
                # å¤±æ•—ã—ãŸã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã¯å¿…ãšç©ºã®DataFrameã§ä»£æ›¿ï¼ˆå‹ä¿è¨¼ï¼‰
                results[timeframe.value] = pd.DataFrame()

        # æœ€çµ‚çš„ãªå‹ç¢ºèª - ã™ã¹ã¦ãŒDataFrameã§ã‚ã‚‹ã“ã¨ã‚’ä¿è¨¼ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        for tf, data in results.items():
            if not isinstance(data, pd.DataFrame):
                data_detail = str(data)[:100] if data else "None"
                self.logger.error(
                    f"å‹ä¸æ•´åˆæ¤œå‡º: {tf} = {type(data)}, "
                    f"ç©ºã®DataFrameã§ä¿®æ­£. è©³ç´°: {data_detail}"
                )
                results[tf] = pd.DataFrame()
            elif not hasattr(data, "empty"):
                self.logger.error(f"DataFrameå±æ€§ä¸æ•´åˆ: {tf}, ç©ºã®DataFrameã§ä¿®æ­£")
                results[tf] = pd.DataFrame()

        self.logger.info(
            f"ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—å®Œäº†: {symbol}",
            extra_data={
                "timeframes": list(results.keys()),
                "total_rows": sum(len(df) for df in results.values()),
                "all_dataframes": all(isinstance(df, pd.DataFrame) for df in results.values()),
            },
        )

        return results

    async def get_latest_prices(self, symbol: str = "BTC/JPY") -> Dict[str, float]:
        """
        æœ€æ–°ä¾¡æ ¼æƒ…å ±ã‚’å…¨ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰å–å¾—

        Args:
            symbol: é€šè²¨ãƒšã‚¢

        Returns:
            ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥æœ€æ–°ä¾¡æ ¼.
        """
        latest_prices = {}

        for timeframe in TimeFrame:
            try:
                request = DataRequest(symbol=symbol, timeframe=timeframe, limit=1)
                df = await self.fetch_ohlcv(request)

                if len(df) > 0:
                    latest_prices[timeframe.value] = float(df["close"].iloc[-1])

            except Exception as e:
                self.logger.warning(f"æœ€æ–°ä¾¡æ ¼å–å¾—å¤±æ•—: {timeframe.value} - {e}")

        return latest_prices

    def clear_cache(self) -> None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢."""
        self._cache.clear()
        self._cache_timestamps.clear()
        self.logger.info("ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")

    def get_cache_info(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±å–å¾—."""
        now = datetime.now()

        cache_info = {
            "total_cached_items": len(self._cache),
            "cache_size_mb": sum(df.memory_usage(deep=True).sum() for df in self._cache.values())
            / 1024
            / 1024,
            "items": [],
        }

        for key, timestamp in self._cache_timestamps.items():
            age_minutes = (now - timestamp).total_seconds() / 60
            is_valid = age_minutes < self.cache_duration_minutes

            cache_info["items"].append(
                {
                    "key": key,
                    "age_minutes": round(age_minutes, 2),
                    "is_valid": is_valid,
                    "rows": len(self._cache[key]),
                }
            )

        return cache_info

    async def fetch_historical_data(
        self,
        symbol: str = "BTC/JPY",
        timeframe: str = "1h",
        since: Optional[datetime] = None,
        limit: int = 1000,
    ) -> pd.DataFrame:
        """
        ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨ã®éå»ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆéåŒæœŸç‰ˆï¼‰

        Args:
            symbol: é€šè²¨ãƒšã‚¢
            timeframe: æ™‚é–“è»¸
            since: é–‹å§‹æ—¥æ™‚
            limit: å–å¾—æ•°åˆ¶é™

        Returns:
            pd.DataFrame: éå»ãƒ‡ãƒ¼ã‚¿
        """
        try:
            # timeframeã‚’TimeFrameã‚¨ãƒŠãƒ ã«å¤‰æ›ï¼ˆç¾åœ¨ã‚µãƒãƒ¼ãƒˆã™ã‚‹2è»¸ã®ã¿ï¼‰
            timeframe_map = {
                "15m": TimeFrame.M15,
                "4h": TimeFrame.H4,
            }

            tf_enum = timeframe_map.get(timeframe.lower(), TimeFrame.H4)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯4æ™‚é–“è¶³

            # DataRequestã‚’ä½œæˆã—ã¦æ—¢å­˜ã®fetch_ohlcvã‚’ä½¿ç”¨
            # datetimeã‚’ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆãƒŸãƒªç§’ï¼‰ã«å¤‰æ›
            since_timestamp = None
            if since is not None:
                since_timestamp = int(since.timestamp() * 1000)

            request = DataRequest(
                symbol=symbol,
                timeframe=tf_enum,
                limit=limit,
                since=since_timestamp,
            )

            # éåŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œ
            data = await self.fetch_ohlcv(request, use_cache=True)

            self.logger.info(f"Historical data fetched: {len(data)} rows for {symbol} {timeframe}")
            return data

        except Exception as e:
            self.logger.error(f"Historical data fetch error: {e}")
            # ç©ºã®DataFrameã‚’è¿”ã™ï¼ˆã‚¨ãƒ©ãƒ¼å¯¾å¿œï¼‰
            return pd.DataFrame()


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_data_pipeline: Optional[DataPipeline] = None


def get_data_pipeline() -> DataPipeline:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å–å¾—."""
    global _data_pipeline

    if _data_pipeline is None:
        _data_pipeline = DataPipeline()

    return _data_pipeline


def fetch_market_data(
    symbol: str = "BTC/JPY", timeframe: str = "1h", limit: int = 1000
) -> pd.DataFrame:
    """
    å¸‚å ´ãƒ‡ãƒ¼ã‚¿å–å¾—ã®ç°¡æ˜“ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

    Args:
        symbol: é€šè²¨ãƒšã‚¢
        timeframe: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ 
        limit: å–å¾—ä»¶æ•°

    Returns:
        OHLCV DataFrame.
    """
    pipeline = get_data_pipeline()

    # TimeFrame enumã«å¤‰æ›
    timeframe_enum = TimeFrame(timeframe)

    request = DataRequest(symbol=symbol, timeframe=timeframe_enum, limit=limit)

    return pipeline.fetch_ohlcv(request)


# Phase 18çµ±åˆ: DataLoaderæ©Ÿèƒ½çµ±åˆï¼ˆãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå°‚ç”¨ï¼‰


class BacktestDataLoader:
    """
    ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆPhase 18çµ±åˆç‰ˆï¼‰

    DataLoaderã‹ã‚‰ã®çµ±åˆæ©Ÿèƒ½ã€‚DataPipelineã‚’åŸºç›¤ã¨ã—ã¦ã€
    ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå°‚ç”¨ã®é•·æœŸãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚’æä¾›ã€‚
    """

    def __init__(self, data_pipeline: Optional[DataPipeline] = None):
        """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–"""
        self.logger = get_logger(__name__)
        self.data_pipeline = data_pipeline or get_data_pipeline()

        # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå°‚ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.data_dir = Path(__file__).parent / "historical"
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # ãƒ‡ãƒ¼ã‚¿å“è³ªè¨­å®šï¼ˆDataLoaderã‹ã‚‰çµ±åˆï¼‰
        self.quality_thresholds = {
            "min_data_points": 1000,  # æœ€å°ãƒ‡ãƒ¼ã‚¿æ•°
            "max_gap_hours": 2,  # æœ€å¤§ãƒ‡ãƒ¼ã‚¿æ¬ ææ™‚é–“
            "volume_threshold": 1000,  # æœ€å°å‡ºæ¥é«˜
            "price_change_limit": 0.2,  # 20%ä»¥ä¸Šã®ä¾¡æ ¼å¤‰å‹•åˆ¶é™
        }

        self.logger.info("BacktestDataLoaderåˆæœŸåŒ–å®Œäº†ï¼ˆPhase 18çµ±åˆç‰ˆï¼‰")

    async def load_historical_data(
        self,
        symbol: str = "BTC/JPY",
        months: int = 6,
        timeframes: List[str] = None,
        force_refresh: bool = False,
    ) -> Dict[str, pd.DataFrame]:
        """
        éå»ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆDataLoaderã‹ã‚‰çµ±åˆï¼‰

        Args:
            symbol: é€šè²¨ãƒšã‚¢
            months: å–å¾—æœˆæ•°
            timeframes: å¯¾è±¡ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ 
            force_refresh: å¼·åˆ¶å†å–å¾—ãƒ•ãƒ©ã‚°

        Returns:
            ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥ãƒ‡ãƒ¼ã‚¿è¾æ›¸
        """
        if timeframes is None:
            timeframes = ["15m", "1h", "4h"]

        self.logger.info(f"éå»ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ï¼ˆçµ±åˆç‰ˆï¼‰: {symbol} {months}ãƒ¶æœˆ {timeframes}")

        # ãƒ‡ãƒ¼ã‚¿å–å¾—æœŸé–“è¨ˆç®—
        from datetime import datetime, timedelta

        end_date = datetime.now()
        start_date = end_date - timedelta(days=months * 30)

        data_dict = {}

        for timeframe in timeframes:
            try:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèªï¼ˆé•·æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
                if not force_refresh:
                    cached_data = await self._load_from_backtest_cache(
                        symbol, timeframe, start_date, end_date
                    )
                    if cached_data is not None:
                        data_dict[timeframe] = cached_data
                        continue

                # ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆDataPipelineã‚’ä½¿ç”¨ï¼‰
                raw_data = await self._fetch_timeframe_data_integrated(
                    symbol, timeframe, start_date, end_date
                )

                # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆçµ±åˆç‰ˆï¼‰
                cleaned_data = await self._validate_and_clean_data_integrated(raw_data, timeframe)

                # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå°‚ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
                await self._save_to_backtest_cache(
                    symbol, timeframe, cleaned_data, start_date, end_date
                )

                data_dict[timeframe] = cleaned_data

                self.logger.info(f"{timeframe}ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†ï¼ˆçµ±åˆç‰ˆï¼‰: {len(cleaned_data)}ä»¶")

            except Exception as e:
                self.logger.error(f"{timeframe}ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ï¼ˆçµ±åˆç‰ˆï¼‰: {e}")
                data_dict[timeframe] = pd.DataFrame()

        # çµ±åˆãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        await self._validate_integrated_data(data_dict)

        self.logger.info(f"éå»ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†ï¼ˆçµ±åˆç‰ˆï¼‰: {len(data_dict)}ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ")
        return data_dict

    async def _fetch_timeframe_data_integrated(
        self, symbol: str, timeframe: str, start_date: datetime, end_date: datetime
    ) -> pd.DataFrame:
        """DataPipelineçµ±åˆãƒ‡ãƒ¼ã‚¿å–å¾—"""
        all_data = []
        current_date = start_date
        chunk_days = 30

        while current_date < end_date:
            chunk_end = min(current_date + timedelta(days=chunk_days), end_date)

            try:
                # DataPipelineã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿å–å¾—
                chunk_data = await self.data_pipeline.fetch_historical_data(
                    symbol=symbol, timeframe=timeframe, since=current_date, limit=2000
                )

                if not chunk_data.empty:
                    # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    chunk_data = chunk_data[
                        (chunk_data.index >= current_date) & (chunk_data.index < chunk_end)
                    ]
                    all_data.append(chunk_data)

                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
                await asyncio.sleep(1.0)

            except Exception as e:
                self.logger.warning(f"ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ï¼ˆçµ±åˆç‰ˆï¼‰: {e}")

            current_date = chunk_end

        # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        if all_data:
            combined_data = pd.concat(all_data, ignore_index=False)
            combined_data = combined_data.sort_index().drop_duplicates()
            return combined_data
        else:
            return pd.DataFrame()

    async def _validate_and_clean_data_integrated(
        self, data: pd.DataFrame, timeframe: str
    ) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆçµ±åˆç‰ˆï¼‰"""
        if data.empty:
            from ..core.exceptions import DataQualityError

            raise DataQualityError(f"{timeframe}: ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ï¼ˆçµ±åˆç‰ˆï¼‰")

        original_length = len(data)

        # åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆDataLoaderã‹ã‚‰çµ±åˆï¼‰
        required_columns = ["open", "high", "low", "close", "volume"]
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            from ..core.exceptions import DataQualityError

            raise DataQualityError(f"å¿…é ˆã‚«ãƒ©ãƒ ä¸è¶³ï¼ˆçµ±åˆç‰ˆï¼‰: {missing_columns}")

        # æ•°å€¤ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        for col in required_columns:
            data[col] = pd.to_numeric(data[col], errors="coerce")

        # NaNå€¤é™¤å»
        data = data.dropna(subset=required_columns)

        # ç•°å¸¸å€¤æ¤œå‡ºãƒ»é™¤å»
        data = self._remove_outliers_integrated(data)

        # ä¾¡æ ¼æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        data = self._validate_price_consistency_integrated(data)

        # å‡ºæ¥é«˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        data = data[data["volume"] >= self.quality_thresholds["volume_threshold"]]

        # ãƒ‡ãƒ¼ã‚¿ååˆ†æ€§ãƒã‚§ãƒƒã‚¯
        final_length = len(data)
        if final_length < self.quality_thresholds["min_data_points"]:
            from ..core.exceptions import DataQualityError

            raise DataQualityError(
                f"{timeframe}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆçµ±åˆç‰ˆï¼‰ {final_length}/{self.quality_thresholds['min_data_points']}"
            )

        self.logger.info(
            f"{timeframe}ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼ˆçµ±åˆç‰ˆï¼‰: "
            f"{original_length}â†’{final_length}ä»¶ ({final_length / original_length:.1%})"
        )

        return data

    def _remove_outliers_integrated(self, data: pd.DataFrame) -> pd.DataFrame:
        """ç•°å¸¸å€¤é™¤å»ï¼ˆçµ±åˆç‰ˆï¼‰"""
        price_changes = data["close"].pct_change().abs()
        change_limit = self.quality_thresholds["price_change_limit"]

        valid_mask = price_changes <= change_limit
        valid_mask.iloc[0] = True  # æœ€åˆã®è¡Œã¯ä¿æŒ

        outlier_count = (~valid_mask).sum()
        if outlier_count > 0:
            self.logger.warning(f"ç•°å¸¸å€¤é™¤å»ï¼ˆçµ±åˆç‰ˆï¼‰: {outlier_count}ä»¶")

        return data[valid_mask]

    def _validate_price_consistency_integrated(self, data: pd.DataFrame) -> pd.DataFrame:
        """ä¾¡æ ¼æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆçµ±åˆç‰ˆï¼‰"""
        valid_mask = (
            (data["high"] >= data["low"])
            & (data["high"] >= data["open"])
            & (data["high"] >= data["close"])
            & (data["low"] <= data["open"])
            & (data["low"] <= data["close"])
            & (data["open"] > 0)
            & (data["high"] > 0)
            & (data["low"] > 0)
            & (data["close"] > 0)
        )

        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            self.logger.warning(f"ä¾¡æ ¼æ•´åˆæ€§ã‚¨ãƒ©ãƒ¼é™¤å»ï¼ˆçµ±åˆç‰ˆï¼‰: {invalid_count}ä»¶")

        return data[valid_mask]

    async def _load_from_backtest_cache(
        self, symbol: str, timeframe: str, start_date: datetime, end_date: datetime
    ) -> Optional[pd.DataFrame]:
        """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨é•·æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿"""
        cache_key = f"backtest_{symbol}_{timeframe}_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}"
        file_path = self.data_dir / f"{cache_key}.csv"

        try:
            if file_path.exists():
                # ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ™‚åˆ»ãƒã‚§ãƒƒã‚¯ï¼ˆ1é€±é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
                import os

                file_age_hours = (datetime.now().timestamp() - os.path.getmtime(file_path)) / 3600

                if file_age_hours < 24 * 7:  # 1é€±é–“ä»¥å†…
                    cached_data = pd.read_csv(file_path, index_col=0, parse_dates=True)
                    self.logger.info(
                        f"{timeframe}ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {len(cached_data)}ä»¶"
                    )
                    return cached_data

        except Exception as e:
            self.logger.debug(f"ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        return None

    async def _save_to_backtest_cache(
        self,
        symbol: str,
        timeframe: str,
        data: pd.DataFrame,
        start_date: datetime,
        end_date: datetime,
    ):
        """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨é•·æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜"""
        cache_key = f"backtest_{symbol}_{timeframe}_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}"
        file_path = self.data_dir / f"{cache_key}.csv"

        try:
            data.to_csv(file_path)
            self.logger.debug(f"{timeframe}ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº†: {len(data)}ä»¶")
        except Exception as e:
            self.logger.warning(f"ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    async def _validate_integrated_data(self, data_dict: Dict[str, pd.DataFrame]):
        """çµ±åˆãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆçµ±åˆç‰ˆï¼‰"""
        date_ranges = {}
        for timeframe, df in data_dict.items():
            if not df.empty:
                date_ranges[timeframe] = (df.index.min(), df.index.max())

        if len(date_ranges) > 1:
            min_start = max(start for start, _ in date_ranges.values())
            max_end = min(end for _, end in date_ranges.values())

            if min_start >= max_end:
                from ..core.exceptions import DataQualityError

                raise DataQualityError("ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã§é‡è¤‡æœŸé–“ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆçµ±åˆç‰ˆï¼‰")

            overlap_days = (max_end - min_start).days
            self.logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿é‡è¤‡æœŸé–“ï¼ˆçµ±åˆç‰ˆï¼‰: {overlap_days}æ—¥")


# ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼å–å¾—ç”¨ã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°
_backtest_data_loader: Optional[BacktestDataLoader] = None


def get_backtest_data_loader() -> BacktestDataLoader:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼å–å¾—"""
    global _backtest_data_loader

    if _backtest_data_loader is None:
        _backtest_data_loader = BacktestDataLoader()

    return _backtest_data_loader
