"""
ScheduledDataFetcher - å®šæ™‚ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ 
Phase 3: å¤–éƒ¨APIå¾©æ´»å®Ÿè£…ã®æœ€çµ‚æ®µéš

æ©Ÿèƒ½:
- å®šæ™‚ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å–å¾—è² è·å‰Šæ¸›
- æŒ‡æ¨™æ¯æœ€é©ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœŸé–“ç®¡ç†
- è‡ªå‹•æ›´æ–°ãƒ»éšœå®³æ™‚å»¶é•·åˆ©ç”¨
- ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ç®¡ç†
"""

import logging
import threading
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, Optional

from ..monitoring.data_quality_monitor import get_quality_monitor
from .multi_source_fetcher import MultiSourceDataFetcher

logger = logging.getLogger(__name__)


class ScheduleType(Enum):
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç¨®åˆ¥"""

    INTERVAL = "interval"  # ä¸€å®šé–“éš”
    CRON = "cron"  # Cronå¼
    ADAPTIVE = "adaptive"  # é©å¿œçš„é–“éš”


@dataclass
class ScheduleConfig:
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š"""

    fetcher_type: str
    schedule_type: ScheduleType
    interval_minutes: int = 60
    cron_expression: Optional[str] = None
    adaptive_min_minutes: int = 30
    adaptive_max_minutes: int = 240
    priority: int = 1
    enabled: bool = True

    # å“è³ªãƒ™ãƒ¼ã‚¹èª¿æ•´
    quality_based_adjustment: bool = True
    high_quality_extend_factor: float = 1.5  # é«˜å“è³ªæ™‚ã¯é–“éš”ã‚’1.5å€ã«
    low_quality_reduce_factor: float = 0.5  # ä½å“è³ªæ™‚ã¯é–“éš”ã‚’0.5å€ã«

    # å¸‚å ´æ™‚é–“ãƒ™ãƒ¼ã‚¹èª¿æ•´
    market_hours_adjustment: bool = True
    market_hours_factor: float = 0.8  # å¸‚å ´æ™‚é–“ä¸­ã¯é–“éš”ã‚’0.8å€ã«
    off_hours_factor: float = 1.2  # å¸‚å ´æ™‚é–“å¤–ã¯é–“éš”ã‚’1.2å€ã«


@dataclass
class ScheduledTask:
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ¸ˆã¿ã‚¿ã‚¹ã‚¯"""

    task_id: str
    fetcher_type: str
    fetcher_instance: MultiSourceDataFetcher
    config: ScheduleConfig
    next_run: datetime
    last_run: Optional[datetime] = None
    last_success: Optional[datetime] = None
    consecutive_failures: int = 0
    total_runs: int = 0
    successful_runs: int = 0

    def success_rate(self) -> float:
        """æˆåŠŸç‡è¨ˆç®—"""
        if self.total_runs == 0:
            return 0.0
        return self.successful_runs / self.total_runs


class ScheduledDataFetcher:
    """å®šæ™‚ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.schedule_config = self.config.get("scheduled_data_fetcher", {})

        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç®¡ç†
        self.scheduled_tasks: Dict[str, ScheduledTask] = {}
        self.running = False
        self.scheduler_thread = None

        # å®Ÿè¡Œçµ±è¨ˆ
        self.execution_stats = {
            "total_scheduled_runs": 0,
            "successful_runs": 0,
            "failed_runs": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "quality_adjustments": 0,
            "market_adjustments": 0,
        }

        # å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
        self.quality_monitor = get_quality_monitor(config)

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š
        self.max_concurrent_fetches = self.schedule_config.get(
            "max_concurrent_fetches", 3
        )
        self.task_timeout_minutes = self.schedule_config.get("task_timeout_minutes", 10)
        self.health_check_interval = self.schedule_config.get(
            "health_check_interval", 300
        )  # 5åˆ†

        # æ‹¡å¼µã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        self.cache_extension_factor = self.schedule_config.get(
            "cache_extension_factor", 2.0
        )
        self.emergency_cache_hours = self.schedule_config.get(
            "emergency_cache_hours", 48
        )

        logger.info("ğŸ”§ ScheduledDataFetcher initialized")
        logger.info(f"  - Max concurrent fetches: {self.max_concurrent_fetches}")
        logger.info(f"  - Task timeout: {self.task_timeout_minutes} minutes")
        logger.info(f"  - Health check interval: {self.health_check_interval} seconds")

    def register_fetcher(
        self,
        fetcher_type: str,
        fetcher_instance: MultiSourceDataFetcher,
        schedule_config: ScheduleConfig,
    ) -> str:
        """ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ç™»éŒ²"""
        task_id = f"{fetcher_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # æ¬¡å›å®Ÿè¡Œæ™‚åˆ»è¨ˆç®—
        next_run = self._calculate_next_run(schedule_config)

        # ã‚¿ã‚¹ã‚¯ä½œæˆ
        task = ScheduledTask(
            task_id=task_id,
            fetcher_type=fetcher_type,
            fetcher_instance=fetcher_instance,
            config=schedule_config,
            next_run=next_run,
        )

        self.scheduled_tasks[task_id] = task

        logger.info(f"ğŸ“… Registered scheduled fetcher: {fetcher_type}")
        logger.info(f"  - Task ID: {task_id}")
        logger.info(f"  - Next run: {next_run}")
        logger.info(f"  - Schedule: {schedule_config.schedule_type.value}")

        return task_id

    def _calculate_next_run(self, schedule_config: ScheduleConfig) -> datetime:
        """æ¬¡å›å®Ÿè¡Œæ™‚åˆ»è¨ˆç®—"""
        now = datetime.now()

        if schedule_config.schedule_type == ScheduleType.INTERVAL:
            # ä¸€å®šé–“éš”ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
            base_interval = schedule_config.interval_minutes

            # å“è³ªãƒ™ãƒ¼ã‚¹èª¿æ•´
            if schedule_config.quality_based_adjustment:
                base_interval = self._adjust_interval_by_quality(
                    base_interval, schedule_config.fetcher_type, schedule_config
                )

            # å¸‚å ´æ™‚é–“ãƒ™ãƒ¼ã‚¹èª¿æ•´
            if schedule_config.market_hours_adjustment:
                base_interval = self._adjust_interval_by_market_hours(
                    base_interval, schedule_config
                )

            return now + timedelta(minutes=base_interval)

        elif schedule_config.schedule_type == ScheduleType.ADAPTIVE:
            # é©å¿œçš„é–“éš”ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
            return self._calculate_adaptive_interval(schedule_config)

        elif schedule_config.schedule_type == ScheduleType.CRON:
            # Cronå¼ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
            return self._calculate_cron_next_run(schedule_config)

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1æ™‚é–“å¾Œ
        return now + timedelta(hours=1)

    def _adjust_interval_by_quality(
        self, base_interval: int, fetcher_type: str, schedule_config: ScheduleConfig
    ) -> int:
        """å“è³ªãƒ™ãƒ¼ã‚¹é–“éš”èª¿æ•´"""
        try:
            # å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰æœ€æ–°ã®å“è³ªã‚¹ã‚³ã‚¢å–å¾—
            quality_summary = self.quality_monitor.get_quality_summary()
            source_stats = quality_summary.get("source_statistics", {})

            # è©²å½“ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã®å“è³ªã‚¹ã‚³ã‚¢å–å¾—
            quality_score = 0.7  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            for source_key, stats in source_stats.items():
                if fetcher_type in source_key:
                    quality_score = stats.get("last_quality_score", 0.7)
                    break

            # å“è³ªã‚¹ã‚³ã‚¢ã«åŸºã¥ãèª¿æ•´
            if quality_score >= 0.9:
                # é«˜å“è³ªï¼šé–“éš”ã‚’å»¶é•·
                adjusted_interval = int(
                    base_interval * schedule_config.high_quality_extend_factor
                )
                self.execution_stats["quality_adjustments"] += 1
                logger.info(
                    "ğŸ” Quality-based interval extension: %d -> %d minutes "
                    "(quality: %.3f)",
                    base_interval,
                    adjusted_interval,
                    quality_score,
                )
            elif quality_score < 0.6:
                # ä½å“è³ªï¼šé–“éš”ã‚’çŸ­ç¸®
                adjusted_interval = int(
                    base_interval * schedule_config.low_quality_reduce_factor
                )
                self.execution_stats["quality_adjustments"] += 1
                logger.info(
                    "ğŸ” Quality-based interval reduction: %d -> %d minutes "
                    "(quality: %.3f)",
                    base_interval,
                    adjusted_interval,
                    quality_score,
                )
            else:
                # é€šå¸¸å“è³ªï¼šãã®ã¾ã¾
                adjusted_interval = base_interval

            return adjusted_interval

        except Exception as e:
            logger.error(f"âŒ Quality-based interval adjustment failed: {e}")
            return base_interval

    def _adjust_interval_by_market_hours(
        self, base_interval: int, schedule_config: ScheduleConfig
    ) -> int:
        """å¸‚å ´æ™‚é–“ãƒ™ãƒ¼ã‚¹é–“éš”èª¿æ•´"""
        try:
            now = datetime.now()

            # æ—¥æœ¬æ™‚é–“ã§ã®å¸‚å ´æ™‚é–“åˆ¤å®šï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
            # å¹³æ—¥9:00-15:00ã‚’ä¸»è¦ãªå¸‚å ´æ™‚é–“ã¨ã—ã¦æ‰±ã†
            is_weekday = now.weekday() < 5  # 0=æœˆæ›œæ—¥, 6=æ—¥æ›œæ—¥
            is_market_hours = 9 <= now.hour < 15

            if is_weekday and is_market_hours:
                # å¸‚å ´æ™‚é–“ä¸­ï¼šé–“éš”ã‚’çŸ­ç¸®
                adjusted_interval = int(
                    base_interval * schedule_config.market_hours_factor
                )
                self.execution_stats["market_adjustments"] += 1
                logger.info(
                    "ğŸª Market hours interval reduction: %d -> %d minutes",
                    base_interval,
                    adjusted_interval,
                )
            else:
                # å¸‚å ´æ™‚é–“å¤–ï¼šé–“éš”ã‚’å»¶é•·
                adjusted_interval = int(
                    base_interval * schedule_config.off_hours_factor
                )
                self.execution_stats["market_adjustments"] += 1
                logger.info(
                    "ğŸŒ™ Off-hours interval extension: %d -> %d minutes",
                    base_interval,
                    adjusted_interval,
                )

            return adjusted_interval

        except Exception as e:
            logger.error(f"âŒ Market hours interval adjustment failed: {e}")
            return base_interval

    def _calculate_adaptive_interval(self, schedule_config: ScheduleConfig) -> datetime:
        """é©å¿œçš„é–“éš”è¨ˆç®—"""
        now = datetime.now()

        # æˆåŠŸç‡ãƒ™ãƒ¼ã‚¹ã®é–“éš”èª¿æ•´
        # é«˜æˆåŠŸç‡ -> é•·ã„é–“éš”
        # ä½æˆåŠŸç‡ -> çŸ­ã„é–“éš”

        # ç°¡æ˜“å®Ÿè£…ï¼šä¸­é–“å€¤ã‚’ä½¿ç”¨
        adaptive_interval = (
            schedule_config.adaptive_min_minutes + schedule_config.adaptive_max_minutes
        ) / 2

        return now + timedelta(minutes=adaptive_interval)

    def _calculate_cron_next_run(self, schedule_config: ScheduleConfig) -> datetime:
        """Cronå¼æ¬¡å›å®Ÿè¡Œæ™‚åˆ»è¨ˆç®—ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰"""
        now = datetime.now()

        # ç°¡æ˜“å®Ÿè£…ï¼š1æ™‚é–“ã”ã¨ã«å®Ÿè¡Œ
        next_hour = now.replace(minute=0, second=0, microsecond=0) + timedelta(hours=1)
        return next_hour

    def start_scheduler(self) -> None:
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é–‹å§‹"""
        if self.running:
            logger.warning("âš ï¸ Scheduler is already running")
            return

        self.running = True
        self.scheduler_thread = threading.Thread(
            target=self._scheduler_loop, daemon=True
        )
        self.scheduler_thread.start()

        logger.info("ğŸš€ ScheduledDataFetcher started")
        logger.info(f"  - Scheduled tasks: {len(self.scheduled_tasks)}")

    def stop_scheduler(self) -> None:
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼åœæ­¢"""
        self.running = False

        if self.scheduler_thread:
            self.scheduler_thread.join(timeout=5)

        logger.info("ğŸ›‘ ScheduledDataFetcher stopped")

    def _scheduler_loop(self) -> None:
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        logger.info("ğŸ“… Scheduler loop started")

        while self.running:
            try:
                current_time = datetime.now()

                # å®Ÿè¡Œã™ã¹ãã‚¿ã‚¹ã‚¯ã‚’æ¤œç´¢
                tasks_to_run = []
                for _task_id, task in self.scheduled_tasks.items():
                    if task.config.enabled and current_time >= task.next_run:
                        tasks_to_run.append(task)

                # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
                tasks_to_run.sort(key=lambda t: t.config.priority)

                # ä¸¦è¡Œå®Ÿè¡Œåˆ¶é™
                if len(tasks_to_run) > self.max_concurrent_fetches:
                    tasks_to_run = tasks_to_run[: self.max_concurrent_fetches]

                # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
                for task in tasks_to_run:
                    self._execute_scheduled_task(task)

                # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
                self._perform_health_check()

                # çŸ­ã„é–“éš”ã§å†ãƒã‚§ãƒƒã‚¯
                time.sleep(30)  # 30ç§’é–“éš”

            except Exception as e:
                logger.error(f"âŒ Scheduler loop error: {e}")
                time.sleep(60)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯1åˆ†å¾…æ©Ÿ

    def _execute_scheduled_task(self, task: ScheduledTask) -> None:
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ¸ˆã¿ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ"""
        try:
            logger.info(f"ğŸ”„ Executing scheduled task: {task.fetcher_type}")

            start_time = time.time()

            # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
            task.last_run = datetime.now()
            task.total_runs += 1
            self.execution_stats["total_scheduled_runs"] += 1

            # ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
            data = task.fetcher_instance.get_data()

            execution_time = time.time() - start_time

            if data is not None and not data.empty:
                # æˆåŠŸå‡¦ç†
                task.last_success = datetime.now()
                task.successful_runs += 1
                task.consecutive_failures = 0
                self.execution_stats["successful_runs"] += 1

                logger.info(
                    f"âœ… Scheduled task completed successfully: {task.fetcher_type}"
                )
                logger.info(f"  - Data records: {len(data)}")
                logger.info(f"  - Execution time: {execution_time:.2f}s")
                logger.info(f"  - Success rate: {task.success_rate():.2f}")

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ/ãƒŸã‚¹çµ±è¨ˆ
                if execution_time < 1.0:  # 1ç§’æœªæº€ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã¨åˆ¤å®š
                    self.execution_stats["cache_hits"] += 1
                else:
                    self.execution_stats["cache_misses"] += 1

            else:
                # å¤±æ•—å‡¦ç†
                task.consecutive_failures += 1
                self.execution_stats["failed_runs"] += 1

                logger.warning(f"âš ï¸ Scheduled task failed: {task.fetcher_type}")
                logger.warning(f"  - Consecutive failures: {task.consecutive_failures}")

                # é€£ç¶šå¤±æ•—æ™‚ã®å¯¾å¿œ
                if task.consecutive_failures >= 3:
                    logger.error(
                        "âŒ Task disabled due to consecutive failures: %s",
                        task.fetcher_type,
                    )
                    task.config.enabled = False

            # æ¬¡å›å®Ÿè¡Œæ™‚åˆ»è¨ˆç®—
            task.next_run = self._calculate_next_run(task.config)

            logger.info(f"ğŸ“… Next run scheduled: {task.next_run}")

        except Exception as e:
            logger.error(
                f"âŒ Scheduled task execution failed: {task.fetcher_type} - {e}"
            )
            task.consecutive_failures += 1
            self.execution_stats["failed_runs"] += 1

    def _perform_health_check(self) -> None:
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        try:
            current_time = datetime.now()

            # ç„¡åŠ¹åŒ–ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®ç¢ºèª
            disabled_tasks = [
                task
                for task in self.scheduled_tasks.values()
                if not task.config.enabled
            ]

            if disabled_tasks:
                logger.warning(f"âš ï¸ Health check: {len(disabled_tasks)} tasks disabled")

                # å›å¾©å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
                for task in disabled_tasks:
                    if task.consecutive_failures >= 5:
                        # 5å›é€£ç¶šå¤±æ•—ã—ãŸã‚¿ã‚¹ã‚¯ã¯ä¸€æ™‚çš„ã«å†æœ‰åŠ¹åŒ–ã‚’è©¦ã™
                        logger.info(
                            f"ğŸ”„ Attempting to re-enable task: {task.fetcher_type}"
                        )
                        task.config.enabled = True
                        task.consecutive_failures = 0
                        task.next_run = current_time + timedelta(minutes=5)

            # çµ±è¨ˆæƒ…å ±ãƒ­ã‚°
            total_runs = self.execution_stats["total_scheduled_runs"]
            if total_runs > 0:
                success_rate = self.execution_stats["successful_runs"] / total_runs
                cache_hit_rate = self.execution_stats["cache_hits"] / total_runs

                logger.info("ğŸ“Š Health check stats:")
                logger.info(f"  - Total runs: {total_runs}")
                logger.info(f"  - Success rate: {success_rate:.2f}")
                logger.info(f"  - Cache hit rate: {cache_hit_rate:.2f}")
                logger.info(
                    "  - Quality adjustments: %d",
                    self.execution_stats["quality_adjustments"],
                )
                logger.info(
                    "  - Market adjustments: %d",
                    self.execution_stats["market_adjustments"],
                )

        except Exception as e:
            logger.error(f"âŒ Health check failed: {e}")

    def get_task_status(self) -> Dict[str, Any]:
        """ã‚¿ã‚¹ã‚¯çŠ¶æ…‹å–å¾—"""
        status = {
            "running": self.running,
            "total_tasks": len(self.scheduled_tasks),
            "enabled_tasks": len(
                [t for t in self.scheduled_tasks.values() if t.config.enabled]
            ),
            "disabled_tasks": len(
                [t for t in self.scheduled_tasks.values() if not t.config.enabled]
            ),
            "execution_stats": self.execution_stats.copy(),
            "tasks": {},
        }

        for task_id, task in self.scheduled_tasks.items():
            status["tasks"][task_id] = {
                "fetcher_type": task.fetcher_type,
                "enabled": task.config.enabled,
                "next_run": task.next_run.isoformat(),
                "last_run": task.last_run.isoformat() if task.last_run else None,
                "last_success": (
                    task.last_success.isoformat() if task.last_success else None
                ),
                "success_rate": task.success_rate(),
                "consecutive_failures": task.consecutive_failures,
                "total_runs": task.total_runs,
                "schedule_type": task.config.schedule_type.value,
                "interval_minutes": task.config.interval_minutes,
            }

        return status

    def get_cache_statistics(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆå–å¾—"""
        stats = {
            "total_cache_requests": self.execution_stats["cache_hits"]
            + self.execution_stats["cache_misses"],
            "cache_hit_rate": 0.0,
            "cache_miss_rate": 0.0,
            "fetcher_cache_info": {},
        }

        total_requests = stats["total_cache_requests"]
        if total_requests > 0:
            stats["cache_hit_rate"] = (
                self.execution_stats["cache_hits"] / total_requests
            )
            stats["cache_miss_rate"] = (
                self.execution_stats["cache_misses"] / total_requests
            )

        # å„ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±
        for _task_id, task in self.scheduled_tasks.items():
            cache_info = task.fetcher_instance.get_cache_info()
            stats["fetcher_cache_info"][task.fetcher_type] = cache_info

        return stats

    def force_refresh_cache(self, fetcher_type: Optional[str] = None) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¼·åˆ¶ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥"""
        results = {}

        target_tasks = []
        if fetcher_type:
            # ç‰¹å®šãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã®ã¿
            target_tasks = [
                task
                for task in self.scheduled_tasks.values()
                if task.fetcher_type == fetcher_type
            ]
        else:
            # å…¨ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼
            target_tasks = list(self.scheduled_tasks.values())

        for task in target_tasks:
            try:
                logger.info(f"ğŸ”„ Force refreshing cache: {task.fetcher_type}")

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
                task.fetcher_instance.cache_data = None
                task.fetcher_instance.cache_timestamp = None

                # ãƒ‡ãƒ¼ã‚¿å–å¾—
                data = task.fetcher_instance.get_data()

                if data is not None and not data.empty:
                    results[task.fetcher_type] = {
                        "success": True,
                        "records": len(data),
                        "timestamp": datetime.now().isoformat(),
                    }
                    logger.info(
                        f"âœ… Cache refreshed: {task.fetcher_type} - {len(data)} records"
                    )
                else:
                    results[task.fetcher_type] = {
                        "success": False,
                        "error": "No data returned",
                        "timestamp": datetime.now().isoformat(),
                    }
                    logger.warning(f"âš ï¸ Cache refresh failed: {task.fetcher_type}")

            except Exception as e:
                results[task.fetcher_type] = {
                    "success": False,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat(),
                }
                logger.error(f"âŒ Cache refresh error: {task.fetcher_type} - {e}")

        return results


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_scheduled_fetcher = None


def get_scheduled_fetcher(
    config: Optional[Dict[str, Any]] = None,
) -> ScheduledDataFetcher:
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ¸ˆã¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰"""
    global _scheduled_fetcher
    if _scheduled_fetcher is None:
        _scheduled_fetcher = ScheduledDataFetcher(config)
    return _scheduled_fetcher


def initialize_scheduled_fetchers(config: Dict[str, Any]) -> ScheduledDataFetcher:
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ¸ˆã¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼åˆæœŸåŒ–"""
    scheduler = get_scheduled_fetcher(config)

    # VIXãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ç™»éŒ²
    try:
        from .vix_fetcher import VIXDataFetcher

        vix_fetcher = VIXDataFetcher(config)
        vix_schedule = ScheduleConfig(
            fetcher_type="vix",
            schedule_type=ScheduleType.INTERVAL,
            interval_minutes=60,  # 1æ™‚é–“é–“éš”
            priority=1,
            quality_based_adjustment=True,
            market_hours_adjustment=True,
        )
        scheduler.register_fetcher("vix", vix_fetcher, vix_schedule)
    except ImportError:
        logger.warning("âš ï¸ VIX fetcher not available for scheduling")

    # Fear&Greedãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ç™»éŒ²
    try:
        from .fear_greed_fetcher import FearGreedDataFetcher

        fg_fetcher = FearGreedDataFetcher(config)
        fg_schedule = ScheduleConfig(
            fetcher_type="fear_greed",
            schedule_type=ScheduleType.INTERVAL,
            interval_minutes=120,  # 2æ™‚é–“é–“éš”
            priority=2,
            quality_based_adjustment=True,
            market_hours_adjustment=True,
        )
        scheduler.register_fetcher("fear_greed", fg_fetcher, fg_schedule)
    except ImportError:
        logger.warning("âš ï¸ Fear&Greed fetcher not available for scheduling")

    return scheduler
