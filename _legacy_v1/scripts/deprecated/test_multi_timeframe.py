#!/usr/bin/env python3
"""
ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Phase 5.1: çµ±åˆãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼ã®ä¸€ç’°ã¨ã—ã¦ã€
ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆ15m/1h/4hï¼‰ãƒ‡ãƒ¼ã‚¿åŒæœŸãƒ»å¤‰æ›ã‚’æ¤œè¨¼
"""

import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

from crypto_bot.data.multi_timeframe_fetcher import MultiTimeframeDataFetcher
from crypto_bot.data.timeframe_synchronizer import TimeframeSynchronizer
from crypto_bot.main import load_config

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class MultiTimeframeTester:
    """ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config_path: str = "config/production/production.yml"):
        """
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.config = load_config(config_path)
        self.test_results = {
            "timeframe_tests": {},
            "synchronization_tests": {},
            "data_quality_tests": {},
            "performance_metrics": {},
            "test_status": "PENDING",
            "execution_time": 0,
            "errors": [],
        }

    def generate_hourly_test_data(self, n_hours: int = 200) -> pd.DataFrame:
        """1æ™‚é–“è¶³ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼‰"""
        logger.info(f"ğŸ“Š Generating {n_hours} hourly OHLCV samples...")

        end_time = datetime.now().replace(minute=0, second=0, microsecond=0)
        timestamps = [end_time - timedelta(hours=i) for i in range(n_hours)]
        timestamps.reverse()

        # ãƒªã‚¢ãƒ«ãªOHLCVãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        base_price = 5000000  # BTC/JPY
        ohlcv_data = []

        for i, ts in enumerate(timestamps):
            # ä¾¡æ ¼ã®å¤‰å‹•
            price_change = np.random.normal(0, 0.01) * base_price * 0.02
            base_price += price_change

            # OHLC
            volatility = abs(np.random.normal(0.005, 0.002))
            high = base_price * (1 + volatility)
            low = base_price * (1 - volatility)
            open_price = base_price + np.random.normal(0, volatility * 0.5) * base_price
            close_price = base_price
            volume = abs(np.random.normal(50, 20))

            ohlcv_data.append(
                {
                    "timestamp": ts,
                    "open": open_price,
                    "high": high,
                    "low": low,
                    "close": close_price,
                    "volume": volume,
                }
            )

        df = pd.DataFrame(ohlcv_data)
        df.set_index("timestamp", inplace=True)

        logger.info(f"âœ… Generated hourly test data: {len(df)} rows")
        return df

    def test_timeframe_conversion(self, hourly_data: pd.DataFrame) -> Dict:
        """ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ”„ Testing timeframe conversion...")
        conversion_results = {}

        try:
            # MultiTimeframeDataFetcheråˆæœŸåŒ–
            fetcher = MultiTimeframeDataFetcher(self.config)

            # 15åˆ†è¶³ã¸ã®å¤‰æ›ãƒ†ã‚¹ãƒˆï¼ˆè£œé–“ï¼‰
            logger.info("  Testing 1h â†’ 15m conversion (interpolation)...")
            data_15m = fetcher._interpolate_to_15m(hourly_data.copy())
            conversion_results["15m"] = {
                "input_size": len(hourly_data),
                "output_size": len(data_15m) if data_15m is not None else 0,
                "expected_ratio": 4,  # 1æ™‚é–“ = 4Ã—15åˆ†
                "actual_ratio": (
                    len(data_15m) / len(hourly_data)
                    if data_15m is not None and len(hourly_data) > 0
                    else 0
                ),
                "success": data_15m is not None and len(data_15m) > 0,
            }

            # 4æ™‚é–“è¶³ã¸ã®å¤‰æ›ãƒ†ã‚¹ãƒˆï¼ˆé›†ç´„ï¼‰
            logger.info("  Testing 1h â†’ 4h conversion (aggregation)...")
            data_4h = fetcher._aggregate_to_4h(hourly_data.copy())
            conversion_results["4h"] = {
                "input_size": len(hourly_data),
                "output_size": len(data_4h) if data_4h is not None else 0,
                "expected_ratio": 0.25,  # 4æ™‚é–“ = 1/4
                "actual_ratio": (
                    len(data_4h) / len(hourly_data)
                    if data_4h is not None and len(hourly_data) > 0
                    else 0
                ),
                "success": data_4h is not None and len(data_4h) > 0,
            }

            # å“è³ªãƒã‚§ãƒƒã‚¯
            for timeframe, result in conversion_results.items():
                if result["success"]:
                    logger.info(
                        f"    âœ… {timeframe}: {result['output_size']} samples (ratio: {result['actual_ratio']:.2f})"
                    )
                else:
                    logger.error(f"    âŒ {timeframe}: Conversion failed")

            return conversion_results

        except Exception as e:
            logger.error(f"âŒ Timeframe conversion test failed: {str(e)}")
            self.test_results["errors"].append(f"Timeframe conversion: {str(e)}")
            return {"error": str(e)}

    def test_data_synchronization(self, hourly_data: pd.DataFrame) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿åŒæœŸãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ”„ Testing data synchronization...")
        sync_results = {}

        try:
            # TimeframeSynchronizeråˆæœŸåŒ–
            synchronizer = TimeframeSynchronizer(self.config)

            # ç•°ãªã‚‹ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
            fetcher = MultiTimeframeDataFetcher(self.config)
            data_15m = fetcher._interpolate_to_15m(hourly_data.copy())
            data_1h = hourly_data.copy()
            data_4h = fetcher._aggregate_to_4h(hourly_data.copy())

            if data_15m is None or data_4h is None:
                raise ValueError("Failed to generate multi-timeframe data")

            # åŒæœŸãƒ†ã‚¹ãƒˆ
            timeframe_data = {"15m": data_15m, "1h": data_1h, "4h": data_4h}

            logger.info("  Synchronizing timeframes...")
            synchronized_data = synchronizer.synchronize_multi_timeframe_data(
                timeframe_data
            )

            # åŒæœŸçµæœã®åˆ†æ
            if synchronized_data:
                sync_results["success"] = True
                sync_results["synchronized_timeframes"] = len(synchronized_data)
                sync_results["common_timestamps"] = {}

                # å…±é€šã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®åˆ†æ
                for timeframe, data in synchronized_data.items():
                    sync_results["common_timestamps"][timeframe] = len(data)
                    logger.info(
                        f"    âœ… {timeframe}: {len(data)} synchronized timestamps"
                    )

                # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                total_records = sum(len(data) for data in synchronized_data.values())
                avg_records = (
                    total_records / len(synchronized_data) if synchronized_data else 0
                )
                sync_results["quality_score"] = min(
                    avg_records / 100, 1.0
                )  # ç°¡æ˜“ã‚¹ã‚³ã‚¢
                logger.info(
                    f"    ğŸ“Š Avg synchronized records per timeframe: {avg_records:.1f}"
                )

            else:
                sync_results["success"] = False
                logger.error("    âŒ Synchronization failed - no output data")

            return sync_results

        except Exception as e:
            logger.error(f"âŒ Data synchronization test failed: {str(e)}")
            self.test_results["errors"].append(f"Data synchronization: {str(e)}")
            return {"error": str(e)}

    def test_data_quality(self, hourly_data: pd.DataFrame) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ” Testing data quality...")
        quality_results = {}

        try:
            # MultiTimeframeFetcher ã§çµ±åˆãƒ‡ãƒ¼ã‚¿å–å¾—
            fetcher = MultiTimeframeDataFetcher(self.config)

            # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸå“è³ªãƒ†ã‚¹ãƒˆ
            timeframes = ["15m", "1h", "4h"]

            for timeframe in timeframes:
                logger.info(f"  Checking {timeframe} data quality...")

                if timeframe == "15m":
                    data = fetcher._interpolate_to_15m(hourly_data.copy())
                elif timeframe == "1h":
                    data = hourly_data.copy()
                elif timeframe == "4h":
                    data = fetcher._aggregate_to_4h(hourly_data.copy())

                if data is not None and not data.empty:
                    quality_metrics = {
                        "total_records": len(data),
                        "nan_percentage": (
                            data.isna().sum().sum() / (len(data) * len(data.columns))
                        )
                        * 100,
                        "duplicate_timestamps": data.index.duplicated().sum(),
                        "price_range_check": {
                            "min_price": float(
                                data[["open", "high", "low", "close"]].min().min()
                            ),
                            "max_price": float(
                                data[["open", "high", "low", "close"]].max().max()
                            ),
                            "price_variance": float(data["close"].var()),
                        },
                        "volume_stats": {
                            "min_volume": float(data["volume"].min()),
                            "max_volume": float(data["volume"].max()),
                            "avg_volume": float(data["volume"].mean()),
                        },
                    }

                    quality_results[timeframe] = quality_metrics
                    logger.info(
                        f"    âœ… {timeframe}: {quality_metrics['total_records']} records, "
                        f"NaN: {quality_metrics['nan_percentage']:.2f}%"
                    )
                else:
                    quality_results[timeframe] = {"error": "No data generated"}
                    logger.error(f"    âŒ {timeframe}: No data generated")

            return quality_results

        except Exception as e:
            logger.error(f"âŒ Data quality test failed: {str(e)}")
            self.test_results["errors"].append(f"Data quality: {str(e)}")
            return {"error": str(e)}

    def test_performance(self, hourly_data: pd.DataFrame) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        logger.info("âš¡ Testing performance...")
        perf_results = {}

        try:
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šï¼ˆç°¡æ˜“ï¼‰
            try:
                import psutil

                process = psutil.Process()
                initial_memory = process.memory_info().rss / 1024 / 1024  # MB
                memory_available = True
            except ImportError:
                logger.warning("âš ï¸ psutil not available, skipping memory monitoring")
                initial_memory = 0
                memory_available = False

            start_time = time.time()

            # MultiTimeframeDataFetcheråˆæœŸåŒ–ã¨å®Ÿè¡Œ
            fetcher = MultiTimeframeDataFetcher(self.config)

            # è¤‡æ•°ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†æ™‚é–“æ¸¬å®š
            conversion_times = {}

            # 15åˆ†è¶³å¤‰æ›
            conv_start = time.time()
            data_15m = fetcher._interpolate_to_15m(hourly_data.copy())
            conversion_times["15m"] = time.time() - conv_start

            # 4æ™‚é–“è¶³å¤‰æ›
            conv_start = time.time()
            data_4h = fetcher._aggregate_to_4h(hourly_data.copy())
            conversion_times["4h"] = time.time() - conv_start

            # åŒæœŸå‡¦ç†æ™‚é–“
            if data_15m is not None and data_4h is not None:
                sync_start = time.time()
                synchronizer = TimeframeSynchronizer(self.config)
                timeframe_data = {"15m": data_15m, "1h": hourly_data, "4h": data_4h}
                synchronized_data = synchronizer.synchronize_multi_timeframe_data(
                    timeframe_data
                )
                sync_time = time.time() - sync_start
            else:
                sync_time = 0
                logger.warning("  âš ï¸ Skipping sync performance test (missing data)")

            total_time = time.time() - start_time
            if memory_available:
                final_memory = process.memory_info().rss / 1024 / 1024  # MB
            else:
                final_memory = 0

            perf_results = {
                "total_execution_time": total_time,
                "conversion_times": conversion_times,
                "synchronization_time": sync_time,
                "memory_usage": {
                    "initial_mb": initial_memory,
                    "final_mb": final_memory,
                    "increase_mb": (
                        final_memory - initial_memory if memory_available else 0
                    ),
                    "monitoring_available": memory_available,
                },
                "throughput": {
                    "records_per_second": (
                        len(hourly_data) / total_time if total_time > 0 else 0
                    )
                },
            }

            logger.info(f"  âš¡ Total time: {total_time:.2f}s")
            if memory_available:
                logger.info(
                    f"  ğŸ’¾ Memory increase: {final_memory - initial_memory:.1f} MB"
                )
            else:
                logger.info("  ğŸ’¾ Memory monitoring: Not available (psutil missing)")
            logger.info(
                f"  ğŸ“Š Throughput: {perf_results['throughput']['records_per_second']:.1f} records/s"
            )

            return perf_results

        except Exception as e:
            logger.error(f"âŒ Performance test failed: {str(e)}")
            self.test_results["errors"].append(f"Performance: {str(e)}")
            return {"error": str(e)}

    def run_comprehensive_test(self) -> Dict:
        """åŒ…æ‹¬çš„ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        start_time = time.time()

        try:
            logger.info("ğŸš€ Starting comprehensive multi-timeframe test...")

            # Step 1: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            test_data = self.generate_hourly_test_data(n_hours=200)

            # Step 2: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›ãƒ†ã‚¹ãƒˆ
            self.test_results["timeframe_tests"] = self.test_timeframe_conversion(
                test_data
            )

            # Step 3: ãƒ‡ãƒ¼ã‚¿åŒæœŸãƒ†ã‚¹ãƒˆ
            self.test_results["synchronization_tests"] = self.test_data_synchronization(
                test_data
            )

            # Step 4: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ†ã‚¹ãƒˆ
            self.test_results["data_quality_tests"] = self.test_data_quality(test_data)

            # Step 5: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            self.test_results["performance_metrics"] = self.test_performance(test_data)

            # ç·åˆåˆ¤å®š
            errors = len(self.test_results["errors"])
            failed_tests = sum(
                1
                for test_result in [
                    self.test_results["timeframe_tests"],
                    self.test_results["synchronization_tests"],
                    self.test_results["data_quality_tests"],
                    self.test_results["performance_metrics"],
                ]
                if "error" in test_result
            )

            if errors == 0 and failed_tests == 0:
                self.test_results["test_status"] = "PASSED"
                logger.info("âœ… All multi-timeframe tests PASSED!")
            else:
                self.test_results["test_status"] = "FAILED"
                logger.error(
                    f"âŒ Multi-timeframe tests FAILED! Errors: {errors}, Failed tests: {failed_tests}"
                )

            # å®Ÿè¡Œæ™‚é–“
            self.test_results["execution_time"] = time.time() - start_time
            logger.info(
                f"\nâ±ï¸ Multi-timeframe test completed in {self.test_results['execution_time']:.2f} seconds"
            )

            # çµæœä¿å­˜
            self.save_results()

            return self.test_results

        except Exception as e:
            logger.error(f"âŒ Comprehensive test failed: {str(e)}")
            self.test_results["test_status"] = "ERROR"
            self.test_results["error"] = str(e)
            return self.test_results

    def save_results(self):
        """ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"test_results/multi_timeframe_test_{timestamp}.json"

        os.makedirs("test_results", exist_ok=True)

        with open(filename, "w") as f:
            json.dump(self.test_results, f, indent=2, default=str)

        logger.info(f"ğŸ’¾ Test results saved to: {filename}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("=" * 80)
    logger.info("Multi-Timeframe Integration Test - Phase 5.1")
    logger.info("=" * 80)

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    tester = MultiTimeframeTester()
    results = tester.run_comprehensive_test()

    # æœ€çµ‚ã‚µãƒãƒªãƒ¼
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“Š FINAL TEST SUMMARY")
    logger.info("=" * 80)
    logger.info(f"Status: {results['test_status']}")
    logger.info(f"Execution Time: {results.get('execution_time', 0):.2f}s")
    logger.info(f"Errors: {len(results.get('errors', []))}")

    # ãƒ†ã‚¹ãƒˆå¤±æ•—æ™‚ã¯éã‚¼ãƒ­ã§çµ‚äº†
    if results["test_status"] not in ["PASSED"]:
        sys.exit(1)


if __name__ == "__main__":
    main()
