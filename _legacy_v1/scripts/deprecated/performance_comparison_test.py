#!/usr/bin/env python3
"""
Phase B2.6.1: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ–°æ—§ç‰¹å¾´é‡ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½æ¯”è¼ƒ:
- æ–°ã‚·ã‚¹ãƒ†ãƒ : BatchFeatureCalculator + TechnicalFeatureEngine + ExternalDataIntegrator
- æ—§ã‚·ã‚¹ãƒ†ãƒ : å€‹åˆ¥df[column] = valueæ“ä½œ (151å›æ–­ç‰‡åŒ–)

æœŸå¾…åŠ¹æœ:
- å‡¦ç†é€Ÿåº¦: 75%å‘ä¸Š (2-4ç§’ â†’ 0.5-1.0ç§’)
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 50%å‰Šæ¸›
"""

import json
import logging
import os
import sys
import time
import tracemalloc
from typing import Any, Dict, Tuple

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

import numpy as np
import pandas as pd

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
import yaml
from memory_profiler import memory_usage

# æ–°ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆPhase B2å®Ÿè£…ï¼‰
from crypto_bot.ml.feature_engines.batch_calculator import BatchFeatureCalculator
from crypto_bot.ml.feature_engines.external_data_engine import ExternalDataIntegrator
from crypto_bot.ml.feature_engines.technical_engine import TechnicalFeatureEngine

# æ—§ã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒç”¨ï¼ˆpreprocessor.pyã®legacyéƒ¨åˆ†ï¼‰
from crypto_bot.ml.preprocessor import FeatureEngineer

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆç›´æ¥pandasä½¿ç”¨ï¼‰


# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class PerformanceTestRunner:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.results = {
            "test_metadata": {
                "timestamp": time.time(),
                "python_version": sys.version,
            },
            "legacy_system": {},
            "batch_system": {},
            "comparison": {},
        }

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        self.config = self._load_config()

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
        self.test_data = self._prepare_test_data()

        logger.info("ğŸ§ª PerformanceTestRunner initialized")

    def _load_config(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        config_path = "/Users/nao/Desktop/bot/config/production/production.yml"
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨è¨­å®šèª¿æ•´
            # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’åˆ¶é™ã—ã¦ãƒ†ã‚¹ãƒˆæ™‚é–“ã‚’çŸ­ç¸®
            test_features = [
                "rsi_14",
                "rsi_7",
                "rsi_21",
                "sma_20",
                "sma_50",
                "ema_12",
                "ema_26",
                "macd",
                "atr_14",
                "stoch",
                "adx",  # é«˜é€Ÿãªãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã®ã¿
            ]

            config["ml"]["extra_features"] = test_features

            logger.info(
                f"ğŸ“‹ Configuration loaded for testing: {len(test_features)} features"
            )

            return config
        except Exception as e:
            logger.error(f"âŒ Failed to load config: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
            return {
                "ml": {
                    "extra_features": ["rsi_14", "sma_20", "ema_12", "macd"],
                    "feat_period": 14,
                    "lags": [1, 2, 3],
                    "rolling_window": 14,
                }
            }

    def _prepare_test_data(self) -> pd.DataFrame:
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        try:
            # CSVãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
            csv_path = "data/btc_usd_2024_hourly.csv"

            if os.path.exists(csv_path):
                logger.info("ğŸ“Š Loading test data from CSV...")
                df = pd.read_csv(csv_path)

                # timestampã‚«ãƒ©ãƒ ã‚’DatetimeIndexã«å¤‰æ›
                if "timestamp" in df.columns:
                    df["timestamp"] = pd.to_datetime(df["timestamp"])
                    df.set_index("timestamp", inplace=True)
                elif df.index.name != "timestamp":
                    # æœ€åˆã®åˆ—ã‚’timestampã¨ã—ã¦ä½¿ç”¨
                    df.index = pd.to_datetime(df.iloc[:, 0])
                    df.index.name = "timestamp"

                # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’æ¸¬å®šç”¨ã«èª¿æ•´ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ã«å°ã•ãï¼‰
                test_size = min(200, len(df))
                test_df = df.tail(test_size).copy()

                logger.info(
                    f"âœ… Test data prepared: {len(test_df)} rows Ã— {len(test_df.columns)} columns"
                )
                return test_df
            else:
                # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
                logger.warning("âš ï¸ CSV not found, generating mock data...")
                return self._generate_mock_data()

        except Exception as e:
            logger.error(f"âŒ Failed to prepare test data: {e}")
            return self._generate_mock_data()

    def _generate_mock_data(self, n_rows: int = 200) -> pd.DataFrame:
        """ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        dates = pd.date_range("2024-01-01", periods=n_rows, freq="H")

        # åŸºæœ¬OHLCVç”Ÿæˆï¼ˆãƒªã‚¢ãƒ«ãªä¾¡æ ¼å‹•ä½œï¼‰
        base_price = 50000.0
        returns = np.random.normal(0, 0.02, n_rows)
        prices = base_price * np.exp(returns.cumsum())

        # OHLCVè¨ˆç®—
        df = pd.DataFrame(
            {
                "timestamp": dates,
                "open": prices * (1 + np.random.normal(0, 0.001, n_rows)),
                "high": prices * (1 + np.abs(np.random.normal(0, 0.005, n_rows))),
                "low": prices * (1 - np.abs(np.random.normal(0, 0.005, n_rows))),
                "close": prices,
                "volume": np.random.lognormal(10, 1, n_rows),
            }
        )

        df.set_index("timestamp", inplace=True)

        logger.info(f"ğŸ“Š Mock data generated: {len(df)} rows")
        return df

    def test_legacy_system_performance(self) -> Dict[str, Any]:
        """æ—§ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå€‹åˆ¥å‡¦ç†ï¼‰ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ•°ï¸ Testing Legacy System Performance...")

        try:
            # ãƒ¡ãƒ¢ãƒªãƒˆãƒ¬ãƒ¼ã‚¹é–‹å§‹
            tracemalloc.start()
            start_time = time.time()

            # FeatureEngineråˆæœŸåŒ–
            fe = FeatureEngineer(self.config)

            # æ—§æ–¹å¼ã§ã®ç‰¹å¾´é‡ç”Ÿæˆï¼ˆãƒãƒƒãƒã‚¨ãƒ³ã‚¸ãƒ³ç„¡åŠ¹åŒ–ï¼‰
            if hasattr(fe, "batch_engines_enabled"):
                fe.batch_engines_enabled = False  # ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ¢ãƒ¼ãƒ‰å¼·åˆ¶

            # ãƒ¡ãƒ¢ãƒªç›£è¦–ä»˜ãç‰¹å¾´é‡ç”Ÿæˆ
            def legacy_transform():
                return fe.transform(self.test_data.copy())

            # memory_profilerã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š
            memory_before = memory_usage()[0]
            memory_during = memory_usage((legacy_transform, ()))
            memory_after = memory_usage()[0]

            # çµæœå–å¾—
            result_df = legacy_transform()

            # æ™‚é–“æ¸¬å®š
            processing_time = time.time() - start_time

            # ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()

            # çµæœè¨˜éŒ²
            legacy_results = {
                "processing_time_seconds": processing_time,
                "input_rows": len(self.test_data),
                "input_columns": len(self.test_data.columns),
                "output_columns": (
                    len(result_df.columns) if result_df is not None else 0
                ),
                "features_generated": (
                    len(result_df.columns) - len(self.test_data.columns)
                    if result_df is not None
                    else 0
                ),
                "memory_before_mb": memory_before,
                "memory_peak_mb": max(memory_during),
                "memory_after_mb": memory_after,
                "tracemalloc_current_mb": current / 1024 / 1024,
                "tracemalloc_peak_mb": peak / 1024 / 1024,
                "features_per_second": (
                    (len(result_df.columns) - len(self.test_data.columns))
                    / processing_time
                    if processing_time > 0 and result_df is not None
                    else 0
                ),
            }

            self.results["legacy_system"] = legacy_results

            logger.info(f"ğŸ“Š Legacy System Results:")
            logger.info(f"  â€¢ Processing Time: {processing_time:.3f}s")
            logger.info(
                f"  â€¢ Features Generated: {legacy_results['features_generated']}"
            )
            logger.info(f"  â€¢ Memory Peak: {legacy_results['memory_peak_mb']:.1f} MB")
            logger.info(
                f"  â€¢ Features/Second: {legacy_results['features_per_second']:.1f}"
            )

            return legacy_results

        except Exception as e:
            error_result = {"error": str(e), "traceback": traceback.format_exc()}
            self.results["legacy_system"] = error_result
            logger.error(f"âŒ Legacy system test failed: {e}")
            return error_result

    def test_batch_system_performance(self) -> Dict[str, Any]:
        """æ–°ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸš€ Testing Batch System Performance...")

        try:
            # ãƒ¡ãƒ¢ãƒªãƒˆãƒ¬ãƒ¼ã‚¹é–‹å§‹
            tracemalloc.start()
            start_time = time.time()

            # ãƒãƒƒãƒã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            batch_calc = BatchFeatureCalculator(self.config)
            technical_engine = TechnicalFeatureEngine(self.config, batch_calc)
            external_integrator = ExternalDataIntegrator(self.config, batch_calc)

            # ãƒ¡ãƒ¢ãƒªç›£è¦–ä»˜ããƒãƒƒãƒå‡¦ç†ï¼ˆFeatureEngineerçµ±åˆãƒ†ã‚¹ãƒˆï¼‰
            def batch_transform():
                # FeatureEngineerçµŒç”±ã§ã®ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆ
                fe_batch = FeatureEngineer(self.config)

                # ãƒãƒƒãƒã‚¨ãƒ³ã‚¸ãƒ³ãŒæœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                if hasattr(fe_batch, "batch_engines_enabled"):
                    if not fe_batch.batch_engines_enabled:
                        logger.warning("ğŸ”§ Force enabling batch engines for batch test")
                        fe_batch.batch_engines_enabled = True

                return fe_batch.transform(self.test_data.copy())

            # memory_profilerã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š
            memory_before = memory_usage()[0]
            memory_during = memory_usage((batch_transform, ()))
            memory_after = memory_usage()[0]

            # çµæœå–å¾—
            result_df = batch_transform()

            # æ™‚é–“æ¸¬å®š
            processing_time = time.time() - start_time

            # ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()

            # ãƒãƒƒãƒã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆå–å¾—
            batch_stats = batch_calc.calculate_batch_efficiency_metrics()

            # çµæœè¨˜éŒ²
            batch_results = {
                "processing_time_seconds": processing_time,
                "input_rows": len(self.test_data),
                "input_columns": len(self.test_data.columns),
                "output_columns": (
                    len(result_df.columns) if result_df is not None else 0
                ),
                "features_generated": (
                    len(result_df.columns) - len(self.test_data.columns)
                    if result_df is not None
                    else 0
                ),
                "memory_before_mb": memory_before,
                "memory_peak_mb": max(memory_during),
                "memory_after_mb": memory_after,
                "tracemalloc_current_mb": current / 1024 / 1024,
                "tracemalloc_peak_mb": peak / 1024 / 1024,
                "features_per_second": (
                    (len(result_df.columns) - len(self.test_data.columns))
                    / processing_time
                    if processing_time > 0 and result_df is not None
                    else 0
                ),
                "batch_system_stats": batch_stats,
            }

            self.results["batch_system"] = batch_results

            logger.info(f"ğŸš€ Batch System Results:")
            logger.info(f"  â€¢ Processing Time: {processing_time:.3f}s")
            logger.info(
                f"  â€¢ Features Generated: {batch_results['features_generated']}"
            )
            logger.info(f"  â€¢ Memory Peak: {batch_results['memory_peak_mb']:.1f} MB")
            logger.info(
                f"  â€¢ Features/Second: {batch_results['features_per_second']:.1f}"
            )

            return batch_results

        except Exception as e:
            error_result = {"error": str(e), "traceback": traceback.format_exc()}
            self.results["batch_system"] = error_result
            logger.error(f"âŒ Batch system test failed: {e}")
            return error_result

    def calculate_performance_comparison(self):
        """æ€§èƒ½æ¯”è¼ƒè¨ˆç®—"""
        if (
            "error" in self.results["legacy_system"]
            or "error" in self.results["batch_system"]
        ):
            logger.warning("âš ï¸ Cannot calculate comparison due to errors in tests")
            return

        legacy = self.results["legacy_system"]
        batch = self.results["batch_system"]

        # é€Ÿåº¦æ”¹å–„è¨ˆç®—
        time_improvement = 0
        if legacy["processing_time_seconds"] > 0:
            time_improvement = (
                (legacy["processing_time_seconds"] - batch["processing_time_seconds"])
                / legacy["processing_time_seconds"]
            ) * 100

        # ãƒ¡ãƒ¢ãƒªæ”¹å–„è¨ˆç®—
        memory_improvement = 0
        if legacy["memory_peak_mb"] > 0:
            memory_improvement = (
                (legacy["memory_peak_mb"] - batch["memory_peak_mb"])
                / legacy["memory_peak_mb"]
            ) * 100

        # ç‰¹å¾´é‡ç”ŸæˆåŠ¹ç‡æ”¹å–„
        efficiency_improvement = 0
        if legacy["features_per_second"] > 0:
            efficiency_improvement = (
                (batch["features_per_second"] - legacy["features_per_second"])
                / legacy["features_per_second"]
            ) * 100

        comparison_results = {
            "speed_improvement_percent": time_improvement,
            "memory_improvement_percent": memory_improvement,
            "efficiency_improvement_percent": efficiency_improvement,
            "target_speed_improvement": 75.0,  # 75%ç›®æ¨™
            "target_memory_improvement": 50.0,  # 50%ç›®æ¨™
            "speed_target_achieved": time_improvement >= 75.0,
            "memory_target_achieved": memory_improvement >= 50.0,
            "overall_success": time_improvement >= 75.0 and memory_improvement >= 50.0,
        }

        self.results["comparison"] = comparison_results

        # çµæœè¡¨ç¤º
        logger.info("ğŸ¯ Performance Comparison Results:")
        logger.info(f"  â€¢ Speed Improvement: {time_improvement:.1f}% (Target: 75%)")
        logger.info(f"  â€¢ Memory Improvement: {memory_improvement:.1f}% (Target: 50%)")
        logger.info(f"  â€¢ Efficiency Improvement: {efficiency_improvement:.1f}%")
        logger.info(
            f"  â€¢ Speed Target Achieved: {'âœ…' if comparison_results['speed_target_achieved'] else 'âŒ'}"
        )
        logger.info(
            f"  â€¢ Memory Target Achieved: {'âœ…' if comparison_results['memory_target_achieved'] else 'âŒ'}"
        )
        logger.info(
            f"  â€¢ Overall Success: {'âœ…' if comparison_results['overall_success'] else 'âŒ'}"
        )

    def save_results(
        self,
        output_path: str = "/Users/nao/Desktop/bot/test_results/performance_comparison.json",
    ):
        """çµæœä¿å­˜"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)

            logger.info(f"ğŸ’¾ Results saved to: {output_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to save results: {e}")

    def run_complete_test(self):
        """å®Œå…¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("ğŸ§ª Starting Complete Performance Test...")

        # æ—§ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
        self.test_legacy_system_performance()

        # æ–°ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
        self.test_batch_system_performance()

        # æ¯”è¼ƒè¨ˆç®—
        self.calculate_performance_comparison()

        # çµæœä¿å­˜
        self.save_results()

        logger.info("ğŸ‰ Performance Test Completed!")

        return self.results


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        runner = PerformanceTestRunner()
        results = runner.run_complete_test()

        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\n" + "=" * 80)
        print("ğŸ¯ PHASE B2.6.1 PERFORMANCE VERIFICATION RESULTS")
        print("=" * 80)

        if "comparison" in results and "overall_success" in results["comparison"]:
            if results["comparison"]["overall_success"]:
                print("ğŸ‰ SUCCESS: Performance targets achieved!")
                print(
                    f"   â€¢ Speed improvement: {results['comparison']['speed_improvement_percent']:.1f}%"
                )
                print(
                    f"   â€¢ Memory improvement: {results['comparison']['memory_improvement_percent']:.1f}%"
                )
            else:
                print("âš ï¸  PARTIAL SUCCESS: Some targets not achieved")
                print(
                    f"   â€¢ Speed improvement: {results['comparison']['speed_improvement_percent']:.1f}% (Target: 75%)"
                )
                print(
                    f"   â€¢ Memory improvement: {results['comparison']['memory_improvement_percent']:.1f}% (Target: 50%)"
                )
        else:
            print("âŒ TEST FAILED: Could not complete comparison")

        print("=" * 80)
        return results

    except Exception as e:
        logger.error(f"âŒ Test execution failed: {e}")
        return None


if __name__ == "__main__":
    main()
