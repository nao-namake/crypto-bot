#!/usr/bin/env python3
"""
97ç‰¹å¾´é‡æœ€é©åŒ–ãƒ»ç‰¹å¾´é‡é¸æŠã«ã‚ˆã‚‹å‹ç‡ãƒ»æç›Šæ”¹å–„ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Phase 2: 127â†’97ç‰¹å¾´é‡æœ€é©åŒ–å¾Œã®ã•ã‚‰ãªã‚‹æ”¹å–„æ¤œè¨¼

ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
1. æ®µéšçš„ç‰¹å¾´é‡å‰Šæ¸›ï¼ˆ97â†’80â†’60â†’40â†’30â†’20ï¼‰
2. å„æ®µéšã§ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ»å‹ç‡ç¢ºèª
3. æœ€é©ãªç‰¹å¾´é‡æ•°ã‚’ç‰¹å®š
4. é‡è¦åº¦ãƒ»ç›¸é–¢åˆ†æã«ã‚ˆã‚‹æœ€çµ‚é¸æŠ
"""

import json
import logging
import os
import sys
from pathlib import Path

import joblib
import lightgbm as lgb
import numpy as np
import pandas as pd
from sklearn.feature_selection import RFE, SelectKBest, f_classif
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.model_selection import train_test_split

sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

from crypto_bot.ml.feature_order_manager import FeatureOrderManager
from crypto_bot.ml.preprocessor import FeatureEngineer

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def create_training_data():
    """97ç‰¹å¾´é‡è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    np.random.seed(42)
    n_rows = 10000

    logger.info(f"97ç‰¹å¾´é‡æœ€é©åŒ–ç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: {n_rows} samples")

    # ã‚ˆã‚Šç¾å®Ÿçš„ãªå¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    t = np.arange(n_rows)

    # è¤‡æ•°ã®å¸‚å ´ã‚µã‚¤ã‚¯ãƒ«
    trend = 0.0001 * t  # é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰
    cycle1 = 0.1 * np.sin(2 * np.pi * t / 1000)  # é•·æœŸã‚µã‚¤ã‚¯ãƒ«
    cycle2 = 0.05 * np.sin(2 * np.pi * t / 100)  # ä¸­æœŸã‚µã‚¤ã‚¯ãƒ«
    noise = np.random.normal(0, 0.02, n_rows)  # ãƒã‚¤ã‚º

    base_price = 100 * np.exp(trend + cycle1 + cycle2 + noise)

    # OHLCVç”Ÿæˆ
    high = base_price * (1 + np.abs(np.random.normal(0, 0.01, n_rows)))
    low = base_price * (1 - np.abs(np.random.normal(0, 0.01, n_rows)))
    volume = np.random.lognormal(mean=10, sigma=1, size=n_rows)

    df = pd.DataFrame(
        {
            "timestamp": pd.date_range("2023-01-01", periods=n_rows, freq="H"),
            "open": base_price,
            "high": high,
            "low": low,
            "close": base_price,
            "volume": volume,
        }
    )

    df.set_index("timestamp", inplace=True)
    return df


def generate_97_features(df, config):
    """97ç‰¹å¾´é‡ç”Ÿæˆ"""
    feature_engineer = FeatureEngineer(config)
    features_df = feature_engineer.transform(df)

    # 97ç‰¹å¾´é‡ã‚·ã‚¹ãƒ†ãƒ ã«æ•´åˆ
    fom = FeatureOrderManager()
    features_df = fom.ensure_97_features_completeness(features_df)

    logger.info(f"97ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {len(features_df.columns)} ç‰¹å¾´é‡")
    return features_df


def create_target(df):
    """æ”¹å–„ã•ã‚ŒãŸã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆ"""
    # ä¾¡æ ¼å¤‰å‹•ç‡è¨ˆç®—
    returns = df["close"].pct_change(periods=5).shift(-5)  # 5æœŸé–“å…ˆã®å¤‰å‹•

    # ã‚ˆã‚Šå³æ ¼ãªé–¾å€¤ï¼ˆãƒã‚¤ã‚ºé™¤å»ï¼‰
    buy_threshold = 0.003  # 0.3%ä»¥ä¸Š
    sell_threshold = -0.003  # -0.3%ä»¥ä¸‹

    target = pd.Series(1, index=returns.index)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆHOLD
    target[returns > buy_threshold] = 2  # BUY
    target[returns < sell_threshold] = 0  # SELL

    logger.info(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: {target.value_counts().to_dict()}")
    return target


def test_feature_count(features_df, target, n_features, test_name=""):
    """æŒ‡å®šç‰¹å¾´é‡æ•°ã§ã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
    logger.info(f"ğŸ§ª {test_name}: {n_features}ç‰¹å¾´é‡ã§ãƒ†ã‚¹ãƒˆé–‹å§‹")

    # ç‰¹å¾´é‡é¸æŠ
    if n_features >= len(features_df.columns):
        selected_features = features_df
    else:
        # Fçµ±è¨ˆé‡ã«ã‚ˆã‚‹é‡è¦ç‰¹å¾´é‡é¸æŠ
        selector = SelectKBest(score_func=f_classif, k=n_features)
        selected_features = pd.DataFrame(
            selector.fit_transform(features_df, target),
            columns=features_df.columns[selector.get_support()],
            index=features_df.index,
        )

    # æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿
    valid_mask = ~(selected_features.isna().any(axis=1) | target.isna())
    X = selected_features[valid_mask]
    y = target[valid_mask]

    if len(X) < 100:
        logger.warning(f"ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {len(X)} samples")
        return None

    # å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )

    # LightGBMãƒ¢ãƒ‡ãƒ«
    model = lgb.LGBMClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=6,
        num_leaves=31,
        random_state=42,
        verbose=-1,
    )

    # å­¦ç¿’
    model.fit(X_train, y_train)

    # äºˆæ¸¬ãƒ»è©•ä¾¡
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_test, y_pred, average="weighted"
    )

    # äºˆæ¸¬åˆ†å¸ƒç¢ºèª
    pred_distribution = pd.Series(y_pred).value_counts().to_dict()

    result = {
        "n_features": n_features,
        "feature_names": list(selected_features.columns),
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1_score": f1,
        "prediction_distribution": pred_distribution,
        "data_points": len(X_test),
    }

    logger.info(f"âœ… {n_features}ç‰¹å¾´é‡çµæœ: Accuracy={accuracy:.4f}, F1={f1:.4f}")
    return result


def run_97_feature_optimization():
    """97ç‰¹å¾´é‡æœ€é©åŒ–å®Ÿè¡Œ"""
    logger.info("ğŸš€ 97ç‰¹å¾´é‡æœ€é©åŒ–é–‹å§‹")

    # è¨­å®šèª­ã¿è¾¼ã¿
    config_path = Path("config/production/production.yml")
    if not config_path.exists():
        logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
        return False

    import yaml

    with open(config_path, "r") as f:
        config = yaml.safe_load(f)

    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    df = create_training_data()
    features_df = generate_97_features(df, config)
    target = create_target(df)

    # æ®µéšçš„ç‰¹å¾´é‡æ•°ãƒ†ã‚¹ãƒˆ
    feature_counts = [97, 80, 60, 40, 30, 20, 15, 10]
    results = []

    for n_features in feature_counts:
        try:
            result = test_feature_count(
                features_df, target, n_features, test_name=f"Stage-{len(results)+1}"
            )
            if result:
                results.append(result)
        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡æ•°{n_features}ã§ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    # çµæœä¿å­˜
    output_dir = Path("results/97_feature_optimization")
    output_dir.mkdir(parents=True, exist_ok=True)

    results_summary = {
        "optimization_date": pd.Timestamp.now().isoformat(),
        "base_features": 97,
        "tested_counts": feature_counts,
        "results": results,
        "best_performance": (
            max(results, key=lambda x: x["f1_score"]) if results else None
        ),
    }

    output_file = output_dir / "optimization_results.json"
    with open(output_file, "w") as f:
        json.dump(results_summary, f, indent=2)

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    logger.info("ğŸ“Š 97ç‰¹å¾´é‡æœ€é©åŒ–çµæœã‚µãƒãƒªãƒ¼:")
    for result in results:
        logger.info(
            f"  {result['n_features']:2d}ç‰¹å¾´é‡: "
            f"Accuracy={result['accuracy']:.4f}, "
            f"F1={result['f1_score']:.4f}"
        )

    if results:
        best = results_summary["best_performance"]
        logger.info(
            f"ğŸ¯ æœ€é©ç‰¹å¾´é‡æ•°: {best['n_features']} " f"(F1={best['f1_score']:.4f})"
        )

    logger.info(f"çµæœä¿å­˜: {output_file}")
    return True


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        success = run_97_feature_optimization()
        if success:
            logger.info("ğŸŠ 97ç‰¹å¾´é‡æœ€é©åŒ–å®Œäº†")
            sys.exit(0)
        else:
            logger.error("âŒ æœ€é©åŒ–å¤±æ•—")
            sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
