"""
å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡ç²¾åº¦æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ - Phase 52.4

MarketRegimeClassifierã®åˆ†é¡ç²¾åº¦ã‚’å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã§æ¤œè¨¼ã€‚
ãƒ¬ãƒ³ã‚¸/ãƒˆãƒ¬ãƒ³ãƒ‰/é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®æ¤œå‡ºç²¾åº¦ã‚’ç¢ºèªã™ã‚‹ã€‚

è¨­å®šç®¡ç†: thresholds.yamlã«æ¤œè¨¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å®šç¾©
æœŸå¾…çµæœ: thresholds.yaml:analysis.regime_verification.target_rangesã«å®šç¾©
"""

import asyncio
import random
import sys
from pathlib import Path
from typing import Optional

import pandas as pd

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.config.threshold_manager import get_threshold
from src.core.logger import get_logger
from src.core.services.market_regime_classifier import MarketRegimeClassifier
from src.core.services.regime_types import RegimeType
from src.features.feature_generator import FeatureGenerator

logger = get_logger()


def load_historical_data(csv_path: str) -> pd.DataFrame:
    """
    å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿

    Args:
        csv_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        pd.DataFrame: å±¥æ­´ãƒ‡ãƒ¼ã‚¿
    """
    logger.info(f"ğŸ“‚ å±¥æ­´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {csv_path}")
    df = pd.read_csv(csv_path)

    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ—ã®å‡¦ç†
    if "datetime" in df.columns:
        # CSVã«æ—¢ã«datetimeã‚«ãƒ©ãƒ ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
        df["timestamp"] = pd.to_datetime(df["datetime"], errors="coerce")
    elif "timestamp_ms" in df.columns:
        # timestamp_msãŒã‚ã‚‹å ´åˆã¯ãƒŸãƒªç§’ã¨ã—ã¦å¤‰æ›
        df["timestamp"] = pd.to_datetime(df["timestamp_ms"], unit="ms", errors="coerce")
    elif "timestamp" in df.columns:
        # timestampã‚«ãƒ©ãƒ ãŒã‚ã‚‹å ´åˆã¯ãã®ã¾ã¾å¤‰æ›ã‚’è©¦ã¿ã‚‹
        # ã¾ãšæ•°å€¤ï¼ˆãƒŸãƒªç§’ï¼‰ã¨ã—ã¦è©¦ã™
        try:
            df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", errors="coerce")
        except (ValueError, TypeError):
            # æ–‡å­—åˆ—ã®å¯èƒ½æ€§ã‚‚ã‚ã‚‹ã®ã§å†è©¦è¡Œ
            df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")

    # ã‚½ãƒ¼ãƒˆã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®šï¼ˆFeatureGeneratorç”¨ï¼‰
    if "timestamp" in df.columns:
        df = df.sort_values("timestamp").reset_index(drop=True)
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’DatetimeIndexã«è¨­å®šï¼ˆFeatureGeneratorãŒæœŸå¾…ã™ã‚‹å½¢å¼ï¼‰
        df = df.set_index("timestamp")

    logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ")
    if isinstance(df.index, pd.DatetimeIndex):
        logger.info(f"   ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç¯„å›²: {df.index.min()} ~ {df.index.max()}")
    return df


async def generate_features_for_all_rows(df: pd.DataFrame) -> pd.DataFrame:
    """
    å…¨è¡Œã«å¯¾ã—ã¦ç‰¹å¾´é‡ã‚’ç”Ÿæˆï¼ˆéåŒæœŸï¼‰

    Args:
        df: å±¥æ­´ãƒ‡ãƒ¼ã‚¿

    Returns:
        pd.DataFrame: ç‰¹å¾´é‡ä»˜ããƒ‡ãƒ¼ã‚¿
    """
    logger.info("ğŸ“Š ç‰¹å¾´é‡ç”Ÿæˆé–‹å§‹")
    generator = FeatureGenerator()

    all_features = []
    for i in range(len(df)):
        try:
            # iè¡Œç›®ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã§ç‰¹å¾´é‡ç”Ÿæˆ
            partial_df = df.iloc[: i + 1].copy()
            features = await generator.generate_features(partial_df)

            # æœ€æ–°è¡Œã®ç‰¹å¾´é‡ã‚’ä¿å­˜
            all_features.append(features.iloc[-1])
        except Exception as e:
            logger.warning(f"âš ï¸ è¡Œ{i}ã®ç‰¹å¾´é‡ç”Ÿæˆå¤±æ•—: {e}")
            # NaNè¡Œã‚’è¿½åŠ ï¼ˆå¾Œã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            all_features.append(pd.Series(dtype=float))

    # DataFrameã«å¤‰æ›
    features_df = pd.DataFrame(all_features).reset_index(drop=True)
    logger.info(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {len(features_df)}è¡Œ")
    return features_df


def classify_all_regimes(features_df: pd.DataFrame) -> list:
    """
    å…¨è¡Œã«å¯¾ã—ã¦å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡ã‚’å®Ÿè¡Œ

    Args:
        features_df: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿

    Returns:
        list[RegimeType]: åˆ†é¡çµæœãƒªã‚¹ãƒˆ
    """
    logger.info("ğŸ¯ å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡é–‹å§‹")
    classifier = MarketRegimeClassifier()

    regimes = []
    for i in range(len(features_df)):
        try:
            # iè¡Œç›®ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã§åˆ†é¡
            partial_df = features_df.iloc[: i + 1].copy()

            # å¿…é ˆã‚«ãƒ©ãƒ ç¢ºèª
            required_columns = ["close", "high", "low", "atr_14", "adx_14"]
            if all(col in partial_df.columns for col in required_columns):
                regime = classifier.classify(partial_df)
                regimes.append(regime)
            else:
                # å¿…é ˆã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯Noneã‚’è¿½åŠ 
                regimes.append(None)
        except Exception as e:
            logger.warning(f"âš ï¸ è¡Œ{i}ã®åˆ†é¡å¤±æ•—: {e}")
            regimes.append(None)

    logger.info(f"âœ… åˆ†é¡å®Œäº†: {len(regimes)}è¡Œ")
    return regimes


def calculate_regime_statistics(regimes: list) -> dict:
    """
    ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡çµ±è¨ˆã‚’è¨ˆç®—

    Args:
        regimes: åˆ†é¡çµæœãƒªã‚¹ãƒˆ

    Returns:
        dict: çµ±è¨ˆæƒ…å ±
    """
    # Noneã‚’é™¤å¤–
    valid_regimes = [r for r in regimes if r is not None]
    total = len(valid_regimes)

    if total == 0:
        return {}

    stats = {
        "total": total,
        "tight_range": sum(1 for r in valid_regimes if r == RegimeType.TIGHT_RANGE),
        "normal_range": sum(1 for r in valid_regimes if r == RegimeType.NORMAL_RANGE),
        "trending": sum(1 for r in valid_regimes if r == RegimeType.TRENDING),
        "high_volatility": sum(1 for r in valid_regimes if r == RegimeType.HIGH_VOLATILITY),
    }

    # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—
    stats["tight_range_pct"] = (stats["tight_range"] / total) * 100
    stats["normal_range_pct"] = (stats["normal_range"] / total) * 100
    stats["trending_pct"] = (stats["trending"] / total) * 100
    stats["high_volatility_pct"] = (stats["high_volatility"] / total) * 100

    # ãƒ¬ãƒ³ã‚¸ç›¸å ´åˆè¨ˆ
    stats["range_total"] = stats["tight_range"] + stats["normal_range"]
    stats["range_total_pct"] = (stats["range_total"] / total) * 100

    return stats


def print_regime_statistics(stats: dict):
    """
    ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡çµ±è¨ˆã‚’è¡¨ç¤º

    Args:
        stats: çµ±è¨ˆæƒ…å ±
    """
    logger.info("=" * 80)
    logger.info("ğŸ“Š å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡çµ±è¨ˆï¼ˆPhase 51.2-Newï¼‰")
    logger.info("=" * 80)

    # ç©ºã®è¾æ›¸ã®å ´åˆï¼ˆå…¨è¡Œå¤±æ•—ï¼‰
    if not stats or "total" not in stats:
        logger.error("âš ï¸ çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…¨ã¦ã®è¡Œã§åˆ†é¡ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
        logger.error("   åŸå› : ç‰¹å¾´é‡ç”Ÿæˆã®å¤±æ•—ã€ã¾ãŸã¯å¿…é ˆã‚«ãƒ©ãƒ ã®ä¸è¶³")
        logger.info("=" * 80)
        return

    logger.info(f"\nğŸ“ˆ ç·ãƒ‡ãƒ¼ã‚¿æ•°: {stats['total']}è¡Œ")

    logger.info("\nã€ãƒ¬ãƒ³ã‚¸ç›¸å ´ã€‘")
    logger.info(f"  ğŸ“Š ç‹­ã„ãƒ¬ãƒ³ã‚¸: {stats['tight_range']}è¡Œ ({stats['tight_range_pct']:.2f}%)")
    logger.info(f"  ğŸ“Š é€šå¸¸ãƒ¬ãƒ³ã‚¸: {stats['normal_range']}è¡Œ ({stats['normal_range_pct']:.2f}%)")
    logger.info(f"  ğŸ“Š ãƒ¬ãƒ³ã‚¸åˆè¨ˆ: {stats['range_total']}è¡Œ ({stats['range_total_pct']:.2f}%)")

    logger.info("\nã€ãƒˆãƒ¬ãƒ³ãƒ‰ç›¸å ´ã€‘")
    logger.info(f"  ğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰: {stats['trending']}è¡Œ ({stats['trending_pct']:.2f}%)")

    logger.info("\nã€é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã€‘")
    logger.info(f"  âš ï¸ é«˜ãƒœãƒ©: {stats['high_volatility']}è¡Œ ({stats['high_volatility_pct']:.2f}%)")

    logger.info("\n" + "=" * 80)
    logger.info("ğŸ¯ ç›®æ¨™é”æˆç¢ºèª")
    logger.info("=" * 80)

    # thresholds.yamlã‹ã‚‰ç›®æ¨™å€¤ã‚’å–å¾—
    range_min = get_threshold("analysis.regime_verification.target_ranges.range_market.min", 70)
    range_max = get_threshold("analysis.regime_verification.target_ranges.range_market.max", 80)
    trending_min = get_threshold(
        "analysis.regime_verification.target_ranges.trending_market.min", 15
    )
    trending_max = get_threshold(
        "analysis.regime_verification.target_ranges.trending_market.max", 20
    )
    volatility_min = get_threshold(
        "analysis.regime_verification.target_ranges.high_volatility.min", 5
    )
    volatility_max = get_threshold(
        "analysis.regime_verification.target_ranges.high_volatility.max", 10
    )

    # ç›®æ¨™å€¤ã¨ã®æ¯”è¼ƒ
    range_target = range_min <= stats["range_total_pct"] <= range_max
    trending_target = trending_min <= stats["trending_pct"] <= trending_max
    volatility_target = volatility_min <= stats["high_volatility_pct"] <= volatility_max

    logger.info(
        f"  ãƒ¬ãƒ³ã‚¸ç›¸å ´ {range_min}-{range_max}%: {'âœ…' if range_target else 'âš ï¸'} ({stats['range_total_pct']:.2f}%)"
    )
    logger.info(
        f"  ãƒˆãƒ¬ãƒ³ãƒ‰ç›¸å ´ {trending_min}-{trending_max}%: {'âœ…' if trending_target else 'âš ï¸'} ({stats['trending_pct']:.2f}%)"
    )
    logger.info(
        f"  é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ {volatility_min}-{volatility_max}%: {'âœ…' if volatility_target else 'âš ï¸'} ({stats['high_volatility_pct']:.2f}%)"
    )

    # ç·åˆåˆ¤å®š
    all_targets = range_target and trending_target and volatility_target
    logger.info(f"\n  ç·åˆåˆ¤å®š: {'âœ… ç›®æ¨™é”æˆ' if all_targets else 'âš ï¸ è¦èª¿æ•´'}")
    logger.info("=" * 80)


def print_random_samples(
    df: pd.DataFrame, features_df: pd.DataFrame, regimes: list, sample_size: Optional[int] = None
):
    """
    ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤ºï¼ˆæ‰‹å‹•æ¤œè¨¼ç”¨ï¼‰

    Args:
        df: å±¥æ­´ãƒ‡ãƒ¼ã‚¿
        features_df: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        regimes: åˆ†é¡çµæœãƒªã‚¹ãƒˆ
        sample_size: ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆNoneã®å ´åˆã¯thresholds.yamlã‹ã‚‰å–å¾—ï¼‰
    """
    if sample_size is None:
        sample_size = get_threshold("analysis.regime_verification.sample_size", 50)

    logger.info("\n" + "=" * 80)
    logger.info(f"ğŸ” ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼ˆ{sample_size}ä»¶ï¼‰")
    logger.info("=" * 80)

    # Noneã§ãªã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    valid_indices = [i for i, r in enumerate(regimes) if r is not None]

    # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    sample_indices = random.sample(valid_indices, min(sample_size, len(valid_indices)))
    sample_indices.sort()

    for idx in sample_indices:
        regime = regimes[idx]
        row = features_df.iloc[idx]

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å–å¾—ï¼ˆindexã‹ã‚‰ï¼‰
        if isinstance(df.index, pd.DatetimeIndex):
            timestamp = df.index[idx]
        else:
            timestamp = "N/A"

        # ä¸»è¦æŒ‡æ¨™å–å¾—
        close = row.get("close", 0)
        atr_14 = row.get("atr_14", 0)
        adx_14 = row.get("adx_14", 0)
        ema_20 = row.get("ema_20", 0)

        # ATRæ¯”è¨ˆç®—
        atr_ratio = (atr_14 / close) if close > 0 else 0

        logger.info(f"\nè¡Œ{idx} | {timestamp} | {regime.value}")
        logger.info(f"  ä¾¡æ ¼: Â¥{close:,.0f} | ATR: {atr_14:.2f} (æ¯”: {atr_ratio:.4f})")
        logger.info(f"  ADX: {adx_14:.2f} | EMA20: Â¥{ema_20:,.0f}")

    logger.info("\n" + "=" * 80)


async def main(limit_rows: int = None):
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆéåŒæœŸï¼‰

    Args:
        limit_rows: ãƒ†ã‚¹ãƒˆç”¨è¡Œæ•°åˆ¶é™ï¼ˆNoneã®å ´åˆã¯å…¨è¡Œå‡¦ç†ï¼‰
    """
    logger.info("ğŸš€ Phase 52.4: å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡ç²¾åº¦æ¤œè¨¼é–‹å§‹")

    # 1. å±¥æ­´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆthresholds.yamlã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹å–å¾—ï¼‰
    csv_path = get_threshold(
        "analysis.regime_verification.default_data_path",
        "src/backtest/data/historical/BTC_JPY_4h.csv",
    )
    df = load_historical_data(csv_path)

    # ãƒ†ã‚¹ãƒˆç”¨: è¡Œæ•°åˆ¶é™
    if limit_rows is not None:
        logger.info(f"âš ï¸ ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: æœ€åˆã®{limit_rows}è¡Œã®ã¿å‡¦ç†")
        df = df.iloc[:limit_rows].copy()

    # 2. ç‰¹å¾´é‡ç”Ÿæˆ
    features_df = await generate_features_for_all_rows(df)

    # 3. å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡
    regimes = classify_all_regimes(features_df)

    # 4. çµ±è¨ˆè¨ˆç®—
    stats = calculate_regime_statistics(regimes)

    # 5. çµ±è¨ˆè¡¨ç¤º
    print_regime_statistics(stats)

    # 6. ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    sample_size = min(50, len([r for r in regimes if r is not None]))
    print_random_samples(df, features_df, regimes, sample_size=sample_size)

    logger.info("\nâœ… Phase 51.2-New: å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡ç²¾åº¦æ¤œè¨¼å®Œäº†")


if __name__ == "__main__":
    # å…¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆPhase 51.2-New æœ€çµ‚æ¤œè¨¼ï¼‰
    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯: asyncio.run(main(limit_rows=100))
    asyncio.run(main())
