#!/usr/bin/env python3
"""
97ç‰¹å¾´é‡ vs 127ç‰¹å¾´é‡ åŠ¹ç‡åŒ–åŠ¹æœæ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Phase 2æœ€é©åŒ–ã®åŠ¹æœã‚’åŒ…æ‹¬çš„ã«æ¤œè¨¼
"""

import json
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import psutil

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class FeatureComparisonAnalyzer:
    """97ç‰¹å¾´é‡vs127ç‰¹å¾´é‡ã®æ¯”è¼ƒåˆ†æ"""

    def __init__(self):
        self.results_97 = {}
        self.results_127 = {}
        self.performance_metrics = {}

    def run_backtest(self, config_path, label):
        """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã¨æ€§èƒ½æ¸¬å®š"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ {label}ç‰¹å¾´é‡ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print(f"{'='*60}")

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
        process = psutil.Process()
        mem_before = process.memory_info().rss / 1024 / 1024  # MB

        # æ™‚é–“æ¸¬å®šé–‹å§‹
        start_time = time.time()

        # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        cmd = [
            sys.executable,
            "-m",
            "crypto_bot.main",
            "backtest",
            "--config",
            config_path,
        ]

        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)

            # å®Ÿè¡Œæ™‚é–“
            execution_time = time.time() - start_time

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            mem_after = process.memory_info().rss / 1024 / 1024  # MB
            mem_used = mem_after - mem_before

            # çµæœè§£æ
            output = result.stdout

            # å–å¼•çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            results_dir = Path("results")
            latest_result = None
            latest_time = 0

            for result_file in results_dir.glob("**/backtest_results_*.json"):
                if result_file.stat().st_mtime > latest_time:
                    latest_time = result_file.stat().st_mtime
                    latest_result = result_file

            backtest_metrics = {}
            if latest_result:
                with open(latest_result, "r") as f:
                    backtest_data = json.load(f)
                    backtest_metrics = {
                        "total_return": backtest_data.get("total_return", 0),
                        "win_rate": backtest_data.get("win_rate", 0),
                        "sharpe_ratio": backtest_data.get("sharpe_ratio", 0),
                        "max_drawdown": backtest_data.get("max_drawdown", 0),
                        "total_trades": backtest_data.get("total_trades", 0),
                    }

            return {
                "execution_time": execution_time,
                "memory_used": mem_used,
                "backtest_metrics": backtest_metrics,
                "success": True,
                "output": output,
            }

        except subprocess.CalledProcessError as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "execution_time": time.time() - start_time,
                "memory_used": 0,
                "backtest_metrics": {},
                "success": False,
                "error": str(e),
            }

    def analyze_efficiency(self):
        """åŠ¹ç‡åŒ–åŠ¹æœã®åˆ†æ"""
        print("\n" + "=" * 60)
        print("ğŸ“Š åŠ¹ç‡åŒ–åŠ¹æœåˆ†æ")
        print("=" * 60)

        # å®Ÿè¡Œæ™‚é–“æ¯”è¼ƒ
        time_97 = self.results_97.get("execution_time", 0)
        time_127 = self.results_127.get("execution_time", 0)
        time_improvement = (
            ((time_127 - time_97) / time_127 * 100) if time_127 > 0 else 0
        )

        print(f"\nâ±ï¸ å®Ÿè¡Œæ™‚é–“:")
        print(f"  127ç‰¹å¾´é‡: {time_127:.2f}ç§’")
        print(f"  97ç‰¹å¾´é‡: {time_97:.2f}ç§’")
        print(f"  æ”¹å–„ç‡: {time_improvement:.1f}%")

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¯”è¼ƒ
        mem_97 = self.results_97.get("memory_used", 0)
        mem_127 = self.results_127.get("memory_used", 0)
        mem_improvement = ((mem_127 - mem_97) / mem_127 * 100) if mem_127 > 0 else 0

        print(f"\nğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:")
        print(f"  127ç‰¹å¾´é‡: {mem_127:.2f}MB")
        print(f"  97ç‰¹å¾´é‡: {mem_97:.2f}MB")
        print(f"  æ”¹å–„ç‡: {mem_improvement:.1f}%")

        # ç‰¹å¾´é‡å‰Šæ¸›åŠ¹æœ
        feature_reduction = (127 - 97) / 127 * 100
        print(f"\nğŸ“‰ ç‰¹å¾´é‡å‰Šæ¸›:")
        print(f"  å‰Šæ¸›æ•°: 30ç‰¹å¾´é‡")
        print(f"  å‰Šæ¸›ç‡: {feature_reduction:.1f}%")

        return {
            "time_improvement": time_improvement,
            "memory_improvement": mem_improvement,
            "feature_reduction": feature_reduction,
        }

    def analyze_performance(self):
        """å–å¼•æ€§èƒ½æ¯”è¼ƒ"""
        print("\n" + "=" * 60)
        print("ğŸ“ˆ å–å¼•æ€§èƒ½æ¯”è¼ƒ")
        print("=" * 60)

        metrics_97 = self.results_97.get("backtest_metrics", {})
        metrics_127 = self.results_127.get("backtest_metrics", {})

        comparison = pd.DataFrame({"127ç‰¹å¾´é‡": metrics_127, "97ç‰¹å¾´é‡": metrics_97})

        print("\n" + comparison.to_string())

        # æ€§èƒ½ç¶­æŒç‡è¨ˆç®—
        if metrics_127.get("total_return", 0) != 0:
            return_retention = (
                metrics_97.get("total_return", 0)
                / metrics_127.get("total_return", 0)
                * 100
            )
        else:
            return_retention = 0

        if metrics_127.get("win_rate", 0) != 0:
            winrate_retention = (
                metrics_97.get("win_rate", 0) / metrics_127.get("win_rate", 0) * 100
            )
        else:
            winrate_retention = 0

        print(f"\nğŸ¯ æ€§èƒ½ç¶­æŒç‡:")
        print(f"  åç›Šç‡ç¶­æŒ: {return_retention:.1f}%")
        print(f"  å‹ç‡ç¶­æŒ: {winrate_retention:.1f}%")

        return {
            "return_retention": return_retention,
            "winrate_retention": winrate_retention,
            "comparison_df": comparison,
        }

    def generate_report(self):
        """ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ ç·åˆè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 60)

        efficiency = self.analyze_efficiency()
        performance = self.analyze_performance()

        # ç·åˆè©•ä¾¡
        print("\nğŸ† Phase 2æœ€é©åŒ–åŠ¹æœ:")
        print(f"  âœ… å®Ÿè¡Œæ™‚é–“: {efficiency['time_improvement']:.1f}%æ”¹å–„")
        print(f"  âœ… ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {efficiency['memory_improvement']:.1f}%å‰Šæ¸›")
        print(f"  âœ… ç‰¹å¾´é‡æ•°: {efficiency['feature_reduction']:.1f}%å‰Šæ¸›")
        print(f"  âœ… åç›Šæ€§èƒ½: {performance['return_retention']:.1f}%ç¶­æŒ")
        print(f"  âœ… å‹ç‡æ€§èƒ½: {performance['winrate_retention']:.1f}%ç¶­æŒ")

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report = {
            "timestamp": datetime.now().isoformat(),
            "efficiency_metrics": efficiency,
            "performance_metrics": performance,
            "results_97": self.results_97,
            "results_127": self.results_127,
            "summary": {
                "optimization_success": True,
                "efficiency_gain": f"{np.mean([efficiency['time_improvement'], efficiency['memory_improvement']]):.1f}%",
                "performance_retention": f"{np.mean([performance['return_retention'], performance['winrate_retention']]):.1f}%",
            },
        }

        # çµæœä¿å­˜
        output_dir = Path("results/feature_comparison")
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"97vs127_comparison_{timestamp}.json"

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)

        print(f"\nğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_file}")

        return report

    def run_comparison(self):
        """æ¯”è¼ƒåˆ†æå®Ÿè¡Œ"""
        print("ğŸ” 97ç‰¹å¾´é‡ vs 127ç‰¹å¾´é‡ åŠ¹ç‡åŒ–åŠ¹æœæ¸¬å®šé–‹å§‹")
        print(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # 127ç‰¹å¾´é‡ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
        config_127 = "config/validation/unified_127_features_backtest.yml"
        if Path(config_127).exists():
            self.results_127 = self.run_backtest(config_127, "127")
        else:
            print(f"âŒ 127ç‰¹å¾´é‡è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_127}")
            return

        # 97ç‰¹å¾´é‡ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
        config_97 = "config/validation/unified_97_features_backtest.yml"
        if Path(config_97).exists():
            self.results_97 = self.run_backtest(config_97, "97")
        else:
            print(f"âŒ 97ç‰¹å¾´é‡è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_97}")
            return

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        if self.results_97.get("success") and self.results_127.get("success"):
            self.generate_report()
        else:
            print("\nâŒ ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")

        print(f"\nå®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    analyzer = FeatureComparisonAnalyzer()
    analyzer.run_comparison()


if __name__ == "__main__":
    main()
