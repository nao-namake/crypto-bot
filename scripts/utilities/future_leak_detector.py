#!/usr/bin/env python
"""
æœªæ¥ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 

Phase 2-3: ChatGPTææ¡ˆæ¡ç”¨
ç‰¹å¾´é‡è¨ˆç®—ã‚„ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã§æœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§ã—ã¦ã„ãªã„ã‹æ¤œè¨¼
ãƒ‡ãƒ—ãƒ­ã‚¤å‰ã«æ½œåœ¨çš„ãƒã‚°ã‚’ç™ºè¦‹ã—ã€å®Ÿé‹ç”¨ã§ã®å¤±æ•—ã‚’é˜²æ­¢
"""

import ast
import json
import logging
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class FutureLeakDetector:
    """æœªæ¥ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯æ¤œå‡ºã‚¯ãƒ©ã‚¹"""

    def __init__(self, report_dir: str = "logs/leak_detection"):
        self.report_dir = Path(report_dir)
        self.report_dir.mkdir(exist_ok=True, parents=True)

        # æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
        self.suspicious_patterns = {
            # å±é™ºãªãƒ‘ã‚¿ãƒ¼ãƒ³
            "future_indexing": [
                r"iloc\[i\s*\+",  # iloc[i + n] ã¯æœªæ¥å‚ç…§ã®å¯èƒ½æ€§
                r"iloc\[.*:\s*i\s*\+",  # iloc[:i+n] ã¯æœªæ¥ã‚’å«ã‚€
                r"shift\(-\d+\)",  # shift(-n) ã¯æœªæ¥ã¸ã®ã‚·ãƒ•ãƒˆ
                r"rolling\(.*center=True",  # center=Trueã¯å‰å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            ],
            # è¦ç¢ºèªãƒ‘ã‚¿ãƒ¼ãƒ³
            "needs_review": [
                r"iloc\[i:",  # iloc[i:] ã¯æœªæ¥å…¨ä½“ã‚’å«ã‚€å¯èƒ½æ€§
                r"\.tail\(",  # tail()ã¯æœ€æ–°ãƒ‡ãƒ¼ã‚¿ï¼ˆæœªæ¥ã®å¯èƒ½æ€§ï¼‰
                r"\.last\(",  # last()ã‚‚åŒæ§˜
                r"future",  # "future"ã¨ã„ã†å¤‰æ•°åã¯ç–‘ã‚ã—ã„
                r"forward",  # "forward"ã‚‚è¦ç¢ºèª
            ],
            # å®‰å…¨ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆï¼‰
            "safe_patterns": [
                r"iloc\[max\(0,\s*i\s*-",  # iloc[max(0, i - n)] ã¯éå»å‚ç…§
                r"shift\([1-9]",  # shift(n) where n > 0 ã¯éå»å‚ç…§
                r"rolling\(\d+\)(?!.*center=True)",  # rolling without center
                r"iloc\[:i\]",  # iloc[:i] ã¯ç¾åœ¨ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿
            ],
        }

        self.issues_found = []
        self.safe_operations = []

    def analyze_feature_code(self, file_path: str) -> List[Dict]:
        """ç‰¹å¾´é‡è¨ˆç®—ã‚³ãƒ¼ãƒ‰ã‚’åˆ†æ"""
        issues = []
        file_path = Path(file_path)

        if not file_path.exists():
            logger.warning(f"File not found: {file_path}")
            return issues

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                lines = content.split("\n")

            # å„è¡Œã‚’ãƒã‚§ãƒƒã‚¯
            for line_num, line in enumerate(lines, 1):
                # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
                if line.strip().startswith("#"):
                    continue

                # å±é™ºãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                for pattern_type, patterns in self.suspicious_patterns.items():
                    if pattern_type == "safe_patterns":
                        continue

                    for pattern in patterns:
                        if re.search(pattern, line):
                            # å®‰å…¨ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã«è©²å½“ã™ã‚‹ã‹ç¢ºèª
                            is_safe = False
                            for safe_pattern in self.suspicious_patterns[
                                "safe_patterns"
                            ]:
                                if re.search(safe_pattern, line):
                                    is_safe = True
                                    break

                            if not is_safe:
                                severity = (
                                    "HIGH"
                                    if pattern_type == "future_indexing"
                                    else "MEDIUM"
                                )
                                issues.append(
                                    {
                                        "file": str(file_path),
                                        "line": line_num,
                                        "code": line.strip(),
                                        "pattern": pattern,
                                        "type": pattern_type,
                                        "severity": severity,
                                        "message": f"Potential future data leak: {pattern}",
                                    }
                                )

            logger.info(f"Analyzed {file_path}: {len(issues)} issues found")

        except Exception as e:
            logger.error(f"Error analyzing {file_path}: {e}")

        return issues

    def validate_dataframe_operations(
        self, df: pd.DataFrame, operations: List[str]
    ) -> Dict:
        """DataFrameã®æ“ä½œã‚’å®Ÿè¡Œã—ã¦æœªæ¥ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’æ¤œè¨¼"""
        validation_results = {
            "safe": [],
            "unsafe": [],
            "errors": [],
        }

        # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨­å®š
        test_index = len(df) // 2  # ä¸­é–“ç‚¹ã§ãƒ†ã‚¹ãƒˆ

        for operation in operations:
            try:
                # æ“ä½œã‚’å®‰å…¨ã«è©•ä¾¡
                if "rolling" in operation:
                    # rollingæ“ä½œã®æ¤œè¨¼
                    window_match = re.search(r"rolling\((\d+)", operation)
                    if window_match:
                        window_size = int(window_match.group(1))
                        if "center=True" in operation:
                            validation_results["unsafe"].append(
                                {
                                    "operation": operation,
                                    "reason": "center=True uses future data",
                                }
                            )
                        else:
                            validation_results["safe"].append(
                                {"operation": operation, "reason": "Backward looking only"}
                            )

                elif "shift" in operation:
                    # shiftæ“ä½œã®æ¤œè¨¼
                    shift_match = re.search(r"shift\(([-\d]+)\)", operation)
                    if shift_match:
                        shift_value = int(shift_match.group(1))
                        if shift_value < 0:
                            validation_results["unsafe"].append(
                                {
                                    "operation": operation,
                                    "reason": f"shift({shift_value}) references future",
                                }
                            )
                        else:
                            validation_results["safe"].append(
                                {
                                    "operation": operation,
                                    "reason": f"shift({shift_value}) references past",
                                }
                            )

                elif "iloc" in operation:
                    # ilocæ“ä½œã®æ¤œè¨¼
                    if re.search(r"iloc\[i\s*\+", operation):
                        validation_results["unsafe"].append(
                            {
                                "operation": operation,
                                "reason": "iloc[i+n] may reference future",
                            }
                        )
                    elif re.search(r"iloc\[:i\]", operation):
                        validation_results["safe"].append(
                            {
                                "operation": operation,
                                "reason": "iloc[:i] only uses past data",
                            }
                        )

            except Exception as e:
                validation_results["errors"].append(
                    {"operation": operation, "error": str(e)}
                )

        return validation_results

    def check_backtest_data_split(
        self, train_data: pd.DataFrame, test_data: pd.DataFrame
    ) -> Dict:
        """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã‚’æ¤œè¨¼"""
        issues = []

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒã‚ã‚‹å ´åˆã®æ¤œè¨¼
        if "timestamp" in train_data.columns and "timestamp" in test_data.columns:
            train_max = pd.to_datetime(train_data["timestamp"]).max()
            test_min = pd.to_datetime(test_data["timestamp"]).min()

            if train_max >= test_min:
                issues.append(
                    {
                        "type": "DATA_OVERLAP",
                        "severity": "CRITICAL",
                        "message": f"Train data overlaps with test data: train_max={train_max}, test_min={test_min}",
                    }
                )

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®æ¤œè¨¼
        if hasattr(train_data.index, "max") and hasattr(test_data.index, "min"):
            if train_data.index.max() >= test_data.index.min():
                issues.append(
                    {
                        "type": "INDEX_OVERLAP",
                        "severity": "HIGH",
                        "message": "Train indices overlap with test indices",
                    }
                )

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "train_shape": train_data.shape,
            "test_shape": test_data.shape,
        }

    def validate_feature_pipeline(
        self, feature_function, sample_data: pd.DataFrame
    ) -> Dict:
        """ç‰¹å¾´é‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’æ¤œè¨¼"""
        validation_report = {
            "timestamp": datetime.now().isoformat(),
            "function_name": feature_function.__name__ if feature_function else "unknown",
            "data_shape": sample_data.shape,
            "checks": [],
        }

        # 1. å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ™‚ç³»åˆ—é †åºç¢ºèª
        if "timestamp" in sample_data.columns:
            timestamps = pd.to_datetime(sample_data["timestamp"])
            is_sorted = timestamps.is_monotonic_increasing
            validation_report["checks"].append(
                {
                    "check": "timestamp_order",
                    "passed": is_sorted,
                    "message": "Data is chronologically sorted"
                    if is_sorted
                    else "Data is NOT chronologically sorted",
                }
            )

        # 2. ç‰¹å¾´é‡ç”Ÿæˆã‚’ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å®Ÿè¡Œ
        try:
            # ãƒ‡ãƒ¼ã‚¿ã®åŠåˆ†ã§ç‰¹å¾´é‡ç”Ÿæˆ
            half_data = sample_data.iloc[: len(sample_data) // 2].copy()
            features_half = feature_function(half_data)

            # å…¨ãƒ‡ãƒ¼ã‚¿ã§ç‰¹å¾´é‡ç”Ÿæˆ
            features_full = feature_function(sample_data.copy())

            # 3. ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
            # å‰åŠã®ãƒ‡ãƒ¼ã‚¿ã§ç”Ÿæˆã—ãŸç‰¹å¾´é‡ãŒã€å…¨ãƒ‡ãƒ¼ã‚¿ã§ç”Ÿæˆã—ãŸç‰¹å¾´é‡ã®å‰åŠã¨ä¸€è‡´ã™ã‚‹ã‹
            common_cols = set(features_half.columns) & set(features_full.columns)
            
            inconsistent_features = []
            for col in common_cols:
                if col in ["timestamp", "date", "index"]:
                    continue
                    
                half_values = features_half[col].dropna()
                full_values = features_full[col].iloc[: len(features_half)].dropna()
                
                if len(half_values) > 0 and len(full_values) > 0:
                    # å€¤ã®æ¯”è¼ƒï¼ˆæµ®å‹•å°æ•°ç‚¹èª¤å·®ã‚’è€ƒæ…®ï¼‰
                    if not np.allclose(
                        half_values.values[:min(len(half_values), len(full_values))],
                        full_values.values[:min(len(half_values), len(full_values))],
                        rtol=1e-5,
                        equal_nan=True
                    ):
                        inconsistent_features.append(col)

            validation_report["checks"].append(
                {
                    "check": "feature_consistency",
                    "passed": len(inconsistent_features) == 0,
                    "message": f"Features are consistent"
                    if len(inconsistent_features) == 0
                    else f"Inconsistent features found: {inconsistent_features}",
                    "details": {
                        "total_features": len(common_cols),
                        "inconsistent_count": len(inconsistent_features),
                    },
                }
            )

        except Exception as e:
            validation_report["checks"].append(
                {
                    "check": "feature_generation",
                    "passed": False,
                    "message": f"Error during feature generation: {str(e)}",
                }
            )

        # 4. çµæœã®è©•ä¾¡
        all_passed = all(check["passed"] for check in validation_report["checks"])
        validation_report["overall_status"] = "PASS" if all_passed else "FAIL"

        return validation_report

    def scan_project(self, project_root: str) -> Dict:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã‚’ã‚¹ã‚­ãƒ£ãƒ³"""
        project_root = Path(project_root)
        all_issues = []

        # ã‚¹ã‚­ãƒ£ãƒ³å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        scan_dirs = ["crypto_bot/ml", "crypto_bot/backtest", "crypto_bot/strategy"]

        for scan_dir in scan_dirs:
            dir_path = project_root / scan_dir
            if not dir_path.exists():
                continue

            # Pythonãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            for py_file in dir_path.rglob("*.py"):
                issues = self.analyze_feature_code(py_file)
                all_issues.extend(issues)

        # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        severity_order = {"CRITICAL": 0, "HIGH": 1, "MEDIUM": 2, "LOW": 3}
        all_issues.sort(key=lambda x: severity_order.get(x.get("severity", "LOW"), 4))

        return {
            "scan_time": datetime.now().isoformat(),
            "total_files_scanned": len(list(project_root.rglob("*.py"))),
            "total_issues": len(all_issues),
            "issues_by_severity": self._count_by_severity(all_issues),
            "issues": all_issues[:50],  # æœ€åˆã®50ä»¶ã®ã¿
        }

    def _count_by_severity(self, issues: List[Dict]) -> Dict:
        """é‡è¦åº¦åˆ¥ã«ã‚«ã‚¦ãƒ³ãƒˆ"""
        counts = {"CRITICAL": 0, "HIGH": 0, "MEDIUM": 0, "LOW": 0}
        for issue in issues:
            severity = issue.get("severity", "LOW")
            counts[severity] = counts.get(severity, 0) + 1
        return counts

    def generate_report(self, scan_results: Dict) -> str:
        """HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        html_path = self.report_dir / f"leak_detection_{timestamp}.html"

        # å…¨ä½“ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
        total_issues = scan_results["total_issues"]
        critical_count = scan_results["issues_by_severity"]["CRITICAL"]
        high_count = scan_results["issues_by_severity"]["HIGH"]

        if critical_count > 0:
            status_color = "red"
            status_text = "CRITICAL ISSUES FOUND"
        elif high_count > 0:
            status_color = "orange"
            status_text = "HIGH PRIORITY ISSUES"
        elif total_issues > 0:
            status_color = "yellow"
            status_text = "MEDIUM PRIORITY ISSUES"
        else:
            status_color = "green"
            status_text = "NO ISSUES DETECTED"

        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Future Data Leak Detection Report</title>
    <style>
        body {{ font-family: 'Segoe UI', Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .header {{ background-color: #2c3e50; color: white; padding: 25px; border-radius: 8px; }}
        .status {{ font-size: 36px; color: {status_color}; font-weight: bold; margin: 20px 0; }}
        .section {{ background-color: white; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .issue {{ padding: 15px; margin: 10px 0; border-left: 4px solid; background-color: #f9f9f9; }}
        .CRITICAL {{ border-color: #e74c3c; background-color: #ffe6e6; }}
        .HIGH {{ border-color: #e67e22; background-color: #fff4e6; }}
        .MEDIUM {{ border-color: #f39c12; background-color: #fffae6; }}
        .LOW {{ border-color: #3498db; background-color: #e6f3ff; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #34495e; color: white; }}
        .code {{ font-family: 'Courier New', monospace; background-color: #2c3e50; color: #ecf0f1; padding: 3px 6px; border-radius: 3px; }}
        .recommendation {{ background-color: #d4edda; padding: 15px; border-radius: 5px; margin: 15px 0; }}
        .metric {{ display: inline-block; padding: 10px 20px; margin: 10px; background-color: white; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ” Future Data Leak Detection Report</h1>
        <p>Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        <div class="status">{status_text}</div>
    </div>
    
    <div class="section">
        <h2>ğŸ“Š Scan Summary</h2>
        <div>
            <span class="metric">ğŸ“ Files Scanned: {scan_results['total_files_scanned']}</span>
            <span class="metric">âš ï¸ Total Issues: {total_issues}</span>
            <span class="metric">ğŸ”´ Critical: {critical_count}</span>
            <span class="metric">ğŸŸ  High: {high_count}</span>
            <span class="metric">ğŸŸ¡ Medium: {scan_results['issues_by_severity']['MEDIUM']}</span>
        </div>
    </div>
    
    <div class="section">
        <h2>âš ï¸ Issues Found</h2>
"""

        if scan_results["issues"]:
            for issue in scan_results["issues"][:20]:  # æœ€åˆã®20ä»¶ã‚’è¡¨ç¤º
                html_content += f"""
        <div class="issue {issue.get('severity', 'LOW')}">
            <strong>{issue.get('severity', 'LOW')}</strong>: {issue.get('message', '')}
            <br>ğŸ“ File: {issue.get('file', 'unknown')}
            <br>ğŸ“ Line {issue.get('line', '?')}: <span class="code">{issue.get('code', '')[:100]}</span>
        </div>
"""
        else:
            html_content += """
        <div class="recommendation">
            âœ… No future data leak issues detected! Your code appears to be safe from look-ahead bias.
        </div>
"""

        html_content += """
    </div>
    
    <div class="section">
        <h2>ğŸ’¡ Recommendations</h2>
        <div class="recommendation">
            <h3>Best Practices to Prevent Future Data Leaks:</h3>
            <ul>
                <li>âœ… Always use <code>shift(1)</code> or higher for past data, never negative values</li>
                <li>âœ… Use <code>rolling(window)</code> without <code>center=True</code></li>
                <li>âœ… When using <code>iloc</code>, ensure indices reference past data only</li>
                <li>âœ… Split train/test data chronologically with no overlap</li>
                <li>âœ… Validate feature consistency: features computed on partial data should match when computed on full data</li>
                <li>âœ… Add timestamp checks in production to ensure no future data access</li>
            </ul>
        </div>
    </div>
    
    <div class="section">
        <h2>ğŸ“ˆ Common Patterns Detected</h2>
        <table>
            <tr>
                <th>Pattern</th>
                <th>Risk Level</th>
                <th>Example</th>
                <th>Safe Alternative</th>
            </tr>
            <tr>
                <td>Future Shift</td>
                <td style="color: red;">HIGH</td>
                <td><code>df.shift(-1)</code></td>
                <td><code>df.shift(1)</code></td>
            </tr>
            <tr>
                <td>Center Rolling</td>
                <td style="color: orange;">MEDIUM</td>
                <td><code>df.rolling(5, center=True)</code></td>
                <td><code>df.rolling(5)</code></td>
            </tr>
            <tr>
                <td>Future Indexing</td>
                <td style="color: red;">HIGH</td>
                <td><code>df.iloc[i:i+10]</code></td>
                <td><code>df.iloc[max(0,i-10):i]</code></td>
            </tr>
        </table>
    </div>
</body>
</html>
"""

        with open(html_path, "w") as f:
            f.write(html_content)

        logger.info(f"HTML report saved: {html_path}")
        return str(html_path)

    def save_json_report(self, scan_results: Dict) -> str:
        """JSONå½¢å¼ã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_path = self.report_dir / f"leak_detection_{timestamp}.json"

        with open(json_path, "w") as f:
            json.dump(scan_results, f, indent=2, default=str)

        logger.info(f"JSON report saved: {json_path}")
        return str(json_path)


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Detect future data leaks in ML pipeline"
    )
    parser.add_argument(
        "--project-root",
        default=".",
        help="Project root directory to scan",
    )
    parser.add_argument(
        "--report-dir",
        default="logs/leak_detection",
        help="Directory for detection reports",
    )
    parser.add_argument(
        "--file",
        help="Specific file to analyze",
    )
    parser.add_argument(
        "--html",
        action="store_true",
        help="Generate HTML report",
    )

    args = parser.parse_args()

    # æ¤œå‡ºå™¨åˆæœŸåŒ–
    detector = FutureLeakDetector(args.report_dir)

    if args.file:
        # ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ
        issues = detector.analyze_feature_code(args.file)
        
        if issues:
            logger.warning(f"âš ï¸ Found {len(issues)} potential issues in {args.file}")
            for issue in issues:
                logger.warning(
                    f"  Line {issue['line']} [{issue['severity']}]: {issue['message']}"
                )
            sys.exit(1)
        else:
            logger.info(f"âœ… No issues found in {args.file}")
            sys.exit(0)
    else:
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ã‚¹ã‚­ãƒ£ãƒ³
        logger.info(f"ğŸ” Scanning project: {args.project_root}")
        scan_results = detector.scan_project(args.project_root)

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        json_path = detector.save_json_report(scan_results)
        
        if args.html:
            html_path = detector.generate_report(scan_results)
            logger.info(f"ğŸ“Š HTML report: {html_path}")

        # ã‚µãƒãƒªãƒ¼å‡ºåŠ›
        logger.info("=" * 50)
        logger.info("ğŸ“Š Scan Complete")
        logger.info(f"ğŸ“ Files scanned: {scan_results['total_files_scanned']}")
        logger.info(f"âš ï¸ Total issues: {scan_results['total_issues']}")
        
        for severity, count in scan_results["issues_by_severity"].items():
            if count > 0:
                logger.info(f"  {severity}: {count}")

        logger.info("=" * 50)

        # çµ‚äº†ã‚³ãƒ¼ãƒ‰
        if scan_results["issues_by_severity"]["CRITICAL"] > 0:
            sys.exit(2)  # Critical issues
        elif scan_results["issues_by_severity"]["HIGH"] > 0:
            sys.exit(1)  # High priority issues
        else:
            sys.exit(0)  # Success or only medium/low issues


if __name__ == "__main__":
    main()