#!/usr/bin/env python3
# =============================================================================
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: scripts/performance_comparison_system.py
# èª¬æ˜:
# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ vs å¾“æ¥æ‰‹æ³•ã®è©³ç´°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ»çµ±è¨ˆåˆ†æã‚·ã‚¹ãƒ†ãƒ 
# å‹ç‡ãƒ»åç›Šæ€§æ”¹å–„åŠ¹æœã®ç§‘å­¦çš„æ¤œè¨¼ãƒ»ä¿¡é ¼åŒºé–“ä»˜ãåˆ†æ
# =============================================================================

import sys
import os
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
import warnings
warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import numpy as np
import pandas as pd
import yaml
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """è©³ç´°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    strategy_name: str
    period_start: datetime
    period_end: datetime
    
    # ãƒªã‚¿ãƒ¼ãƒ³æŒ‡æ¨™
    total_return: float
    annualized_return: float
    cumulative_return: float
    
    # ãƒªã‚¹ã‚¯æŒ‡æ¨™
    volatility: float
    sharpe_ratio: float
    sortino_ratio: float
    calmar_ratio: float
    max_drawdown: float
    
    # å–å¼•æŒ‡æ¨™
    total_trades: int
    win_rate: float
    profit_factor: float
    avg_win: float
    avg_loss: float
    avg_trade_duration: float
    
    # MLç‰¹æœ‰æŒ‡æ¨™
    prediction_accuracy: float
    precision: float
    recall: float
    f1_score: float
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç‰¹æœ‰æŒ‡æ¨™
    avg_confidence: Optional[float] = None
    confidence_accuracy_correlation: Optional[float] = None
    ensemble_diversity: Optional[float] = None
    
    # çµ±è¨ˆæƒ…å ±
    sample_size: int = 0
    confidence_interval_95: Optional[Tuple[float, float]] = None


@dataclass
class ComparisonResult:
    """æ¯”è¼ƒçµæœ"""
    metric_name: str
    traditional_value: float
    ensemble_value: float
    improvement: float
    improvement_pct: float
    statistical_significance: float
    confidence_interval: Tuple[float, float]
    effect_size: float
    interpretation: str


@dataclass
class StatisticalTest:
    """çµ±è¨ˆæ¤œå®šçµæœ"""
    test_name: str
    statistic: float
    p_value: float
    effect_size: float
    confidence_interval: Tuple[float, float]
    interpretation: str
    is_significant: bool


class PerformanceComparisonSystem:
    """åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config_path: str = None):
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        
        Parameters:
        -----------
        config_path : str
            è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.config = self._load_config(config_path)
        self.traditional_data = []
        self.ensemble_data = []
        self.comparison_results = {}
        self.statistical_tests = {}
        
        # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.output_dir = Path(self.config.get('output_dir', 
                                             project_root / "results" / "performance_comparison"))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("Performance Comparison System initialized")
    
    def _load_config(self, config_path: str = None) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        if config_path is None:
            config_path = project_root / "config" / "performance_comparison.yml"
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"Config loaded from: {config_path}")
            return config
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š"""
        return {
            'comparison': {
                'confidence_level': 0.95,
                'minimum_sample_size': 50,
                'significance_threshold': 0.05,
                'effect_size_thresholds': {
                    'small': 0.2,
                    'medium': 0.5,
                    'large': 0.8
                }
            },
            'metrics': {
                'primary': ['total_return', 'sharpe_ratio', 'win_rate', 'max_drawdown'],
                'secondary': ['volatility', 'profit_factor', 'avg_win', 'avg_loss'],
                'ml_specific': ['prediction_accuracy', 'precision', 'recall', 'f1_score']
            },
            'visualization': {
                'enable_plots': True,
                'plot_formats': ['png', 'pdf'],
                'figure_size': [12, 8],
                'style': 'seaborn'
            }
        }
    
    def load_trading_results(self, traditional_file: str, ensemble_file: str):
        """å–å¼•çµæœãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            # å¾“æ¥æ‰‹æ³•çµæœ
            if os.path.exists(traditional_file):
                self.traditional_data = self._load_results_file(traditional_file, 'traditional')
                logger.info(f"Traditional results loaded: {len(self.traditional_data)} records")
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœ
            if os.path.exists(ensemble_file):
                self.ensemble_data = self._load_results_file(ensemble_file, 'ensemble')
                logger.info(f"Ensemble results loaded: {len(self.ensemble_data)} records")
                
        except Exception as e:
            logger.error(f"Failed to load trading results: {e}")
            self._generate_sample_data()
    
    def _load_results_file(self, file_path: str, strategy_type: str) -> List[PerformanceMetrics]:
        """çµæœãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã‚„ãƒ©ã‚¤ãƒ–ãƒˆãƒ¬ãƒ¼ãƒ‰çµæœã‚’èª­ã¿è¾¼ã¿
        # ã“ã“ã§ã¯æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        return self._generate_sample_metrics(strategy_type)
    
    def _generate_sample_metrics(self, strategy_type: str, count: int = 100) -> List[PerformanceMetrics]:
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ"""
        np.random.seed(42 if strategy_type == 'traditional' else 43)
        
        metrics_list = []
        base_date = datetime.now() - timedelta(days=count)
        
        for i in range(count):
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½
            base_return = 0.02 if strategy_type == 'traditional' else 0.025
            base_winrate = 0.58 if strategy_type == 'traditional' else 0.63
            base_sharpe = 1.2 if strategy_type == 'traditional' else 1.5
            
            # ãƒã‚¤ã‚ºè¿½åŠ 
            total_return = base_return + np.random.normal(0, 0.01)
            win_rate = np.clip(base_winrate + np.random.normal(0, 0.03), 0, 1)
            sharpe_ratio = base_sharpe + np.random.normal(0, 0.2)
            
            metrics = PerformanceMetrics(
                strategy_name=strategy_type,
                period_start=base_date + timedelta(days=i),
                period_end=base_date + timedelta(days=i+1),
                total_return=total_return,
                annualized_return=total_return * 252,
                cumulative_return=(1 + total_return) ** (i + 1) - 1,
                volatility=np.random.uniform(0.15, 0.25),
                sharpe_ratio=sharpe_ratio,
                sortino_ratio=sharpe_ratio * 1.2,
                calmar_ratio=abs(total_return) / np.random.uniform(0.05, 0.15),
                max_drawdown=-np.random.uniform(0.03, 0.12),
                total_trades=np.random.randint(10, 50),
                win_rate=win_rate,
                profit_factor=win_rate / (1 - win_rate) * np.random.uniform(1.1, 1.8),
                avg_win=np.random.uniform(0.02, 0.05),
                avg_loss=-np.random.uniform(0.015, 0.03),
                avg_trade_duration=np.random.uniform(2, 24),
                prediction_accuracy=win_rate + np.random.normal(0, 0.02),
                precision=win_rate + np.random.normal(0, 0.03),
                recall=win_rate + np.random.normal(0, 0.02),
                f1_score=win_rate + np.random.normal(0, 0.025),
                avg_confidence=np.random.uniform(0.6, 0.85) if strategy_type == 'ensemble' else None,
                confidence_accuracy_correlation=np.random.uniform(0.3, 0.7) if strategy_type == 'ensemble' else None,
                ensemble_diversity=np.random.uniform(0.4, 0.8) if strategy_type == 'ensemble' else None,
                sample_size=1
            )
            
            metrics_list.append(metrics)
        
        return metrics_list
    
    def _generate_sample_data(self):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        logger.info("Generating sample performance data...")
        self.traditional_data = self._generate_sample_metrics('traditional', 100)
        self.ensemble_data = self._generate_sample_metrics('ensemble', 100)
    
    def run_comprehensive_comparison(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„æ¯”è¼ƒåˆ†æå®Ÿè¡Œ"""
        logger.info("Starting comprehensive performance comparison...")
        
        if not self.traditional_data or not self.ensemble_data:
            logger.error("No data available for comparison")
            return {}
        
        # åŸºæœ¬çµ±è¨ˆæ¯”è¼ƒ
        basic_comparison = self._run_basic_comparison()
        
        # çµ±è¨ˆçš„æ¤œå®š
        statistical_tests = self._run_statistical_tests()
        
        # æ™‚ç³»åˆ—åˆ†æ
        time_series_analysis = self._run_time_series_analysis()
        
        # MLç‰¹æœ‰åˆ†æ
        ml_analysis = self._run_ml_specific_analysis()
        
        # ç·åˆè©•ä¾¡
        overall_assessment = self._run_overall_assessment()
        
        # çµæœçµ±åˆ
        comprehensive_results = {
            'basic_comparison': basic_comparison,
            'statistical_tests': statistical_tests,
            'time_series_analysis': time_series_analysis,
            'ml_analysis': ml_analysis,
            'overall_assessment': overall_assessment,
            'metadata': {
                'comparison_date': datetime.now().isoformat(),
                'traditional_sample_size': len(self.traditional_data),
                'ensemble_sample_size': len(self.ensemble_data),
                'comparison_period': self._get_comparison_period()
            }
        }
        
        self.comparison_results = comprehensive_results
        return comprehensive_results
    
    def _run_basic_comparison(self) -> Dict[str, ComparisonResult]:
        """åŸºæœ¬çµ±è¨ˆæ¯”è¼ƒ"""
        logger.info("Running basic statistical comparison...")
        
        primary_metrics = self.config['metrics']['primary']
        results = {}
        
        for metric in primary_metrics:
            traditional_values = [getattr(m, metric) for m in self.traditional_data]
            ensemble_values = [getattr(m, metric) for m in self.ensemble_data]
            
            # åŸºæœ¬çµ±è¨ˆ
            trad_mean = np.mean(traditional_values)
            ens_mean = np.mean(ensemble_values)
            improvement = ens_mean - trad_mean
            improvement_pct = (improvement / abs(trad_mean)) * 100 if trad_mean != 0 else 0
            
            # çµ±è¨ˆçš„æœ‰æ„æ€§
            t_stat, p_value = stats.ttest_ind(ensemble_values, traditional_values)
            
            # åŠ¹æœã‚µã‚¤ã‚ºï¼ˆCohen's dï¼‰
            pooled_std = np.sqrt(((len(traditional_values) - 1) * np.var(traditional_values, ddof=1) +
                                 (len(ensemble_values) - 1) * np.var(ensemble_values, ddof=1)) /
                                (len(traditional_values) + len(ensemble_values) - 2))
            effect_size = improvement / pooled_std if pooled_std != 0 else 0
            
            # ä¿¡é ¼åŒºé–“
            se_diff = pooled_std * np.sqrt(1/len(traditional_values) + 1/len(ensemble_values))
            df = len(traditional_values) + len(ensemble_values) - 2
            t_critical = stats.t.ppf(0.975, df)
            ci_lower = improvement - t_critical * se_diff
            ci_upper = improvement + t_critical * se_diff
            
            # è§£é‡ˆ
            interpretation = self._interpret_improvement(improvement_pct, effect_size, p_value)
            
            results[metric] = ComparisonResult(
                metric_name=metric,
                traditional_value=trad_mean,
                ensemble_value=ens_mean,
                improvement=improvement,
                improvement_pct=improvement_pct,
                statistical_significance=p_value,
                confidence_interval=(ci_lower, ci_upper),
                effect_size=effect_size,
                interpretation=interpretation
            )
        
        return results
    
    def _run_statistical_tests(self) -> Dict[str, StatisticalTest]:
        """çµ±è¨ˆçš„æ¤œå®šå®Ÿè¡Œ"""
        logger.info("Running statistical tests...")
        
        tests = {}
        primary_metrics = self.config['metrics']['primary']
        
        for metric in primary_metrics:
            traditional_values = [getattr(m, metric) for m in self.traditional_data]
            ensemble_values = [getattr(m, metric) for m in self.ensemble_data]
            
            # Welch's t-test (ç­‰åˆ†æ•£ã‚’ä»®å®šã—ãªã„)
            t_stat, p_value = stats.ttest_ind(ensemble_values, traditional_values, equal_var=False)
            
            # Mann-Whitney U test (ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯)
            u_stat, u_p_value = stats.mannwhitneyu(ensemble_values, traditional_values, alternative='two-sided')
            
            # åŠ¹æœã‚µã‚¤ã‚º
            effect_size = self._calculate_cohens_d(ensemble_values, traditional_values)
            
            # ä¿¡é ¼åŒºé–“ï¼ˆbootstrapï¼‰
            ci_lower, ci_upper = self._bootstrap_confidence_interval(
                ensemble_values, traditional_values, metric
            )
            
            # æ¤œå®šçµæœè§£é‡ˆ
            is_significant = p_value < self.config['comparison']['significance_threshold']
            interpretation = self._interpret_statistical_test(p_value, effect_size, is_significant)
            
            tests[f"{metric}_ttest"] = StatisticalTest(
                test_name=f"Welch's t-test ({metric})",
                statistic=t_stat,
                p_value=p_value,
                effect_size=effect_size,
                confidence_interval=(ci_lower, ci_upper),
                interpretation=interpretation,
                is_significant=is_significant
            )
            
            tests[f"{metric}_mannwhitney"] = StatisticalTest(
                test_name=f"Mann-Whitney U ({metric})",
                statistic=u_stat,
                p_value=u_p_value,
                effect_size=effect_size,
                confidence_interval=(ci_lower, ci_upper),
                interpretation=f"Non-parametric: {interpretation}",
                is_significant=u_p_value < self.config['comparison']['significance_threshold']
            )
        
        return tests
    
    def _run_time_series_analysis(self) -> Dict[str, Any]:
        """æ™‚ç³»åˆ—åˆ†æ"""
        logger.info("Running time series analysis...")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—ã§æ•´ç†
        traditional_ts = self._create_time_series(self.traditional_data)
        ensemble_ts = self._create_time_series(self.ensemble_data)
        
        analysis = {}
        
        # ãƒªã‚¿ãƒ¼ãƒ³æ¨ç§»æ¯”è¼ƒ
        analysis['return_trend'] = self._analyze_return_trend(traditional_ts, ensemble_ts)
        
        # ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³åˆ†æ
        analysis['drawdown_analysis'] = self._analyze_drawdown_patterns(traditional_ts, ensemble_ts)
        
        # å‹ç‡ã®å®‰å®šæ€§
        analysis['winrate_stability'] = self._analyze_winrate_stability(traditional_ts, ensemble_ts)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒç¶šæ€§
        analysis['performance_persistence'] = self._analyze_performance_persistence(
            traditional_ts, ensemble_ts
        )
        
        return analysis
    
    def _run_ml_specific_analysis(self) -> Dict[str, Any]:
        """MLç‰¹æœ‰åˆ†æ"""
        logger.info("Running ML-specific analysis...")
        
        analysis = {}
        
        # äºˆæ¸¬ç²¾åº¦åˆ†æ
        analysis['prediction_accuracy'] = self._analyze_prediction_accuracy()
        
        # ä¿¡é ¼åº¦ã¨ç²¾åº¦ã®ç›¸é–¢ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®ã¿ï¼‰
        if any(m.avg_confidence for m in self.ensemble_data if m.avg_confidence):
            analysis['confidence_accuracy_correlation'] = self._analyze_confidence_accuracy()
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¤šæ§˜æ€§åŠ¹æœ
        if any(m.ensemble_diversity for m in self.ensemble_data if m.ensemble_diversity):
            analysis['diversity_effect'] = self._analyze_ensemble_diversity()
        
        # éå­¦ç¿’æ¤œå‡º
        analysis['overfitting_detection'] = self._detect_overfitting_signs()
        
        return analysis
    
    def _run_overall_assessment(self) -> Dict[str, Any]:
        """ç·åˆè©•ä¾¡"""
        logger.info("Running overall assessment...")
        
        assessment = {}
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        assessment['overall_score'] = self._calculate_overall_score()
        
        # ãƒªã‚¹ã‚¯èª¿æ•´å¾Œãƒªã‚¿ãƒ¼ãƒ³
        assessment['risk_adjusted_return'] = self._calculate_risk_adjusted_metrics()
        
        # å®Ÿç”¨æ€§è©•ä¾¡
        assessment['practical_significance'] = self._evaluate_practical_significance()
        
        # æ¨å¥¨äº‹é …
        assessment['recommendations'] = self._generate_recommendations()
        
        return assessment
    
    def _calculate_cohens_d(self, group1: List[float], group2: List[float]) -> float:
        """Cohen's dåŠ¹æœã‚µã‚¤ã‚ºè¨ˆç®—"""
        n1, n2 = len(group1), len(group2)
        pooled_std = np.sqrt(((n1 - 1) * np.var(group1, ddof=1) + 
                             (n2 - 1) * np.var(group2, ddof=1)) / (n1 + n2 - 2))
        return (np.mean(group1) - np.mean(group2)) / pooled_std if pooled_std != 0 else 0
    
    def _bootstrap_confidence_interval(self, group1: List[float], group2: List[float], 
                                     metric: str, n_bootstrap: int = 1000) -> Tuple[float, float]:
        """ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ä¿¡é ¼åŒºé–“"""
        np.random.seed(42)
        bootstrap_diffs = []
        
        for _ in range(n_bootstrap):
            sample1 = np.random.choice(group1, size=len(group1), replace=True)
            sample2 = np.random.choice(group2, size=len(group2), replace=True)
            bootstrap_diffs.append(np.mean(sample1) - np.mean(sample2))
        
        ci_level = self.config['comparison']['confidence_level']
        alpha = 1 - ci_level
        lower_percentile = (alpha / 2) * 100
        upper_percentile = (1 - alpha / 2) * 100
        
        return np.percentile(bootstrap_diffs, [lower_percentile, upper_percentile])
    
    def _interpret_improvement(self, improvement_pct: float, effect_size: float, p_value: float) -> str:
        """æ”¹å–„åŠ¹æœè§£é‡ˆ"""
        significance_threshold = self.config['comparison']['significance_threshold']
        effect_thresholds = self.config['comparison']['effect_size_thresholds']
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§
        significant = p_value < significance_threshold
        
        # åŠ¹æœã‚µã‚¤ã‚º
        if abs(effect_size) < effect_thresholds['small']:
            size_interpretation = "negligible"
        elif abs(effect_size) < effect_thresholds['medium']:
            size_interpretation = "small"
        elif abs(effect_size) < effect_thresholds['large']:
            size_interpretation = "medium"
        else:
            size_interpretation = "large"
        
        # ç·åˆè§£é‡ˆ
        if significant and improvement_pct > 0:
            return f"Statistically significant improvement ({improvement_pct:.2f}%, {size_interpretation} effect)"
        elif significant and improvement_pct < 0:
            return f"Statistically significant decline ({improvement_pct:.2f}%, {size_interpretation} effect)"
        else:
            return f"No significant difference ({improvement_pct:.2f}%, {size_interpretation} effect)"
    
    def _interpret_statistical_test(self, p_value: float, effect_size: float, is_significant: bool) -> str:
        """çµ±è¨ˆæ¤œå®šçµæœè§£é‡ˆ"""
        if is_significant:
            if effect_size > 0.8:
                return f"Strong evidence of improvement (p={p_value:.4f}, large effect)"
            elif effect_size > 0.5:
                return f"Moderate evidence of improvement (p={p_value:.4f}, medium effect)"
            elif effect_size > 0.2:
                return f"Weak evidence of improvement (p={p_value:.4f}, small effect)"
            else:
                return f"Significant but minimal improvement (p={p_value:.4f})"
        else:
            return f"No significant difference (p={p_value:.4f})"
    
    def _create_time_series(self, data: List[PerformanceMetrics]) -> pd.DataFrame:
        """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
        df_data = []
        for metrics in data:
            df_data.append({
                'date': metrics.period_start,
                'total_return': metrics.total_return,
                'win_rate': metrics.win_rate,
                'sharpe_ratio': metrics.sharpe_ratio,
                'max_drawdown': metrics.max_drawdown,
                'total_trades': metrics.total_trades
            })
        
        df = pd.DataFrame(df_data)
        df['date'] = pd.to_datetime(df['date'])
        return df.sort_values('date').reset_index(drop=True)
    
    def _analyze_return_trend(self, traditional_ts: pd.DataFrame, ensemble_ts: pd.DataFrame) -> Dict:
        """ãƒªã‚¿ãƒ¼ãƒ³æ¨ç§»åˆ†æ"""
        return {
            'traditional_trend': np.polyfit(range(len(traditional_ts)), traditional_ts['total_return'], 1)[0],
            'ensemble_trend': np.polyfit(range(len(ensemble_ts)), ensemble_ts['total_return'], 1)[0],
            'trend_improvement': np.polyfit(range(len(ensemble_ts)), ensemble_ts['total_return'], 1)[0] - 
                               np.polyfit(range(len(traditional_ts)), traditional_ts['total_return'], 1)[0]
        }
    
    def _analyze_drawdown_patterns(self, traditional_ts: pd.DataFrame, ensemble_ts: pd.DataFrame) -> Dict:
        """ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        return {
            'traditional_avg_drawdown': traditional_ts['max_drawdown'].mean(),
            'ensemble_avg_drawdown': ensemble_ts['max_drawdown'].mean(),
            'drawdown_improvement': ensemble_ts['max_drawdown'].mean() - traditional_ts['max_drawdown'].mean(),
            'drawdown_volatility_reduction': traditional_ts['max_drawdown'].std() - ensemble_ts['max_drawdown'].std()
        }
    
    def _analyze_winrate_stability(self, traditional_ts: pd.DataFrame, ensemble_ts: pd.DataFrame) -> Dict:
        """å‹ç‡å®‰å®šæ€§åˆ†æ"""
        return {
            'traditional_winrate_std': traditional_ts['win_rate'].std(),
            'ensemble_winrate_std': ensemble_ts['win_rate'].std(),
            'stability_improvement': traditional_ts['win_rate'].std() - ensemble_ts['win_rate'].std()
        }
    
    def _analyze_performance_persistence(self, traditional_ts: pd.DataFrame, ensemble_ts: pd.DataFrame) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒç¶šæ€§åˆ†æ"""
        # ç§»å‹•å¹³å‡ãƒ™ãƒ¼ã‚¹ã®åˆ†æ
        window = min(10, len(traditional_ts) // 4)
        
        trad_rolling = traditional_ts['total_return'].rolling(window).mean()
        ens_rolling = ensemble_ts['total_return'].rolling(window).mean()
        
        return {
            'traditional_persistence': trad_rolling.std(),
            'ensemble_persistence': ens_rolling.std(),
            'persistence_improvement': trad_rolling.std() - ens_rolling.std()
        }
    
    def _analyze_prediction_accuracy(self) -> Dict:
        """äºˆæ¸¬ç²¾åº¦åˆ†æ"""
        trad_accuracy = [m.prediction_accuracy for m in self.traditional_data]
        ens_accuracy = [m.prediction_accuracy for m in self.ensemble_data]
        
        return {
            'traditional_avg_accuracy': np.mean(trad_accuracy),
            'ensemble_avg_accuracy': np.mean(ens_accuracy),
            'accuracy_improvement': np.mean(ens_accuracy) - np.mean(trad_accuracy),
            'accuracy_consistency_improvement': np.std(trad_accuracy) - np.std(ens_accuracy)
        }
    
    def _analyze_confidence_accuracy(self) -> Dict:
        """ä¿¡é ¼åº¦ã¨ç²¾åº¦ã®ç›¸é–¢åˆ†æ"""
        ensemble_confidence = [m.avg_confidence for m in self.ensemble_data if m.avg_confidence]
        ensemble_accuracy = [m.prediction_accuracy for m in self.ensemble_data if m.avg_confidence]
        
        if len(ensemble_confidence) > 10:
            correlation = np.corrcoef(ensemble_confidence, ensemble_accuracy)[0, 1]
            return {
                'confidence_accuracy_correlation': correlation,
                'interpretation': 'Strong correlation' if abs(correlation) > 0.7 else 
                               'Moderate correlation' if abs(correlation) > 0.5 else 'Weak correlation'
            }
        return {'confidence_accuracy_correlation': None, 'interpretation': 'Insufficient data'}
    
    def _analyze_ensemble_diversity(self) -> Dict:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¤šæ§˜æ€§åŠ¹æœåˆ†æ"""
        diversity_scores = [m.ensemble_diversity for m in self.ensemble_data if m.ensemble_diversity]
        accuracy_scores = [m.prediction_accuracy for m in self.ensemble_data if m.ensemble_diversity]
        
        if len(diversity_scores) > 10:
            diversity_effect = np.corrcoef(diversity_scores, accuracy_scores)[0, 1]
            return {
                'avg_diversity': np.mean(diversity_scores),
                'diversity_accuracy_correlation': diversity_effect,
                'optimal_diversity_range': (np.percentile(diversity_scores, 25), np.percentile(diversity_scores, 75))
            }
        return {'avg_diversity': None, 'diversity_accuracy_correlation': None}
    
    def _detect_overfitting_signs(self) -> Dict:
        """éå­¦ç¿’å…†å€™æ¤œå‡º"""
        # ç²¾åº¦ã¨åç›Šæ€§ã®ä¹–é›¢ã‚’ãƒã‚§ãƒƒã‚¯
        ens_accuracy = [m.prediction_accuracy for m in self.ensemble_data]
        ens_returns = [m.total_return for m in self.ensemble_data]
        
        accuracy_return_correlation = np.corrcoef(ens_accuracy, ens_returns)[0, 1]
        
        return {
            'accuracy_return_correlation': accuracy_return_correlation,
            'overfitting_risk': 'High' if accuracy_return_correlation < 0.3 else 
                              'Medium' if accuracy_return_correlation < 0.6 else 'Low',
            'recommendation': 'Monitor for overfitting' if accuracy_return_correlation < 0.5 else 'Healthy correlation'
        }
    
    def _calculate_overall_score(self) -> Dict:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        # é‡ã¿ä»˜ãã‚¹ã‚³ã‚¢è¨ˆç®—
        weights = {
            'total_return': 0.3,
            'sharpe_ratio': 0.25,
            'win_rate': 0.2,
            'max_drawdown': 0.25  # è² ã®æŒ‡æ¨™ãªã®ã§æ”¹å–„ã¯æ­£ã®åŠ¹æœ
        }
        
        total_score = 0
        for metric, weight in weights.items():
            if metric in self.comparison_results.get('basic_comparison', {}):
                improvement = self.comparison_results['basic_comparison'][metric].improvement_pct
                if metric == 'max_drawdown':  # ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ã¯è² ã®æŒ‡æ¨™
                    improvement = -improvement
                total_score += improvement * weight
        
        return {
            'weighted_improvement_score': total_score,
            'score_interpretation': 'Excellent' if total_score > 10 else
                                   'Good' if total_score > 5 else
                                   'Moderate' if total_score > 0 else 'Poor'
        }
    
    def _calculate_risk_adjusted_metrics(self) -> Dict:
        """ãƒªã‚¹ã‚¯èª¿æ•´å¾Œãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
        trad_sharpe = [m.sharpe_ratio for m in self.traditional_data]
        ens_sharpe = [m.sharpe_ratio for m in self.ensemble_data]
        
        trad_sortino = [m.sortino_ratio for m in self.traditional_data]
        ens_sortino = [m.sortino_ratio for m in self.ensemble_data]
        
        return {
            'sharpe_improvement': np.mean(ens_sharpe) - np.mean(trad_sharpe),
            'sortino_improvement': np.mean(ens_sortino) - np.mean(trad_sortino),
            'risk_adjusted_superiority': (np.mean(ens_sharpe) - np.mean(trad_sharpe)) > 0.1
        }
    
    def _evaluate_practical_significance(self) -> Dict:
        """å®Ÿç”¨æ€§è©•ä¾¡"""
        # å®Ÿéš›ã®ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ã®æ„ç¾©ã‚’è©•ä¾¡
        improvements = {}
        
        if 'basic_comparison' in self.comparison_results:
            for metric, result in self.comparison_results['basic_comparison'].items():
                improvements[metric] = result.improvement_pct
        
        # å®Ÿç”¨çš„ãªæ”¹å–„é–¾å€¤
        practical_thresholds = {
            'total_return': 2.0,  # 2%ä»¥ä¸Šã®æ”¹å–„
            'sharpe_ratio': 5.0,  # 5%ä»¥ä¸Šã®æ”¹å–„
            'win_rate': 3.0,      # 3%ä»¥ä¸Šã®æ”¹å–„
            'max_drawdown': -10.0  # 10%ä»¥ä¸Šã®ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³å‰Šæ¸›
        }
        
        practical_improvements = {}
        for metric, threshold in practical_thresholds.items():
            if metric in improvements:
                improvement = improvements[metric]
                if metric == 'max_drawdown':
                    practical_improvements[metric] = improvement < threshold
                else:
                    practical_improvements[metric] = improvement > threshold
        
        return {
            'practical_improvements': practical_improvements,
            'overall_practical_significance': sum(practical_improvements.values()) >= 2,
            'deployment_recommendation': 'Recommended' if sum(practical_improvements.values()) >= 3 else
                                       'Consider' if sum(practical_improvements.values()) >= 2 else
                                       'Not recommended'
        }
    
    def _generate_recommendations(self) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        if hasattr(self, 'comparison_results') and 'basic_comparison' in self.comparison_results:
            basic_results = self.comparison_results['basic_comparison']
            
            # ãƒªã‚¿ãƒ¼ãƒ³æ”¹å–„
            if 'total_return' in basic_results and basic_results['total_return'].improvement_pct > 5:
                recommendations.append("Strong return improvement observed - proceed with deployment")
            
            # ãƒªã‚¹ã‚¯å‰Šæ¸›
            if 'max_drawdown' in basic_results and basic_results['max_drawdown'].improvement < 0:
                recommendations.append("Reduced drawdown risk - improves portfolio stability")
            
            # å‹ç‡å‘ä¸Š
            if 'win_rate' in basic_results and basic_results['win_rate'].improvement_pct > 3:
                recommendations.append("Improved win rate supports trading psychology")
            
            # ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª
            if 'sharpe_ratio' in basic_results and basic_results['sharpe_ratio'].improvement_pct > 10:
                recommendations.append("Excellent risk-adjusted return improvement")
        
        if not recommendations:
            recommendations.append("Monitor performance over longer period for conclusive results")
        
        return recommendations
    
    def _get_comparison_period(self) -> Dict:
        """æ¯”è¼ƒæœŸé–“å–å¾—"""
        if self.traditional_data and self.ensemble_data:
            return {
                'start_date': min(m.period_start for m in self.traditional_data + self.ensemble_data).isoformat(),
                'end_date': max(m.period_end for m in self.traditional_data + self.ensemble_data).isoformat(),
                'total_days': (max(m.period_end for m in self.traditional_data + self.ensemble_data) - 
                              min(m.period_start for m in self.traditional_data + self.ensemble_data)).days
            }
        return {}
    
    def generate_detailed_report(self) -> str:
        """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if not self.comparison_results:
            return "No comparison results available"
        
        report_lines = []
        report_lines.append("ğŸ” ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ vs å¾“æ¥æ‰‹æ³• - è©³ç´°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ")
        report_lines.append("=" * 80)
        
        # å®Ÿè¡Œã‚µãƒãƒªãƒ¼
        metadata = self.comparison_results.get('metadata', {})
        report_lines.append(f"\nğŸ“Š åˆ†ææ¦‚è¦:")
        report_lines.append(f"  æ¯”è¼ƒå®Ÿè¡Œæ—¥æ™‚: {metadata.get('comparison_date', 'N/A')}")
        report_lines.append(f"  å¾“æ¥æ‰‹æ³•ã‚µãƒ³ãƒ—ãƒ«æ•°: {metadata.get('traditional_sample_size', 0)}")
        report_lines.append(f"  ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚µãƒ³ãƒ—ãƒ«æ•°: {metadata.get('ensemble_sample_size', 0)}")
        
        # åŸºæœ¬æ¯”è¼ƒçµæœ
        if 'basic_comparison' in self.comparison_results:
            report_lines.append(f"\nğŸ“ˆ åŸºæœ¬ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ:")
            for metric, result in self.comparison_results['basic_comparison'].items():
                report_lines.append(f"  {metric}:")
                report_lines.append(f"    å¾“æ¥æ‰‹æ³•: {result.traditional_value:.4f}")
                report_lines.append(f"    ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: {result.ensemble_value:.4f}")
                report_lines.append(f"    æ”¹å–„åŠ¹æœ: {result.improvement_pct:+.2f}% (åŠ¹æœã‚µã‚¤ã‚º: {result.effect_size:.3f})")
                report_lines.append(f"    æœ‰æ„æ€§: p={result.statistical_significance:.4f}")
                report_lines.append(f"    è§£é‡ˆ: {result.interpretation}")
        
        # ç·åˆè©•ä¾¡
        if 'overall_assessment' in self.comparison_results:
            assessment = self.comparison_results['overall_assessment']
            report_lines.append(f"\nğŸ† ç·åˆè©•ä¾¡:")
            
            if 'overall_score' in assessment:
                score_info = assessment['overall_score']
                report_lines.append(f"  ç·åˆæ”¹å–„ã‚¹ã‚³ã‚¢: {score_info.get('weighted_improvement_score', 0):.2f}")
                report_lines.append(f"  è©•ä¾¡: {score_info.get('score_interpretation', 'N/A')}")
            
            if 'practical_significance' in assessment:
                practical = assessment['practical_significance']
                report_lines.append(f"  å®Ÿç”¨æ€§è©•ä¾¡: {practical.get('deployment_recommendation', 'N/A')}")
            
            if 'recommendations' in assessment:
                report_lines.append(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
                for rec in assessment['recommendations']:
                    report_lines.append(f"  â€¢ {rec}")
        
        # MLç‰¹æœ‰åˆ†æ
        if 'ml_analysis' in self.comparison_results:
            ml_analysis = self.comparison_results['ml_analysis']
            report_lines.append(f"\nğŸ¤– MLç‰¹æœ‰åˆ†æ:")
            
            if 'prediction_accuracy' in ml_analysis:
                acc = ml_analysis['prediction_accuracy']
                report_lines.append(f"  äºˆæ¸¬ç²¾åº¦æ”¹å–„: {acc.get('accuracy_improvement', 0):.3f}")
                report_lines.append(f"  ç²¾åº¦å®‰å®šæ€§æ”¹å–„: {acc.get('accuracy_consistency_improvement', 0):.3f}")
        
        report_lines.append(f"\n" + "=" * 80)
        
        return "\n".join(report_lines)
    
    def save_results(self, filename_prefix: str = "performance_comparison") -> Path:
        """çµæœä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONçµæœä¿å­˜
        json_file = self.output_dir / f"{filename_prefix}_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            # DataClassã‚’è¾æ›¸ã«å¤‰æ›ã—ã¦ä¿å­˜
            serializable_results = self._make_serializable(self.comparison_results)
            import json
            json.dump(serializable_results, f, indent=2, ensure_ascii=False, default=str)
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_file = self.output_dir / f"{filename_prefix}_report_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(self.generate_detailed_report())
        
        logger.info(f"Results saved to: {json_file}")
        logger.info(f"Report saved to: {report_file}")
        
        return self.output_dir
    
    def _make_serializable(self, obj):
        """JSONåºåˆ—åŒ–å¯èƒ½ãªå½¢å¼ã«å¤‰æ›"""
        if hasattr(obj, '__dict__'):
            return {k: self._make_serializable(v) for k, v in obj.__dict__.items()}
        elif isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ” ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ vs å¾“æ¥æ‰‹æ³• - è©³ç´°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 70)
    
    try:
        # æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        comparison_system = PerformanceComparisonSystem()
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ‡ãƒ¢å®Ÿè¡Œ
        print("\nğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸæ¯”è¼ƒåˆ†æå®Ÿè¡Œä¸­...")
        comparison_system._generate_sample_data()
        
        # åŒ…æ‹¬çš„æ¯”è¼ƒå®Ÿè¡Œ
        results = comparison_system.run_comprehensive_comparison()
        
        # çµæœè¡¨ç¤º
        report = comparison_system.generate_detailed_report()
        print(report)
        
        # çµæœä¿å­˜
        output_path = comparison_system.save_results()
        print(f"\nğŸ’¾ è©³ç´°çµæœä¿å­˜å®Œäº†: {output_path}")
        
        print("\nâœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒåˆ†æå®Œäº†")
        
    except Exception as e:
        logger.error(f"Performance comparison failed: {e}")
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    main()