#!/usr/bin/env python3
"""
Walk-Forward Validation ã‚¹ã‚¯ãƒªãƒ—ãƒˆ - Phase 61

éå­¦ç¿’ã‚’æ’é™¤ã—ãŸãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã€‚
ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ–¹å¼ã§è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã‚’åˆ†é›¢ã—ã€
çœŸã®MLäºˆæ¸¬åŠ›ã‚’è©•ä¾¡ã™ã‚‹ã€‚

ä½¿ç”¨æ–¹æ³•:
    # å®Ÿè¡Œ
    python scripts/backtest/walk_forward_validation.py

    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ç¢ºèªã®ã¿ï¼‰
    python scripts/backtest/walk_forward_validation.py --dry-run

    # è©³ç´°ãƒ­ã‚°
    python scripts/backtest/walk_forward_validation.py --verbose

    # CIã®æœ€æ–°Walk-Forwardçµæœã‚’å–å¾—ã—ã¦åˆ†æ
    python scripts/backtest/walk_forward_validation.py --from-ci
"""

import argparse
import asyncio
import json
import logging
import os
import pickle
import shutil
import sys
import tempfile
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import yaml

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.logger import get_logger


class WalkForwardValidator:
    """
    Walk-Forwardæ¤œè¨¼ã‚¯ãƒ©ã‚¹

    ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ–¹å¼ã§ä»¥ä¸‹ã‚’å®Ÿè¡Œ:
    1. è¨“ç·´æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã§MLãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
    2. ãƒ†ã‚¹ãƒˆæœŸé–“ã§ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    3. å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®çµæœã‚’é›†è¨ˆãƒ»è©•ä¾¡
    """

    def __init__(self, config_path: str = None, verbose: bool = False):
        """
        åˆæœŸåŒ–

        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            verbose: è©³ç´°ãƒ­ã‚°å‡ºåŠ›
        """
        self.logger = get_logger(__name__)
        self.verbose = verbose

        if verbose:
            logging.getLogger().setLevel(logging.DEBUG)

        # è¨­å®šèª­ã¿è¾¼ã¿
        if config_path is None:
            config_path = project_root / "config/core/wf_config.yaml"
        self.config = self._load_config(config_path)

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦è¨­å®š
        wf_config = self.config.get("walk_forward", {})
        self.train_days = wf_config.get("train_days", 60)
        self.test_days = wf_config.get("test_days", 30)
        self.step_days = wf_config.get("step_days", 30)

        # ãƒ‡ãƒ¼ã‚¿æœŸé–“
        self.data_start = datetime.strptime(wf_config.get("data_start", "2025-07-01"), "%Y-%m-%d")
        self.data_end = datetime.strptime(wf_config.get("data_end", "2025-12-31"), "%Y-%m-%d")

        # MLè¨­å®š
        ml_config = wf_config.get("ml_training", {})
        self.n_classes = ml_config.get("n_classes", 3)
        self.use_smote = ml_config.get("use_smote", True)
        self.optimize = ml_config.get("optimize", False)
        self.n_trials = ml_config.get("n_trials", 10)
        self.target_threshold = ml_config.get("target_threshold", 0.0005)

        # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨­å®š
        bt_config = wf_config.get("backtest", {})
        self.symbol = bt_config.get("symbol", "BTC/JPY")
        self.timeframes = bt_config.get("timeframes", ["15m", "4h"])
        self.initial_balance = bt_config.get("initial_balance", 500000)

        # å‡ºåŠ›è¨­å®š
        output_config = wf_config.get("output", {})
        self.output_dir = Path(output_config.get("dir", "docs/æ¤œè¨¼è¨˜éŒ²"))
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # å®‰å®šæ€§é–¾å€¤
        stability = wf_config.get("stability_thresholds", {})
        self.min_pf = stability.get("min_pf", 1.0)
        self.max_pf_std = stability.get("max_pf_std", 0.3)
        self.min_win_rate = stability.get("min_win_rate", 0.45)
        self.max_wf_vs_full_diff = stability.get("max_wf_vs_full_diff", 0.20)

        # çµæœæ ¼ç´
        self.window_results: List[Dict] = []
        self.full_backtest_result: Optional[Dict] = None

        self.logger.info("=" * 60)
        self.logger.info("Walk-Forward Validation åˆæœŸåŒ–å®Œäº†")
        self.logger.info(f"  è¨“ç·´æœŸé–“: {self.train_days}æ—¥")
        self.logger.info(f"  ãƒ†ã‚¹ãƒˆæœŸé–“: {self.test_days}æ—¥")
        self.logger.info(f"  ã‚¹ãƒ†ãƒƒãƒ—: {self.step_days}æ—¥")
        self.logger.info(f"  ãƒ‡ãƒ¼ã‚¿æœŸé–“: {self.data_start.date()} ~ {self.data_end.date()}")
        self.logger.info("=" * 60)

    def _load_config(self, config_path: str) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            self.logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def generate_windows(self) -> List[Dict]:
        """
        ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ç”Ÿæˆ

        Returns:
            ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒªã‚¹ãƒˆ [{"window_id": 1, "train_start": ..., "train_end": ..., "test_start": ..., "test_end": ...}, ...]
        """
        windows = []
        window_id = 1

        # æœ€åˆã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é–‹å§‹ä½ç½®
        train_start = self.data_start

        while True:
            train_end = train_start + timedelta(days=self.train_days)
            test_start = train_end + timedelta(days=1)
            test_end = test_start + timedelta(days=self.test_days - 1)

            # ãƒ‡ãƒ¼ã‚¿æœŸé–“ã‚’è¶…ãˆãŸã‚‰çµ‚äº†
            if test_end > self.data_end:
                break

            windows.append(
                {
                    "window_id": window_id,
                    "train_start": train_start,
                    "train_end": train_end,
                    "test_start": test_start,
                    "test_end": test_end,
                }
            )

            self.logger.info(
                f"Window {window_id}: "
                f"Train {train_start.date()} ~ {train_end.date()} | "
                f"Test {test_start.date()} ~ {test_end.date()}"
            )

            # æ¬¡ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã¸
            train_start = train_start + timedelta(days=self.step_days)
            window_id += 1

        self.logger.info(f"åˆè¨ˆ {len(windows)} ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ç”Ÿæˆ")
        return windows

    async def run_window(self, window: Dict) -> Dict:
        """
        å˜ä¸€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

        Args:
            window: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æƒ…å ±

        Returns:
            ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦çµæœ
        """
        window_id = window["window_id"]
        self.logger.info("=" * 60)
        self.logger.info(f"Window {window_id} é–‹å§‹")
        self.logger.info("=" * 60)

        result = {
            "window_id": window_id,
            "train_period": {
                "start": window["train_start"].isoformat(),
                "end": window["train_end"].isoformat(),
            },
            "test_period": {
                "start": window["test_start"].isoformat(),
                "end": window["test_end"].isoformat(),
            },
            "ml_training": {},
            "backtest": {},
            "status": "pending",
        }

        try:
            # Step 1: MLãƒ¢ãƒ‡ãƒ«è¨“ç·´
            self.logger.info(f"[Window {window_id}] Step 1: MLãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹")
            ml_result = await self._train_ml_model(window)
            result["ml_training"] = ml_result

            if ml_result.get("status") != "success":
                result["status"] = "ml_training_failed"
                return result

            # Step 2: ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            self.logger.info(f"[Window {window_id}] Step 2: ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–‹å§‹")
            bt_result = await self._run_backtest(window)
            result["backtest"] = bt_result

            if bt_result.get("status") != "success":
                result["status"] = "backtest_failed"
                return result

            result["status"] = "success"
            self.logger.info(
                f"[Window {window_id}] å®Œäº†: PF={bt_result.get('profit_factor', 'N/A')}"
            )

        except Exception as e:
            self.logger.error(f"[Window {window_id}] ã‚¨ãƒ©ãƒ¼: {e}")
            result["status"] = "error"
            result["error"] = str(e)

        return result

    async def _train_ml_model(self, window: Dict) -> Dict:
        """
        MLãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ¥ï¼‰

        Args:
            window: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æƒ…å ±

        Returns:
            è¨“ç·´çµæœ
        """
        window_id = window["window_id"]
        train_start = window["train_start"]
        train_end = window["train_end"]

        result = {
            "status": "pending",
            "train_period": f"{train_start.date()} ~ {train_end.date()}",
        }

        try:
            # NewSystemMLModelCreatorã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            from scripts.ml.create_ml_models import NewSystemMLModelCreator

            # è¨“ç·´æ—¥æ•°è¨ˆç®—
            train_days = (train_end - train_start).days

            self.logger.info(
                f"  è¨“ç·´æœŸé–“: {train_days}æ—¥ ({train_start.date()} ~ {train_end.date()})"
            )

            # ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            creator = NewSystemMLModelCreator(
                verbose=self.verbose,
                target_threshold=self.target_threshold,
                n_classes=self.n_classes,
                use_smote=self.use_smote,
                optimize=self.optimize,
                n_trials=self.n_trials,
                models_to_train=["full"],  # fullãƒ¢ãƒ‡ãƒ«ã®ã¿
                stacking=False,
            )

            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆæœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ã¯creatorå†…éƒ¨ã§å®Ÿæ–½ï¼‰
            # ä¸€æ™‚çš„ã«thresholds.yamlã®æ—¥ä»˜ã‚’ä¸Šæ›¸ã
            original_thresholds = self._backup_thresholds()
            self._set_training_period(train_start, train_end)

            try:
                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Ÿè¡Œ
                # Note: days=365ã§å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆcutoff_dateãƒ•ã‚£ãƒ«ã‚¿å¯¾ç­–ï¼‰
                # å®Ÿéš›ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯ä¸‹ã®maskã§è¡Œã†
                features, target = await creator.prepare_training_data_async(days=365)

                # æœŸé–“ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if hasattr(features, "index") and len(features) > 0:
                    mask = (features.index >= train_start) & (features.index <= train_end)
                    features = features[mask]
                    target = target[mask]

                self.logger.info(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(features)}ã‚µãƒ³ãƒ—ãƒ«")

                if len(features) < 100:
                    self.logger.warning(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {len(features)}ã‚µãƒ³ãƒ—ãƒ«")
                    result["status"] = "insufficient_data"
                    return result

                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                training_result = creator.train_models(features, target)

                # ä¸€æ™‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆWindowåˆ¥ï¼‰
                temp_model_path = Path(f"models/walk_forward/window_{window_id}")
                temp_model_path.mkdir(parents=True, exist_ok=True)

                # ProductionEnsembleã¨ã—ã¦ä¿å­˜
                ensemble = creator._create_ensemble(creator.models)
                ensemble_path = temp_model_path / "ensemble_full.pkl"
                with open(ensemble_path, "wb") as f:
                    pickle.dump(ensemble, f)

                result["status"] = "success"
                result["model_path"] = str(ensemble_path)
                result["samples"] = len(features)
                result["metrics"] = training_result.get("evaluation", {})

            finally:
                # thresholds.yamlå¾©å…ƒ
                self._restore_thresholds(original_thresholds)

        except Exception as e:
            self.logger.error(f"  MLè¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback

            self.logger.error(traceback.format_exc())
            result["status"] = "error"
            result["error"] = str(e)

        return result

    async def _run_backtest(self, window: Dict) -> Dict:
        """
        ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ¥ï¼‰

        Args:
            window: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æƒ…å ±

        Returns:
            ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœ
        """
        window_id = window["window_id"]
        test_start = window["test_start"]
        test_end = window["test_end"]

        result = {
            "status": "pending",
            "test_period": f"{test_start.date()} ~ {test_end.date()}",
        }

        try:
            # Windowåˆ¥ãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
            temp_model_path = Path(f"models/walk_forward/window_{window_id}/ensemble_full.pkl")
            prod_model_path = Path("models/production/ensemble_full.pkl")

            if not temp_model_path.exists():
                result["status"] = "model_not_found"
                return result

            # æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            prod_backup_path = Path("models/production/ensemble_full.pkl.backup")
            if prod_model_path.exists():
                shutil.copy(prod_model_path, prod_backup_path)

            # Windowåˆ¥ãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªã«ã‚³ãƒ”ãƒ¼
            shutil.copy(temp_model_path, prod_model_path)

            try:
                # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæœŸé–“è¨­å®š
                self._set_backtest_period(test_start, test_end)

                # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆmain.py --mode backtestç›¸å½“ï¼‰
                # Phase 61.1: ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰è¨­å®šã‚’æ­£ã—ãè¡Œã†
                from src.core.config import (
                    load_config,
                    set_backtest_log_level,
                    set_backtest_mode,
                )
                from src.core.orchestration import create_trading_orchestrator

                # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰è¨­å®šï¼ˆmain.pyã¨åŒã˜æ–¹æ³•ï¼‰
                os.environ["LOG_LEVEL"] = "WARNING"
                os.environ["BACKTEST_MODE"] = "true"
                set_backtest_mode(True)
                set_backtest_log_level("WARNING")

                # cmdline_mode="backtest"ã§config.mode=backtestã«ãªã‚‹
                config = load_config("config/core/unified.yaml", cmdline_mode="backtest")
                orchestrator = await create_trading_orchestrator(config=config, logger=self.logger)

                # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                success = await orchestrator.runner.run()

                if success:
                    # çµæœå–å¾—
                    bt_result = self._extract_backtest_result(window_id)
                    result.update(bt_result)
                    result["status"] = "success"
                else:
                    result["status"] = "backtest_failed"

            finally:
                # æœ¬ç•ªãƒ¢ãƒ‡ãƒ«å¾©å…ƒ
                if prod_backup_path.exists():
                    shutil.copy(prod_backup_path, prod_model_path)
                    prod_backup_path.unlink()

        except Exception as e:
            self.logger.error(f"  ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            import traceback

            self.logger.error(traceback.format_exc())
            result["status"] = "error"
            result["error"] = str(e)

        return result

    def _extract_backtest_result(self, window_id: int) -> Dict:
        """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã‚’æŠ½å‡º"""
        result = {}

        try:
            # æœ€æ–°ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœJSONã‚’å–å¾—
            log_dir = Path("src/backtest/logs")
            json_files = sorted(
                log_dir.glob("backtest_*.json"), key=lambda x: x.stat().st_mtime, reverse=True
            )

            if json_files:
                with open(json_files[0], "r", encoding="utf-8") as f:
                    bt_data = json.load(f)

                # ä¸»è¦æŒ‡æ¨™ã‚’æŠ½å‡º
                summary = bt_data.get("summary", {})
                result["total_trades"] = summary.get("total_trades", 0)
                result["win_rate"] = summary.get("win_rate", 0)
                result["profit_factor"] = summary.get("profit_factor", 0)
                result["total_pnl"] = summary.get("total_pnl", 0)
                result["max_drawdown"] = summary.get("max_drawdown_pct", 0)

        except Exception as e:
            self.logger.warning(f"ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        return result

    def _backup_thresholds(self) -> Dict:
        """thresholds.yamlã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        try:
            with open("config/core/thresholds.yaml", "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception:
            return {}

    def _restore_thresholds(self, original: Dict):
        """thresholds.yamlã®å¾©å…ƒ"""
        try:
            with open("config/core/thresholds.yaml", "w", encoding="utf-8") as f:
                yaml.dump(original, f, default_flow_style=False, allow_unicode=True)
        except Exception as e:
            self.logger.warning(f"thresholds.yamlå¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")

    def _set_training_period(self, start: datetime, end: datetime):
        """è¨“ç·´æœŸé–“ã®è¨­å®šï¼ˆthresholds.yamlæ›´æ–°ï¼‰"""
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿åé›†ã®ãŸã‚ã«thresholds.yamlã‚’æ›´æ–°ã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆ
        pass  # NewSystemMLModelCreatorãŒå†…éƒ¨ã§å‡¦ç†

    def _set_backtest_period(self, start: datetime, end: datetime):
        """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæœŸé–“ã®è¨­å®š"""
        try:
            with open("config/core/thresholds.yaml", "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)

            if "execution" not in config:
                config["execution"] = {}

            config["execution"]["backtest_use_fixed_dates"] = True
            config["execution"]["backtest_start_date"] = start.strftime("%Y-%m-%d")
            config["execution"]["backtest_end_date"] = end.strftime("%Y-%m-%d")

            with open("config/core/thresholds.yaml", "w", encoding="utf-8") as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)

        except Exception as e:
            self.logger.error(f"ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæœŸé–“è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")

    async def run_all(self) -> Dict:
        """
        å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å®Ÿè¡Œãƒ»é›†è¨ˆ

        Returns:
            é›†è¨ˆçµæœ
        """
        self.logger.info("=" * 60)
        self.logger.info("Walk-Forward Validation é–‹å§‹")
        self.logger.info("=" * 60)

        start_time = datetime.now()

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ç”Ÿæˆ
        windows = self.generate_windows()

        if not windows:
            self.logger.error("ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return {"status": "error", "message": "No windows generated"}

        # æœ¬ç•ªthresholds.yamlãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        original_thresholds = self._backup_thresholds()

        try:
            # å„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’é †æ¬¡å®Ÿè¡Œ
            for window in windows:
                result = await self.run_window(window)
                self.window_results.append(result)

                # é€²æ—è¡¨ç¤º
                completed = len(self.window_results)
                total = len(windows)
                self.logger.info(f"é€²æ—: {completed}/{total} ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å®Œäº†")

        finally:
            # thresholds.yamlå¾©å…ƒ
            self._restore_thresholds(original_thresholds)

        # é›†è¨ˆãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        elapsed = datetime.now() - start_time
        self.logger.info(f"å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å®Œäº†: å®Ÿè¡Œæ™‚é–“ {elapsed}")

        aggregate_result = self._aggregate_results()
        self._generate_report(aggregate_result)

        return aggregate_result

    def _aggregate_results(self) -> Dict:
        """çµæœé›†è¨ˆ"""
        successful_windows = [
            w
            for w in self.window_results
            if w.get("status") == "success" and w.get("backtest", {}).get("profit_factor", 0) > 0
        ]

        if not successful_windows:
            return {
                "status": "no_successful_windows",
                "window_results": self.window_results,
            }

        # PFçµ±è¨ˆ
        pfs = [w["backtest"]["profit_factor"] for w in successful_windows]
        win_rates = [w["backtest"]["win_rate"] for w in successful_windows]
        pnls = [w["backtest"]["total_pnl"] for w in successful_windows]

        aggregate = {
            "status": "success",
            "execution_time": datetime.now().isoformat(),
            "total_windows": len(self.window_results),
            "successful_windows": len(successful_windows),
            "failed_windows": len(self.window_results) - len(successful_windows),
            "aggregate_metrics": {
                "mean_pf": float(np.mean(pfs)),
                "std_pf": float(np.std(pfs)),
                "min_pf": float(np.min(pfs)),
                "max_pf": float(np.max(pfs)),
                "mean_win_rate": float(np.mean(win_rates)),
                "std_win_rate": float(np.std(win_rates)),
                "total_pnl": float(np.sum(pnls)),
                "mean_pnl_per_window": float(np.mean(pnls)),
            },
            "stability_assessment": self._assess_stability(pfs, win_rates),
            "window_details": self.window_results,
        }

        return aggregate

    def _assess_stability(self, pfs: List[float], win_rates: List[float]) -> Dict:
        """å®‰å®šæ€§è©•ä¾¡"""
        std_pf = np.std(pfs)
        all_positive = all(pf > self.min_pf for pf in pfs)
        low_variance = std_pf < self.max_pf_std

        # å®‰å®šæ€§åˆ¤å®š
        if all_positive and low_variance:
            stability = "stable"
            risk = "low"
        elif all_positive or low_variance:
            stability = "moderate"
            risk = "medium"
        else:
            stability = "unstable"
            risk = "high"

        return {
            "is_stable": stability == "stable",
            "stability_level": stability,
            "overfitting_risk": risk,
            "all_windows_profitable": all_positive,
            "low_variance": low_variance,
            "criteria": {
                "min_pf_threshold": self.min_pf,
                "max_std_threshold": self.max_pf_std,
            },
        }

    def _generate_report(self, aggregate: Dict):
        """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # JSONä¿å­˜
        json_path = self.output_dir / f"walk_forward_{timestamp}.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(aggregate, f, indent=2, ensure_ascii=False, default=str)
        self.logger.info(f"JSONãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {json_path}")

        # Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        md_path = self.output_dir / f"walk_forward_{timestamp}.md"
        md_content = self._generate_markdown_report(aggregate)
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(md_content)
        self.logger.info(f"Markdownãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {md_path}")

    def _generate_markdown_report(self, aggregate: Dict) -> str:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        metrics = aggregate.get("aggregate_metrics", {})
        stability = aggregate.get("stability_assessment", {})

        md = f"""# Walk-Forward Validation ãƒ¬ãƒãƒ¼ãƒˆ

**å®Ÿè¡Œæ—¥æ™‚**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

---

## ã‚µãƒãƒªãƒ¼

| é …ç›® | å€¤ |
|------|-----|
| ç·ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ•° | {aggregate.get('total_windows', 0)} |
| æˆåŠŸã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ | {aggregate.get('successful_windows', 0)} |
| å¤±æ•—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ | {aggregate.get('failed_windows', 0)} |

---

## é›†è¨ˆæŒ‡æ¨™

| æŒ‡æ¨™ | å€¤ |
|------|-----|
| å¹³å‡PF | {metrics.get('mean_pf', 0):.2f} |
| PFæ¨™æº–åå·® | {metrics.get('std_pf', 0):.3f} |
| æœ€å°PF | {metrics.get('min_pf', 0):.2f} |
| æœ€å¤§PF | {metrics.get('max_pf', 0):.2f} |
| å¹³å‡å‹ç‡ | {metrics.get('mean_win_rate', 0) * 100:.1f}% |
| ç·æç›Š | Â¥{metrics.get('total_pnl', 0):,.0f} |

---

## å®‰å®šæ€§è©•ä¾¡

| é …ç›® | çµæœ |
|------|------|
| å®‰å®šæ€§ãƒ¬ãƒ™ãƒ« | {stability.get('stability_level', 'N/A')} |
| éå­¦ç¿’ãƒªã‚¹ã‚¯ | {stability.get('overfitting_risk', 'N/A')} |
| å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é»’å­— | {'Yes' if stability.get('all_windows_profitable') else 'No'} |
| ä½åˆ†æ•£ | {'Yes' if stability.get('low_variance') else 'No'} |

---

## ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ¥çµæœ

| Window | è¨“ç·´æœŸé–“ | ãƒ†ã‚¹ãƒˆæœŸé–“ | PF | å‹ç‡ | æç›Š | çŠ¶æ…‹ |
|--------|---------|----------|-----|------|------|------|
"""
        for w in aggregate.get("window_details", []):
            train_period = w.get("train_period", {})
            test_period = w.get("test_period", {})
            bt = w.get("backtest", {})

            train_str = f"{train_period.get('start', '')[:10]} ~ {train_period.get('end', '')[:10]}"
            test_str = f"{test_period.get('start', '')[:10]} ~ {test_period.get('end', '')[:10]}"

            md += f"| {w.get('window_id', 'N/A')} | {train_str} | {test_str} | {bt.get('profit_factor', 0):.2f} | {bt.get('win_rate', 0) * 100:.1f}% | Â¥{bt.get('total_pnl', 0):,.0f} | {w.get('status', 'N/A')} |\n"

        md += f"""
---

## è©•ä¾¡åŸºæº–

| æŒ‡æ¨™ | è‰¯å¥½ | æ³¨æ„ | å±é™º |
|------|------|------|------|
| PFæ¨™æº–åå·® | < 0.2 | 0.2-0.4 | > 0.4 |
| å…¨Window PF | å…¨ã¦ > 1.0 | 1ã¤ < 1.0 | è¤‡æ•° < 1.0 |
| WF vs Fullå·® | < 10% | 10-20% | > 20% |

---

**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().isoformat()}
"""
        return md

    def dry_run(self):
        """ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ç¢ºèªã®ã¿ï¼‰"""
        self.logger.info("=" * 60)
        self.logger.info("Walk-Forward Validation ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³")
        self.logger.info("=" * 60)

        windows = self.generate_windows()

        self.logger.info("")
        self.logger.info("ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ§‹æˆ:")
        for w in windows:
            self.logger.info(
                f"  Window {w['window_id']}: "
                f"Train {w['train_start'].date()} ~ {w['train_end'].date()} | "
                f"Test {w['test_start'].date()} ~ {w['test_end'].date()}"
            )

        self.logger.info("")
        self.logger.info(f"åˆè¨ˆ: {len(windows)} ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦")
        self.logger.info(f"äºˆæƒ³å®Ÿè¡Œæ™‚é–“: {len(windows) * 30}åˆ† ~ {len(windows) * 60}åˆ†")


class CIIntegration:
    """GitHub Actions CIé€£æºã‚¯ãƒ©ã‚¹"""

    WORKFLOW_NAME = "walk_forward.yml"
    ARTIFACT_NAME = "walk-forward-results"
    DOWNLOAD_DIR = Path("docs/æ¤œè¨¼è¨˜éŒ²/ci_downloads/walk_forward")

    @classmethod
    def fetch_latest_result(cls) -> Tuple[Optional[str], Optional[str]]:
        """
        æœ€æ–°ã®CI Walk-Forwardçµæœã‚’å–å¾—

        Returns:
            (json_path, run_info): JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¨å®Ÿè¡Œæƒ…å ±ã®ã‚¿ãƒ—ãƒ«
            å¤±æ•—æ™‚ã¯ (None, error_message)
        """
        import subprocess

        print("ğŸ” CIæœ€æ–°Walk-Forwardçµæœã‚’æ¤œç´¢ä¸­...")

        # gh CLIç¢ºèª
        if not cls._check_gh_cli():
            return None, "gh CLI ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“"

        # æœ€æ–°ã®æˆåŠŸã—ãŸå®Ÿè¡Œã‚’å–å¾—
        run_id, run_info = cls._get_latest_successful_run()
        if not run_id:
            return None, run_info

        print(f"âœ… æœ€æ–°å®Ÿè¡Œã‚’æ¤œå‡º: Run ID {run_id}")
        print(f"   {run_info}")

        # artifactãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        json_path = cls._download_artifact(run_id)
        if not json_path:
            return None, "artifactã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ"

        return json_path, run_info

    @classmethod
    def _check_gh_cli(cls) -> bool:
        """gh CLI ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª"""
        import subprocess

        try:
            result = subprocess.run(
                ["gh", "--version"],
                capture_output=True,
                text=True,
                timeout=10,
            )
            return result.returncode == 0
        except (subprocess.SubprocessError, FileNotFoundError):
            return False

    @classmethod
    def _get_latest_successful_run(cls) -> Tuple[Optional[str], str]:
        """æœ€æ–°ã®æˆåŠŸã—ãŸå®Ÿè¡Œã‚’å–å¾—"""
        import subprocess

        try:
            result = subprocess.run(
                [
                    "gh",
                    "run",
                    "list",
                    "--workflow",
                    cls.WORKFLOW_NAME,
                    "--status",
                    "success",
                    "--limit",
                    "1",
                    "--json",
                    "databaseId,createdAt,displayTitle",
                ],
                capture_output=True,
                text=True,
                timeout=30,
            )

            if result.returncode != 0:
                return None, f"gh run list å¤±æ•—: {result.stderr}"

            runs = json.loads(result.stdout)
            if not runs:
                return None, "æˆåŠŸã—ãŸWalk-Forwardå®Ÿè¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"

            run = runs[0]
            run_id = str(run["databaseId"])
            created_at = run["createdAt"]
            title = run.get("displayTitle", "Walk-Forward Validation")

            return run_id, f"å®Ÿè¡Œæ—¥æ™‚: {created_at}, ã‚¿ã‚¤ãƒˆãƒ«: {title}"

        except subprocess.TimeoutExpired:
            return None, "gh run list ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ"
        except json.JSONDecodeError:
            return None, "gh run list ã®å‡ºåŠ›ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—"
        except Exception as e:
            return None, f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}"

    @classmethod
    def _download_artifact(cls, run_id: str) -> Optional[str]:
        """artifactã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦JSONãƒ‘ã‚¹ã‚’è¿”ã™"""
        import subprocess

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
        cls.DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)

        # æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªã‚¢
        for f in cls.DOWNLOAD_DIR.glob("*"):
            if f.is_file():
                f.unlink()
            elif f.is_dir():
                shutil.rmtree(f)

        print(f"ğŸ“¥ artifact ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ (Run ID: {run_id})...")

        try:
            result = subprocess.run(
                [
                    "gh",
                    "run",
                    "download",
                    run_id,
                    "--name",
                    cls.ARTIFACT_NAME,
                    "--dir",
                    str(cls.DOWNLOAD_DIR),
                ],
                capture_output=True,
                text=True,
                timeout=120,
            )

            if result.returncode != 0:
                print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {result.stderr}")
                return None

            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            json_files = list(cls.DOWNLOAD_DIR.glob("**/walk_forward_*.json"))
            if not json_files:
                print("âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None

            # æœ€æ–°ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
            json_path = max(json_files, key=lambda p: p.stat().st_mtime)
            print(f"âœ… JSONãƒ•ã‚¡ã‚¤ãƒ«å–å¾—: {json_path}")

            return str(json_path)

        except subprocess.TimeoutExpired:
            print("âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            return None
        except Exception as e:
            print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return None


def analyze_ci_result(json_path: str):
    """CIçµæœã‚’åˆ†æã—ã¦è¡¨ç¤º"""
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    print("\n" + "=" * 60)
    print("ğŸ“Š Walk-Forward Validation åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 60)

    # åŸºæœ¬æƒ…å ±
    print(f"\nå®Ÿè¡Œæ—¥æ™‚: {data.get('execution_time', 'N/A')}")
    print(f"ç·ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ•°: {data.get('total_windows', 0)}")
    print(f"æˆåŠŸã‚¦ã‚£ãƒ³ãƒ‰ã‚¦: {data.get('successful_windows', 0)}")
    print(f"å¤±æ•—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦: {data.get('failed_windows', 0)}")

    # é›†è¨ˆæŒ‡æ¨™
    metrics = data.get("aggregate_metrics", {})
    if metrics:
        print("\nã€é›†è¨ˆæŒ‡æ¨™ã€‘")
        print(f"  å¹³å‡PF: {metrics.get('mean_pf', 0):.2f} (Â±{metrics.get('std_pf', 0):.3f})")
        print(f"  æœ€å°PF: {metrics.get('min_pf', 0):.2f}")
        print(f"  æœ€å¤§PF: {metrics.get('max_pf', 0):.2f}")
        print(f"  å¹³å‡å‹ç‡: {metrics.get('mean_win_rate', 0) * 100:.1f}%")
        print(f"  ç·æç›Š: Â¥{metrics.get('total_pnl', 0):,.0f}")
        print(f"  ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å¹³å‡æç›Š: Â¥{metrics.get('mean_pnl_per_window', 0):,.0f}")

    # å®‰å®šæ€§è©•ä¾¡
    stability = data.get("stability_assessment", {})
    if stability:
        print("\nã€å®‰å®šæ€§è©•ä¾¡ã€‘")
        print(f"  å®‰å®šæ€§ãƒ¬ãƒ™ãƒ«: {stability.get('stability_level', 'N/A')}")
        print(f"  éå­¦ç¿’ãƒªã‚¹ã‚¯: {stability.get('overfitting_risk', 'N/A')}")
        print(f"  å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é»’å­—: {'Yes' if stability.get('all_windows_profitable') else 'No'}")
        print(f"  ä½åˆ†æ•£: {'Yes' if stability.get('low_variance') else 'No'}")

    # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ¥çµæœ
    window_details = data.get("window_details", [])
    if window_details:
        print("\nã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ¥çµæœã€‘")
        print("-" * 60)
        for w in window_details:
            bt = w.get("backtest", {})
            test_period = w.get("test_period", {})
            test_str = f"{test_period.get('start', '')[:10]} ~ {test_period.get('end', '')[:10]}"
            status_emoji = "âœ…" if w.get("status") == "success" else "âŒ"
            print(
                f"  {status_emoji} Window {w.get('window_id', 'N/A')}: "
                f"PF={bt.get('profit_factor', 0):.2f}, "
                f"å‹ç‡={bt.get('win_rate', 0) * 100:.1f}%, "
                f"Â¥{bt.get('total_pnl', 0):+,.0f} "
                f"({test_str})"
            )

    # è©•ä¾¡ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 60)
    print("ğŸ’¡ è©•ä¾¡ã‚µãƒãƒªãƒ¼")
    print("=" * 60)

    if stability.get("is_stable"):
        print("  âœ… å®‰å®šæ€§: è‰¯å¥½ - éå­¦ç¿’ãƒªã‚¹ã‚¯ã¯ä½ã„ã¨è©•ä¾¡")
    elif stability.get("stability_level") == "moderate":
        print("  âš ï¸ å®‰å®šæ€§: ä¸­ç¨‹åº¦ - ä¸€éƒ¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ä¸å®‰å®š")
    else:
        print("  âŒ å®‰å®šæ€§: ä¸å®‰å®š - éå­¦ç¿’ã®å¯èƒ½æ€§ã‚ã‚Š")

    if metrics.get("mean_pf", 0) >= 1.5:
        print("  âœ… åç›Šæ€§: å„ªè‰¯ (å¹³å‡PF â‰¥ 1.5)")
    elif metrics.get("mean_pf", 0) >= 1.0:
        print("  âš ï¸ åç›Šæ€§: è¨±å®¹ç¯„å›² (å¹³å‡PF â‰¥ 1.0)")
    else:
        print("  âŒ åç›Šæ€§: è¦æ”¹å–„ (å¹³å‡PF < 1.0)")

    print("=" * 60 + "\n")


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(description="Walk-Forward Validation - Phase 61")
    parser.add_argument("--dry-run", action="store_true", help="ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ç¢ºèªã®ã¿ï¼‰")
    parser.add_argument("--verbose", "-v", action="store_true", help="è©³ç´°ãƒ­ã‚°å‡ºåŠ›")
    parser.add_argument("--config", type=str, help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    parser.add_argument(
        "--from-ci",
        action="store_true",
        help="CIã®æœ€æ–°Walk-Forwardçµæœã‚’è‡ªå‹•å–å¾—ã—ã¦åˆ†æ",
    )
    parser.add_argument(
        "json_path",
        nargs="?",
        help="åˆ†æã™ã‚‹JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆ--from-ciæ™‚ã¯ä¸è¦ï¼‰",
    )
    args = parser.parse_args()

    # CIé€£æºãƒ¢ãƒ¼ãƒ‰ or ãƒ­ãƒ¼ã‚«ãƒ«JSONåˆ†æãƒ¢ãƒ¼ãƒ‰
    if args.from_ci or args.json_path:
        json_path = args.json_path

        if args.from_ci:
            json_path, run_info = CIIntegration.fetch_latest_result()
            if not json_path:
                print(f"âŒ CIã‹ã‚‰ã®å–å¾—ã«å¤±æ•—: {run_info}")
                sys.exit(1)
            print(f"ğŸ“Š CIå®Ÿè¡Œæƒ…å ±: {run_info}")

        if json_path:
            analyze_ci_result(json_path)
            print("âœ… Walk-Forward åˆ†æå®Œäº†")
        return

    # é€šå¸¸å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
    validator = WalkForwardValidator(config_path=args.config, verbose=args.verbose)

    if args.dry_run:
        validator.dry_run()
    else:
        result = await validator.run_all()

        # æœ€çµ‚çµæœè¡¨ç¤º
        print("\n" + "=" * 60)
        print("Walk-Forward Validation å®Œäº†")
        print("=" * 60)

        if result.get("status") == "success":
            metrics = result.get("aggregate_metrics", {})
            stability = result.get("stability_assessment", {})

            print(f"å¹³å‡PF: {metrics.get('mean_pf', 0):.2f} (Â±{metrics.get('std_pf', 0):.3f})")
            print(f"ç·æç›Š: Â¥{metrics.get('total_pnl', 0):,.0f}")
            print(f"å®‰å®šæ€§: {stability.get('stability_level', 'N/A')}")
            print(f"éå­¦ç¿’ãƒªã‚¹ã‚¯: {stability.get('overfitting_risk', 'N/A')}")
        else:
            print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {result.get('status', 'unknown')}")


if __name__ == "__main__":
    asyncio.run(main())
