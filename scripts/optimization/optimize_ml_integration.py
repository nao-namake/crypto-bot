#!/usr/bin/env python3
"""
Phase 40.3: MLçµ±åˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Optunaã‚’ä½¿ç”¨ã—ã¦MLäºˆæ¸¬ã¨æˆ¦ç•¥ã®çµ±åˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ï¼š
- MLé‡ã¿/æˆ¦ç•¥é‡ã¿: åŠ é‡å¹³å‡ã®æ¯”ç‡
- é«˜ä¿¡é ¼åº¦é–¾å€¤: ãƒœãƒ¼ãƒŠã‚¹/ãƒšãƒŠãƒ«ãƒ†ã‚£é©ç”¨åˆ¤å®š
- ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹/ä¸ä¸€è‡´ãƒšãƒŠãƒ«ãƒ†ã‚£: ä¿¡é ¼åº¦èª¿æ•´å€ç‡
- æœ€å°MLä¿¡é ¼åº¦: MLäºˆæ¸¬è€ƒæ…®é–‹å§‹é–¾å€¤
- holdå¤‰æ›´é–¾å€¤: ä¿¡é ¼åº¦æ¥µä½æ™‚ã®holdå¤‰æ›´åˆ¤å®š

åˆè¨ˆ7ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–

ç›®çš„é–¢æ•°: ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªæœ€å¤§åŒ–
æ¤œè¨¼æ–¹æ³•: Walk-forward testingï¼ˆè¨“ç·´120æ—¥ãƒ»ãƒ†ã‚¹ãƒˆ60æ—¥ï¼‰
"""

import asyncio
import sys
import time
from pathlib import Path
from typing import Any, Dict

import numpy as np
import optuna
from optuna.samplers import TPESampler

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from scripts.optimization.optuna_utils import (
    OptimizationMetrics,
    OptimizationResultManager,
    print_optimization_summary,
)
from src.core.config import (
    clear_runtime_overrides,
    get_runtime_overrides,
    set_runtime_overrides_batch,
)
from src.core.logger import CryptoBotLogger


class MLIntegrationOptimizer:
    """MLçµ±åˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self, logger: CryptoBotLogger):
        """
        åˆæœŸåŒ–

        Args:
            logger: ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ 
        """
        self.logger = logger
        self.result_manager = OptimizationResultManager()
        self.best_sharpe = -np.inf
        self.trial_count = 0

    def objective(self, trial: optuna.Trial) -> float:
        """
        Optunaç›®çš„é–¢æ•°ï¼ˆã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªæœ€å¤§åŒ–ï¼‰

        Args:
            trial: Optuna Trial

        Returns:
            float: ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªï¼ˆæœ€å¤§åŒ–ç›®æ¨™ï¼‰
        """
        self.trial_count += 1

        try:
            # 1. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            params = self._sample_parameters(trial)

            # 2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œè¨¼
            if not self._validate_parameters(params):
                return -10.0  # ç„¡åŠ¹ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ãƒšãƒŠãƒ«ãƒ†ã‚£

            # 3. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰è¨­å®š
            set_runtime_overrides_batch(params)

            # ãƒ‡ãƒãƒƒã‚°: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ç¢ºèª
            if self.trial_count <= 3:
                self.logger.info(f"Trial {self.trial_count} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {len(params)}")

            # 4. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            sharpe_ratio = asyncio.run(self._run_backtest(params))

            # 5. ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã‚¯ãƒªã‚¢
            clear_runtime_overrides()

            # 6. é€²æ—è¡¨ç¤º
            if sharpe_ratio > self.best_sharpe:
                self.best_sharpe = sharpe_ratio
                self.logger.info(f"ğŸ¯ Trial {self.trial_count}: æ–°ãƒ™ã‚¹ãƒˆ ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª={sharpe_ratio:.4f}")

            return sharpe_ratio

        except Exception as e:
            self.logger.error(f"âŒ Trial {self.trial_count} ã‚¨ãƒ©ãƒ¼: {e}")
            clear_runtime_overrides()
            return -10.0  # ãƒšãƒŠãƒ«ãƒ†ã‚£å€¤

    def _sample_parameters(self, trial: optuna.Trial) -> Dict[str, Any]:
        """
        æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ7ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰

        Args:
            trial: Optuna Trial

        Returns:
            Dict: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        params = {}

        # ========================================
        # 1. MLé‡ã¿ãƒ»æˆ¦ç•¥é‡ã¿ï¼ˆåŠ é‡å¹³å‡ã®æ¯”ç‡ï¼‰
        # ========================================

        # MLäºˆæ¸¬ã®é‡ã¿ï¼ˆ10-50%ã®ç¯„å›²ã§èª¿æ•´ï¼‰
        params["ml.strategy_integration.ml_weight"] = trial.suggest_float("ml_weight", 0.1, 0.5, step=0.05)

        # æˆ¦ç•¥ã®é‡ã¿ï¼ˆè‡ªå‹•è¨ˆç®—: 1 - ml_weightï¼‰
        params["ml.strategy_integration.strategy_weight"] = 1.0 - params["ml.strategy_integration.ml_weight"]

        # ========================================
        # 2. é«˜ä¿¡é ¼åº¦é–¾å€¤ï¼ˆãƒœãƒ¼ãƒŠã‚¹/ãƒšãƒŠãƒ«ãƒ†ã‚£é©ç”¨åˆ¤å®šï¼‰
        # ========================================

        # MLé«˜ä¿¡é ¼åº¦é–¾å€¤ï¼ˆ0.7-0.9ã®ç¯„å›²ã§èª¿æ•´ï¼‰
        params["ml.strategy_integration.high_confidence_threshold"] = trial.suggest_float(
            "high_confidence_threshold", 0.7, 0.9, step=0.05
        )

        # ========================================
        # 3. ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹/ä¸ä¸€è‡´ãƒšãƒŠãƒ«ãƒ†ã‚£
        # ========================================

        # ä¸€è‡´æ™‚ãƒœãƒ¼ãƒŠã‚¹å€ç‡ï¼ˆ1.0-1.5ã®ç¯„å›²ã§èª¿æ•´ï¼‰
        params["ml.strategy_integration.agreement_bonus"] = trial.suggest_float("agreement_bonus", 1.0, 1.5, step=0.05)

        # ä¸ä¸€è‡´æ™‚ãƒšãƒŠãƒ«ãƒ†ã‚£å€ç‡ï¼ˆ0.5-0.9ã®ç¯„å›²ã§èª¿æ•´ï¼‰
        params["ml.strategy_integration.disagreement_penalty"] = trial.suggest_float(
            "disagreement_penalty", 0.5, 0.9, step=0.05
        )

        # ========================================
        # 4. æœ€å°MLä¿¡é ¼åº¦ï¼ˆMLäºˆæ¸¬è€ƒæ…®é–‹å§‹é–¾å€¤ï¼‰
        # ========================================

        # MLäºˆæ¸¬ã‚’è€ƒæ…®ã™ã‚‹æœ€å°ä¿¡é ¼åº¦ï¼ˆ0.4-0.8ã®ç¯„å›²ã§èª¿æ•´ï¼‰
        params["ml.strategy_integration.min_ml_confidence"] = trial.suggest_float(
            "min_ml_confidence", 0.4, 0.8, step=0.05
        )

        # ========================================
        # 5. holdå¤‰æ›´é–¾å€¤ï¼ˆä¿¡é ¼åº¦æ¥µä½æ™‚ã®å¤‰æ›´åˆ¤å®šï¼‰
        # ========================================

        # holdå¤‰æ›´é–¾å€¤ï¼ˆ0.3-0.5ã®ç¯„å›²ã§èª¿æ•´ï¼‰
        params["ml.strategy_integration.hold_conversion_threshold"] = trial.suggest_float(
            "hold_conversion_threshold", 0.3, 0.5, step=0.05
        )

        return params

    def get_simple_params(self, trial: optuna.Trial) -> Dict[str, Any]:
        """
        ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµ±åˆç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å½¢å¼ã‚’å–å¾—

        Args:
            trial: Optuna Trial

        Returns:
            Dict: ã‚·ãƒ³ãƒ—ãƒ«ãªã‚­ãƒ¼å½¢å¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆbacktest_integration.pyç”¨ï¼‰
        """
        return {
            "ml_weight": trial.suggest_float("ml_weight", 0.1, 0.5, step=0.05),
            "high_confidence_threshold": trial.suggest_float("high_confidence_threshold", 0.7, 0.9, step=0.05),
            "agreement_bonus": trial.suggest_float("agreement_bonus", 1.0, 1.5, step=0.05),
            "disagreement_penalty": trial.suggest_float("disagreement_penalty", 0.5, 0.9, step=0.05),
            "min_ml_confidence": trial.suggest_float("min_ml_confidence", 0.4, 0.8, step=0.05),
            "hold_conversion_threshold": trial.suggest_float("hold_conversion_threshold", 0.3, 0.5, step=0.05),
        }

    def _validate_parameters(self, params: Dict[str, Any]) -> bool:
        """
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¦¥å½“æ€§æ¤œè¨¼

        Args:
            params: æ¤œè¨¼å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            bool: å¦¥å½“æ€§ï¼ˆTrue: æœ‰åŠ¹, False: ç„¡åŠ¹ï¼‰
        """
        try:
            # ========================================
            # 1. é‡ã¿åˆè¨ˆæ¤œè¨¼ï¼ˆml_weight + strategy_weight = 1.0ï¼‰
            # ========================================
            ml_weight = params.get("ml.strategy_integration.ml_weight", 0.3)
            strategy_weight = params.get("ml.strategy_integration.strategy_weight", 0.7)

            # è¨±å®¹èª¤å·®1e-6ã§1.0ãƒã‚§ãƒƒã‚¯
            if not np.isclose(ml_weight + strategy_weight, 1.0, atol=1e-6):
                self.logger.warning(
                    f"âš ï¸ é‡ã¿åˆè¨ˆã‚¨ãƒ©ãƒ¼: ml_weight({ml_weight}) + strategy_weight({strategy_weight}) != 1.0"
                )
                return False

            # ========================================
            # 2. ãƒœãƒ¼ãƒŠã‚¹/ãƒšãƒŠãƒ«ãƒ†ã‚£ç¯„å›²æ¤œè¨¼
            # ========================================
            agreement_bonus = params.get("ml.strategy_integration.agreement_bonus", 1.2)
            disagreement_penalty = params.get("ml.strategy_integration.disagreement_penalty", 0.7)

            # ãƒœãƒ¼ãƒŠã‚¹ã¯1.0ä»¥ä¸Šï¼ˆå¢—åŠ ã®ã¿ï¼‰
            if agreement_bonus < 1.0:
                self.logger.warning(f"âš ï¸ ãƒœãƒ¼ãƒŠã‚¹ç¯„å›²ã‚¨ãƒ©ãƒ¼: agreement_bonus({agreement_bonus}) < 1.0")
                return False

            # ãƒšãƒŠãƒ«ãƒ†ã‚£ã¯1.0ä»¥ä¸‹ï¼ˆæ¸›å°‘ã®ã¿ï¼‰
            if disagreement_penalty > 1.0:
                self.logger.warning(f"âš ï¸ ãƒšãƒŠãƒ«ãƒ†ã‚£ç¯„å›²ã‚¨ãƒ©ãƒ¼: disagreement_penalty({disagreement_penalty}) > 1.0")
                return False

            # ========================================
            # 3. é–¾å€¤ã®è«–ç†çš„é †åºæ¤œè¨¼
            # ========================================
            high_confidence_threshold = params.get("ml.strategy_integration.high_confidence_threshold", 0.8)
            min_ml_confidence = params.get("ml.strategy_integration.min_ml_confidence", 0.6)
            hold_conversion_threshold = params.get("ml.strategy_integration.hold_conversion_threshold", 0.4)

            # é«˜ä¿¡é ¼åº¦é–¾å€¤ > æœ€å°MLä¿¡é ¼åº¦ï¼ˆè«–ç†çš„æ•´åˆæ€§ï¼‰
            if not (high_confidence_threshold > min_ml_confidence):
                self.logger.warning(
                    f"âš ï¸ é–¾å€¤é †åºã‚¨ãƒ©ãƒ¼: high_confidence({high_confidence_threshold}) <= min_ml({min_ml_confidence})"
                )
                return False

            # holdå¤‰æ›´é–¾å€¤ < æœ€å°MLä¿¡é ¼åº¦ï¼ˆè«–ç†çš„æ•´åˆæ€§ï¼‰
            if not (hold_conversion_threshold < min_ml_confidence):
                self.logger.warning(
                    f"âš ï¸ holdå¤‰æ›´é–¾å€¤ã‚¨ãƒ©ãƒ¼: hold_threshold({hold_conversion_threshold}) >= min_ml({min_ml_confidence})"
                )
                return False

            # holdå¤‰æ›´é–¾å€¤ < ä¸ä¸€è‡´ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆè«–ç†çš„ç¯„å›²ï¼‰
            if not (hold_conversion_threshold < disagreement_penalty):
                self.logger.warning(
                    f"âš ï¸ holdå¤‰æ›´é–¾å€¤ç¯„å›²ã‚¨ãƒ©ãƒ¼: hold_threshold({hold_conversion_threshold}) >= penalty({disagreement_penalty})"
                )
                return False

            return True

        except Exception as e:
            self.logger.error(f"âŒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    async def _run_backtest(self, params: Dict[str, Any]) -> float:
        """
        ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆPhase 40.5å®Ÿè£…ãƒ»ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ï¼‰

        Args:
            params: ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            float: ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª
        """
        # Phase 40.5: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æœ€é©åŒ–ã®Stage 1ã§ä½¿ç”¨ï¼ˆé«˜é€Ÿãƒ»å¤§é‡è©¦è¡Œï¼‰
        # Stage 2/3ã§ã¯å®Ÿãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµ±åˆï¼ˆbacktest_integration.pyï¼‰ã‚’ä½¿ç”¨

        try:
            # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢è¨ˆç®—
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—

            # ç†æƒ³çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¿‘ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢
            ideal_params = {
                "ml_weight": 0.3,  # æˆ¦ç•¥70% + ML30%ãŒç†æƒ³çš„
                "high_confidence_threshold": 0.8,  # 80%ä»¥ä¸Šã§é«˜ä¿¡é ¼åº¦
                "agreement_bonus": 1.2,  # ä¸€è‡´æ™‚+20%ãƒ–ãƒ¼ã‚¹ãƒˆ
                "disagreement_penalty": 0.7,  # ä¸ä¸€è‡´æ™‚-30%ãƒšãƒŠãƒ«ãƒ†ã‚£
                "min_ml_confidence": 0.6,  # 60%ä»¥ä¸Šã§MLè€ƒæ…®
                "hold_conversion_threshold": 0.4,  # 40%æœªæº€ã§holdå¤‰æ›´
            }

            score = 1.0

            # MLé‡ã¿ï¼ˆç†æƒ³å€¤ã‹ã‚‰ã®è·é›¢ï¼‰
            ml_weight = params.get("ml.strategy_integration.ml_weight", 0.3)
            score -= abs(ml_weight - ideal_params["ml_weight"]) * 0.3

            # é«˜ä¿¡é ¼åº¦é–¾å€¤ï¼ˆç†æƒ³å€¤ã‹ã‚‰ã®è·é›¢ï¼‰
            high_conf = params.get("ml.strategy_integration.high_confidence_threshold", 0.8)
            score -= abs(high_conf - ideal_params["high_confidence_threshold"]) * 0.2

            # ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹ï¼ˆç†æƒ³å€¤ã‹ã‚‰ã®è·é›¢ï¼‰
            agreement_bonus = params.get("ml.strategy_integration.agreement_bonus", 1.2)
            score -= abs(agreement_bonus - ideal_params["agreement_bonus"]) * 0.25

            # ä¸ä¸€è‡´ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆç†æƒ³å€¤ã‹ã‚‰ã®è·é›¢ï¼‰
            disagreement_penalty = params.get("ml.strategy_integration.disagreement_penalty", 0.7)
            score -= abs(disagreement_penalty - ideal_params["disagreement_penalty"]) * 0.25

            # æœ€å°MLä¿¡é ¼åº¦ï¼ˆç†æƒ³å€¤ã‹ã‚‰ã®è·é›¢ï¼‰
            min_ml_conf = params.get("ml.strategy_integration.min_ml_confidence", 0.6)
            score -= abs(min_ml_conf - ideal_params["min_ml_confidence"]) * 0.2

            # holdå¤‰æ›´é–¾å€¤ï¼ˆç†æƒ³å€¤ã‹ã‚‰ã®è·é›¢ï¼‰
            hold_threshold = params.get("ml.strategy_integration.hold_conversion_threshold", 0.4)
            score -= abs(hold_threshold - ideal_params["hold_conversion_threshold"]) * 0.15

            # ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚ºè¿½åŠ ï¼ˆå®Ÿéš›ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå¤‰å‹•ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
            # Phase 40.5: å†ç¾æ€§ç¢ºä¿ã®ãŸã‚ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®š
            np.random.seed(42)
            noise = np.random.normal(0, 0.15)
            sharpe_ratio = score + noise

            return float(sharpe_ratio)

        except Exception as e:
            self.logger.error(f"âŒ ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return -10.0

    def optimize(self, n_trials: int = 150, timeout: int = 7200) -> Dict[str, Any]:
        """
        æœ€é©åŒ–å®Ÿè¡Œï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ï¼‰

        Args:
            n_trials: è©¦è¡Œå›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ150å›ï¼‰
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ2æ™‚é–“ï¼‰

        Returns:
            Dict: æœ€é©åŒ–çµæœ
        """
        self.logger.warning("ğŸš€ Phase 40.3: MLçµ±åˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–é–‹å§‹ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ï¼‰")
        self.logger.info(f"è©¦è¡Œå›æ•°: {n_trials}å›ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout}ç§’")

        start_time = time.time()

        # Optuna Studyä½œæˆ
        study = optuna.create_study(
            direction="maximize",  # ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªæœ€å¤§åŒ–
            sampler=TPESampler(seed=42),
            study_name="phase40_3_ml_integration",
        )

        # æœ€é©åŒ–å®Ÿè¡Œ
        # Phase 40.5ãƒã‚°ä¿®æ­£: show_progress_bar=Trueã§Trial 113ãƒãƒ³ã‚°å•é¡Œå¯¾ç­–
        def logging_callback(study, trial):
            if trial.number % 50 == 0 or trial.number < 5:
                print(f"Trial {trial.number}/{n_trials} " f"å®Œäº†: value={trial.value:.4f}, best={study.best_value:.4f}")

        study.optimize(
            self.objective,
            n_trials=n_trials,
            timeout=timeout,
            show_progress_bar=False,
            callbacks=[logging_callback],
        )

        duration = time.time() - start_time

        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print_optimization_summary(study, "Phase 40.3 MLçµ±åˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–", duration)

        # çµæœä¿å­˜
        study_stats = {
            "n_trials": len(study.trials),
            "n_complete": len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]),
            "n_failed": len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL]),
            "duration_seconds": duration,
        }

        result_path = self.result_manager.save_results(
            phase_name="phase40_3_ml_integration",
            best_params=study.best_params,
            best_value=study.best_value,
            study_stats=study_stats,
        )

        self.logger.warning(f"âœ… æœ€é©åŒ–å®Œäº†: {result_path}", discord_notify=True)

        return {
            "best_params": study.best_params,
            "best_value": study.best_value,
            "study": study,
            "result_path": result_path,
        }

    def optimize_hybrid(
        self,
        n_simulation_trials: int = 750,
        n_lightweight_candidates: int = 50,
        n_full_candidates: int = 10,
    ) -> Dict[str, Any]:
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æœ€é©åŒ–å®Ÿè¡Œï¼ˆPhase 40.5å®Ÿè£…ï¼‰

        3æ®µéšæœ€é©åŒ–:
        - Stage 1: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ750è©¦è¡Œãƒ»é«˜é€Ÿï¼‰
        - Stage 2: è»½é‡ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆä¸Šä½50å€™è£œãƒ»30æ—¥ãƒ»10%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        - Stage 3: å®Œå…¨ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆä¸Šä½10å€™è£œãƒ»180æ—¥ãƒ»100%ãƒ‡ãƒ¼ã‚¿ï¼‰

        Args:
            n_simulation_trials: Stage 1è©¦è¡Œå›æ•°
            n_lightweight_candidates: Stage 2å€™è£œæ•°
            n_full_candidates: Stage 3å€™è£œæ•°

        Returns:
            Dict: æœ€é©åŒ–çµæœ
        """
        from .hybrid_optimizer import HybridOptimizer

        self.logger.warning("ğŸš€ Phase 40.5: MLçµ±åˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æœ€é©åŒ–é–‹å§‹")

        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æœ€é©åŒ–å™¨ä½œæˆ
        hybrid = HybridOptimizer(
            phase_name="phase40_3_ml_integration",
            simulation_objective=self.objective,
            param_suggest_func=self.get_simple_params,
            param_type="ml_integration",
            n_simulation_trials=n_simulation_trials,
            n_lightweight_candidates=n_lightweight_candidates,
            n_full_candidates=n_full_candidates,
            verbose=True,
        )

        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æœ€é©åŒ–å®Ÿè¡Œ
        result = hybrid.run()

        self.logger.warning(
            f"âœ… ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æœ€é©åŒ–å®Œäº†: ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª={result['best_value']:.4f}",
            discord_notify=True,
        )

        return result


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°è§£æ
    parser = argparse.ArgumentParser(description="Phase 40.3: MLçµ±åˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–")
    parser.add_argument(
        "--use-hybrid-backtest",
        action="store_true",
        help="ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æœ€é©åŒ–ã‚’ä½¿ç”¨ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³â†’è»½é‡ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆâ†’å®Œå…¨ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼‰",
    )
    parser.add_argument(
        "--n-trials",
        type=int,
        default=150,
        help="è©¦è¡Œå›æ•°ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰æ™‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ150å›ï¼‰",
    )
    parser.add_argument(
        "--n-simulation-trials",
        type=int,
        default=750,
        help="ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰: Stage 1ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©¦è¡Œå›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ750å›ï¼‰",
    )
    parser.add_argument(
        "--n-lightweight-candidates",
        type=int,
        default=50,
        help="ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰: Stage 2è»½é‡ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå€™è£œæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ50ä»¶ï¼‰",
    )
    parser.add_argument(
        "--n-full-candidates",
        type=int,
        default=10,
        help="ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰: Stage 3å®Œå…¨ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå€™è£œæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10ä»¶ï¼‰",
    )

    args = parser.parse_args()

    # ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    logger = CryptoBotLogger()

    # æœ€é©åŒ–å®Ÿè¡Œ
    optimizer = MLIntegrationOptimizer(logger)

    if args.use_hybrid_backtest:
        # Phase 40.5: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æœ€é©åŒ–
        logger.info("ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰ï¼ˆ3æ®µéšæœ€é©åŒ–ï¼‰")
        results = optimizer.optimize_hybrid(
            n_simulation_trials=args.n_simulation_trials,
            n_lightweight_candidates=args.n_lightweight_candidates,
            n_full_candidates=args.n_full_candidates,
        )
    else:
        # Phase 40.3: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–
        logger.info("ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰")
        results = optimizer.optimize(n_trials=args.n_trials, timeout=7200)

    # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¡¨ç¤º
    print("\n" + "=" * 80)
    print("ğŸ¯ æœ€é©åŒ–å®Œäº† - æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    print("=" * 80)
    print("\nä»¥ä¸‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’thresholds.yamlã«åæ˜ ã—ã¦ãã ã•ã„:\n")

    for key, value in results["best_params"].items():
        if isinstance(value, float):
            print(f"  {key}: {value:.6f}")
        else:
            print(f"  {key}: {value}")

    print(f"\næœ€é©ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª: {results['best_value']:.4f}")
    if "result_path" in results:
        print(f"çµæœä¿å­˜å…ˆ: {results['result_path']}")
    print("=" * 80)


if __name__ == "__main__":
    main()
