#!/usr/bin/env python3
"""
Phase 40.4: MLãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Optunaã‚’ä½¿ç”¨ã—ã¦MLãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBMãƒ»XGBoostãƒ»RandomForestï¼‰ã®
ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ï¼š

3ãƒ¢ãƒ‡ãƒ«ãƒ»åˆè¨ˆ30ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
- LightGBM: 10ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- XGBoost: 10ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- RandomForest: 10ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

ç›®çš„é–¢æ•°: äºˆæ¸¬ç²¾åº¦ï¼ˆF1ã‚¹ã‚³ã‚¢ã¾ãŸã¯AUCï¼‰æœ€å¤§åŒ–
æ¤œè¨¼æ–¹æ³•: Walk-forward testingï¼ˆè¨“ç·´120æ—¥ãƒ»ãƒ†ã‚¹ãƒˆ60æ—¥ï¼‰
"""

import asyncio
import sys
import time
from pathlib import Path
from typing import Any, Dict

import numpy as np
import optuna
from optuna.samplers import TPESampler

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from scripts.optimization.optuna_utils import (
    OptimizationMetrics,
    OptimizationResultManager,
    print_optimization_summary,
)
from src.core.config import (
    clear_runtime_overrides,
    get_runtime_overrides,
    set_runtime_overrides_batch,
)
from src.core.logger import CryptoBotLogger


class MLHyperparameterOptimizer:
    """MLãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self, logger: CryptoBotLogger):
        """
        åˆæœŸåŒ–

        Args:
            logger: ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ 
        """
        self.logger = logger
        self.result_manager = OptimizationResultManager()
        self.best_accuracy = -np.inf
        self.trial_count = 0

    def objective(self, trial: optuna.Trial) -> float:
        """
        Optunaç›®çš„é–¢æ•°ï¼ˆäºˆæ¸¬ç²¾åº¦æœ€å¤§åŒ–ï¼‰

        Args:
            trial: Optuna Trial

        Returns:
            float: äºˆæ¸¬ç²¾åº¦ï¼ˆF1ã‚¹ã‚³ã‚¢ãƒ»æœ€å¤§åŒ–ç›®æ¨™ï¼‰
        """
        self.trial_count += 1

        try:
            # 1. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            params = self._sample_parameters(trial)

            # 2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œè¨¼
            if not self._validate_parameters(params):
                return -10.0  # ç„¡åŠ¹ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ãƒšãƒŠãƒ«ãƒ†ã‚£

            # 3. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰è¨­å®š
            set_runtime_overrides_batch(params)

            # ãƒ‡ãƒãƒƒã‚°: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ç¢ºèª
            if self.trial_count <= 3:
                self.logger.info(f"Trial {self.trial_count} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {len(params)}")

            # 4. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            accuracy = asyncio.run(self._run_backtest(params))

            # 5. ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã‚¯ãƒªã‚¢
            clear_runtime_overrides()

            # 6. é€²æ—è¡¨ç¤º
            if accuracy > self.best_accuracy:
                self.best_accuracy = accuracy
                self.logger.info(f"ğŸ¯ Trial {self.trial_count}: æ–°ãƒ™ã‚¹ãƒˆ äºˆæ¸¬ç²¾åº¦={accuracy:.4f}")

            return accuracy

        except Exception as e:
            self.logger.error(f"âŒ Trial {self.trial_count} ã‚¨ãƒ©ãƒ¼: {e}")
            clear_runtime_overrides()
            return -10.0  # ãƒšãƒŠãƒ«ãƒ†ã‚£å€¤

    def _sample_parameters(self, trial: optuna.Trial) -> Dict[str, Any]:
        """
        æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ30ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰

        Args:
            trial: Optuna Trial

        Returns:
            Dict: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        params = {}

        # ========================================
        # LightGBMï¼ˆ10ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
        # ========================================

        # 1. num_leaves: ãƒ„ãƒªãƒ¼ã®è‘‰ã®æ•°ï¼ˆ20-150ï¼‰
        params["models.lgbm.num_leaves"] = trial.suggest_int("lgbm_num_leaves", 20, 150, step=10)

        # 2. learning_rate: å­¦ç¿’ç‡ï¼ˆ0.01-0.3ãƒ»å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        params["models.lgbm.learning_rate"] = trial.suggest_float(
            "lgbm_learning_rate", 0.01, 0.3, log=True
        )

        # 3. n_estimators: ãƒ„ãƒªãƒ¼ã®æ•°ï¼ˆ50-500ï¼‰
        params["models.lgbm.n_estimators"] = trial.suggest_int(
            "lgbm_n_estimators", 50, 500, step=50
        )

        # 4. max_depth: ãƒ„ãƒªãƒ¼ã®æœ€å¤§æ·±ã•ï¼ˆ3-15ãƒ»-1ã‚‚å¯ï¼‰
        max_depth = trial.suggest_int("lgbm_max_depth", 3, 15)
        # 50%ã®ç¢ºç‡ã§-1ï¼ˆç„¡åˆ¶é™ï¼‰ã‚’è©¦ã™
        if trial.suggest_categorical("lgbm_max_depth_unlimited", [True, False]):
            max_depth = -1
        params["models.lgbm.max_depth"] = max_depth

        # 5. min_child_samples: è‘‰ãƒãƒ¼ãƒ‰ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆ10-100ï¼‰
        params["models.lgbm.min_child_samples"] = trial.suggest_int(
            "lgbm_min_child_samples", 10, 100, step=10
        )

        # 6. feature_fraction: ç‰¹å¾´é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡ï¼ˆ0.6-1.0ï¼‰
        params["models.lgbm.feature_fraction"] = trial.suggest_float(
            "lgbm_feature_fraction", 0.6, 1.0, step=0.05
        )

        # 7. bagging_fraction: ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡ï¼ˆ0.6-1.0ï¼‰
        params["models.lgbm.bagging_fraction"] = trial.suggest_float(
            "lgbm_bagging_fraction", 0.6, 1.0, step=0.05
        )

        # 8. bagging_freq: ãƒã‚®ãƒ³ã‚°é »åº¦ï¼ˆ0-10ï¼‰
        # bagging_fraction < 1.0 ã®å ´åˆã®ã¿æœ‰åŠ¹
        if params["models.lgbm.bagging_fraction"] < 1.0:
            params["models.lgbm.bagging_freq"] = trial.suggest_int("lgbm_bagging_freq", 1, 10)
        else:
            params["models.lgbm.bagging_freq"] = 0

        # 9. reg_alpha: L1æ­£å‰‡åŒ–ï¼ˆ0.0-10.0ãƒ»å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        params["models.lgbm.reg_alpha"] = trial.suggest_float(
            "lgbm_reg_alpha", 1e-8, 10.0, log=True
        )

        # 10. reg_lambda: L2æ­£å‰‡åŒ–ï¼ˆ0.0-10.0ãƒ»å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        params["models.lgbm.reg_lambda"] = trial.suggest_float(
            "lgbm_reg_lambda", 1e-8, 10.0, log=True
        )

        # ========================================
        # XGBoostï¼ˆ10ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
        # ========================================

        # 1. max_depth: ãƒ„ãƒªãƒ¼ã®æœ€å¤§æ·±ã•ï¼ˆ3-15ï¼‰
        params["models.xgb.max_depth"] = trial.suggest_int("xgb_max_depth", 3, 15)

        # 2. learning_rate: å­¦ç¿’ç‡ï¼ˆ0.01-0.3ãƒ»å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        params["models.xgb.learning_rate"] = trial.suggest_float(
            "xgb_learning_rate", 0.01, 0.3, log=True
        )

        # 3. n_estimators: ãƒ„ãƒªãƒ¼ã®æ•°ï¼ˆ50-500ï¼‰
        params["models.xgb.n_estimators"] = trial.suggest_int("xgb_n_estimators", 50, 500, step=50)

        # 4. min_child_weight: å­ãƒãƒ¼ãƒ‰ã®æœ€å°é‡ã¿ï¼ˆ1-10ï¼‰
        params["models.xgb.min_child_weight"] = trial.suggest_int("xgb_min_child_weight", 1, 10)

        # 5. subsample: ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡ï¼ˆ0.6-1.0ï¼‰
        params["models.xgb.subsample"] = trial.suggest_float("xgb_subsample", 0.6, 1.0, step=0.05)

        # 6. colsample_bytree: ç‰¹å¾´é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡ï¼ˆ0.6-1.0ï¼‰
        params["models.xgb.colsample_bytree"] = trial.suggest_float(
            "xgb_colsample_bytree", 0.6, 1.0, step=0.05
        )

        # 7. gamma: åˆ†å‰²ã®ãŸã‚ã®æœ€å°æå¤±å‰Šæ¸›ï¼ˆ0.0-5.0ï¼‰
        params["models.xgb.gamma"] = trial.suggest_float("xgb_gamma", 0.0, 5.0, step=0.5)

        # 8. alpha: L1æ­£å‰‡åŒ–ï¼ˆ0.0-10.0ãƒ»å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        params["models.xgb.alpha"] = trial.suggest_float("xgb_alpha", 1e-8, 10.0, log=True)

        # 9. lambda: L2æ­£å‰‡åŒ–ï¼ˆ0.0-10.0ãƒ»å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        params["models.xgb.lambda"] = trial.suggest_float("xgb_lambda", 1e-8, 10.0, log=True)

        # 10. scale_pos_weight: æ­£ä¾‹ã®é‡ã¿ï¼ˆ0.5-2.0ï¼‰
        params["models.xgb.scale_pos_weight"] = trial.suggest_float(
            "xgb_scale_pos_weight", 0.5, 2.0, step=0.1
        )

        # ========================================
        # RandomForestï¼ˆ10ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
        # ========================================

        # 1. n_estimators: ãƒ„ãƒªãƒ¼ã®æ•°ï¼ˆ50-500ï¼‰
        params["models.rf.n_estimators"] = trial.suggest_int("rf_n_estimators", 50, 500, step=50)

        # 2. max_depth: ãƒ„ãƒªãƒ¼ã®æœ€å¤§æ·±ã•ï¼ˆ3-30ãƒ»Noneã‚‚å¯ï¼‰
        rf_max_depth = trial.suggest_int("rf_max_depth", 3, 30)
        # 30%ã®ç¢ºç‡ã§Noneï¼ˆç„¡åˆ¶é™ï¼‰ã‚’è©¦ã™
        if trial.suggest_categorical("rf_max_depth_none", [True, False, False]):
            rf_max_depth = None
        params["models.rf.max_depth"] = rf_max_depth

        # 3. min_samples_split: åˆ†å‰²ã®ãŸã‚ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆ2-20ï¼‰
        params["models.rf.min_samples_split"] = trial.suggest_int("rf_min_samples_split", 2, 20)

        # 4. min_samples_leaf: è‘‰ãƒãƒ¼ãƒ‰ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆ1-10ï¼‰
        params["models.rf.min_samples_leaf"] = trial.suggest_int("rf_min_samples_leaf", 1, 10)

        # 5. max_features: åˆ†å‰²æ™‚ã®æœ€å¤§ç‰¹å¾´é‡æ•°ï¼ˆsqrt/log2/0.5-1.0ï¼‰
        max_features_type = trial.suggest_categorical(
            "rf_max_features_type", ["sqrt", "log2", "float"]
        )
        if max_features_type == "float":
            params["models.rf.max_features"] = trial.suggest_float(
                "rf_max_features_float", 0.5, 1.0, step=0.1
            )
        else:
            params["models.rf.max_features"] = max_features_type

        # 6. max_leaf_nodes: æœ€å¤§è‘‰ãƒãƒ¼ãƒ‰æ•°ï¼ˆ10-100ãƒ»Noneã‚‚å¯ï¼‰
        rf_max_leaf_nodes = trial.suggest_int("rf_max_leaf_nodes", 10, 100, step=10)
        # 50%ã®ç¢ºç‡ã§Noneï¼ˆç„¡åˆ¶é™ï¼‰ã‚’è©¦ã™
        if trial.suggest_categorical("rf_max_leaf_nodes_none", [True, False]):
            rf_max_leaf_nodes = None
        params["models.rf.max_leaf_nodes"] = rf_max_leaf_nodes

        # 7. min_impurity_decrease: ä¸ç´”åº¦æ¸›å°‘ã®æœ€å°å€¤ï¼ˆ0.0-0.1ï¼‰
        params["models.rf.min_impurity_decrease"] = trial.suggest_float(
            "rf_min_impurity_decrease", 0.0, 0.1, step=0.01
        )

        # 8. bootstrap: ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆTrue/Falseï¼‰
        params["models.rf.bootstrap"] = trial.suggest_categorical("rf_bootstrap", [True, False])

        # 9. oob_score: Out-of-bag scoreä½¿ç”¨ï¼ˆTrue/Falseï¼‰
        # bootstrap=Trueã®å ´åˆã®ã¿æœ‰åŠ¹
        if params["models.rf.bootstrap"]:
            params["models.rf.oob_score"] = trial.suggest_categorical("rf_oob_score", [True, False])
        else:
            params["models.rf.oob_score"] = False

        # 10. class_weight: ã‚¯ãƒ©ã‚¹é‡ã¿ï¼ˆbalanced/balanced_subsample/Noneï¼‰
        params["models.rf.class_weight"] = trial.suggest_categorical(
            "rf_class_weight", ["balanced", "balanced_subsample", None]
        )

        return params

    def _validate_parameters(self, params: Dict[str, Any]) -> bool:
        """
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¦¥å½“æ€§æ¤œè¨¼

        Args:
            params: æ¤œè¨¼å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            bool: å¦¥å½“æ€§ï¼ˆTrue: æœ‰åŠ¹, False: ç„¡åŠ¹ï¼‰
        """
        try:
            # ========================================
            # LightGBMæ¤œè¨¼
            # ========================================

            # bagging_fraction < 1.0 ã®å ´åˆã€bagging_freq > 0 ãŒå¿…è¦
            bagging_fraction = params.get("models.lgbm.bagging_fraction", 1.0)
            bagging_freq = params.get("models.lgbm.bagging_freq", 0)

            if bagging_fraction < 1.0 and bagging_freq == 0:
                self.logger.warning(
                    f"âš ï¸ LightGBMæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: bagging_fraction({bagging_fraction}) < 1.0 "
                    f"but bagging_freq({bagging_freq}) = 0"
                )
                return False

            # num_leaves vs max_depth ã®æ•´åˆæ€§
            num_leaves = params.get("models.lgbm.num_leaves", 31)
            max_depth = params.get("models.lgbm.max_depth", -1)

            if max_depth > 0:
                # max_depth ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€num_leaves <= 2^max_depth
                max_leaves_for_depth = 2**max_depth
                if num_leaves > max_leaves_for_depth:
                    self.logger.warning(
                        f"âš ï¸ LightGBMæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: num_leaves({num_leaves}) > "
                        f"2^max_depth({max_leaves_for_depth})"
                    )
                    return False

            # ========================================
            # XGBoostæ¤œè¨¼
            # ========================================

            # min_child_weight ã¨ max_depth ã®ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
            min_child_weight = params.get("models.xgb.min_child_weight", 1)
            xgb_max_depth = params.get("models.xgb.max_depth", 6)

            # æ¥µç«¯ãªçµ„ã¿åˆã‚ã›ã‚’å›é¿ï¼ˆæ·±ã„ãƒ„ãƒªãƒ¼ + é«˜ã„min_child_weight â†’ å­¦ç¿’ä¸è¶³ï¼‰
            if xgb_max_depth > 12 and min_child_weight > 7:
                self.logger.warning(
                    f"âš ï¸ XGBoostæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: æ¥µç«¯ãªçµ„ã¿åˆã‚ã› "
                    f"max_depth({xgb_max_depth}) > 12 and min_child_weight({min_child_weight}) > 7"
                )
                return False

            # ========================================
            # RandomForestæ¤œè¨¼
            # ========================================

            # oob_score=True ã®å ´åˆã€bootstrap=True ãŒå¿…è¦
            bootstrap = params.get("models.rf.bootstrap", True)
            oob_score = params.get("models.rf.oob_score", False)

            if oob_score and not bootstrap:
                self.logger.warning("âš ï¸ RandomForestæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: oob_score=True but bootstrap=False")
                return False

            # min_samples_split > min_samples_leaf ãŒå¿…è¦
            min_samples_split = params.get("models.rf.min_samples_split", 2)
            min_samples_leaf = params.get("models.rf.min_samples_leaf", 1)

            if min_samples_split <= min_samples_leaf:
                self.logger.warning(
                    f"âš ï¸ RandomForestæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: min_samples_split({min_samples_split}) <= "
                    f"min_samples_leaf({min_samples_leaf})"
                )
                return False

            return True

        except Exception as e:
            self.logger.error(f"âŒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    async def _run_backtest(self, params: Dict[str, Any]) -> float:
        """
        MLå­¦ç¿’ãƒ»è©•ä¾¡å®Ÿè¡Œï¼ˆPhase 40.5å®Ÿè£…ãƒ»ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ï¼‰

        Args:
            params: ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            float: äºˆæ¸¬ç²¾åº¦ï¼ˆF1ã‚¹ã‚³ã‚¢ï¼‰
        """
        # Phase 40.5: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®äºˆæ¸¬ç²¾åº¦è©•ä¾¡
        # MLãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ãƒ¢ãƒ‡ãƒ«å­¦ç¿’æ™‚ã«æœ€é©åŒ–ã™ã‚‹ãŸã‚ã€
        # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµ±åˆã§ã¯ãªãã€ç›´æ¥MLå­¦ç¿’ãƒ»è©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹æƒ³å®š
        # ç¾åœ¨ã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè£…ï¼ˆç†æƒ³çš„ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ã®è·é›¢ã§è©•ä¾¡ï¼‰

        try:
            # æ³¨: å®Ÿéš›ã®MLå­¦ç¿’ãƒ»è©•ä¾¡ã¯ `scripts/ml/create_ml_models.py` ã§å®Ÿè£…
            # from scripts.ml.create_ml_models import train_and_evaluate_models
            # accuracy = train_and_evaluate_models(params)

            # Phase 40.4: ãƒ€ãƒŸãƒ¼å®Ÿè£…ï¼ˆç†æƒ³çš„ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¿‘ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰

            # ç†æƒ³çš„ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆçµŒé¨“çš„ã«è‰¯å¥½ãªå€¤ï¼‰
            ideal_params = {
                # LightGBM
                "lgbm_num_leaves": 50,
                "lgbm_learning_rate": 0.05,
                "lgbm_n_estimators": 150,
                "lgbm_max_depth": 7,
                "lgbm_min_child_samples": 30,
                "lgbm_feature_fraction": 0.8,
                "lgbm_bagging_fraction": 0.8,
                "lgbm_bagging_freq": 5,
                "lgbm_reg_alpha": 0.1,
                "lgbm_reg_lambda": 0.1,
                # XGBoost
                "xgb_max_depth": 6,
                "xgb_learning_rate": 0.05,
                "xgb_n_estimators": 150,
                "xgb_min_child_weight": 3,
                "xgb_subsample": 0.8,
                "xgb_colsample_bytree": 0.8,
                "xgb_gamma": 0.5,
                "xgb_alpha": 0.1,
                "xgb_lambda": 1.0,
                "xgb_scale_pos_weight": 1.0,
                # RandomForest
                "rf_n_estimators": 150,
                "rf_max_depth": 15,
                "rf_min_samples_split": 5,
                "rf_min_samples_leaf": 2,
                "rf_max_features": "sqrt",
                "rf_max_leaf_nodes": 50,
                "rf_min_impurity_decrease": 0.01,
                "rf_bootstrap": True,
                "rf_oob_score": True,
                "rf_class_weight": "balanced",
            }

            score = 1.0

            # ========================================
            # LightGBMã‚¹ã‚³ã‚¢è¨ˆç®—
            # ========================================

            # num_leavesï¼ˆé‡ã¿: 0.05ï¼‰
            lgbm_num_leaves = params.get("models.lgbm.num_leaves", 50)
            score -= abs(lgbm_num_leaves - ideal_params["lgbm_num_leaves"]) / 200.0 * 0.05

            # learning_rateï¼ˆé‡ã¿: 0.1ï¼‰
            lgbm_lr = params.get("models.lgbm.learning_rate", 0.05)
            score -= abs(np.log(lgbm_lr) - np.log(ideal_params["lgbm_learning_rate"])) * 0.1

            # n_estimatorsï¼ˆé‡ã¿: 0.05ï¼‰
            lgbm_n_estimators = params.get("models.lgbm.n_estimators", 150)
            score -= abs(lgbm_n_estimators - ideal_params["lgbm_n_estimators"]) / 500.0 * 0.05

            # max_depthï¼ˆé‡ã¿: 0.05ï¼‰
            lgbm_max_depth = params.get("models.lgbm.max_depth", 7)
            if lgbm_max_depth == -1:
                lgbm_max_depth = 15  # -1ã‚’é«˜ã„å€¤ã¨ã—ã¦æ‰±ã†
            score -= abs(lgbm_max_depth - ideal_params["lgbm_max_depth"]) / 20.0 * 0.05

            # feature_fractionï¼ˆé‡ã¿: 0.03ï¼‰
            lgbm_feature_fraction = params.get("models.lgbm.feature_fraction", 0.8)
            score -= abs(lgbm_feature_fraction - ideal_params["lgbm_feature_fraction"]) * 0.03

            # bagging_fractionï¼ˆé‡ã¿: 0.03ï¼‰
            lgbm_bagging_fraction = params.get("models.lgbm.bagging_fraction", 0.8)
            score -= abs(lgbm_bagging_fraction - ideal_params["lgbm_bagging_fraction"]) * 0.03

            # ========================================
            # XGBoostã‚¹ã‚³ã‚¢è¨ˆç®—
            # ========================================

            # learning_rateï¼ˆé‡ã¿: 0.1ï¼‰
            xgb_lr = params.get("models.xgb.learning_rate", 0.05)
            score -= abs(np.log(xgb_lr) - np.log(ideal_params["xgb_learning_rate"])) * 0.1

            # max_depthï¼ˆé‡ã¿: 0.05ï¼‰
            xgb_max_depth = params.get("models.xgb.max_depth", 6)
            score -= abs(xgb_max_depth - ideal_params["xgb_max_depth"]) / 20.0 * 0.05

            # n_estimatorsï¼ˆé‡ã¿: 0.05ï¼‰
            xgb_n_estimators = params.get("models.xgb.n_estimators", 150)
            score -= abs(xgb_n_estimators - ideal_params["xgb_n_estimators"]) / 500.0 * 0.05

            # subsampleï¼ˆé‡ã¿: 0.03ï¼‰
            xgb_subsample = params.get("models.xgb.subsample", 0.8)
            score -= abs(xgb_subsample - ideal_params["xgb_subsample"]) * 0.03

            # colsample_bytreeï¼ˆé‡ã¿: 0.03ï¼‰
            xgb_colsample = params.get("models.xgb.colsample_bytree", 0.8)
            score -= abs(xgb_colsample - ideal_params["xgb_colsample_bytree"]) * 0.03

            # ========================================
            # RandomForestã‚¹ã‚³ã‚¢è¨ˆç®—
            # ========================================

            # n_estimatorsï¼ˆé‡ã¿: 0.05ï¼‰
            rf_n_estimators = params.get("models.rf.n_estimators", 150)
            score -= abs(rf_n_estimators - ideal_params["rf_n_estimators"]) / 500.0 * 0.05

            # max_depthï¼ˆé‡ã¿: 0.05ï¼‰
            rf_max_depth = params.get("models.rf.max_depth", 15)
            if rf_max_depth is None:
                rf_max_depth = 30  # Noneã‚’é«˜ã„å€¤ã¨ã—ã¦æ‰±ã†
            score -= abs(rf_max_depth - ideal_params["rf_max_depth"]) / 40.0 * 0.05

            # min_samples_splitï¼ˆé‡ã¿: 0.03ï¼‰
            rf_min_samples_split = params.get("models.rf.min_samples_split", 5)
            score -= abs(rf_min_samples_split - ideal_params["rf_min_samples_split"]) / 20.0 * 0.03

            # max_featuresï¼ˆé‡ã¿: 0.03ï¼‰
            rf_max_features = params.get("models.rf.max_features", "sqrt")
            if rf_max_features == ideal_params["rf_max_features"]:
                score += 0.03  # å®Œå…¨ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹

            # ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚ºè¿½åŠ ï¼ˆå®Ÿéš›ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå¤‰å‹•ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
            # Phase 40.5: å†ç¾æ€§ç¢ºä¿ã®ãŸã‚ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®š
            np.random.seed(42)
            noise = np.random.normal(0, 0.15)
            f1_score = max(0.0, min(1.0, score + noise))

            return float(f1_score)

        except Exception as e:
            self.logger.error(f"âŒ ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return -10.0

    def optimize(self, n_trials: int = 300, timeout: int = 14400) -> Dict[str, Any]:
        """
        æœ€é©åŒ–å®Ÿè¡Œ

        Args:
            n_trials: è©¦è¡Œå›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ300å›ï¼‰
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4æ™‚é–“ï¼‰

        Returns:
            Dict: æœ€é©åŒ–çµæœ
        """
        self.logger.warning("ğŸš€ Phase 40.4: MLãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–é–‹å§‹")
        self.logger.info(f"è©¦è¡Œå›æ•°: {n_trials}å›ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout}ç§’")

        start_time = time.time()

        # Optuna Studyä½œæˆ
        study = optuna.create_study(
            direction="maximize",  # äºˆæ¸¬ç²¾åº¦æœ€å¤§åŒ–
            sampler=TPESampler(seed=42),
            study_name="phase40_4_ml_hyperparameters",
        )

        # æœ€é©åŒ–å®Ÿè¡Œ
        # Phase 40.5ãƒã‚°ä¿®æ­£: show_progress_bar=Trueã§Trial 113ãƒãƒ³ã‚°å•é¡Œå¯¾ç­–
        def logging_callback(study, trial):
            if trial.number % 50 == 0 or trial.number < 5:
                print(
                    f"Trial {trial.number}/{n_trials} "
                    f"å®Œäº†: value={trial.value:.4f}, best={study.best_value:.4f}"
                )

        study.optimize(
            self.objective,
            n_trials=n_trials,
            timeout=timeout,
            show_progress_bar=False,
            callbacks=[logging_callback],
        )

        duration = time.time() - start_time

        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print_optimization_summary(study, "Phase 40.4 MLãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–", duration)

        # çµæœä¿å­˜
        study_stats = {
            "n_trials": len(study.trials),
            "n_complete": len(
                [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]
            ),
            "n_failed": len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL]),
            "duration_seconds": duration,
        }

        result_path = self.result_manager.save_results(
            phase_name="phase40_4_ml_hyperparameters",
            best_params=study.best_params,
            best_value=study.best_value,
            study_stats=study_stats,
        )

        self.logger.warning(f"âœ… æœ€é©åŒ–å®Œäº†: {result_path}", discord_notify=True)

        return {
            "best_params": study.best_params,
            "best_value": study.best_value,
            "study": study,
            "result_path": result_path,
        }


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    # ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    logger = CryptoBotLogger()

    # æœ€é©åŒ–å®Ÿè¡Œ
    optimizer = MLHyperparameterOptimizer(logger)

    # Phase 40.4: è©¦è¡Œå›æ•°300å›ãƒ»ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ4æ™‚é–“
    results = optimizer.optimize(n_trials=300, timeout=14400)

    # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¡¨ç¤º
    print("\n" + "=" * 80)
    print("ğŸ¯ æœ€é©åŒ–å®Œäº† - æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    print("=" * 80)
    print("\nä»¥ä¸‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’thresholds.yamlã«åæ˜ ã—ã¦ãã ã•ã„:\n")

    for key, value in results["best_params"].items():
        print(f"  {key}: {value}")

    print(f"\næœ€é©äºˆæ¸¬ç²¾åº¦ï¼ˆF1ã‚¹ã‚³ã‚¢ï¼‰: {results['best_value']:.4f}")
    print(f"çµæœä¿å­˜å…ˆ: {results['result_path']}")
    print("=" * 80)


if __name__ == "__main__":
    main()
