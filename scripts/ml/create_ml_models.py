#!/usr/bin/env python3
"""
æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨MLãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ - Phase 52.4ï¼ˆ6æˆ¦ç•¥ãƒ»55ç‰¹å¾´é‡ãƒ»3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰

æ©Ÿèƒ½:
- **2æ®µéšMLãƒ¢ãƒ‡ãƒ«ç”Ÿæˆ** - fullï¼ˆ55ç‰¹å¾´é‡ï¼‰ãƒ»basicï¼ˆ49ç‰¹å¾´é‡ï¼‰
- **è¨­å®šé§†å‹•å‹** - feature_order.jsonå®Œå…¨æº–æ‹ ãƒ»strategies.yamlå‹•çš„ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã‚¼ãƒ­
- **6æˆ¦ç•¥çµ±åˆ** - StrategyLoaderå‹•çš„ãƒ­ãƒ¼ãƒ‰ï¼ˆATRBased/DonchianChannel/ADXTrendStrength/BBReversal/StochasticReversal/MACDEMACrossoverï¼‰
- **ãƒ¢ãƒ‡ãƒ«åˆ¥ç‰¹å¾´é‡é¸æŠ** - feature_order.jsonã‚«ãƒ†ã‚´ãƒªãƒ¼å®šç¾©ã«åŸºã¥ãè‡ªå‹•é¸æŠ
- **çµ±åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ** - å…¨ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’1ã¤ã®JSONã«é›†ç´„ï¼ˆensemble_metadata.jsonï¼‰
- **çœŸã®3ã‚¯ãƒ©ã‚¹åˆ†é¡** - 0=sell, 1=hold, 2=buy
- **Strategy-Aware ML** - å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’ï¼ˆè¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã®ä¸€è²«æ€§ç¢ºä¿ï¼‰
- **TimeSeriesSplitãƒ»SMOTEãƒ»Optunaæœ€é©åŒ–**
- models/production/ ã«ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆfull/basicï¼‰

Phase 52.4å®Œäº†æˆæœ: 6æˆ¦ç•¥çµ±åˆãƒ»55ç‰¹å¾´é‡ã‚·ã‚¹ãƒ†ãƒ ãƒ»çœŸã®3ã‚¯ãƒ©ã‚¹åˆ†é¡ãƒ»è¨­å®šé§†å‹•å‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

ä½¿ç”¨æ–¹æ³•:
    # ä¸¡ãƒ¢ãƒ‡ãƒ«ä¸€æ‹¬å­¦ç¿’ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ»æ¨å¥¨ï¼‰
    python scripts/ml/create_ml_models.py --n-classes 3 --threshold 0.005 --optimize --n-trials 50 --verbose

    # fullãƒ¢ãƒ‡ãƒ«ã®ã¿å­¦ç¿’ï¼ˆç·Šæ€¥æ™‚ï¼‰
    python scripts/ml/create_ml_models.py --model full --n-classes 3 --threshold 0.005 --optimize --n-trials 50 --verbose

    # basicãƒ¢ãƒ‡ãƒ«ã®ã¿å­¦ç¿’ï¼ˆç·Šæ€¥æ™‚ï¼‰
    python scripts/ml/create_ml_models.py --model basic --n-classes 3 --threshold 0.005 --optimize --n-trials 50 --verbose
"""

import argparse
import asyncio
import json
import logging
import pickle
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, Tuple

import numpy as np
import optuna
import pandas as pd
from imblearn.over_sampling import SMOTE
from lightgbm import LGBMClassifier
from optuna.samplers import TPESampler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import TimeSeriesSplit
from xgboost import XGBClassifier

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆscripts/ml -> botï¼‰
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from src.backtest.scripts.collect_historical_csv import HistoricalDataCollector
    from src.core.config import load_config
    from src.core.config.feature_manager import _feature_manager  # Phase 50.7
    from src.core.logger import get_logger
    from src.data.data_pipeline import DataPipeline, DataRequest, TimeFrame
    from src.features.feature_generator import FeatureGenerator
    from src.ml.ensemble import ProductionEnsemble
    from src.strategies.base.strategy_manager import StrategyManager
except ImportError as e:
    print(f"âŒ æ–°ã‚·ã‚¹ãƒ†ãƒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    print("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)


class NewSystemMLModelCreator:
    """æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨MLãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ»å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ."""

    def __init__(
        self,
        config_path: str = "config/core/unified.yaml",
        verbose: bool = False,
        target_threshold: float = 0.005,
        n_classes: int = 2,
        use_smote: bool = False,
        optimize: bool = False,
        n_trials: int = 20,
        models_to_train: list = None,
    ):
        """
        åˆæœŸåŒ–ï¼ˆPhase 52.4å¯¾å¿œãƒ»ä¸€æ‹¬ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼‰

        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            verbose: è©³ç´°ãƒ­ã‚°å‡ºåŠ›
            target_threshold: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé–¾å€¤
            n_classes: ã‚¯ãƒ©ã‚¹æ•° 2 or 3
            use_smote: SMOTEã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä½¿ç”¨
            optimize: Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ä½¿ç”¨
            n_trials: Optunaè©¦è¡Œå›æ•°
            models_to_train: è¨“ç·´ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆ ["full", "basic"] ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ä¸¡æ–¹
        """
        self.config_path = config_path
        self.models_to_train = models_to_train or ["full", "basic"]
        self.current_model_type = "full"  # ãƒ«ãƒ¼ãƒ—å‡¦ç†ä¸­ã«å‹•çš„ã«è¨­å®š
        self.verbose = verbose
        self.target_threshold = target_threshold  # Phase 39.2
        self.n_classes = n_classes  # Phase 39.2
        self.use_smote = use_smote  # Phase 39.4
        self.optimize = optimize  # Phase 39.5
        self.n_trials = n_trials  # Phase 39.5

        # ãƒ­ã‚°è¨­å®š
        self.logger = get_logger()
        if verbose:
            logging.getLogger().setLevel(logging.DEBUG)

        # Phase 51.9-6A: ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›å…ˆã‚’logs/ml/ã«å¤‰æ›´
        from datetime import datetime

        log_dir = Path("logs/ml")
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = log_dir / f"ml_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
        logging.getLogger().addHandler(file_handler)
        self.logger.info(f"ğŸ“ ãƒ­ã‚°å‡ºåŠ›å…ˆ: {log_file}")

        # è¨­å®šèª­ã¿è¾¼ã¿
        try:
            self.config = load_config(config_path)
            self.logger.info(f"âœ… è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†: {config_path}")
        except Exception as e:
            self.logger.error(f"âŒ è¨­å®šèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            raise

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.training_dir = Path("models/training")
        self.production_dir = Path("models/production")
        self.training_dir.mkdir(parents=True, exist_ok=True)
        self.production_dir.mkdir(parents=True, exist_ok=True)

        # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
        try:
            self.data_pipeline = DataPipeline()
            self.logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å¤±æ•—: {e}")
            raise

        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        self.feature_generator = FeatureGenerator()

        # Phase 29: ç‰¹å¾´é‡å®šç¾©ä¸€å…ƒåŒ–å¯¾å¿œï¼ˆfeature_managerã‹ã‚‰å–å¾—ï¼‰
        from src.core.config.feature_manager import get_feature_names

        self.expected_features = get_feature_names()

        self.logger.info(f"ğŸ¯ å¯¾è±¡ç‰¹å¾´é‡: {len(self.expected_features)}å€‹ï¼ˆæ–°ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–æ¸ˆã¿ï¼‰")
        self.logger.info(
            f"ğŸ¯ Phase 39.2 ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å®š: é–¾å€¤={target_threshold:.1%}, ã‚¯ãƒ©ã‚¹æ•°={n_classes}"
        )

        # MLãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆPhase 39.3-39.4å¯¾å¿œãƒ»Phase 51.9-6A: 3ã‚¯ãƒ©ã‚¹å¯¾å¿œï¼‰
        # LightGBMè¨­å®š
        lgb_params = {
            "n_estimators": 200,
            "learning_rate": 0.1,
            "max_depth": 8,
            "num_leaves": 31,
            "random_state": 42,
            "verbose": -1,
            "class_weight": "balanced",  # Phase 39.4
        }
        if self.n_classes == 3:
            lgb_params["objective"] = "multiclass"
            lgb_params["num_class"] = 3

        # XGBoostè¨­å®š
        xgb_params = {
            "n_estimators": 200,
            "learning_rate": 0.1,
            "max_depth": 8,
            "random_state": 42,
            "verbosity": 0,
        }
        if self.n_classes == 3:
            xgb_params["objective"] = "multi:softprob"
            xgb_params["num_class"] = 3
            xgb_params["eval_metric"] = "mlogloss"
        else:
            xgb_params["eval_metric"] = "logloss"

        # RandomForestè¨­å®šï¼ˆè‡ªå‹•æ¤œå‡ºã™ã‚‹ãŒæ˜ç¤ºçš„ã«è¨­å®šï¼‰
        rf_params = {
            "n_estimators": 200,
            "max_depth": 12,
            "random_state": 42,
            "n_jobs": -1,
            "class_weight": "balanced",  # Phase 39.4
        }

        self.models = {
            "lightgbm": LGBMClassifier(**lgb_params),
            "xgboost": XGBClassifier(**xgb_params),
            "random_forest": RandomForestClassifier(**rf_params),
        }

    async def _load_real_historical_data(self, days: int) -> pd.DataFrame:
        """
        å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆPhase 39.1: MLå®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼‰

        Args:
            days: åé›†æ—¥æ•°

        Returns:
            pd.DataFrame: OHLCV ãƒ‡ãƒ¼ã‚¿
        """
        self.logger.info(f"ğŸ“Š Phase 39.1: å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹ï¼ˆéå»{days}æ—¥åˆ†ï¼‰")

        csv_path = Path("src/backtest/data/historical/BTC_JPY_4h.csv")

        # ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆå­˜åœ¨ã—ãªã„ã€ã¾ãŸã¯å¤ã„å ´åˆï¼‰
        if not csv_path.exists():
            self.logger.info("ğŸ’¾ å±¥æ­´ãƒ‡ãƒ¼ã‚¿æœªå­˜åœ¨ - è‡ªå‹•åé›†é–‹å§‹")
            try:
                collector = HistoricalDataCollector()
                await collector.collect_data(symbol="BTC/JPY", days=days, timeframes=["4h"])
                self.logger.info("âœ… ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†")
            except Exception as e:
                self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿åé›†å¤±æ•—: {e}")
                raise

        # CSVèª­ã¿è¾¼ã¿
        try:
            df = pd.read_csv(csv_path)

            # timestamp ã‚’datetimeã«å¤‰æ›
            df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
            df.set_index("timestamp", inplace=True)

            # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿
            cutoff_date = datetime.now() - timedelta(days=days)
            df = df[df.index >= cutoff_date]

            # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
            if df.isnull().any().any():
                self.logger.warning("âš ï¸ æ¬ æå€¤ã‚’æ¤œå‡º - ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ")
                df = df.dropna()

            self.logger.info(
                f"âœ… å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ã‚µãƒ³ãƒ—ãƒ« "
                f"({df.index.min().strftime('%Y-%m-%d')} ã€œ {df.index.max().strftime('%Y-%m-%d')})"
            )

            # OHLCV ã‚«ãƒ©ãƒ ã®ã¿è¿”å´
            return df[["open", "high", "low", "close", "volume"]].copy()

        except Exception as e:
            self.logger.error(f"âŒ CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    async def prepare_training_data_async(self, days: int = 180) -> Tuple[pd.DataFrame, pd.Series]:
        """
        Phase 51.9: ãƒ¢ãƒ‡ãƒ«åˆ¥ç‰¹å¾´é‡é¸æŠï¼ˆ2æ®µéšã‚·ã‚¹ãƒ†ãƒ ãƒ»6æˆ¦ç•¥çµ±åˆï¼‰

        model_type="full": 55ç‰¹å¾´é‡ï¼ˆå…¨ç‰¹å¾´é‡ä½¿ç”¨ãƒ»6æˆ¦ç•¥ä¿¡å·å«ã‚€ï¼‰
        model_type="basic": 49ç‰¹å¾´é‡ï¼ˆæˆ¦ç•¥ä¿¡å·é™¤å¤–ï¼‰
        """
        model_name = (
            "fullï¼ˆ55ç‰¹å¾´é‡ï¼‰" if self.current_model_type == "full" else "basicï¼ˆ49ç‰¹å¾´é‡ï¼‰"
        )
        self.logger.info(f"ğŸ“Š Phase 51.9: å®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’é–‹å§‹ï¼ˆéå»{days}æ—¥åˆ†ãƒ»{model_name}ï¼‰")

        try:
            # Phase 39.1: å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = await self._load_real_historical_data(days)

            self.logger.info(f"âœ… åŸºæœ¬ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(df)}è¡Œ")

            # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆ55ç‰¹å¾´é‡ã‚·ã‚¹ãƒ†ãƒ ï¼‰
            features_df = await self.feature_generator.generate_features(df)

            # Phase 41.8: æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ç‰¹å¾´é‡ã‚’å‰Šé™¤ï¼ˆå¾Œã§å®Ÿæˆ¦ç•¥ä¿¡å·ã§ç½®ãæ›ãˆã‚‹ï¼‰
            # generate_features() ã¯æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ã‚’0.0ã§è‡ªå‹•ç”Ÿæˆã™ã‚‹ãŒã€Phase 41.8ã§ã¯å®Ÿæˆ¦ç•¥ä¿¡å·ã‚’ä½¿ç”¨
            strategy_signal_cols = [
                col for col in features_df.columns if col.startswith("strategy_signal_")
            ]
            if strategy_signal_cols:
                features_df = features_df.drop(columns=strategy_signal_cols)
                self.logger.info(
                    f"âœ… æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ç‰¹å¾´é‡å‰Šé™¤: {len(strategy_signal_cols)}å€‹ï¼ˆå®Ÿæˆ¦ç•¥ä¿¡å·ã§ç½®ãæ›ãˆï¼‰"
                )

            # Phase 51.9: å®Ÿæˆ¦ç•¥ä¿¡å·ç”Ÿæˆï¼ˆ49â†’55ç‰¹å¾´é‡ãƒ»6æˆ¦ç•¥çµ±åˆï¼‰
            # Note: éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿéš›ã«æˆ¦ç•¥ã‚’å®Ÿè¡Œã—ã€æœ¬ç‰©ã®æˆ¦ç•¥ä¿¡å·ã‚’ç”Ÿæˆ
            #       ã“ã‚Œã«ã‚ˆã‚Šè¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã®ä¸€è²«æ€§ã‚’ç¢ºä¿
            #       ç‰¹å¾´é‡ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™ï¼ˆæˆ¦ç•¥ã¯ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’å¿…è¦ã¨ã™ã‚‹ãŸã‚ï¼‰
            strategy_signals_df = await self._generate_real_strategy_signals_for_training(
                features_df
            )

            # Phase 51.9: åŸºæœ¬ç‰¹å¾´é‡ï¼ˆ49ï¼‰ + å®Ÿæˆ¦ç•¥ä¿¡å·ï¼ˆ6ï¼‰ = 55ç‰¹å¾´é‡ã‚’çµåˆ
            features_df = pd.concat([features_df, strategy_signals_df], axis=1)

            # Phase 51.9: ç‰¹å¾´é‡æ•´åˆæ€§ç¢ºä¿ï¼ˆ55ç‰¹å¾´é‡å›ºå®šã‚·ã‚¹ãƒ†ãƒ ï¼‰
            features_df = self._ensure_feature_consistency(features_df)

            # Phase 52.4 CRITICAL FIX: ç‰¹å¾´é‡é¸æŠã¯run()å†…ã®ãƒ«ãƒ¼ãƒ—ã§å®Ÿè¡Œ
            # ã“ã“ã§ã¯å…¨55ç‰¹å¾´é‡ã‚’ç”Ÿæˆã—ã€run()ã§ãƒ¢ãƒ‡ãƒ«åˆ¥ã«é¸æŠã™ã‚‹

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆPhase 39.2: é–¾å€¤ãƒ»ã‚¯ãƒ©ã‚¹æ•°å¯¾å¿œï¼‰
            target = self._generate_target(df, self.target_threshold, self.n_classes)

            # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
            features_df, target = self._clean_data(features_df, target)

            self.logger.info(
                f"âœ… Phase 52.4: å®Ÿãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº† - {len(features_df)}ã‚µãƒ³ãƒ—ãƒ«ã€{len(features_df.columns)}ç‰¹å¾´é‡ï¼ˆå…¨ç‰¹å¾´é‡ï¼‰"
            )
            return features_df, target

        except Exception as e:
            self.logger.error(f"âŒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def prepare_training_data(self, days: int = 180) -> Tuple[pd.DataFrame, pd.Series]:
        """å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆåŒæœŸãƒ©ãƒƒãƒ‘ãƒ¼ãƒ»å¾Œæ–¹äº’æ›æ€§ï¼‰"""
        return asyncio.run(self.prepare_training_data_async(days))

    def _generate_sample_data(self, samples: int) -> pd.DataFrame:
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰."""
        self.logger.info(f"ğŸ”§ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: {samples}ã‚µãƒ³ãƒ—ãƒ«")

        # æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        dates = pd.date_range(end=datetime.now(), periods=samples, freq="h")

        # ãƒªã‚¢ãƒ«ãªBTCä¾¡æ ¼å‹•å‘ã‚’æ¨¡æ“¬
        np.random.seed(42)
        base_price = 5000000  # 5M JPY
        prices = []
        current_price = base_price

        for i in range(samples):
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼ˆæ™‚é–“å¸¯ã«ã‚ˆã‚‹å¤‰å‹•å¹…èª¿æ•´ï¼‰
            hour = dates[i].hour
            volatility = 0.015 if 8 <= hour <= 20 else 0.008  # æ—¥ä¸­é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            change = np.random.normal(0, volatility)
            current_price *= 1 + change
            prices.append(current_price)

        # OHLCV ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        df = pd.DataFrame(
            {
                "timestamp": dates,
                "open": prices,
                "high": [p * (1 + abs(np.random.normal(0, 0.005))) for p in prices],
                "low": [p * (1 - abs(np.random.normal(0, 0.005))) for p in prices],
                "close": prices,
                "volume": np.random.lognormal(8, 1, samples),  # ãƒªã‚¢ãƒ«ãªå‡ºæ¥é«˜åˆ†å¸ƒ
            }
        )

        df.set_index("timestamp", inplace=True)
        return df

    async def _generate_real_strategy_signals_for_training(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Strategy-Aware ML: å®Ÿéš›ã®æˆ¦ç•¥ä¿¡å·ã‚’ç”Ÿæˆï¼ˆéå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰

        å„æ™‚ç‚¹ã§6æˆ¦ç•¥ã‚’å®Ÿè¡Œã—ã€æœ¬ç‰©ã®æˆ¦ç•¥ä¿¡å·ã‚’è¨ˆç®—ã€‚
        ã“ã‚Œã«ã‚ˆã‚Šè¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã®ä¸€è²«æ€§ã‚’ç¢ºä¿ã€‚

        Args:
            df: OHLCVä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿

        Returns:
            pd.DataFrame: æˆ¦ç•¥ä¿¡å·6åˆ—ã®DataFrame (index aligned with df)
        """
        # strategies.yamlã‹ã‚‰å‹•çš„ãƒ­ãƒ¼ãƒ‰ï¼ˆ6æˆ¦ç•¥å¯¾å¿œï¼‰
        from src.strategies.strategy_loader import StrategyLoader

        strategy_loader = StrategyLoader("config/core/strategies.yaml")
        loaded_strategies = strategy_loader.load_strategies()
        strategy_names = [s["metadata"]["name"] for s in loaded_strategies]

        self.logger.info(
            f"ğŸ“Š Phase 51.9: å®Ÿæˆ¦ç•¥ä¿¡å·ç”Ÿæˆé–‹å§‹ï¼ˆéå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰{len(strategy_names)}æˆ¦ç•¥å®Ÿè¡Œï¼‰"
        )
        self.logger.info(f"   æˆ¦ç•¥ãƒªã‚¹ãƒˆ: {strategy_names}")

        # çµæœæ ¼ç´ç”¨DataFrame
        strategy_signals = pd.DataFrame(index=df.index)

        try:
            # StrategyManageråˆæœŸåŒ– + å…¨æˆ¦ç•¥ç™»éŒ²
            strategy_manager = StrategyManager()

            # Phase 51.9: å…¨æˆ¦ç•¥ã‚’StrategyManagerã«ç™»éŒ²
            for strategy_data in loaded_strategies:
                strategy_manager.register_strategy(
                    strategy_data["instance"], weight=strategy_data["weight"]
                )

            self.logger.info(f"âœ… StrategyManageråˆæœŸåŒ–å®Œäº† - {len(loaded_strategies)}æˆ¦ç•¥ç™»éŒ²")

            # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–
            self.data_pipeline.set_backtest_data({"4h": df.copy()})
            self.logger.info("âœ… DataPipelineãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰è¨­å®šå®Œäº†")

            # å„æ™‚ç‚¹ã§æˆ¦ç•¥å®Ÿè¡Œï¼ˆlook-ahead biaså›é¿ã®ãŸã‚é †æ¬¡å‡¦ç†ï¼‰
            total_points = len(df)
            processed = 0

            for i in range(len(df)):
                # ç¾åœ¨æ™‚ç‚¹ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼ˆæœªæ¥ãƒ‡ãƒ¼ã‚¿æ¼æ´©é˜²æ­¢ï¼‰
                current_data = df.iloc[: i + 1]

                # æœ€ä½é™ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒå¿…è¦ï¼ˆç‰¹å¾´é‡è¨ˆç®—ã®ãŸã‚ï¼‰
                if len(current_data) < 50:
                    # ãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚ã¯0ã§åŸ‹ã‚ã‚‹
                    for strategy_name in strategy_names:
                        strategy_signals.loc[
                            current_data.index[-1], f"strategy_signal_{strategy_name}"
                        ] = 0.0
                    continue

                try:
                    # DataPipelineæ›´æ–°ï¼ˆãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å½¢å¼ï¼‰
                    self.data_pipeline.set_backtest_data({"4h": current_data.copy()})

                    # å€‹åˆ¥æˆ¦ç•¥ä¿¡å·å–å¾—ï¼ˆPhase 51.7 Day 7: å˜ä¸€DataFrameã¨ã—ã¦æ¸¡ã™ï¼‰
                    signals = strategy_manager.get_individual_strategy_signals(current_data)

                    # action Ã— confidence ã‚’è¨ˆç®—ã—ã¦æ ¼ç´
                    current_timestamp = current_data.index[-1]
                    for strategy_name in strategy_names:
                        if strategy_name in signals:
                            action = signals[strategy_name]["action"]
                            confidence = signals[strategy_name]["confidence"]

                            # Phase 51.9: æ”¹å–„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆhold=0å•é¡Œè§£æ±ºï¼‰
                            # æ–°æ–¹å¼: 0.0-1.0ã®é€£ç¶šå€¤ï¼ˆå…¨ã¦éã‚¼ãƒ­ï¼‰
                            # - hold: 0.5ï¼ˆä¸­ç«‹ï¼‰
                            # - buy: 0.5 + (confidence * 0.5) = 0.5-1.0ç¯„å›²
                            # - sell: 0.5 - (confidence * 0.5) = 0.0-0.5ç¯„å›²
                            if action == "buy":
                                signal_value = 0.5 + (confidence * 0.5)
                            elif action == "sell":
                                signal_value = 0.5 - (confidence * 0.5)
                            else:  # hold
                                signal_value = 0.5

                            strategy_signals.loc[
                                current_timestamp, f"strategy_signal_{strategy_name}"
                            ] = signal_value
                        else:
                            # æˆ¦ç•¥ä¿¡å·ãŒå¾—ã‚‰ã‚Œãªã„å ´åˆã¯holdæ‰±ã„ï¼ˆ0.5ï¼‰
                            strategy_signals.loc[
                                current_timestamp, f"strategy_signal_{strategy_name}"
                            ] = 0.5

                except Exception as e:
                    # Phase 51.9: ã‚¨ãƒ©ãƒ¼æ™‚ã¯holdï¼ˆ0.5ï¼‰ã§åŸ‹ã‚ã‚‹ï¼ˆå­¦ç¿’ç¶™ç¶šï¼‰
                    self.logger.warning(f"âš ï¸ æ™‚ç‚¹{i}ã§æˆ¦ç•¥å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}, hold(0.5)ã§åŸ‹ã‚ã¾ã™")
                    for strategy_name in strategy_names:
                        strategy_signals.loc[
                            current_data.index[-1], f"strategy_signal_{strategy_name}"
                        ] = 0.5

                # é€²æ—è¡¨ç¤ºï¼ˆ10%ã”ã¨ï¼‰
                processed += 1
                if processed % max(1, total_points // 10) == 0:
                    progress = (processed / total_points) * 100
                    self.logger.info(
                        f"ğŸ“Š Phase 41.8: æˆ¦ç•¥ä¿¡å·ç”Ÿæˆé€²æ— {processed}/{total_points} ({progress:.1f}%)"
                    )

            # Phase 51.9: æ¬ æå€¤ã‚’holdï¼ˆ0.5ï¼‰ã§åŸ‹ã‚ã‚‹
            strategy_signals.fillna(0.5, inplace=True)

            self.logger.info(
                f"âœ… Phase 51.9: å®Ÿæˆ¦ç•¥ä¿¡å·ç”Ÿæˆå®Œäº† - {len(strategy_signals)}è¡Œ Ã— {len(strategy_names)}æˆ¦ç•¥"
            )
            # Phase 51.9: buy/sellç‡ï¼ˆholdä»¥å¤–ã®ç‡ï¼‰ã‚’è¡¨ç¤º
            buy_sell_rate = (
                (strategy_signals != 0.5).sum().sum()
                / (len(strategy_signals) * len(strategy_names))
                * 100
            )
            self.logger.info(
                f"ğŸ“Š Phase 51.9: æˆ¦ç•¥ä¿¡å·çµ±è¨ˆ - buy/sellç‡ï¼ˆholdä»¥å¤–ï¼‰: {buy_sell_rate:.1f}%"
            )

            return strategy_signals

        except Exception as e:
            self.logger.error(f"âŒ Phase 41.8: å®Ÿæˆ¦ç•¥ä¿¡å·ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            # Phase 51.9: ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆhold=0.5åŸ‹ã‚ï¼‰
            self.logger.warning("âš ï¸ Phase 51.9: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ - æˆ¦ç•¥ä¿¡å·ã‚’holdï¼ˆ0.5ï¼‰åŸ‹ã‚")
            for strategy_name in strategy_names:
                strategy_signals[f"strategy_signal_{strategy_name}"] = 0.5
            return strategy_signals

    def _ensure_feature_consistency(self, features_df: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾´é‡æ•´åˆæ€§ç¢ºä¿ï¼ˆPhase 51.9: 55ç‰¹å¾´é‡å¯¾å¿œãƒ»6æˆ¦ç•¥çµ±åˆï¼‰"""
        # ä¸è¶³ç‰¹å¾´é‡ã®0åŸ‹ã‚
        for feature in self.expected_features:
            if feature not in features_df.columns:
                features_df[feature] = 0.0
                self.logger.warning(f"âš ï¸ ä¸è¶³ç‰¹å¾´é‡ã‚’0åŸ‹ã‚: {feature}")

        # ç‰¹å¾´é‡ã®ã¿é¸æŠ - Phase 51.9: 55ç‰¹å¾´é‡å¯¾å¿œ
        features_df = features_df[self.expected_features]

        expected_count = len(self.expected_features)
        if len(features_df.columns) != expected_count:
            self.logger.warning(f"âš ï¸ ç‰¹å¾´é‡æ•°ä¸ä¸€è‡´: {len(features_df.columns)} != {expected_count}")

        return features_df

    def _select_features_by_level(self, features_df: pd.DataFrame) -> pd.DataFrame:
        """
        Phase 51.9: ãƒ¢ãƒ‡ãƒ«åˆ¥ç‰¹å¾´é‡é¸æŠï¼ˆ2æ®µéšã‚·ã‚¹ãƒ†ãƒ ãƒ»è¨­å®šé§†å‹•å‹ãƒ»6æˆ¦ç•¥çµ±åˆï¼‰

        model_type="full": å…¨55ç‰¹å¾´é‡ä½¿ç”¨ï¼ˆ6æˆ¦ç•¥ä¿¡å·å«ã‚€ï¼‰
        model_type="basic": æˆ¦ç•¥ä¿¡å·6å€‹ã‚’é™¤å¤–ï¼ˆ49ç‰¹å¾´é‡ï¼‰

        Args:
            features_df: å…¨ç‰¹å¾´é‡ã‚’å«ã‚€DataFrame

        Returns:
            pd.DataFrame: ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ãŸç‰¹å¾´é‡ã®ã¿ã‚’å«ã‚€DataFrame
        """
        if self.current_model_type == "full":
            # full: å…¨55ç‰¹å¾´é‡ä½¿ç”¨
            self.logger.info(f"ğŸ“Š Phase 51.9: full ãƒ¢ãƒ‡ãƒ« - å…¨{len(features_df.columns)}ç‰¹å¾´é‡ä½¿ç”¨")
            return features_df

        # basic: æˆ¦ç•¥ä¿¡å·ã‚’é™¤å¤–ï¼ˆ49ç‰¹å¾´é‡ï¼‰
        # è¨­å®šé§†å‹•å‹: strategy_signal_ ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã§å‹•çš„æ¤œç´¢
        strategy_signal_features = [
            col for col in features_df.columns if col.startswith("strategy_signal_")
        ]

        features_df = features_df.drop(columns=strategy_signal_features, errors="ignore")
        self.logger.info(
            f"ğŸ“Š Phase 51.9: basic ãƒ¢ãƒ‡ãƒ« - æˆ¦ç•¥ä¿¡å·{len(strategy_signal_features)}å€‹ã‚’é™¤å¤– â†’ {len(features_df.columns)}ç‰¹å¾´é‡"
        )
        return features_df

    def _generate_target(
        self,
        df: pd.DataFrame,
        threshold: float = 0.005,
        n_classes: int = 2,
    ) -> pd.Series:
        """
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆPhase 39.2: é–¾å€¤æœ€é©åŒ–ãƒ»3ã‚¯ãƒ©ã‚¹åˆ†é¡å¯¾å¿œï¼‰

        Args:
            df: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
            threshold: BUYé–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.5%ï¼‰
            n_classes: ã‚¯ãƒ©ã‚¹æ•°ï¼ˆ2ã¾ãŸã¯3ï¼‰

        Returns:
            pd.Series: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ©ãƒ™ãƒ«
                2ã‚¯ãƒ©ã‚¹: 0=HOLD/SELL, 1=BUY
                3ã‚¯ãƒ©ã‚¹: 0=SELL, 1=HOLD, 2=BUY
        """
        # 1æ™‚é–“å¾Œã®ä¾¡æ ¼å¤‰å‹•ç‡ï¼ˆ4æ™‚é–“è¶³ãªã®ã§4æ™‚é–“å¾Œï¼‰
        price_change = df["close"].pct_change(periods=1).shift(-1)

        if n_classes == 2:
            # Phase 39.2: é–¾å€¤0.3%â†’0.5%ã«å¤‰æ›´ï¼ˆãƒã‚¤ã‚ºå‰Šæ¸›ï¼‰
            target = (price_change > threshold).astype(int)

            buy_ratio = target.mean()
            self.logger.info(
                f"ğŸ“Š Phase 39.2 ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒï¼ˆé–¾å€¤{threshold:.1%}ï¼‰: "
                f"BUY {buy_ratio:.1%}, OTHER {1 - buy_ratio:.1%}"
            )

        elif n_classes == 3:
            # Phase 39.2: 3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆBUY/HOLD/SELLï¼‰
            sell_threshold = -threshold

            # 0: SELL, 1: HOLD, 2: BUY
            target = pd.Series(1, index=df.index, dtype=int)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆHOLD
            target[price_change > threshold] = 2  # BUY
            target[price_change < sell_threshold] = 0  # SELL

            distribution = target.value_counts(normalize=True).sort_index()
            self.logger.info(
                f"ğŸ“Š Phase 39.2 3ã‚¯ãƒ©ã‚¹åˆ†å¸ƒï¼ˆé–¾å€¤Â±{threshold:.1%}ï¼‰: "
                f"SELL {distribution.get(0, 0):.1%}, "
                f"HOLD {distribution.get(1, 0):.1%}, "
                f"BUY {distribution.get(2, 0):.1%}"
            )

        else:
            raise ValueError(f"Unsupported n_classes: {n_classes} (must be 2 or 3)")

        return target

    def _clean_data(
        self, features_df: pd.DataFrame, target: pd.Series
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°."""
        # NaNé™¤å»
        valid_mask = ~(features_df.isna().any(axis=1) | target.isna())
        features_clean = features_df[valid_mask].copy()
        target_clean = target[valid_mask].copy()

        # ç„¡é™å€¤é™¤å»
        features_clean.replace([np.inf, -np.inf], np.nan, inplace=True)
        features_clean.fillna(0, inplace=True)

        removed_samples = len(features_df) - len(features_clean)
        if removed_samples > 0:
            self.logger.info(f"ğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°: {removed_samples}ã‚µãƒ³ãƒ—ãƒ«é™¤å»")

        return features_clean, target_clean

    def _objective_lightgbm(
        self, trial: optuna.Trial, X_train: pd.DataFrame, y_train: pd.Series
    ) -> float:
        """Phase 39.5: LightGBMæœ€é©åŒ–objectiveé–¢æ•°ï¼ˆPhase 51.9-6A: 3ã‚¯ãƒ©ã‚¹å¯¾å¿œï¼‰"""
        params = {
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
            "max_depth": trial.suggest_int("max_depth", 3, 15),
            "n_estimators": trial.suggest_int("n_estimators", 50, 300),
            "num_leaves": trial.suggest_int("num_leaves", 20, 100),
            "random_state": 42,
            "verbose": -1,
            "class_weight": "balanced",
        }

        # Phase 51.9-6A: 3ã‚¯ãƒ©ã‚¹åˆ†é¡å¯¾å¿œ
        if self.n_classes == 3:
            params["objective"] = "multiclass"
            params["num_class"] = 3

        model = LGBMClassifier(**params)

        # TimeSeriesSplit CV
        tscv = TimeSeriesSplit(n_splits=3)
        scores = []
        for train_idx, val_idx in tscv.split(X_train):
            X_cv_train = X_train.iloc[train_idx]
            y_cv_train = y_train.iloc[train_idx]
            X_cv_val = X_train.iloc[val_idx]
            y_cv_val = y_train.iloc[val_idx]

            model.fit(X_cv_train, y_cv_train)
            y_pred = model.predict(X_cv_val)
            score = f1_score(y_cv_val, y_pred, average="weighted")
            scores.append(score)

        return np.mean(scores)

    def _objective_xgboost(
        self, trial: optuna.Trial, X_train: pd.DataFrame, y_train: pd.Series
    ) -> float:
        """Phase 39.5: XGBoostæœ€é©åŒ–objectiveé–¢æ•°ï¼ˆPhase 51.9-6A: 3ã‚¯ãƒ©ã‚¹å¯¾å¿œï¼‰"""
        params = {
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
            "max_depth": trial.suggest_int("max_depth", 3, 15),
            "n_estimators": trial.suggest_int("n_estimators", 50, 300),
            "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
            "random_state": 42,
            "verbosity": 0,
        }

        # Phase 51.9-6A: 3ã‚¯ãƒ©ã‚¹åˆ†é¡å¯¾å¿œ
        if self.n_classes == 3:
            params["objective"] = "multi:softprob"
            params["num_class"] = 3
            params["eval_metric"] = "mlogloss"
        else:
            params["eval_metric"] = "logloss"
            # scale_pos_weightå‹•çš„è¨­å®šï¼ˆ2ã‚¯ãƒ©ã‚¹åˆ†é¡ã®ã¿ï¼‰
            pos_count = y_train.sum()
            neg_count = len(y_train) - pos_count
            if pos_count > 0:
                params["scale_pos_weight"] = neg_count / pos_count

        model = XGBClassifier(**params)

        # TimeSeriesSplit CV
        tscv = TimeSeriesSplit(n_splits=3)
        scores = []
        for train_idx, val_idx in tscv.split(X_train):
            X_cv_train = X_train.iloc[train_idx]
            y_cv_train = y_train.iloc[train_idx]
            X_cv_val = X_train.iloc[val_idx]
            y_cv_val = y_train.iloc[val_idx]

            model.fit(X_cv_train, y_cv_train)
            y_pred = model.predict(X_cv_val)
            score = f1_score(y_cv_val, y_pred, average="weighted")
            scores.append(score)

        return np.mean(scores)

    def _objective_random_forest(
        self, trial: optuna.Trial, X_train: pd.DataFrame, y_train: pd.Series
    ) -> float:
        """Phase 39.5: RandomForestæœ€é©åŒ–objectiveé–¢æ•°"""
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 50, 300),
            "max_depth": trial.suggest_int("max_depth", 5, 20),
            "min_samples_split": trial.suggest_int("min_samples_split", 2, 20),
            "random_state": 42,
            "n_jobs": -1,
            "class_weight": "balanced",
        }

        model = RandomForestClassifier(**params)

        # TimeSeriesSplit CV
        tscv = TimeSeriesSplit(n_splits=3)
        scores = []
        for train_idx, val_idx in tscv.split(X_train):
            X_cv_train = X_train.iloc[train_idx]
            y_cv_train = y_train.iloc[train_idx]
            X_cv_val = X_train.iloc[val_idx]
            y_cv_val = y_train.iloc[val_idx]

            model.fit(X_cv_train, y_cv_train)
            y_pred = model.predict(X_cv_val)
            score = f1_score(y_cv_val, y_pred, average="weighted")
            scores.append(score)

        return np.mean(scores)

    def optimize_hyperparameters(
        self, features: pd.DataFrame, target: pd.Series, n_trials: int = 20
    ) -> Dict[str, Dict[str, Any]]:
        """
        Phase 39.5: Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–

        Args:
            features: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡
            target: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            n_trials: è©¦è¡Œå›æ•°

        Returns:
            Dict: å„ãƒ¢ãƒ‡ãƒ«ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        self.logger.info(f"ğŸ”¬ Phase 39.5: Optunaæœ€é©åŒ–é–‹å§‹ï¼ˆ{n_trials}è©¦è¡Œï¼‰")

        # Optunaãƒ­ã‚°æŠ‘åˆ¶
        optuna.logging.set_verbosity(optuna.logging.WARNING)

        optimal_params = {}
        optimization_results = {
            "created_at": datetime.now().isoformat(),
            "n_trials": n_trials,
            "models": {},
        }

        for model_name in ["lightgbm", "xgboost", "random_forest"]:
            self.logger.info(f"ğŸ“Š {model_name} æœ€é©åŒ–é–‹å§‹")

            try:
                # Objectiveé–¢æ•°é¸æŠï¼ˆE731: flake8 lambdaå›é¿ï¼‰
                def objective_func(trial: optuna.Trial) -> float:
                    if model_name == "lightgbm":
                        return self._objective_lightgbm(trial, features, target)
                    elif model_name == "xgboost":
                        return self._objective_xgboost(trial, features, target)
                    else:  # random_forest
                        return self._objective_random_forest(trial, features, target)

                # Optuna Studyä½œæˆãƒ»æœ€é©åŒ–å®Ÿè¡Œ
                study = optuna.create_study(direction="maximize", sampler=TPESampler(seed=42))
                study.optimize(objective_func, n_trials=n_trials, show_progress_bar=False)

                # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
                best_params = study.best_params
                best_score = study.best_value

                optimal_params[model_name] = best_params
                optimization_results["models"][model_name] = {
                    "best_params": best_params,
                    "best_score": float(best_score),
                    "n_trials": n_trials,
                }

                self.logger.info(
                    f"âœ… {model_name} æœ€é©åŒ–å®Œäº† - Best F1: {best_score:.4f}, "
                    f"Best params: {best_params}"
                )

            except Exception as e:
                self.logger.error(f"âŒ {model_name} æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                optimization_results["models"][model_name] = {"error": str(e)}

        # Phase 52.4: Optunaçµæœã¯thresholds.yaml:optuna_optimizedã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«çµ±åˆæ¸ˆã¿
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã¯ä¸è¦ï¼ˆè¨­å®šä¸€å…ƒåŒ–ï¼‰
        self.logger.info("ğŸ’¾ Optunaæœ€é©åŒ–å®Œäº†ï¼ˆçµæœã¯thresholds.yaml:optuna_optimizedã«çµ±åˆæ¸ˆã¿ï¼‰")

        return optimal_params

    def train_models(
        self, features: pd.DataFrame, target: pd.Series, dry_run: bool = False
    ) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Ÿè¡Œï¼ˆPhase 39.3-39.4å¯¾å¿œï¼‰"""
        self.logger.info("ğŸ¤– Phase 39.3-39.4 MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹")

        if dry_run:
            self.logger.info("ğŸ” ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³: å®Ÿéš›ã®å­¦ç¿’ã¯ã‚¹ã‚­ãƒƒãƒ—")
            return {"dry_run": True, "models": list(self.models.keys())}

        results = {}
        trained_models = {}

        # Phase 39.3: Train/Val/Test split (70/15/15)
        n_samples = len(features)
        train_size = int(n_samples * 0.70)
        val_size = int(n_samples * 0.15)

        X_train = features.iloc[:train_size]
        y_train = target.iloc[:train_size]
        X_val = features.iloc[train_size : train_size + val_size]
        y_val = target.iloc[train_size : train_size + val_size]
        X_test = features.iloc[train_size + val_size :]
        y_test = target.iloc[train_size + val_size :]

        self.logger.info(
            f"ğŸ“Š Phase 39.3: Train/Val/Test split - "
            f"Train: {len(X_train)} ({len(X_train) / n_samples:.1%}), "
            f"Val: {len(X_val)} ({len(X_val) / n_samples:.1%}), "
            f"Test: {len(X_test)} ({len(X_test) / n_samples:.1%})"
        )

        # Phase 39.5: Optuna hyperparameter optimization
        if self.optimize:
            self.logger.info("ğŸ”¬ Phase 39.5: Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–é–‹å§‹")
            optimal_params = self.optimize_hyperparameters(
                pd.concat([X_train, X_val]),
                pd.concat([y_train, y_val]),
                self.n_trials,
            )

            # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨
            for model_name in self.models.keys():
                if model_name in optimal_params:
                    self.models[model_name].set_params(**optimal_params[model_name])
                    self.logger.info(f"âœ… {model_name}: æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨å®Œäº†")

        # Phase 39.3: TimeSeriesSplit n_splits=5 for Cross Validation
        tscv = TimeSeriesSplit(n_splits=5)
        self.logger.info("ğŸ“Š Phase 39.3: TimeSeriesSplit n_splits=5 for CV")

        # Phase 39.4: XGBoost scale_pos_weightå‹•çš„è¨­å®š
        if self.n_classes == 2:
            pos_count = y_train.sum()
            neg_count = len(y_train) - pos_count
            if pos_count > 0:
                scale_pos_weight = neg_count / pos_count
                self.models["xgboost"].set_params(scale_pos_weight=scale_pos_weight)
                self.logger.info(f"ğŸ“Š Phase 39.4: XGBoost scale_pos_weight={scale_pos_weight:.2f}")

        for model_name, model in self.models.items():
            self.logger.info(f"ğŸ“ˆ {model_name} å­¦ç¿’é–‹å§‹")

            try:
                # Phase 39.3: Cross Validation with Early Stopping
                cv_scores = []

                for train_idx, val_idx in tscv.split(X_train):
                    X_cv_train = X_train.iloc[train_idx]
                    y_cv_train = y_train.iloc[train_idx]
                    X_cv_val = X_train.iloc[val_idx]
                    y_cv_val = y_train.iloc[val_idx]

                    # Phase 39.4: SMOTE Oversampling (CV fold)
                    if self.use_smote and self.n_classes == 2:
                        try:
                            smote = SMOTE(random_state=42)
                            X_cv_train_resampled, y_cv_train_resampled = smote.fit_resample(
                                X_cv_train, y_cv_train
                            )
                            # Convert back to DataFrame to preserve feature names
                            X_cv_train = pd.DataFrame(
                                X_cv_train_resampled, columns=X_cv_train.columns
                            )
                            y_cv_train = pd.Series(y_cv_train_resampled)
                            if len(X_cv_train_resampled) > len(X_cv_train):
                                self.logger.debug(
                                    f"ğŸ“Š Phase 39.4: SMOTEé©ç”¨ - CV fold "
                                    f"{len(train_idx)}â†’{len(X_cv_train_resampled)}ã‚µãƒ³ãƒ—ãƒ«"
                                )
                        except Exception as e:
                            self.logger.warning(
                                f"âš ï¸ SMOTEé©ç”¨å¤±æ•—ï¼ˆCV foldï¼‰: {e}, å…ƒãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ç¶™ç¶š"
                            )

                    # Phase 39.3: Early Stopping for LightGBM and XGBoost
                    if model_name == "lightgbm":
                        try:
                            model.fit(
                                X_cv_train,
                                y_cv_train,
                                eval_set=[(X_cv_val, y_cv_val)],
                                callbacks=[
                                    # LightGBM 4.0+ uses callbacks instead of early_stopping_rounds
                                    __import__("lightgbm").early_stopping(
                                        stopping_rounds=20, verbose=False
                                    )
                                ],
                            )
                        except ValueError as e:
                            # Handle unseen labels in CV folds (small datasets)
                            if "previously unseen labels" in str(e):
                                model.fit(X_cv_train, y_cv_train)
                            else:
                                raise
                    elif model_name == "xgboost":
                        # XGBoost 2.0+ uses callbacks for early stopping
                        try:
                            from xgboost import callback as xgb_callback

                            model.fit(
                                X_cv_train,
                                y_cv_train,
                                eval_set=[(X_cv_val, y_cv_val)],
                                callbacks=[xgb_callback.EarlyStopping(rounds=20)],
                                verbose=False,
                            )
                        except Exception:
                            # Fallback: train without early stopping
                            model.fit(X_cv_train, y_cv_train)
                    else:
                        # RandomForest doesn't support early stopping
                        model.fit(X_cv_train, y_cv_train)

                    # äºˆæ¸¬ãƒ»è©•ä¾¡
                    y_pred = model.predict(X_cv_val)
                    score = f1_score(y_cv_val, y_pred, average="weighted")
                    cv_scores.append(score)

                # Phase 39.3: Final model training on Train+Val with Early Stopping
                X_train_val = pd.concat([X_train, X_val])
                y_train_val = pd.concat([y_train, y_val])

                # Phase 39.4: SMOTE Oversampling (Final training)
                if self.use_smote and self.n_classes == 2:
                    try:
                        smote = SMOTE(random_state=42)
                        X_train_val_resampled, y_train_val_resampled = smote.fit_resample(
                            X_train_val, y_train_val
                        )
                        # Convert back to DataFrame to preserve feature names
                        X_train_val = pd.DataFrame(
                            X_train_val_resampled, columns=X_train_val.columns
                        )
                        y_train_val = pd.Series(y_train_val_resampled)
                        self.logger.info(
                            f"ğŸ“Š Phase 39.4: SMOTEé©ç”¨ï¼ˆFinal trainingï¼‰ - "
                            f"{len(X_train) + len(X_val)}â†’{len(X_train_val_resampled)}ã‚µãƒ³ãƒ—ãƒ«"
                        )
                    except Exception as e:
                        self.logger.warning(
                            f"âš ï¸ SMOTEé©ç”¨å¤±æ•—ï¼ˆFinal trainingï¼‰: {e}, å…ƒãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ç¶™ç¶š"
                        )

                if model_name == "lightgbm":
                    model.fit(
                        X_train_val,
                        y_train_val,
                        eval_set=[(X_test, y_test)],
                        callbacks=[
                            __import__("lightgbm").early_stopping(stopping_rounds=20, verbose=False)
                        ],
                    )
                    self.logger.info(
                        f"ğŸ“Š Phase 39.3: {model_name} Early Stopping enabled (rounds=20)"
                    )
                elif model_name == "xgboost":
                    # XGBoost 2.0+ uses callbacks for early stopping
                    try:
                        from xgboost import callback as xgb_callback

                        model.fit(
                            X_train_val,
                            y_train_val,
                            eval_set=[(X_test, y_test)],
                            callbacks=[xgb_callback.EarlyStopping(rounds=20)],
                            verbose=False,
                        )
                        self.logger.info(
                            f"ğŸ“Š Phase 39.3: {model_name} Early Stopping enabled (rounds=20)"
                        )
                    except Exception as e:
                        # Fallback: train without early stopping
                        self.logger.warning(
                            f"âš ï¸ XGBoost Early Stopping failed: {e}, training without it"
                        )
                        model.fit(X_train_val, y_train_val)
                else:
                    # RandomForest: Train on Train+Val without early stopping
                    model.fit(X_train_val, y_train_val)

                # Test set evaluation
                y_test_pred = model.predict(X_test)
                test_metrics = {
                    "accuracy": accuracy_score(y_test, y_test_pred),
                    "f1_score": f1_score(y_test, y_test_pred, average="weighted"),
                    "precision": precision_score(
                        y_test, y_test_pred, average="weighted", zero_division=0
                    ),
                    "recall": recall_score(
                        y_test, y_test_pred, average="weighted", zero_division=0
                    ),
                    "cv_f1_mean": np.mean(cv_scores),
                    "cv_f1_std": np.std(cv_scores),
                }

                results[model_name] = test_metrics
                trained_models[model_name] = model

                self.logger.info(
                    f"âœ… {model_name} å­¦ç¿’å®Œäº† - Test F1: {test_metrics['f1_score']:.3f}, "
                    f"CV F1: {test_metrics['cv_f1_mean']:.3f}Â±{test_metrics['cv_f1_std']:.3f}"
                )

            except Exception as e:
                self.logger.error(f"âŒ {model_name} å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
                results[model_name] = {"error": str(e)}

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆindividual_modelsã®ã¿ã‚’ä½¿ç”¨ãƒ»å¾ªç’°å‚ç…§é˜²æ­¢ï¼‰
        if len(trained_models) >= 2:
            try:
                # ProductionEnsembleè‡ªä½“ã‚’å«ã¾ãªã„ã‚ˆã†ã«å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®ã¿æ¸¡ã™
                individual_models_only = {
                    k: v for k, v in trained_models.items() if k != "production_ensemble"
                }
                ensemble_model = self._create_ensemble(individual_models_only)
                trained_models["production_ensemble"] = ensemble_model
                self.logger.info("âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†ï¼ˆå¾ªç’°å‚ç…§é˜²æ­¢å¯¾å¿œï¼‰")
            except Exception as e:
                self.logger.error(f"âŒ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

        return {
            "results": results,
            "models": trained_models,
            "feature_names": list(features.columns),
            "training_samples": len(features),
        }

    def _create_ensemble(self, models: Dict) -> ProductionEnsemble:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆProductionEnsembleã‚¯ãƒ©ã‚¹ä½¿ç”¨ï¼‰."""
        try:
            self.logger.info("ğŸ”§ ProductionEnsembleã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
            ensemble_model = ProductionEnsemble(models)
            self.logger.info("âœ… ProductionEnsembleã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†")
            return ensemble_model
        except Exception as e:
            self.logger.error(f"âŒ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def save_models(self, training_results: Dict[str, Any]) -> Dict[str, str]:
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆå€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ï¼štrainingã€çµ±åˆãƒ¢ãƒ‡ãƒ«ï¼šproductionï¼‰."""
        self.logger.info("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜é–‹å§‹")

        saved_files = {}
        models = training_results.get("models", {})

        for model_name, model in models.items():
            try:
                if model_name == "production_ensemble":
                    # Phase 51.5-A Fix: feature_order.jsonã‹ã‚‰è¨­å®šé§†å‹•å‹ã§ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«åå–å¾—
                    target_model_type = self.current_model_type
                    model_config = _feature_manager.get_feature_level_info()
                    model_filename = model_config[target_model_type].get(
                        "model_file", "ensemble_full.pkl"
                    )

                    # æœ¬ç•ªç”¨çµ±åˆãƒ¢ãƒ‡ãƒ«ã¯productionãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
                    model_file = self.production_dir / model_filename
                    with open(model_file, "wb") as f:
                        pickle.dump(model, f)

                    # Gitæƒ…å ±å–å¾—
                    try:
                        git_commit = self._get_git_info()
                    except Exception:
                        git_commit = {"commit": "unknown", "branch": "unknown"}

                    # æœ¬ç•ªç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆPhase 50.9å®Œäº†: å¤–éƒ¨APIå®Œå…¨å‰Šé™¤ãƒ»2æ®µéšGraceful Degradationï¼‰
                    production_metadata = {
                        "created_at": datetime.now().isoformat(),
                        "model_type": "ProductionEnsemble",
                        "model_file": str(model_file),
                        "version": "1.0.0",
                        "phase": "Phase 50.9",  # Phase 50.9å®Œäº†: å¤–éƒ¨APIå®Œå…¨å‰Šé™¤ãƒ»ã‚·ãƒ³ãƒ—ãƒ«è¨­è¨ˆå›å¸°
                        "status": "production_ready",
                        "feature_names": training_results.get("feature_names", []),
                        "individual_models": [
                            k for k in model.models.keys() if k != "production_ensemble"
                        ],
                        "model_weights": model.weights,
                        "performance_metrics": training_results.get("results", {}),
                        "training_info": {
                            "samples": training_results.get("training_samples", 0),
                            "feature_count": len(training_results.get("feature_names", [])),
                            "training_duration_seconds": getattr(self, "_training_start_time", 0),
                        },
                        "git_info": git_commit,
                        "notes": "Phase 52.4å®Œäº†ãƒ»6æˆ¦ç•¥çµ±åˆãƒ»55ç‰¹å¾´é‡ã‚·ã‚¹ãƒ†ãƒ ãƒ»çœŸã®3ã‚¯ãƒ©ã‚¹åˆ†é¡ãƒ»2æ®µéšGraceful Degradationãƒ»TimeSeriesSplit n_splits=5ãƒ»Early Stoppingãƒ»SMOTEãƒ»Optunaæœ€é©åŒ–",
                    }

                    # Phase 51.5-A Fix: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«åæ±ºå®š
                    # fullãƒ¢ãƒ‡ãƒ«ã¯æ¤œè¨¼ç”¨ã«production_model_metadata.jsonã«ä¿å­˜
                    # basicãƒ¢ãƒ‡ãƒ«ã¯åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                    if self.current_model_type == "full":
                        production_metadata_file = (
                            self.production_dir / "production_model_metadata.json"
                        )
                    else:
                        production_metadata_file = (
                            self.production_dir
                            / f"production_model_metadata_{self.current_model_type}.json"
                        )

                    with open(production_metadata_file, "w", encoding="utf-8") as f:
                        json.dump(
                            production_metadata,
                            f,
                            indent=2,
                            ensure_ascii=False,
                        )

                    self.logger.info(f"âœ… æœ¬ç•ªç”¨çµ±åˆãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_file}")
                else:
                    # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã¯trainingãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
                    model_file = self.training_dir / f"{model_name}_model.pkl"
                    with open(model_file, "wb") as f:
                        pickle.dump(model, f)

                    self.logger.info(f"âœ… å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_file}")

                saved_files[model_name] = str(model_file)

            except Exception as e:
                self.logger.error(f"âŒ {model_name} ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        # å­¦ç¿’ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆPhase 51.9å®Œäº†: çœŸã®3ã‚¯ãƒ©ã‚¹åˆ†é¡ãƒ»6æˆ¦ç•¥çµ±åˆãƒ»Strategy-Aware MLãƒ»å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’ï¼‰
        training_metadata = {
            "created_at": datetime.now().isoformat(),
            "feature_names": training_results.get("feature_names", []),
            "training_samples": training_results.get("training_samples", 0),
            "model_metrics": training_results.get("results", {}),
            "model_files": saved_files,
            "config_path": self.config_path,
            "phase": "Phase 51.9",  # Phase 51.9å®Œäº†: çœŸã®3ã‚¯ãƒ©ã‚¹åˆ†é¡å®Ÿè£…
            "notes": "Phase 51.9å®Œäº†ãƒ»çœŸã®3ã‚¯ãƒ©ã‚¹åˆ†é¡å®Ÿè£…ï¼ˆmulticlass paramsè¿½åŠ ï¼‰ãƒ»6æˆ¦ç•¥çµ±åˆï¼ˆATRBased/DonchianChannel/ADXTrendStrength/BBReversal/StochasticReversal/MACDEMACrossoverï¼‰ãƒ»å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’ï¼ˆè¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã®ä¸€è²«æ€§ç¢ºä¿ï¼‰ãƒ»55ç‰¹å¾´é‡ãƒ»é–¾å€¤0.5%ãƒ»CV n_splits=5ãƒ»Early Stoppingãƒ»SMOTEãƒ»Optunaæœ€é©åŒ–ãƒ»å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’çµæœ",
        }

        training_metadata_file = self.training_dir / "training_metadata.json"
        with open(training_metadata_file, "w", encoding="utf-8") as f:
            json.dump(training_metadata, f, indent=2, ensure_ascii=False)

        self.logger.info(f"âœ… å­¦ç¿’ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {training_metadata_file}")
        return saved_files

    def validate_models(self, saved_files: Dict[str, str]) -> bool:
        """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ï¼ˆæœ¬ç•ªç”¨ãƒ¢ãƒ‡ãƒ«é‡ç‚¹ï¼‰."""
        self.logger.info("ğŸ” ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼é–‹å§‹")

        validation_passed = True

        for model_name, model_file in saved_files.items():
            try:
                # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
                with open(model_file, "rb") as f:
                    model = pickle.load(f)

                # åŸºæœ¬å±æ€§ãƒã‚§ãƒƒã‚¯
                if hasattr(model, "predict"):
                    self.logger.info(f"âœ… {model_name}: predict ãƒ¡ã‚½ãƒƒãƒ‰ç¢ºèª")
                else:
                    self.logger.error(f"âŒ {model_name}: predict ãƒ¡ã‚½ãƒƒãƒ‰ãªã—")
                    validation_passed = False
                    continue

                # Phase 50.9: ãƒ¢ãƒ‡ãƒ«åˆ¥ç‰¹å¾´é‡æ•°å¯¾å¿œã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬ãƒ†ã‚¹ãƒˆ
                # ãƒ¢ãƒ‡ãƒ«ã®å®Ÿéš›ã®ç‰¹å¾´é‡æ•°ã‚’å–å¾—ï¼ˆLightGBMãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ï¼‰
                if hasattr(model, "models") and "lightgbm" in model.models:
                    # ProductionEnsembleã®å ´åˆ
                    lgbm_model = model.models["lightgbm"]
                    if hasattr(lgbm_model, "n_features_in_"):
                        n_features = lgbm_model.n_features_in_
                    elif hasattr(lgbm_model, "_n_features"):
                        n_features = lgbm_model._n_features
                    else:
                        n_features = len(self.expected_features)
                elif hasattr(model, "n_features_in_"):
                    n_features = model.n_features_in_
                else:
                    n_features = len(self.expected_features)

                # ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ãŸç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’å–å¾—
                feature_list = self.expected_features[:n_features]
                sample_features_array = np.random.random((5, n_features))
                sample_features = pd.DataFrame(sample_features_array, columns=feature_list)
                prediction = model.predict(sample_features)

                if len(prediction) == 5:
                    self.logger.info(f"âœ… {model_name}: ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬æˆåŠŸ")
                else:
                    self.logger.error(f"âŒ {model_name}: ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬å¤±æ•—")
                    validation_passed = False
                    continue

                # æœ¬ç•ªç”¨ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°æ¤œè¨¼
                if model_name == "production_ensemble":
                    self.logger.info("ğŸ¯ æœ¬ç•ªç”¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è©³ç´°æ¤œè¨¼é–‹å§‹")

                    # predict_proba ãƒ¡ã‚½ãƒƒãƒ‰ç¢ºèªï¼ˆPhase 51.9-6D: 3ã‚¯ãƒ©ã‚¹å¯¾å¿œï¼‰
                    if hasattr(model, "predict_proba"):
                        probabilities = model.predict_proba(sample_features)
                        expected_shape = (5, self.n_classes)
                        if probabilities.shape == expected_shape:
                            self.logger.info(f"âœ… predict_proba ç¢ºèªæˆåŠŸï¼ˆ{self.n_classes}ã‚¯ãƒ©ã‚¹ï¼‰")
                        else:
                            self.logger.error(
                                f"âŒ predict_proba å½¢çŠ¶ä¸æ­£: {probabilities.shape} "
                                f"!= {expected_shape}"
                            )
                            validation_passed = False

                    # Phase 50.9: ãƒ¢ãƒ‡ãƒ«åˆ¥ç‰¹å¾´é‡æ•°å¯¾å¿œ - get_model_infoç¢ºèª
                    if hasattr(model, "get_model_info"):
                        info = model.get_model_info()
                        # ã™ã§ã«å–å¾—æ¸ˆã¿ã®n_featuresï¼ˆãƒ¢ãƒ‡ãƒ«åˆ¥ï¼‰ã‚’ä½¿ç”¨
                        if info.get("n_features") == n_features:
                            self.logger.info(f"âœ… get_model_info ç¢ºèªæˆåŠŸï¼ˆ{n_features}ç‰¹å¾´é‡ï¼‰")
                        else:
                            self.logger.error(
                                f"âŒ get_model_info ç‰¹å¾´é‡æ•°ä¸æ­£: "
                                f"{info.get('n_features')} != {n_features}"
                            )
                            validation_passed = False

                    self.logger.info("ğŸ¯ æœ¬ç•ªç”¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è©³ç´°æ¤œè¨¼å®Œäº†")

            except Exception as e:
                self.logger.error(f"âŒ {model_name} æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
                validation_passed = False

        if validation_passed:
            self.logger.info("ğŸ‰ å…¨ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼æˆåŠŸï¼")
        else:
            self.logger.error("ğŸ’¥ ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼å¤±æ•—")

        return validation_passed

    def _get_git_info(self) -> Dict[str, str]:
        """Gitæƒ…å ±å–å¾—ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ç”¨ï¼‰."""
        import subprocess

        try:
            # Git commit hashå–å¾—
            commit = subprocess.check_output(
                ["git", "rev-parse", "HEAD"], text=True, cwd=project_root
            ).strip()

            # Git branchå–å¾—
            branch = subprocess.check_output(
                ["git", "rev-parse", "--abbrev-ref", "HEAD"],
                text=True,
                cwd=project_root,
            ).strip()

            return {"commit": commit, "commit_short": commit[:8], "branch": branch}
        except Exception as e:
            self.logger.warning(f"Gitæƒ…å ±å–å¾—å¤±æ•—: {e}")
            return {"commit": "unknown", "commit_short": "unknown", "branch": "unknown"}

    def _archive_existing_models(self) -> bool:
        """
        æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆPhase 52.4: 2æ®µéšã‚·ã‚¹ãƒ†ãƒ ï¼‰

        Phase 52.4å¯¾å¿œãƒ¢ãƒ‡ãƒ«ï¼š
        - ensemble_full.pklï¼ˆ55ç‰¹å¾´é‡ï¼‰
        - ensemble_basic.pklï¼ˆ49ç‰¹å¾´é‡ï¼‰
        """
        try:
            # 2æ®µéšãƒ¢ãƒ‡ãƒ«ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œ
            level_files = [
                "ensemble_full.pkl",
                "ensemble_basic.pkl",
            ]

            archived_any = False
            for model_filename in level_files:
                production_model = self.production_dir / model_filename

                if production_model.exists():
                    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
                    archive_dir = Path("models/archive")
                    archive_dir.mkdir(exist_ok=True)

                    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    model_name = model_filename.replace(".pkl", "")
                    archive_model = archive_dir / f"{model_name}_{timestamp}.pkl"

                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼
                    import shutil

                    shutil.copy2(production_model, archive_model)

                    self.logger.info(f"âœ… æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å®Œäº†: {archive_model}")
                    archived_any = True

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
            production_metadata = self.production_dir / "production_model_metadata.json"
            if production_metadata.exists():
                archive_dir = Path("models/archive")
                archive_dir.mkdir(exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                archive_metadata = archive_dir / f"production_model_metadata_{timestamp}.json"

                import shutil

                shutil.copy2(production_metadata, archive_metadata)
                self.logger.info(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å®Œäº†: {archive_metadata}")

            if not archived_any:
                self.logger.info("ğŸ“‚ æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ãªã— - ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¹ã‚­ãƒƒãƒ—")

            return True

        except Exception as e:
            self.logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def run(self, dry_run: bool = False, days: int = 180) -> bool:
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œå‡¦ç†ï¼ˆPhase 51.5-A Fix: ä¸€æ‹¬ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼‰."""
        try:
            self.logger.info(f"ğŸš€ Phase 51.9: MLãƒ¢ãƒ‡ãƒ«ä½œæˆé–‹å§‹ - è¨“ç·´å¯¾è±¡: {self.models_to_train}")

            # 0. æ—¢å­˜ãƒ¢ãƒ‡ãƒ«è‡ªå‹•ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆPhase 29: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†å¼·åŒ–ï¼‰
            if not dry_run:
                if not self._archive_existing_models():
                    self.logger.warning("âš ï¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¤±æ•— - å‡¦ç†ç¶šè¡Œ")

            # 1. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆ1å›ã®ã¿ãƒ»å…¨55ç‰¹å¾´é‡ç”Ÿæˆï¼‰
            # æˆ¦ç•¥ä¿¡å·ç”ŸæˆãŒæœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ã€1å›ã ã‘å®Ÿè¡Œ
            self.logger.info("ğŸ“Š Phase 52.4: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«å…±é€šï¼‰")
            # ä¸€æ™‚çš„ã«current_model_typeã‚’"full"ã«è¨­å®šã—ã¦å…¨ç‰¹å¾´é‡ç”Ÿæˆ
            self.current_model_type = "full"
            features_full, target = self.prepare_training_data(days)
            self.logger.info("âœ… å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†ï¼ˆå…¨55ç‰¹å¾´é‡ç”Ÿæˆæ¸ˆã¿ï¼‰")

            # 2. å„ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ï¼ˆãƒ«ãƒ¼ãƒ—å‡¦ç†ï¼‰
            all_saved_files = {}
            for model_type in self.models_to_train:
                self.current_model_type = model_type
                model_name = "fullï¼ˆ55ç‰¹å¾´é‡ï¼‰" if model_type == "full" else "basicï¼ˆ49ç‰¹å¾´é‡ï¼‰"

                self.logger.info("")
                self.logger.info("=" * 80)
                self.logger.info(f"ğŸ“Š Phase 52.4: {model_name}ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹")
                self.logger.info("=" * 80)

                # ãƒ¢ãƒ‡ãƒ«åˆ¥ç‰¹å¾´é‡é¸æŠï¼ˆCRITICAL FIX: ãƒ«ãƒ¼ãƒ—å†…ã§å†å®Ÿè¡Œï¼‰
                features = self._select_features_by_level(features_full.copy())
                self.logger.info(
                    f"âœ… ç‰¹å¾´é‡é¸æŠå®Œäº†: {len(features.columns)}å€‹ï¼ˆ{model_type}ãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰"
                )

                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                training_results = self.train_models(features, target, dry_run)

                if dry_run:
                    self.logger.info(f"ğŸ” {model_name}ãƒ¢ãƒ‡ãƒ« ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Œäº†")
                    continue

                # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
                saved_files = self.save_models(training_results)
                all_saved_files.update(saved_files)

                self.logger.info(f"âœ… {model_name}ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»ä¿å­˜å®Œäº†")

            if dry_run:
                self.logger.info("ğŸ” å…¨ãƒ¢ãƒ‡ãƒ« ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Œäº†")
                return True

            # 3. æ¤œè¨¼ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«ï¼‰
            self.logger.info("")
            self.logger.info("=" * 80)
            self.logger.info("ğŸ” Phase 51.5-A Fix: å…¨ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼é–‹å§‹")
            self.logger.info("=" * 80)

            validation_passed = self.validate_models(all_saved_files)

            if validation_passed:
                self.logger.info("âœ… Phase 51.5-A Fix: å…¨MLãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº† - å®Ÿå–å¼•æº–å‚™å®Œäº†")
                return True
            else:
                self.logger.error("âŒ MLãƒ¢ãƒ‡ãƒ«ä½œæˆå¤±æ•—")
                return False

        except Exception as e:
            self.logger.error(f"ğŸ’¥ MLãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆPhase 39.1-39.5å®Œäº†ï¼‰"""
    parser = argparse.ArgumentParser(
        description="æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨MLãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆPhase 39.1-39.5å®Œäº†ãƒ»MLä¿¡é ¼åº¦å‘ä¸ŠæœŸï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆå®Ÿéš›ã®å­¦ç¿’ãƒ»ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰",
    )
    parser.add_argument("--verbose", action="store_true", help="è©³ç´°ãƒ­ã‚°å‡ºåŠ›")
    parser.add_argument(
        "--days",
        type=int,
        default=180,
        help="å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æœŸé–“ï¼ˆæ—¥æ•°ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 180æ—¥ï¼‰",
    )
    parser.add_argument("--config", default="config/core/unified.yaml", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")

    # Phase 39.2: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å®šå¼•æ•°
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.005,
        help="Phase 39.2: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.5%%ï¼‰",
    )
    parser.add_argument(
        "--n-classes",
        type=int,
        default=2,
        choices=[2, 3],
        help="Phase 39.2: ã‚¯ãƒ©ã‚¹æ•° 2ï¼ˆBUY/OTHERï¼‰ or 3ï¼ˆBUY/HOLD/SELLï¼‰",
    )

    # Phase 39.4: SMOTEè¨­å®šå¼•æ•°
    parser.add_argument(
        "--use-smote",
        action="store_true",
        help="Phase 39.4: SMOTEã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æœ‰åŠ¹åŒ–ï¼ˆã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–ï¼‰",
    )

    # Phase 39.5: Optunaæœ€é©åŒ–è¨­å®šå¼•æ•°
    parser.add_argument(
        "--optimize",
        action="store_true",
        help="Phase 39.5: Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–æœ‰åŠ¹åŒ–",
    )
    parser.add_argument(
        "--n-trials",
        type=int,
        default=20,
        help="Phase 39.5: Optunaæœ€é©åŒ–è©¦è¡Œå›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰",
    )

    # Phase 51.5-B: ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆä¸€æ‹¬ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼‰
    parser.add_argument(
        "--model",
        type=str,
        default="both",
        choices=["both", "full", "basic"],
        help="Phase 51.5-B: è¨“ç·´ã™ã‚‹ãƒ¢ãƒ‡ãƒ« both=ä¸¡æ–¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨å¥¨ï¼‰/full=fullã®ã¿/basic=basicã®ã¿",
    )

    args = parser.parse_args()

    # ãƒ¢ãƒ‡ãƒ«é¸æŠã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
    if args.model == "both":
        models_to_train = ["full", "basic"]
    elif args.model == "full":
        models_to_train = ["full"]
    elif args.model == "basic":
        models_to_train = ["basic"]
    else:
        models_to_train = ["full", "basic"]

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Ÿè¡Œï¼ˆPhase 51.5-Bå¯¾å¿œï¼‰
    creator = NewSystemMLModelCreator(
        config_path=args.config,
        models_to_train=models_to_train,
        verbose=args.verbose,
        target_threshold=args.threshold,
        n_classes=args.n_classes,
        use_smote=args.use_smote,
        optimize=args.optimize,
        n_trials=args.n_trials,
    )

    success = creator.run(dry_run=args.dry_run, days=args.days)

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
