#!/usr/bin/env python3
"""
æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨MLãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ.

Phase 9å¯¾å¿œ: 12ç‰¹å¾´é‡æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ç”¨ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®retrain_97_features_model.pyã‚’å‚è€ƒã«æ–°ã‚·ã‚¹ãƒ†ãƒ æ§‹é€ ã§å®Ÿè£…

æ©Ÿèƒ½:
- 12ç‰¹å¾´é‡ã§ã® LightGBMãƒ»XGBoostãƒ»RandomForest ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’
- æ–°ã‚·ã‚¹ãƒ†ãƒ  src/ æ§‹é€ ã«å¯¾å¿œ
- models/production/ ã«ãƒ¢ãƒ‡ãƒ«ä¿å­˜
- å®Ÿå–å¼•å‰ã®å“è³ªä¿è¨¼ãƒ»æ€§èƒ½æ¤œè¨¼

ä½¿ç”¨æ–¹æ³•:
    python scripts/create_ml_models.py [--dry-run] [--verbose].
"""

import argparse
import json
import logging
import pickle
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Tuple

import numpy as np
import pandas as pd
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import TimeSeriesSplit
from xgboost import XGBClassifier

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from src.core.config import load_config
    from src.core.logger import get_logger
    from src.data.data_pipeline import DataPipeline, DataRequest, TimeFrame
    from src.features.technical import TechnicalIndicators
    from src.ml.ensemble import ProductionEnsemble
except ImportError as e:
    print(f"âŒ æ–°ã‚·ã‚¹ãƒ†ãƒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    print("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)


class NewSystemMLModelCreator:
    """æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨MLãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ»å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ."""

    def __init__(self, config_path: str = "config/core/base.yaml", verbose: bool = False):
        """åˆæœŸåŒ–."""
        self.config_path = config_path
        self.verbose = verbose

        # ãƒ­ã‚°è¨­å®š
        self.logger = get_logger()
        if verbose:
            logging.getLogger().setLevel(logging.DEBUG)

        # è¨­å®šèª­ã¿è¾¼ã¿
        try:
            self.config = load_config(config_path)
            self.logger.info(f"âœ… è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†: {config_path}")
        except Exception as e:
            self.logger.error(f"âŒ è¨­å®šèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            raise

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.training_dir = Path("models/training")
        self.production_dir = Path("models/production")
        self.training_dir.mkdir(parents=True, exist_ok=True)
        self.production_dir.mkdir(parents=True, exist_ok=True)

        # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
        try:
            self.data_pipeline = DataPipeline()
            self.logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å¤±æ•—: {e}")
            raise

        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        self.technical_indicators = TechnicalIndicators()

        # 12ç‰¹å¾´é‡å®šç¾©ï¼ˆæ–°ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–æ¸ˆã¿ï¼‰
        self.expected_features = [
            "close",
            "volume",
            "returns_1",
            "rsi_14",
            "macd",
            "macd_signal",
            "atr_14",
            "bb_position",
            "ema_20",
            "ema_50",
            "zscore",
            "volume_ratio",
        ]

        self.logger.info(f"ğŸ¯ å¯¾è±¡ç‰¹å¾´é‡: {len(self.expected_features)}å€‹ï¼ˆæ–°ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–æ¸ˆã¿ï¼‰")

        # MLãƒ¢ãƒ‡ãƒ«è¨­å®š
        self.models = {
            "lightgbm": LGBMClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=8,
                num_leaves=31,
                random_state=42,
                verbose=-1,
            ),
            "xgboost": XGBClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=8,
                random_state=42,
                eval_metric="logloss",
                verbosity=0,
            ),
            "random_forest": RandomForestClassifier(
                n_estimators=200,
                max_depth=12,
                random_state=42,
                n_jobs=-1,
            ),
        }

    def prepare_training_data(self, days: int = 180) -> Tuple[pd.DataFrame, pd.Series]:
        """å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆæ–°ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼‰."""
        self.logger.info(f"ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹ï¼ˆéå»{days}æ—¥åˆ†ï¼‰")

        try:
            # éå»ãƒ‡ãƒ¼ã‚¿å–å¾—

            # 1æ™‚é–“è¶³ãƒ‡ãƒ¼ã‚¿å–å¾—
            request = DataRequest(
                symbol="BTC/JPY",
                timeframe=TimeFrame.H1,
                limit=days * 24,
                since=None,  # 1æ—¥24æ™‚é–“
            )
            df = self.data_pipeline.fetch_ohlcv(request)

            if df is None or len(df) < 100:
                self.logger.warning("âŒ å®Ÿãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—ã€ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ")
                df = self._generate_sample_data(days * 24)

            self.logger.info(f"âœ… åŸºæœ¬ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(df)}è¡Œ")

            # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
            features_df = self.technical_indicators.generate_all_features(df)

            # 12ç‰¹å¾´é‡ã¸ã®æ•´åˆæ€§ç¢ºä¿
            features_df = self._ensure_feature_consistency(features_df)

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆä¾¡æ ¼å¤‰å‹•ã«ã‚ˆã‚‹åˆ†é¡ï¼‰
            target = self._generate_target(df)

            # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
            features_df, target = self._clean_data(features_df, target)

            self.logger.info(
                f"âœ… å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(features_df)}ã‚µãƒ³ãƒ—ãƒ«ã€{len(features_df.columns)}ç‰¹å¾´é‡"
            )
            return features_df, target

        except Exception as e:
            self.logger.error(f"âŒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _generate_sample_data(self, samples: int) -> pd.DataFrame:
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰."""
        self.logger.info(f"ğŸ”§ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: {samples}ã‚µãƒ³ãƒ—ãƒ«")

        # æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        dates = pd.date_range(end=datetime.now(), periods=samples, freq="h")

        # ãƒªã‚¢ãƒ«ãªBTCä¾¡æ ¼å‹•å‘ã‚’æ¨¡æ“¬
        np.random.seed(42)
        base_price = 5000000  # 5M JPY
        prices = []
        current_price = base_price

        for i in range(samples):
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼ˆæ™‚é–“å¸¯ã«ã‚ˆã‚‹å¤‰å‹•å¹…èª¿æ•´ï¼‰
            hour = dates[i].hour
            volatility = 0.015 if 8 <= hour <= 20 else 0.008  # æ—¥ä¸­é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            change = np.random.normal(0, volatility)
            current_price *= 1 + change
            prices.append(current_price)

        # OHLCV ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        df = pd.DataFrame(
            {
                "timestamp": dates,
                "open": prices,
                "high": [p * (1 + abs(np.random.normal(0, 0.005))) for p in prices],
                "low": [p * (1 - abs(np.random.normal(0, 0.005))) for p in prices],
                "close": prices,
                "volume": np.random.lognormal(8, 1, samples),  # ãƒªã‚¢ãƒ«ãªå‡ºæ¥é«˜åˆ†å¸ƒ
            }
        )

        df.set_index("timestamp", inplace=True)
        return df

    def _ensure_feature_consistency(self, features_df: pd.DataFrame) -> pd.DataFrame:
        """12ç‰¹å¾´é‡ã¸ã®æ•´åˆæ€§ç¢ºä¿."""
        # ä¸è¶³ç‰¹å¾´é‡ã®0åŸ‹ã‚
        for feature in self.expected_features:
            if feature not in features_df.columns:
                features_df[feature] = 0.0
                self.logger.warning(f"âš ï¸ ä¸è¶³ç‰¹å¾´é‡ã‚’0åŸ‹ã‚: {feature}")

        # 12ç‰¹å¾´é‡ã®ã¿é¸æŠ
        features_df = features_df[self.expected_features]

        if len(features_df.columns) != 12:
            self.logger.warning(f"âš ï¸ ç‰¹å¾´é‡æ•°ä¸ä¸€è‡´: {len(features_df.columns)} != 12")

        return features_df

    def _generate_target(self, df: pd.DataFrame) -> pd.Series:
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆä¾¡æ ¼å¤‰å‹•ã«ã‚ˆã‚‹åˆ†é¡ï¼‰."""
        # 1æ™‚é–“å¾Œã®ä¾¡æ ¼å¤‰å‹•ç‡
        price_change = df["close"].pct_change(periods=1).shift(-1)

        # 0.3%ä»¥ä¸Šã®ä¸Šæ˜‡ã‚’BUYï¼ˆ1ï¼‰ã€ãã‚Œä»¥å¤–ã‚’HOLD/SELLï¼ˆ0ï¼‰
        target = (price_change > 0.003).astype(int)

        buy_ratio = target.mean()
        self.logger.info(f"ğŸ“Š ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: BUY {buy_ratio:.1%}, HOLD/SELL {1 - buy_ratio:.1%}")

        return target

    def _clean_data(
        self, features_df: pd.DataFrame, target: pd.Series
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°."""
        # NaNé™¤å»
        valid_mask = ~(features_df.isna().any(axis=1) | target.isna())
        features_clean = features_df[valid_mask].copy()
        target_clean = target[valid_mask].copy()

        # ç„¡é™å€¤é™¤å»
        features_clean.replace([np.inf, -np.inf], np.nan, inplace=True)
        features_clean.fillna(0, inplace=True)

        removed_samples = len(features_df) - len(features_clean)
        if removed_samples > 0:
            self.logger.info(f"ğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°: {removed_samples}ã‚µãƒ³ãƒ—ãƒ«é™¤å»")

        return features_clean, target_clean

    def train_models(
        self, features: pd.DataFrame, target: pd.Series, dry_run: bool = False
    ) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Ÿè¡Œ."""
        self.logger.info("ğŸ¤– MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹")

        if dry_run:
            self.logger.info("ğŸ” ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³: å®Ÿéš›ã®å­¦ç¿’ã¯ã‚¹ã‚­ãƒƒãƒ—")
            return {"dry_run": True, "models": list(self.models.keys())}

        results = {}
        trained_models = {}

        # TimeSeriesSplit ã«ã‚ˆã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿å¯¾å¿œã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        tscv = TimeSeriesSplit(n_splits=3)

        for model_name, model in self.models.items():
            self.logger.info(f"ğŸ“ˆ {model_name} å­¦ç¿’é–‹å§‹")

            try:
                # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡
                cv_scores = []

                for train_idx, val_idx in tscv.split(features):
                    X_train, X_val = (
                        features.iloc[train_idx],
                        features.iloc[val_idx],
                    )
                    y_train, y_val = (
                        target.iloc[train_idx],
                        target.iloc[val_idx],
                    )

                    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆDataFrameã®ã¾ã¾æ¸¡ã—ã¦sklearnè­¦å‘Šå›é¿ï¼‰
                    model.fit(X_train, y_train)

                    # äºˆæ¸¬ãƒ»è©•ä¾¡
                    y_pred = model.predict(X_val)
                    score = f1_score(y_val, y_pred, average="weighted")
                    cv_scores.append(score)

                # å…¨ãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆDataFrameã®ã¾ã¾æ¸¡ã—ã¦sklearnè­¦å‘Šå›é¿ï¼‰
                # featuresãŒDataFrameã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºå®Ÿã«ã™ã‚‹
                if not isinstance(features, pd.DataFrame):
                    features = pd.DataFrame(features, columns=self.expected_features)
                model.fit(features, target)

                # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
                y_pred = model.predict(features)
                metrics = {
                    "accuracy": accuracy_score(target, y_pred),
                    "f1_score": f1_score(target, y_pred, average="weighted"),
                    "precision": precision_score(target, y_pred, average="weighted"),
                    "recall": recall_score(target, y_pred, average="weighted"),
                    "cv_f1_mean": np.mean(cv_scores),
                    "cv_f1_std": np.std(cv_scores),
                }

                results[model_name] = metrics
                trained_models[model_name] = model

                self.logger.info(
                    f"âœ… {model_name} å­¦ç¿’å®Œäº† - F1: {metrics['f1_score']:.3f}, "
                    f"CV F1: {metrics['cv_f1_mean']:.3f}Â±{metrics['cv_f1_std']:.3f}"
                )

            except Exception as e:
                self.logger.error(f"âŒ {model_name} å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
                results[model_name] = {"error": str(e)}

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆindividual_modelsã®ã¿ã‚’ä½¿ç”¨ãƒ»å¾ªç’°å‚ç…§é˜²æ­¢ï¼‰
        if len(trained_models) >= 2:
            try:
                # ProductionEnsembleè‡ªä½“ã‚’å«ã¾ãªã„ã‚ˆã†ã«å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®ã¿æ¸¡ã™
                individual_models_only = {
                    k: v for k, v in trained_models.items() if k != "production_ensemble"
                }
                ensemble_model = self._create_ensemble(individual_models_only)
                trained_models["production_ensemble"] = ensemble_model
                self.logger.info("âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†ï¼ˆå¾ªç’°å‚ç…§é˜²æ­¢å¯¾å¿œï¼‰")
            except Exception as e:
                self.logger.error(f"âŒ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

        return {
            "results": results,
            "models": trained_models,
            "feature_names": list(features.columns),
            "training_samples": len(features),
        }

    def _create_ensemble(self, models: Dict) -> ProductionEnsemble:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆProductionEnsembleã‚¯ãƒ©ã‚¹ä½¿ç”¨ï¼‰."""
        try:
            self.logger.info("ğŸ”§ ProductionEnsembleã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
            ensemble_model = ProductionEnsemble(models)
            self.logger.info("âœ… ProductionEnsembleã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†")
            return ensemble_model
        except Exception as e:
            self.logger.error(f"âŒ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def save_models(self, training_results: Dict[str, Any]) -> Dict[str, str]:
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆå€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ï¼štrainingã€çµ±åˆãƒ¢ãƒ‡ãƒ«ï¼šproductionï¼‰."""
        self.logger.info("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜é–‹å§‹")

        saved_files = {}
        models = training_results.get("models", {})

        for model_name, model in models.items():
            try:
                if model_name == "production_ensemble":
                    # æœ¬ç•ªç”¨çµ±åˆãƒ¢ãƒ‡ãƒ«ã¯productionãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
                    model_file = self.production_dir / "production_ensemble.pkl"
                    with open(model_file, "wb") as f:
                        pickle.dump(model, f)

                    # æœ¬ç•ªç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
                    production_metadata = {
                        "created_at": datetime.now().isoformat(),
                        "model_type": "ProductionEnsemble",
                        "model_file": str(model_file),
                        "phase": "Phase 9",
                        "status": "production_ready",
                        "feature_names": training_results.get("feature_names", []),
                        "individual_models": [
                            k for k in model.models.keys() if k != "production_ensemble"
                        ],
                        "model_weights": model.weights,
                        "notes": "æœ¬ç•ªç”¨çµ±åˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ãƒ»å®Ÿå–å¼•ç”¨æœ€é©åŒ–æ¸ˆã¿ãƒ»å¾ªç’°å‚ç…§ä¿®æ­£",
                    }

                    production_metadata_file = (
                        self.production_dir / "production_model_metadata.json"
                    )
                    with open(production_metadata_file, "w", encoding="utf-8") as f:
                        json.dump(
                            production_metadata,
                            f,
                            indent=2,
                            ensure_ascii=False,
                        )

                    self.logger.info(f"âœ… æœ¬ç•ªç”¨çµ±åˆãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_file}")
                else:
                    # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã¯trainingãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
                    model_file = self.training_dir / f"{model_name}_model.pkl"
                    with open(model_file, "wb") as f:
                        pickle.dump(model, f)

                    self.logger.info(f"âœ… å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_file}")

                saved_files[model_name] = str(model_file)

            except Exception as e:
                self.logger.error(f"âŒ {model_name} ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        # å­¦ç¿’ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        training_metadata = {
            "created_at": datetime.now().isoformat(),
            "feature_names": training_results.get("feature_names", []),
            "training_samples": training_results.get("training_samples", 0),
            "model_metrics": training_results.get("results", {}),
            "model_files": saved_files,
            "config_path": self.config_path,
            "phase": "Phase 9",
            "notes": "å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’çµæœãƒ»trainingç”¨ä¿å­˜",
        }

        training_metadata_file = self.training_dir / "training_metadata.json"
        with open(training_metadata_file, "w", encoding="utf-8") as f:
            json.dump(training_metadata, f, indent=2, ensure_ascii=False)

        self.logger.info(f"âœ… å­¦ç¿’ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {training_metadata_file}")
        return saved_files

    def validate_models(self, saved_files: Dict[str, str]) -> bool:
        """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ï¼ˆæœ¬ç•ªç”¨ãƒ¢ãƒ‡ãƒ«é‡ç‚¹ï¼‰."""
        self.logger.info("ğŸ” ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼é–‹å§‹")

        validation_passed = True

        for model_name, model_file in saved_files.items():
            try:
                # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
                with open(model_file, "rb") as f:
                    model = pickle.load(f)

                # åŸºæœ¬å±æ€§ãƒã‚§ãƒƒã‚¯
                if hasattr(model, "predict"):
                    self.logger.info(f"âœ… {model_name}: predict ãƒ¡ã‚½ãƒƒãƒ‰ç¢ºèª")
                else:
                    self.logger.error(f"âŒ {model_name}: predict ãƒ¡ã‚½ãƒƒãƒ‰ãªã—")
                    validation_passed = False
                    continue

                # ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬ãƒ†ã‚¹ãƒˆï¼ˆDataFrameã§sklearnè­¦å‘Šå›é¿ï¼‰
                sample_features_array = np.random.random((5, 12))  # 12ç‰¹å¾´é‡
                sample_features = pd.DataFrame(
                    sample_features_array, columns=self.expected_features
                )
                prediction = model.predict(sample_features)

                if len(prediction) == 5:
                    self.logger.info(f"âœ… {model_name}: ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬æˆåŠŸ")
                else:
                    self.logger.error(f"âŒ {model_name}: ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬å¤±æ•—")
                    validation_passed = False
                    continue

                # æœ¬ç•ªç”¨ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°æ¤œè¨¼
                if model_name == "production_ensemble":
                    self.logger.info("ğŸ¯ æœ¬ç•ªç”¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è©³ç´°æ¤œè¨¼é–‹å§‹")

                    # predict_proba ãƒ¡ã‚½ãƒƒãƒ‰ç¢ºèª
                    if hasattr(model, "predict_proba"):
                        probabilities = model.predict_proba(sample_features)
                        if probabilities.shape == (5, 2):
                            self.logger.info("âœ… predict_proba ç¢ºèªæˆåŠŸ")
                        else:
                            self.logger.error(f"âŒ predict_proba å½¢çŠ¶ä¸æ­£: {probabilities.shape}")
                            validation_passed = False

                    # get_model_info ãƒ¡ã‚½ãƒƒãƒ‰ç¢ºèª
                    if hasattr(model, "get_model_info"):
                        info = model.get_model_info()
                        if info.get("n_features") == 12:
                            self.logger.info("âœ… get_model_info ç¢ºèªæˆåŠŸ")
                        else:
                            self.logger.error("âŒ get_model_info ç‰¹å¾´é‡æ•°ä¸æ­£")
                            validation_passed = False

                    self.logger.info("ğŸ¯ æœ¬ç•ªç”¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è©³ç´°æ¤œè¨¼å®Œäº†")

            except Exception as e:
                self.logger.error(f"âŒ {model_name} æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
                validation_passed = False

        if validation_passed:
            self.logger.info("ğŸ‰ å…¨ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼æˆåŠŸï¼")
        else:
            self.logger.error("ğŸ’¥ ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼å¤±æ•—")

        return validation_passed

    def run(self, dry_run: bool = False, days: int = 180) -> bool:
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œå‡¦ç†."""
        try:
            self.logger.info("ğŸš€ æ–°ã‚·ã‚¹ãƒ†ãƒ MLãƒ¢ãƒ‡ãƒ«ä½œæˆé–‹å§‹")

            # 1. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™
            features, target = self.prepare_training_data(days)

            # 2. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            training_results = self.train_models(features, target, dry_run)

            if dry_run:
                self.logger.info("ğŸ” ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Œäº†")
                return True

            # 3. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            saved_files = self.save_models(training_results)

            # 4. æ¤œè¨¼
            validation_passed = self.validate_models(saved_files)

            if validation_passed:
                self.logger.info("âœ… MLãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº† - å®Ÿå–å¼•æº–å‚™å®Œäº†")
                return True
            else:
                self.logger.error("âŒ MLãƒ¢ãƒ‡ãƒ«ä½œæˆå¤±æ•—")
                return False

        except Exception as e:
            self.logger.error(f"ğŸ’¥ MLãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°."""
    parser = argparse.ArgumentParser(
        description="æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨MLãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆå®Ÿéš›ã®å­¦ç¿’ãƒ»ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰",
    )
    parser.add_argument("--verbose", action="store_true", help="è©³ç´°ãƒ­ã‚°å‡ºåŠ›")
    parser.add_argument(
        "--days",
        type=int,
        default=180,
        help="å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æœŸé–“ï¼ˆæ—¥æ•°ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 180æ—¥ï¼‰",
    )
    parser.add_argument("--config", default="config/core/base.yaml", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")

    args = parser.parse_args()

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Ÿè¡Œ
    creator = NewSystemMLModelCreator(config_path=args.config, verbose=args.verbose)

    success = creator.run(dry_run=args.dry_run, days=args.days)

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
