#!/usr/bin/env python3
"""
æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨MLãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ - Phase 51.9å®Œäº†ç‰ˆï¼ˆ6æˆ¦ç•¥ãƒ»55ç‰¹å¾´é‡ãƒ»3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰

Phase 51.9å¯¾å¿œ: çœŸã®3ã‚¯ãƒ©ã‚¹åˆ†é¡å®Ÿè£…ãƒ»55ç‰¹å¾´é‡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆ6æˆ¦ç•¥çµ±åˆï¼‰
Phase 51.7 Day 7å¯¾å¿œ: 6æˆ¦ç•¥çµ±åˆãƒ»å‹•çš„ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒ»55ç‰¹å¾´é‡ã‚·ã‚¹ãƒ†ãƒ 
Phase 50.9å¯¾å¿œ: å¤–éƒ¨APIå®Œå…¨å‰Šé™¤ãƒ»ã‚·ãƒ³ãƒ—ãƒ«è¨­è¨ˆå›å¸°ãƒ»2æ®µéšGraceful Degradation
Phase 41.8å¯¾å¿œ: å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’ï¼ˆè¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã®ä¸€è²«æ€§ç¢ºä¿ï¼‰

æ©Ÿèƒ½:
- **2æ®µéšMLãƒ¢ãƒ‡ãƒ«ç”Ÿæˆ** - fullï¼ˆ55ç‰¹å¾´é‡ï¼‰ãƒ»basicï¼ˆ49ç‰¹å¾´é‡ï¼‰
- **è¨­å®šé§†å‹•å‹** - feature_order.jsonå®Œå…¨æº–æ‹ ãƒ»strategies.yamlå‹•çš„ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã‚¼ãƒ­
- **6æˆ¦ç•¥çµ±åˆ** - StrategyLoaderå‹•çš„ãƒ­ãƒ¼ãƒ‰ï¼ˆATRBased/DonchianChannel/ADXTrendStrength/BBReversal/StochasticReversal/MACDEMACrossoverï¼‰
- **å¤–éƒ¨APIå®Œå…¨å‰Šé™¤** - ã‚·ã‚¹ãƒ†ãƒ å®‰å®šæ€§å‘ä¸Šãƒ»ã‚¼ãƒ­ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ å®Ÿç¾
- **ãƒ¢ãƒ‡ãƒ«åˆ¥ç‰¹å¾´é‡é¸æŠ** - feature_order.jsonã‚«ãƒ†ã‚´ãƒªãƒ¼å®šç¾©ã«åŸºã¥ãè‡ªå‹•é¸æŠ
- **çµ±åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ** - å…¨ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’1ã¤ã®JSONã«é›†ç´„ï¼ˆensemble_metadata.jsonï¼‰
- Phase 41.8: å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’ - éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿéš›ã«æˆ¦ç•¥ã‚’å®Ÿè¡Œã—ã¦å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- Phase 40.6: Feature Engineeringæ‹¡å¼µ - 15â†’49åŸºæœ¬ç‰¹å¾´é‡
- Phase 39.1-39.5: å®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’ãƒ»TimeSeriesSplitãƒ»SMOTEãƒ»Optunaæœ€é©åŒ–
- æ–°ã‚·ã‚¹ãƒ†ãƒ  src/ æ§‹é€ å¯¾å¿œ
- models/production/ ã«ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆfull/basicï¼‰

Phase 51.9å®Œäº†æˆæœ: 55ç‰¹å¾´é‡å›ºå®šã‚·ã‚¹ãƒ†ãƒ ï¼ˆ6æˆ¦ç•¥çµ±åˆï¼‰ãƒ»çœŸã®3ã‚¯ãƒ©ã‚¹åˆ†é¡ãƒ»å‹•çš„ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒ»è¨­å®šé§†å‹•å‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

ä½¿ç”¨æ–¹æ³•:
    # æ¨å¥¨: ä¸¡ãƒ¢ãƒ‡ãƒ«ä¸€æ‹¬å­¦ç¿’ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å®Ÿè¡Œï¼‰
    python scripts/ml/create_ml_models.py --optimize --n-trials 50 --verbose

    # fullãƒ¢ãƒ‡ãƒ«ã®ã¿å­¦ç¿’ï¼ˆç·Šæ€¥æ™‚ï¼‰
    python scripts/ml/create_ml_models.py --model full --optimize --n-trials 50 --verbose

    # basicãƒ¢ãƒ‡ãƒ«ã®ã¿å­¦ç¿’ï¼ˆç·Šæ€¥æ™‚ï¼‰
    python scripts/ml/create_ml_models.py --model basic --optimize --n-trials 50 --verbose
"""

import argparse
import asyncio
import json
import logging
import pickle
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import optuna
import pandas as pd
from imblearn.over_sampling import SMOTE
from lightgbm import LGBMClassifier
from optuna.samplers import TPESampler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import TimeSeriesSplit
from xgboost import XGBClassifier

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆscripts/ml -> botï¼‰
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from src.backtest.scripts.collect_historical_csv import HistoricalDataCollector
    from src.core.config import load_config
    from src.core.config.feature_manager import _feature_manager  # Phase 50.7
    from src.core.logger import get_logger
    from src.data.data_pipeline import DataPipeline, DataRequest, TimeFrame
    from src.features.feature_generator import FeatureGenerator
    from src.ml.ensemble import ProductionEnsemble, StackingEnsemble  # Phase 59.7: Stackingè¿½åŠ 
    from src.strategies.base.strategy_manager import StrategyManager  # Phase 41.8
except ImportError as e:
    print(f"âŒ æ–°ã‚·ã‚¹ãƒ†ãƒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    print("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)


class NewSystemMLModelCreator:
    """æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨MLãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ»å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ."""

    def __init__(
        self,
        config_path: str = "config/core/unified.yaml",
        verbose: bool = False,
        target_threshold: float = 0.0005,  # Phase 55.8: 0.05%æ¨å¥¨ï¼ˆHOLDç‡é©æ­£åŒ–ï¼‰
        n_classes: int = 3,  # Phase 55.6: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3ã‚¯ãƒ©ã‚¹ï¼ˆBUY/HOLD/SELLï¼‰
        use_smote: bool = True,  # Phase 55.6: SMOTEãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœ‰åŠ¹ï¼ˆã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–ï¼‰
        optimize: bool = False,
        n_trials: int = 20,
        models_to_train: list = None,
        stacking: bool = False,  # Phase 59.7: Stacking Meta-Learneræœ‰åŠ¹åŒ–
    ):
        """
        åˆæœŸåŒ–ï¼ˆPhase 51.5-Bå¯¾å¿œãƒ»ä¸€æ‹¬ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼‰

        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            verbose: è©³ç´°ãƒ­ã‚°å‡ºåŠ›
            target_threshold: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé–¾å€¤ï¼ˆPhase 39.2ï¼‰
            n_classes: ã‚¯ãƒ©ã‚¹æ•° 2 or 3ï¼ˆPhase 39.2ï¼‰
            use_smote: SMOTEã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä½¿ç”¨ï¼ˆPhase 39.4ï¼‰
            optimize: Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ä½¿ç”¨ï¼ˆPhase 39.5ï¼‰
            n_trials: Optunaè©¦è¡Œå›æ•°ï¼ˆPhase 39.5ï¼‰
            models_to_train: è¨“ç·´ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆ ["full", "basic"] ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ä¸¡æ–¹
            stacking: Phase 59.7 Stacking Meta-Learneræœ‰åŠ¹åŒ–
        """
        self.config_path = config_path
        self.models_to_train = models_to_train or ["full", "basic"]
        self.current_model_type = "full"  # ãƒ«ãƒ¼ãƒ—å‡¦ç†ä¸­ã«å‹•çš„ã«è¨­å®š
        self.verbose = verbose
        self.target_threshold = target_threshold  # Phase 39.2
        self.n_classes = n_classes  # Phase 39.2
        self.use_smote = use_smote  # Phase 39.4
        self.optimize = optimize  # Phase 39.5
        self.n_trials = n_trials  # Phase 39.5
        self.stacking = stacking  # Phase 59.7: Stacking Meta-Learner

        # ãƒ­ã‚°è¨­å®š
        self.logger = get_logger()
        if verbose:
            logging.getLogger().setLevel(logging.DEBUG)

        # Phase 51.9-6A: ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›å…ˆã‚’logs/ml/ã«å¤‰æ›´
        from datetime import datetime

        log_dir = Path("logs/ml")
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = log_dir / f"ml_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
        logging.getLogger().addHandler(file_handler)
        self.logger.info(f"ğŸ“ ãƒ­ã‚°å‡ºåŠ›å…ˆ: {log_file}")

        # è¨­å®šèª­ã¿è¾¼ã¿
        try:
            self.config = load_config(config_path)
            self.logger.info(f"âœ… è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†: {config_path}")
        except Exception as e:
            self.logger.error(f"âŒ è¨­å®šèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            raise

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.training_dir = Path("models/training")
        self.production_dir = Path("models/production")
        self.optuna_dir = Path("models/optuna")  # Phase 39.5
        self.training_dir.mkdir(parents=True, exist_ok=True)
        self.production_dir.mkdir(parents=True, exist_ok=True)
        self.optuna_dir.mkdir(parents=True, exist_ok=True)  # Phase 39.5

        # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
        try:
            self.data_pipeline = DataPipeline()
            self.logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å¤±æ•—: {e}")
            raise

        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        self.feature_generator = FeatureGenerator()

        # Phase 29: ç‰¹å¾´é‡å®šç¾©ä¸€å…ƒåŒ–å¯¾å¿œï¼ˆfeature_managerã‹ã‚‰å–å¾—ï¼‰
        from src.core.config.feature_manager import get_feature_names

        self.expected_features = get_feature_names()

        self.logger.info(f"ğŸ¯ å¯¾è±¡ç‰¹å¾´é‡: {len(self.expected_features)}å€‹ï¼ˆæ–°ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–æ¸ˆã¿ï¼‰")
        self.logger.info(
            f"ğŸ¯ Phase 39.2 ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å®š: é–¾å€¤={target_threshold:.1%}, ã‚¯ãƒ©ã‚¹æ•°={n_classes}"
        )

        # Phase 55.6: ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚’åˆ¥ãƒ¡ã‚½ãƒƒãƒ‰ã«åˆ†é›¢
        self._initialize_models()

    def _initialize_models(self):
        """
        Phase 55.6: MLãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åˆæœŸåŒ–

        å„ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®è¨“ç·´å‰ã«å‘¼ã³å‡ºã™ã“ã¨ã§ã€
        å‰å›ã®è¨“ç·´çŠ¶æ…‹ãŒãƒªãƒ¼ã‚¯ã—ãªã„ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹ã€‚
        """
        # LightGBMè¨­å®š
        lgb_params = {
            "n_estimators": 200,
            "learning_rate": 0.1,
            "max_depth": 8,
            "num_leaves": 31,
            "random_state": 42,
            "verbose": -1,
            "class_weight": "balanced",  # Phase 39.4
        }
        if self.n_classes == 3:
            lgb_params["objective"] = "multiclass"
            lgb_params["num_class"] = 3

        # XGBoostè¨­å®š
        xgb_params = {
            "n_estimators": 200,
            "learning_rate": 0.1,
            "max_depth": 8,
            "random_state": 42,
            "verbosity": 0,
        }
        if self.n_classes == 3:
            xgb_params["objective"] = "multi:softprob"
            xgb_params["num_class"] = 3
            xgb_params["eval_metric"] = "mlogloss"
        else:
            xgb_params["eval_metric"] = "logloss"

        # RandomForestè¨­å®šï¼ˆPhase 53.2: GCP gVisoräº’æ›æ€§ã®ãŸã‚n_jobs=1ï¼‰
        rf_params = {
            "n_estimators": 200,
            "max_depth": 12,
            "random_state": 42,
            "n_jobs": 1,  # Phase 53.2: GCP gVisor fork()åˆ¶é™å¯¾å¿œ
            "class_weight": "balanced",  # Phase 39.4
        }

        self.models = {
            "lightgbm": LGBMClassifier(**lgb_params),
            "xgboost": XGBClassifier(**xgb_params),
            "random_forest": RandomForestClassifier(**rf_params),
        }

    async def _load_real_historical_data(self, days: int) -> pd.DataFrame:
        """
        å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆPhase 39.1: MLå®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼‰

        Args:
            days: åé›†æ—¥æ•°

        Returns:
            pd.DataFrame: OHLCV ãƒ‡ãƒ¼ã‚¿
        """
        self.logger.info(f"ğŸ“Š Phase 39.1: å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹ï¼ˆéå»{days}æ—¥åˆ†ï¼‰")

        csv_path = Path("src/backtest/data/historical/btc_jpy_15m.csv")

        # ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆå­˜åœ¨ã—ãªã„ã€ã¾ãŸã¯å¤ã„å ´åˆï¼‰
        if not csv_path.exists():
            self.logger.info("ğŸ’¾ å±¥æ­´ãƒ‡ãƒ¼ã‚¿æœªå­˜åœ¨ - è‡ªå‹•åé›†é–‹å§‹")
            try:
                collector = HistoricalDataCollector()
                await collector.collect_data(symbol="BTC/JPY", days=days, timeframes=["15m"])
                self.logger.info("âœ… ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†")
            except Exception as e:
                self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿åé›†å¤±æ•—: {e}")
                raise

        # CSVèª­ã¿è¾¼ã¿
        try:
            df = pd.read_csv(csv_path)

            # timestamp ã‚’datetimeã«å¤‰æ›
            df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
            df.set_index("timestamp", inplace=True)

            # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿
            cutoff_date = datetime.now() - timedelta(days=days)
            df = df[df.index >= cutoff_date]

            # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
            if df.isnull().any().any():
                self.logger.warning("âš ï¸ æ¬ æå€¤ã‚’æ¤œå‡º - ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ")
                df = df.dropna()

            self.logger.info(
                f"âœ… å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ã‚µãƒ³ãƒ—ãƒ« "
                f"({df.index.min().strftime('%Y-%m-%d')} ã€œ {df.index.max().strftime('%Y-%m-%d')})"
            )

            # OHLCV ã‚«ãƒ©ãƒ ã®ã¿è¿”å´
            return df[["open", "high", "low", "close", "volume"]].copy()

        except Exception as e:
            self.logger.error(f"âŒ CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    async def prepare_training_data_async(self, days: int = 180) -> Tuple[pd.DataFrame, pd.Series]:
        """
        Phase 51.9: ãƒ¢ãƒ‡ãƒ«åˆ¥ç‰¹å¾´é‡é¸æŠï¼ˆ2æ®µéšã‚·ã‚¹ãƒ†ãƒ ãƒ»6æˆ¦ç•¥çµ±åˆï¼‰

        model_type="full": 55ç‰¹å¾´é‡ï¼ˆå…¨ç‰¹å¾´é‡ä½¿ç”¨ãƒ»6æˆ¦ç•¥ä¿¡å·å«ã‚€ï¼‰
        model_type="basic": 49ç‰¹å¾´é‡ï¼ˆæˆ¦ç•¥ä¿¡å·é™¤å¤–ï¼‰
        """
        model_name = (
            "fullï¼ˆ55ç‰¹å¾´é‡ï¼‰" if self.current_model_type == "full" else "basicï¼ˆ49ç‰¹å¾´é‡ï¼‰"
        )
        self.logger.info(f"ğŸ“Š Phase 51.9: å®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’é–‹å§‹ï¼ˆéå»{days}æ—¥åˆ†ãƒ»{model_name}ï¼‰")

        try:
            # Phase 39.1: å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = await self._load_real_historical_data(days)

            self.logger.info(f"âœ… åŸºæœ¬ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(df)}è¡Œ")

            # Phase 50.9: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆ62ç‰¹å¾´é‡ãƒ»å¤–éƒ¨APIå®Œå…¨å‰Šé™¤æ¸ˆã¿ï¼‰
            features_df = await self.feature_generator.generate_features(df)

            # Phase 41.8: æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ç‰¹å¾´é‡ã‚’å‰Šé™¤ï¼ˆå¾Œã§å®Ÿæˆ¦ç•¥ä¿¡å·ã§ç½®ãæ›ãˆã‚‹ï¼‰
            # generate_features() ã¯æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ã‚’0.0ã§è‡ªå‹•ç”Ÿæˆã™ã‚‹ãŒã€Phase 41.8ã§ã¯å®Ÿæˆ¦ç•¥ä¿¡å·ã‚’ä½¿ç”¨
            strategy_signal_cols = [
                col for col in features_df.columns if col.startswith("strategy_signal_")
            ]
            if strategy_signal_cols:
                features_df = features_df.drop(columns=strategy_signal_cols)
                self.logger.info(
                    f"âœ… æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ç‰¹å¾´é‡å‰Šé™¤: {len(strategy_signal_cols)}å€‹ï¼ˆå®Ÿæˆ¦ç•¥ä¿¡å·ã§ç½®ãæ›ãˆï¼‰"
                )

            # Phase 51.9: å®Ÿæˆ¦ç•¥ä¿¡å·ç”Ÿæˆï¼ˆ49â†’55ç‰¹å¾´é‡ãƒ»6æˆ¦ç•¥çµ±åˆï¼‰
            # Note: éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿéš›ã«æˆ¦ç•¥ã‚’å®Ÿè¡Œã—ã€æœ¬ç‰©ã®æˆ¦ç•¥ä¿¡å·ã‚’ç”Ÿæˆ
            #       ã“ã‚Œã«ã‚ˆã‚Šè¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã®ä¸€è²«æ€§ã‚’ç¢ºä¿
            #       ç‰¹å¾´é‡ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™ï¼ˆæˆ¦ç•¥ã¯ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’å¿…è¦ã¨ã™ã‚‹ãŸã‚ï¼‰
            strategy_signals_df = await self._generate_real_strategy_signals_for_training(
                features_df
            )

            # Phase 51.9: åŸºæœ¬ç‰¹å¾´é‡ï¼ˆ49ï¼‰ + å®Ÿæˆ¦ç•¥ä¿¡å·ï¼ˆ6ï¼‰ = 55ç‰¹å¾´é‡ã‚’çµåˆ
            features_df = pd.concat([features_df, strategy_signals_df], axis=1)

            # Phase 51.9: ç‰¹å¾´é‡æ•´åˆæ€§ç¢ºä¿ï¼ˆ55ç‰¹å¾´é‡å›ºå®šã‚·ã‚¹ãƒ†ãƒ ï¼‰
            features_df = self._ensure_feature_consistency(features_df)

            # Phase 55.6 Fix: ãƒ¢ãƒ‡ãƒ«åˆ¥ç‰¹å¾´é‡é¸æŠã¯run()å†…ã®ãƒ«ãƒ¼ãƒ—ã§å®Ÿè¡Œ
            # ã“ã“ã§ã¯å…¨55ç‰¹å¾´é‡ã‚’è¿”ã™

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆPhase 39.2: é–¾å€¤ãƒ»ã‚¯ãƒ©ã‚¹æ•°å¯¾å¿œï¼‰
            target = self._generate_target(df, self.target_threshold, self.n_classes)

            # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
            features_df, target = self._clean_data(features_df, target)

            self.logger.info(
                f"âœ… Phase 55.6: å®Ÿãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº† - {len(features_df)}ã‚µãƒ³ãƒ—ãƒ«ã€{len(features_df.columns)}ç‰¹å¾´é‡ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«å…±é€šï¼‰"
            )
            return features_df, target

        except Exception as e:
            self.logger.error(f"âŒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def prepare_training_data(self, days: int = 180) -> Tuple[pd.DataFrame, pd.Series]:
        """å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆåŒæœŸãƒ©ãƒƒãƒ‘ãƒ¼ãƒ»å¾Œæ–¹äº’æ›æ€§ï¼‰"""
        return asyncio.run(self.prepare_training_data_async(days))

    def _generate_sample_data(self, samples: int) -> pd.DataFrame:
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰."""
        self.logger.info(f"ğŸ”§ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: {samples}ã‚µãƒ³ãƒ—ãƒ«")

        # æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        dates = pd.date_range(end=datetime.now(), periods=samples, freq="h")

        # ãƒªã‚¢ãƒ«ãªBTCä¾¡æ ¼å‹•å‘ã‚’æ¨¡æ“¬
        np.random.seed(42)
        base_price = 5000000  # 5M JPY
        prices = []
        current_price = base_price

        for i in range(samples):
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼ˆæ™‚é–“å¸¯ã«ã‚ˆã‚‹å¤‰å‹•å¹…èª¿æ•´ï¼‰
            hour = dates[i].hour
            volatility = 0.015 if 8 <= hour <= 20 else 0.008  # æ—¥ä¸­é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            change = np.random.normal(0, volatility)
            current_price *= 1 + change
            prices.append(current_price)

        # OHLCV ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        df = pd.DataFrame(
            {
                "timestamp": dates,
                "open": prices,
                "high": [p * (1 + abs(np.random.normal(0, 0.005))) for p in prices],
                "low": [p * (1 - abs(np.random.normal(0, 0.005))) for p in prices],
                "close": prices,
                "volume": np.random.lognormal(8, 1, samples),  # ãƒªã‚¢ãƒ«ãªå‡ºæ¥é«˜åˆ†å¸ƒ
            }
        )

        df.set_index("timestamp", inplace=True)
        return df

    async def _generate_real_strategy_signals_for_training(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Phase 41.8: å®Ÿéš›ã®æˆ¦ç•¥ä¿¡å·ã‚’ç”Ÿæˆï¼ˆéå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰

        å„æ™‚ç‚¹ã§5æˆ¦ç•¥ã‚’å®Ÿè¡Œã—ã€æœ¬ç‰©ã®æˆ¦ç•¥ä¿¡å·ã‚’è¨ˆç®—ã€‚
        ã“ã‚Œã«ã‚ˆã‚Šè¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã®ä¸€è²«æ€§ã‚’ç¢ºä¿ã€‚

        Args:
            df: OHLCVä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿

        Returns:
            pd.DataFrame: æˆ¦ç•¥ä¿¡å·5åˆ—ã®DataFrame (index aligned with df)
        """
        # Phase 51.7 Day 7: strategies.yamlã‹ã‚‰å‹•çš„ãƒ­ãƒ¼ãƒ‰ï¼ˆ6æˆ¦ç•¥å¯¾å¿œï¼‰
        from src.strategies.strategy_loader import StrategyLoader

        strategy_loader = StrategyLoader("config/strategies.yaml")
        loaded_strategies = strategy_loader.load_strategies()
        strategy_names = [s["metadata"]["name"] for s in loaded_strategies]

        self.logger.info(
            f"ğŸ“Š Phase 51.9: å®Ÿæˆ¦ç•¥ä¿¡å·ç”Ÿæˆé–‹å§‹ï¼ˆéå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰{len(strategy_names)}æˆ¦ç•¥å®Ÿè¡Œï¼‰"
        )
        self.logger.info(f"   æˆ¦ç•¥ãƒªã‚¹ãƒˆ: {strategy_names}")

        # çµæœæ ¼ç´ç”¨DataFrame
        strategy_signals = pd.DataFrame(index=df.index)

        try:
            # StrategyManageråˆæœŸåŒ– + å…¨æˆ¦ç•¥ç™»éŒ²
            strategy_manager = StrategyManager()

            # Phase 51.9: å…¨æˆ¦ç•¥ã‚’StrategyManagerã«ç™»éŒ²
            for strategy_data in loaded_strategies:
                strategy_manager.register_strategy(
                    strategy_data["instance"], weight=strategy_data["weight"]
                )

            self.logger.info(f"âœ… StrategyManageråˆæœŸåŒ–å®Œäº† - {len(loaded_strategies)}æˆ¦ç•¥ç™»éŒ²")

            # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–
            self.data_pipeline.set_backtest_data({"15m": df.copy()})
            self.logger.info("âœ… DataPipelineãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰è¨­å®šå®Œäº†")

            # å„æ™‚ç‚¹ã§æˆ¦ç•¥å®Ÿè¡Œï¼ˆlook-ahead biaså›é¿ã®ãŸã‚é †æ¬¡å‡¦ç†ï¼‰
            total_points = len(df)
            processed = 0

            for i in range(len(df)):
                # ç¾åœ¨æ™‚ç‚¹ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼ˆæœªæ¥ãƒ‡ãƒ¼ã‚¿æ¼æ´©é˜²æ­¢ï¼‰
                current_data = df.iloc[: i + 1]

                # æœ€ä½é™ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒå¿…è¦ï¼ˆç‰¹å¾´é‡è¨ˆç®—ã®ãŸã‚ï¼‰
                if len(current_data) < 50:
                    # ãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚ã¯0ã§åŸ‹ã‚ã‚‹
                    for strategy_name in strategy_names:
                        strategy_signals.loc[
                            current_data.index[-1], f"strategy_signal_{strategy_name}"
                        ] = 0.0
                    continue

                try:
                    # DataPipelineæ›´æ–°ï¼ˆãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å½¢å¼ï¼‰
                    self.data_pipeline.set_backtest_data({"15m": current_data.copy()})

                    # å€‹åˆ¥æˆ¦ç•¥ä¿¡å·å–å¾—ï¼ˆPhase 51.7 Day 7: å˜ä¸€DataFrameã¨ã—ã¦æ¸¡ã™ï¼‰
                    signals = strategy_manager.get_individual_strategy_signals(current_data)

                    # action Ã— confidence ã‚’è¨ˆç®—ã—ã¦æ ¼ç´
                    current_timestamp = current_data.index[-1]
                    for strategy_name in strategy_names:
                        if strategy_name in signals:
                            action = signals[strategy_name]["action"]
                            confidence = signals[strategy_name]["confidence"]

                            # Phase 51.9: æ”¹å–„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆhold=0å•é¡Œè§£æ±ºï¼‰
                            # æ–°æ–¹å¼: 0.0-1.0ã®é€£ç¶šå€¤ï¼ˆå…¨ã¦éã‚¼ãƒ­ï¼‰
                            # - hold: 0.5ï¼ˆä¸­ç«‹ï¼‰
                            # - buy: 0.5 + (confidence * 0.5) = 0.5-1.0ç¯„å›²
                            # - sell: 0.5 - (confidence * 0.5) = 0.0-0.5ç¯„å›²
                            if action == "buy":
                                signal_value = 0.5 + (confidence * 0.5)
                            elif action == "sell":
                                signal_value = 0.5 - (confidence * 0.5)
                            else:  # hold
                                signal_value = 0.5

                            strategy_signals.loc[
                                current_timestamp, f"strategy_signal_{strategy_name}"
                            ] = signal_value
                        else:
                            # æˆ¦ç•¥ä¿¡å·ãŒå¾—ã‚‰ã‚Œãªã„å ´åˆã¯holdæ‰±ã„ï¼ˆ0.5ï¼‰
                            strategy_signals.loc[
                                current_timestamp, f"strategy_signal_{strategy_name}"
                            ] = 0.5

                except Exception as e:
                    # Phase 51.9: ã‚¨ãƒ©ãƒ¼æ™‚ã¯holdï¼ˆ0.5ï¼‰ã§åŸ‹ã‚ã‚‹ï¼ˆå­¦ç¿’ç¶™ç¶šï¼‰
                    self.logger.warning(f"âš ï¸ æ™‚ç‚¹{i}ã§æˆ¦ç•¥å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}, hold(0.5)ã§åŸ‹ã‚ã¾ã™")
                    for strategy_name in strategy_names:
                        strategy_signals.loc[
                            current_data.index[-1], f"strategy_signal_{strategy_name}"
                        ] = 0.5

                # é€²æ—è¡¨ç¤ºï¼ˆ10%ã”ã¨ï¼‰
                processed += 1
                if processed % max(1, total_points // 10) == 0:
                    progress = (processed / total_points) * 100
                    self.logger.info(
                        f"ğŸ“Š Phase 41.8: æˆ¦ç•¥ä¿¡å·ç”Ÿæˆé€²æ— {processed}/{total_points} ({progress:.1f}%)"
                    )

            # Phase 51.9: æ¬ æå€¤ã‚’holdï¼ˆ0.5ï¼‰ã§åŸ‹ã‚ã‚‹
            strategy_signals.fillna(0.5, inplace=True)

            self.logger.info(
                f"âœ… Phase 51.9: å®Ÿæˆ¦ç•¥ä¿¡å·ç”Ÿæˆå®Œäº† - {len(strategy_signals)}è¡Œ Ã— {len(strategy_names)}æˆ¦ç•¥"
            )
            # Phase 51.9: buy/sellç‡ï¼ˆholdä»¥å¤–ã®ç‡ï¼‰ã‚’è¡¨ç¤º
            buy_sell_rate = (
                (strategy_signals != 0.5).sum().sum()
                / (len(strategy_signals) * len(strategy_names))
                * 100
            )
            self.logger.info(
                f"ğŸ“Š Phase 51.9: æˆ¦ç•¥ä¿¡å·çµ±è¨ˆ - buy/sellç‡ï¼ˆholdä»¥å¤–ï¼‰: {buy_sell_rate:.1f}%"
            )

            return strategy_signals

        except Exception as e:
            self.logger.error(f"âŒ Phase 41.8: å®Ÿæˆ¦ç•¥ä¿¡å·ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            # Phase 51.9: ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆhold=0.5åŸ‹ã‚ï¼‰
            self.logger.warning("âš ï¸ Phase 51.9: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ - æˆ¦ç•¥ä¿¡å·ã‚’holdï¼ˆ0.5ï¼‰åŸ‹ã‚")
            for strategy_name in strategy_names:
                strategy_signals[f"strategy_signal_{strategy_name}"] = 0.5
            return strategy_signals

    def _ensure_feature_consistency(self, features_df: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾´é‡æ•´åˆæ€§ç¢ºä¿ï¼ˆPhase 51.9: 55ç‰¹å¾´é‡å¯¾å¿œãƒ»6æˆ¦ç•¥çµ±åˆï¼‰"""
        # ä¸è¶³ç‰¹å¾´é‡ã®0åŸ‹ã‚
        for feature in self.expected_features:
            if feature not in features_df.columns:
                features_df[feature] = 0.0
                self.logger.warning(f"âš ï¸ ä¸è¶³ç‰¹å¾´é‡ã‚’0åŸ‹ã‚: {feature}")

        # ç‰¹å¾´é‡ã®ã¿é¸æŠ - Phase 51.9: 55ç‰¹å¾´é‡å¯¾å¿œ
        features_df = features_df[self.expected_features]

        expected_count = len(self.expected_features)
        if len(features_df.columns) != expected_count:
            self.logger.warning(f"âš ï¸ ç‰¹å¾´é‡æ•°ä¸ä¸€è‡´: {len(features_df.columns)} != {expected_count}")

        return features_df

    def _select_features_by_level(self, features_df: pd.DataFrame) -> pd.DataFrame:
        """
        Phase 51.9: ãƒ¢ãƒ‡ãƒ«åˆ¥ç‰¹å¾´é‡é¸æŠï¼ˆ2æ®µéšã‚·ã‚¹ãƒ†ãƒ ãƒ»è¨­å®šé§†å‹•å‹ãƒ»6æˆ¦ç•¥çµ±åˆï¼‰

        model_type="full": å…¨55ç‰¹å¾´é‡ä½¿ç”¨ï¼ˆ6æˆ¦ç•¥ä¿¡å·å«ã‚€ï¼‰
        model_type="basic": æˆ¦ç•¥ä¿¡å·6å€‹ã‚’é™¤å¤–ï¼ˆ49ç‰¹å¾´é‡ï¼‰

        Args:
            features_df: å…¨ç‰¹å¾´é‡ã‚’å«ã‚€DataFrame

        Returns:
            pd.DataFrame: ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ãŸç‰¹å¾´é‡ã®ã¿ã‚’å«ã‚€DataFrame
        """
        if self.current_model_type == "full":
            # full: å…¨55ç‰¹å¾´é‡ä½¿ç”¨
            self.logger.info(f"ğŸ“Š Phase 51.9: full ãƒ¢ãƒ‡ãƒ« - å…¨{len(features_df.columns)}ç‰¹å¾´é‡ä½¿ç”¨")
            return features_df

        # basic: æˆ¦ç•¥ä¿¡å·ã‚’é™¤å¤–ï¼ˆ49ç‰¹å¾´é‡ï¼‰
        # è¨­å®šé§†å‹•å‹: strategy_signal_ ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã§å‹•çš„æ¤œç´¢
        strategy_signal_features = [
            col for col in features_df.columns if col.startswith("strategy_signal_")
        ]

        features_df = features_df.drop(columns=strategy_signal_features, errors="ignore")
        self.logger.info(
            f"ğŸ“Š Phase 51.9: basic ãƒ¢ãƒ‡ãƒ« - æˆ¦ç•¥ä¿¡å·{len(strategy_signal_features)}å€‹ã‚’é™¤å¤– â†’ {len(features_df.columns)}ç‰¹å¾´é‡"
        )
        return features_df

    def _generate_target(
        self,
        df: pd.DataFrame,
        threshold: float = 0.0005,  # Phase 55.8: 0.05%æ¨å¥¨
        n_classes: int = 3,  # Phase 55.6: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3ã‚¯ãƒ©ã‚¹
    ) -> pd.Series:
        """
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆPhase 39.2: é–¾å€¤æœ€é©åŒ–ãƒ»3ã‚¯ãƒ©ã‚¹åˆ†é¡å¯¾å¿œï¼‰

        Args:
            df: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
            threshold: BUYé–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.5%ï¼‰
            n_classes: ã‚¯ãƒ©ã‚¹æ•°ï¼ˆ2ã¾ãŸã¯3ï¼‰

        Returns:
            pd.Series: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ©ãƒ™ãƒ«
                2ã‚¯ãƒ©ã‚¹: 0=HOLD/SELL, 1=BUY
                3ã‚¯ãƒ©ã‚¹: 0=SELL, 1=HOLD, 2=BUY
        """
        # 1æ™‚é–“å¾Œã®ä¾¡æ ¼å¤‰å‹•ç‡ï¼ˆ4æ™‚é–“è¶³ãªã®ã§4æ™‚é–“å¾Œï¼‰
        price_change = df["close"].pct_change(periods=1).shift(-1)

        if n_classes == 2:
            # Phase 39.2: é–¾å€¤0.3%â†’0.5%ã«å¤‰æ›´ï¼ˆãƒã‚¤ã‚ºå‰Šæ¸›ï¼‰
            target = (price_change > threshold).astype(int)

            buy_ratio = target.mean()
            self.logger.info(
                f"ğŸ“Š Phase 39.2 ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒï¼ˆé–¾å€¤{threshold:.1%}ï¼‰: "
                f"BUY {buy_ratio:.1%}, OTHER {1 - buy_ratio:.1%}"
            )

        elif n_classes == 3:
            # Phase 39.2: 3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆBUY/HOLD/SELLï¼‰
            sell_threshold = -threshold

            # 0: SELL, 1: HOLD, 2: BUY
            target = pd.Series(1, index=df.index, dtype=int)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆHOLD
            target[price_change > threshold] = 2  # BUY
            target[price_change < sell_threshold] = 0  # SELL

            distribution = target.value_counts(normalize=True).sort_index()
            self.logger.info(
                f"ğŸ“Š Phase 39.2 3ã‚¯ãƒ©ã‚¹åˆ†å¸ƒï¼ˆé–¾å€¤Â±{threshold:.1%}ï¼‰: "
                f"SELL {distribution.get(0, 0):.1%}, "
                f"HOLD {distribution.get(1, 0):.1%}, "
                f"BUY {distribution.get(2, 0):.1%}"
            )

        else:
            raise ValueError(f"Unsupported n_classes: {n_classes} (must be 2 or 3)")

        return target

    def _clean_data(
        self, features_df: pd.DataFrame, target: pd.Series
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°."""
        # NaNé™¤å»
        valid_mask = ~(features_df.isna().any(axis=1) | target.isna())
        features_clean = features_df[valid_mask].copy()
        target_clean = target[valid_mask].copy()

        # ç„¡é™å€¤é™¤å»
        features_clean.replace([np.inf, -np.inf], np.nan, inplace=True)
        features_clean.fillna(0, inplace=True)

        removed_samples = len(features_df) - len(features_clean)
        if removed_samples > 0:
            self.logger.info(f"ğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°: {removed_samples}ã‚µãƒ³ãƒ—ãƒ«é™¤å»")

        return features_clean, target_clean

    def _objective_lightgbm(
        self, trial: optuna.Trial, X_train: pd.DataFrame, y_train: pd.Series
    ) -> float:
        """Phase 39.5: LightGBMæœ€é©åŒ–objectiveé–¢æ•°ï¼ˆPhase 51.9-6A: 3ã‚¯ãƒ©ã‚¹å¯¾å¿œï¼‰"""
        params = {
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
            "max_depth": trial.suggest_int("max_depth", 3, 15),
            "n_estimators": trial.suggest_int("n_estimators", 50, 300),
            "num_leaves": trial.suggest_int("num_leaves", 20, 100),
            "random_state": 42,
            "verbose": -1,
            "class_weight": "balanced",
        }

        # Phase 51.9-6A: 3ã‚¯ãƒ©ã‚¹åˆ†é¡å¯¾å¿œ
        if self.n_classes == 3:
            params["objective"] = "multiclass"
            params["num_class"] = 3

        model = LGBMClassifier(**params)

        # TimeSeriesSplit CV
        tscv = TimeSeriesSplit(n_splits=3)
        scores = []
        for train_idx, val_idx in tscv.split(X_train):
            X_cv_train = X_train.iloc[train_idx]
            y_cv_train = y_train.iloc[train_idx]
            X_cv_val = X_train.iloc[val_idx]
            y_cv_val = y_train.iloc[val_idx]

            model.fit(X_cv_train, y_cv_train)
            y_pred = model.predict(X_cv_val)
            score = f1_score(y_cv_val, y_pred, average="weighted")
            scores.append(score)

        return np.mean(scores)

    def _objective_xgboost(
        self, trial: optuna.Trial, X_train: pd.DataFrame, y_train: pd.Series
    ) -> float:
        """Phase 39.5: XGBoostæœ€é©åŒ–objectiveé–¢æ•°ï¼ˆPhase 51.9-6A: 3ã‚¯ãƒ©ã‚¹å¯¾å¿œï¼‰"""
        params = {
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
            "max_depth": trial.suggest_int("max_depth", 3, 15),
            "n_estimators": trial.suggest_int("n_estimators", 50, 300),
            "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
            "random_state": 42,
            "verbosity": 0,
        }

        # Phase 51.9-6A: 3ã‚¯ãƒ©ã‚¹åˆ†é¡å¯¾å¿œ
        if self.n_classes == 3:
            params["objective"] = "multi:softprob"
            params["num_class"] = 3
            params["eval_metric"] = "mlogloss"
        else:
            params["eval_metric"] = "logloss"
            # scale_pos_weightå‹•çš„è¨­å®šï¼ˆ2ã‚¯ãƒ©ã‚¹åˆ†é¡ã®ã¿ï¼‰
            pos_count = y_train.sum()
            neg_count = len(y_train) - pos_count
            if pos_count > 0:
                params["scale_pos_weight"] = neg_count / pos_count

        model = XGBClassifier(**params)

        # TimeSeriesSplit CV
        tscv = TimeSeriesSplit(n_splits=3)
        scores = []
        for train_idx, val_idx in tscv.split(X_train):
            X_cv_train = X_train.iloc[train_idx]
            y_cv_train = y_train.iloc[train_idx]
            X_cv_val = X_train.iloc[val_idx]
            y_cv_val = y_train.iloc[val_idx]

            model.fit(X_cv_train, y_cv_train)
            y_pred = model.predict(X_cv_val)
            score = f1_score(y_cv_val, y_pred, average="weighted")
            scores.append(score)

        return np.mean(scores)

    def _objective_random_forest(
        self, trial: optuna.Trial, X_train: pd.DataFrame, y_train: pd.Series
    ) -> float:
        """Phase 39.5: RandomForestæœ€é©åŒ–objectiveé–¢æ•°"""
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 50, 300),
            "max_depth": trial.suggest_int("max_depth", 5, 20),
            "min_samples_split": trial.suggest_int("min_samples_split", 2, 20),
            "random_state": 42,
            "n_jobs": 1,  # Phase 53.2: GCP gVisor fork()åˆ¶é™å¯¾å¿œ
            "class_weight": "balanced",
        }

        model = RandomForestClassifier(**params)

        # TimeSeriesSplit CV
        tscv = TimeSeriesSplit(n_splits=3)
        scores = []
        for train_idx, val_idx in tscv.split(X_train):
            X_cv_train = X_train.iloc[train_idx]
            y_cv_train = y_train.iloc[train_idx]
            X_cv_val = X_train.iloc[val_idx]
            y_cv_val = y_train.iloc[val_idx]

            model.fit(X_cv_train, y_cv_train)
            y_pred = model.predict(X_cv_val)
            score = f1_score(y_cv_val, y_pred, average="weighted")
            scores.append(score)

        return np.mean(scores)

    def optimize_hyperparameters(
        self, features: pd.DataFrame, target: pd.Series, n_trials: int = 20
    ) -> Dict[str, Dict[str, Any]]:
        """
        Phase 39.5: Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–

        Args:
            features: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡
            target: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            n_trials: è©¦è¡Œå›æ•°

        Returns:
            Dict: å„ãƒ¢ãƒ‡ãƒ«ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        self.logger.info(f"ğŸ”¬ Phase 39.5: Optunaæœ€é©åŒ–é–‹å§‹ï¼ˆ{n_trials}è©¦è¡Œï¼‰")

        # Optunaãƒ­ã‚°æŠ‘åˆ¶
        optuna.logging.set_verbosity(optuna.logging.WARNING)

        optimal_params = {}
        optimization_results = {
            "created_at": datetime.now().isoformat(),
            "n_trials": n_trials,
            "models": {},
        }

        for model_name in ["lightgbm", "xgboost", "random_forest"]:
            self.logger.info(f"ğŸ“Š {model_name} æœ€é©åŒ–é–‹å§‹")

            try:
                # Objectiveé–¢æ•°é¸æŠï¼ˆE731: flake8 lambdaå›é¿ï¼‰
                def objective_func(trial: optuna.Trial) -> float:
                    if model_name == "lightgbm":
                        return self._objective_lightgbm(trial, features, target)
                    elif model_name == "xgboost":
                        return self._objective_xgboost(trial, features, target)
                    else:  # random_forest
                        return self._objective_random_forest(trial, features, target)

                # Optuna Studyä½œæˆãƒ»æœ€é©åŒ–å®Ÿè¡Œ
                study = optuna.create_study(direction="maximize", sampler=TPESampler(seed=42))
                study.optimize(objective_func, n_trials=n_trials, show_progress_bar=False)

                # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
                best_params = study.best_params
                best_score = study.best_value

                optimal_params[model_name] = best_params
                optimization_results["models"][model_name] = {
                    "best_params": best_params,
                    "best_score": float(best_score),
                    "n_trials": n_trials,
                }

                self.logger.info(
                    f"âœ… {model_name} æœ€é©åŒ–å®Œäº† - Best F1: {best_score:.4f}, "
                    f"Best params: {best_params}"
                )

            except Exception as e:
                self.logger.error(f"âŒ {model_name} æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                optimization_results["models"][model_name] = {"error": str(e)}

        # çµæœä¿å­˜
        try:
            results_file = self.optuna_dir / "phase39_5_results.json"
            with open(results_file, "w", encoding="utf-8") as f:
                json.dump(optimization_results, f, indent=2, ensure_ascii=False)
            self.logger.info(f"ğŸ’¾ æœ€é©åŒ–çµæœä¿å­˜: {results_file}")
        except Exception as e:
            self.logger.error(f"âŒ æœ€é©åŒ–çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        return optimal_params

    def train_models(
        self, features: pd.DataFrame, target: pd.Series, dry_run: bool = False
    ) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Ÿè¡Œï¼ˆPhase 39.3-39.4å¯¾å¿œï¼‰"""
        self.logger.info("ğŸ¤– Phase 39.3-39.4 MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹")

        if dry_run:
            self.logger.info("ğŸ” ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³: å®Ÿéš›ã®å­¦ç¿’ã¯ã‚¹ã‚­ãƒƒãƒ—")
            return {"dry_run": True, "models": list(self.models.keys())}

        results = {}
        trained_models = {}

        # Phase 39.3: Train/Val/Test split (70/15/15)
        n_samples = len(features)
        train_size = int(n_samples * 0.70)
        val_size = int(n_samples * 0.15)

        X_train = features.iloc[:train_size]
        y_train = target.iloc[:train_size]
        X_val = features.iloc[train_size : train_size + val_size]
        y_val = target.iloc[train_size : train_size + val_size]
        X_test = features.iloc[train_size + val_size :]
        y_test = target.iloc[train_size + val_size :]

        self.logger.info(
            f"ğŸ“Š Phase 39.3: Train/Val/Test split - "
            f"Train: {len(X_train)} ({len(X_train) / n_samples:.1%}), "
            f"Val: {len(X_val)} ({len(X_val) / n_samples:.1%}), "
            f"Test: {len(X_test)} ({len(X_test) / n_samples:.1%})"
        )

        # Phase 39.5: Optuna hyperparameter optimization
        if self.optimize:
            self.logger.info("ğŸ”¬ Phase 39.5: Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–é–‹å§‹")
            optimal_params = self.optimize_hyperparameters(
                pd.concat([X_train, X_val]),
                pd.concat([y_train, y_val]),
                self.n_trials,
            )

            # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨
            for model_name in self.models.keys():
                if model_name in optimal_params:
                    self.models[model_name].set_params(**optimal_params[model_name])
                    self.logger.info(f"âœ… {model_name}: æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨å®Œäº†")

        # Phase 39.3: TimeSeriesSplit n_splits=5 for Cross Validation
        tscv = TimeSeriesSplit(n_splits=5)
        self.logger.info("ğŸ“Š Phase 39.3: TimeSeriesSplit n_splits=5 for CV")

        # Phase 39.4: XGBoost scale_pos_weightå‹•çš„è¨­å®š
        if self.n_classes == 2:
            pos_count = y_train.sum()
            neg_count = len(y_train) - pos_count
            if pos_count > 0:
                scale_pos_weight = neg_count / pos_count
                self.models["xgboost"].set_params(scale_pos_weight=scale_pos_weight)
                self.logger.info(f"ğŸ“Š Phase 39.4: XGBoost scale_pos_weight={scale_pos_weight:.2f}")

        for model_name, model in self.models.items():
            self.logger.info(f"ğŸ“ˆ {model_name} å­¦ç¿’é–‹å§‹")

            try:
                # Phase 39.3: Cross Validation with Early Stopping
                cv_scores = []

                for train_idx, val_idx in tscv.split(X_train):
                    X_cv_train = X_train.iloc[train_idx]
                    y_cv_train = y_train.iloc[train_idx]
                    X_cv_val = X_train.iloc[val_idx]
                    y_cv_val = y_train.iloc[val_idx]

                    # Phase 39.4: SMOTE Oversampling (CV fold) - Phase 54.8: 3ã‚¯ãƒ©ã‚¹å¯¾å¿œ
                    if self.use_smote:
                        try:
                            # Phase 54.8: sampling_strategy='auto'ã§å…¨ã‚¯ãƒ©ã‚¹ã‚’majorityã‚¯ãƒ©ã‚¹æ•°ã«æƒãˆã‚‹
                            smote = SMOTE(sampling_strategy="auto", k_neighbors=5, random_state=42)
                            X_cv_train_resampled, y_cv_train_resampled = smote.fit_resample(
                                X_cv_train, y_cv_train
                            )
                            # Convert back to DataFrame to preserve feature names
                            X_cv_train = pd.DataFrame(
                                X_cv_train_resampled, columns=X_cv_train.columns
                            )
                            y_cv_train = pd.Series(y_cv_train_resampled)
                            # Phase 54.8: ã‚¯ãƒ©ã‚¹åˆ†å¸ƒç¢ºèªãƒ­ã‚°
                            class_dist = pd.Series(y_cv_train_resampled).value_counts(
                                normalize=True
                            )
                            self.logger.debug(
                                f"ğŸ“Š Phase 54.8: SMOTEé©ç”¨ï¼ˆCV foldï¼‰ - "
                                f"{len(train_idx)}â†’{len(X_cv_train_resampled)}ã‚µãƒ³ãƒ—ãƒ«"
                            )
                            self.logger.debug(
                                f"   SMOTEå¾Œã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: "
                                + ", ".join([f"Class {k}: {v:.1%}" for k, v in class_dist.items()])
                            )
                        except Exception as e:
                            self.logger.warning(
                                f"âš ï¸ SMOTEé©ç”¨å¤±æ•—ï¼ˆCV foldï¼‰: {e}, å…ƒãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ç¶™ç¶š"
                            )

                    # Phase 39.3: Early Stopping for LightGBM and XGBoost
                    if model_name == "lightgbm":
                        try:
                            model.fit(
                                X_cv_train,
                                y_cv_train,
                                eval_set=[(X_cv_val, y_cv_val)],
                                callbacks=[
                                    # LightGBM 4.0+ uses callbacks instead of early_stopping_rounds
                                    __import__("lightgbm").early_stopping(
                                        stopping_rounds=20, verbose=False
                                    )
                                ],
                            )
                        except ValueError as e:
                            # Handle unseen labels in CV folds (small datasets)
                            if "previously unseen labels" in str(e):
                                model.fit(X_cv_train, y_cv_train)
                            else:
                                raise
                    elif model_name == "xgboost":
                        # XGBoost 2.0+ uses callbacks for early stopping
                        try:
                            from xgboost import callback as xgb_callback

                            model.fit(
                                X_cv_train,
                                y_cv_train,
                                eval_set=[(X_cv_val, y_cv_val)],
                                callbacks=[xgb_callback.EarlyStopping(rounds=20)],
                                verbose=False,
                            )
                        except Exception:
                            # Fallback: train without early stopping
                            model.fit(X_cv_train, y_cv_train)
                    else:
                        # RandomForest doesn't support early stopping
                        model.fit(X_cv_train, y_cv_train)

                    # äºˆæ¸¬ãƒ»è©•ä¾¡
                    y_pred = model.predict(X_cv_val)
                    score = f1_score(y_cv_val, y_pred, average="weighted")
                    cv_scores.append(score)

                # Phase 39.3: Final model training on Train+Val with Early Stopping
                X_train_val = pd.concat([X_train, X_val])
                y_train_val = pd.concat([y_train, y_val])

                # Phase 39.4: SMOTE Oversampling (Final training) - Phase 54.8: 3ã‚¯ãƒ©ã‚¹å¯¾å¿œ
                if self.use_smote:
                    try:
                        # Phase 54.8: sampling_strategy='auto'ã§å…¨ã‚¯ãƒ©ã‚¹ã‚’majorityã‚¯ãƒ©ã‚¹æ•°ã«æƒãˆã‚‹
                        smote = SMOTE(sampling_strategy="auto", k_neighbors=5, random_state=42)
                        X_train_val_resampled, y_train_val_resampled = smote.fit_resample(
                            X_train_val, y_train_val
                        )
                        # Convert back to DataFrame to preserve feature names
                        X_train_val = pd.DataFrame(
                            X_train_val_resampled, columns=X_train_val.columns
                        )
                        y_train_val = pd.Series(y_train_val_resampled)
                        # Phase 54.8: ã‚¯ãƒ©ã‚¹åˆ†å¸ƒç¢ºèªãƒ­ã‚°
                        class_dist = pd.Series(y_train_val_resampled).value_counts(normalize=True)
                        self.logger.info(
                            f"ğŸ“Š Phase 54.8: SMOTEé©ç”¨ï¼ˆFinal trainingï¼‰ - "
                            f"{len(X_train) + len(X_val)}â†’{len(X_train_val_resampled)}ã‚µãƒ³ãƒ—ãƒ«"
                        )
                        self.logger.info(
                            f"   SMOTEå¾Œã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: "
                            + ", ".join([f"Class {k}: {v:.1%}" for k, v in class_dist.items()])
                        )
                    except Exception as e:
                        self.logger.warning(
                            f"âš ï¸ SMOTEé©ç”¨å¤±æ•—ï¼ˆFinal trainingï¼‰: {e}, å…ƒãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ç¶™ç¶š"
                        )

                if model_name == "lightgbm":
                    model.fit(
                        X_train_val,
                        y_train_val,
                        eval_set=[(X_test, y_test)],
                        callbacks=[
                            __import__("lightgbm").early_stopping(stopping_rounds=20, verbose=False)
                        ],
                    )
                    self.logger.info(
                        f"ğŸ“Š Phase 39.3: {model_name} Early Stopping enabled (rounds=20)"
                    )
                elif model_name == "xgboost":
                    # XGBoost 2.0+ uses callbacks for early stopping
                    try:
                        from xgboost import callback as xgb_callback

                        model.fit(
                            X_train_val,
                            y_train_val,
                            eval_set=[(X_test, y_test)],
                            callbacks=[xgb_callback.EarlyStopping(rounds=20)],
                            verbose=False,
                        )
                        self.logger.info(
                            f"ğŸ“Š Phase 39.3: {model_name} Early Stopping enabled (rounds=20)"
                        )
                    except Exception as e:
                        # Fallback: train without early stopping
                        self.logger.warning(
                            f"âš ï¸ XGBoost Early Stopping failed: {e}, training without it"
                        )
                        model.fit(X_train_val, y_train_val)
                else:
                    # RandomForest: Train on Train+Val without early stopping
                    model.fit(X_train_val, y_train_val)

                # Test set evaluation
                y_test_pred = model.predict(X_test)
                test_metrics = {
                    "accuracy": accuracy_score(y_test, y_test_pred),
                    "f1_score": f1_score(y_test, y_test_pred, average="weighted"),
                    "precision": precision_score(
                        y_test, y_test_pred, average="weighted", zero_division=0
                    ),
                    "recall": recall_score(
                        y_test, y_test_pred, average="weighted", zero_division=0
                    ),
                    "cv_f1_mean": np.mean(cv_scores),
                    "cv_f1_std": np.std(cv_scores),
                }

                results[model_name] = test_metrics
                trained_models[model_name] = model

                self.logger.info(
                    f"âœ… {model_name} å­¦ç¿’å®Œäº† - Test F1: {test_metrics['f1_score']:.3f}, "
                    f"CV F1: {test_metrics['cv_f1_mean']:.3f}Â±{test_metrics['cv_f1_std']:.3f}"
                )

            except Exception as e:
                self.logger.error(f"âŒ {model_name} å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
                results[model_name] = {"error": str(e)}

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆindividual_modelsã®ã¿ã‚’ä½¿ç”¨ãƒ»å¾ªç’°å‚ç…§é˜²æ­¢ï¼‰
        if len(trained_models) >= 2:
            try:
                # ProductionEnsembleè‡ªä½“ã‚’å«ã¾ãªã„ã‚ˆã†ã«å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®ã¿æ¸¡ã™
                individual_models_only = {
                    k: v for k, v in trained_models.items() if k != "production_ensemble"
                }
                ensemble_model = self._create_ensemble(individual_models_only)
                trained_models["production_ensemble"] = ensemble_model
                self.logger.info("âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†ï¼ˆå¾ªç’°å‚ç…§é˜²æ­¢å¯¾å¿œï¼‰")

                # Phase 59.7: Stacking Meta-Learnerè¨“ç·´
                if self.stacking:
                    try:
                        stacking_ensemble, stacking_metadata = self._train_stacking_ensemble(
                            features, target, individual_models_only
                        )
                        trained_models["stacking_ensemble"] = stacking_ensemble
                        results["stacking"] = stacking_metadata
                        self.logger.info("âœ… Phase 59.7: Stacking Ensembleä½œæˆå®Œäº†")
                    except Exception as e:
                        self.logger.error(f"âŒ Phase 59.7: Stacking Ensembleä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
                        import traceback

                        traceback.print_exc()

            except Exception as e:
                self.logger.error(f"âŒ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

        return {
            "results": results,
            "models": trained_models,
            "feature_names": list(features.columns),
            "training_samples": len(features),
        }

    def _create_ensemble(self, models: Dict) -> ProductionEnsemble:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆProductionEnsembleã‚¯ãƒ©ã‚¹ä½¿ç”¨ï¼‰."""
        try:
            self.logger.info("ğŸ”§ ProductionEnsembleã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
            ensemble_model = ProductionEnsemble(models)
            self.logger.info("âœ… ProductionEnsembleã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†")
            return ensemble_model
        except Exception as e:
            self.logger.error(f"âŒ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise

    # ========================================
    # Phase 59.7: Stacking Meta-Learner
    # ========================================

    def _generate_oof_predictions(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        base_models: Dict[str, Any],
        n_splits: int = 5,
    ) -> np.ndarray:
        """
        Phase 59.7: Out-of-Foldäºˆæ¸¬ã‚’ç”Ÿæˆã—ã¦ãƒ¡ã‚¿ç‰¹å¾´é‡ã‚’ä½œæˆ

        TimeSeriesSplit CVã‚’ä½¿ç”¨ã—ã¦å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡ã‚’ç”Ÿæˆã€‚
        ã“ã‚Œã«ã‚ˆã‚Šãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é˜²ãã¤ã¤Meta-Learnerç”¨ã®ç‰¹å¾´é‡ã‚’ä½œæˆã€‚

        Args:
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿
            base_models: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¾æ›¸ï¼ˆæœªè¨“ç·´ï¼‰
            n_splits: CVã®åˆ†å‰²æ•°

        Returns:
            np.ndarray: OOFäºˆæ¸¬ (n_samples, n_models * n_classes)
        """
        self.logger.info(f"ğŸ“Š Phase 59.7: OOFäºˆæ¸¬ç”Ÿæˆé–‹å§‹ï¼ˆn_splits={n_splits}ï¼‰")

        n_samples = len(X)
        n_models = len(base_models)
        n_classes = self.n_classes

        # OOFäºˆæ¸¬æ ¼ç´é…åˆ—ï¼ˆn_samples Ã— (n_models Ã— n_classes)ï¼‰
        oof_preds = np.zeros((n_samples, n_models * n_classes))
        model_names = list(base_models.keys())

        tscv = TimeSeriesSplit(n_splits=n_splits)

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            self.logger.info(
                f"   Fold {fold + 1}/{n_splits}: train={len(train_idx)}, val={len(val_idx)}"
            )

            X_train_fold = X.iloc[train_idx]
            y_train_fold = y.iloc[train_idx]
            X_val_fold = X.iloc[val_idx]

            # SMOTEé©ç”¨ï¼ˆFoldæ¯ï¼‰
            if self.use_smote:
                try:
                    smote = SMOTE(sampling_strategy="auto", k_neighbors=5, random_state=42)
                    X_train_resampled, y_train_resampled = smote.fit_resample(
                        X_train_fold, y_train_fold
                    )
                    X_train_fold = pd.DataFrame(X_train_resampled, columns=X_train_fold.columns)
                    y_train_fold = pd.Series(y_train_resampled)
                except Exception as e:
                    self.logger.warning(f"âš ï¸ SMOTEé©ç”¨å¤±æ•—ï¼ˆFold {fold + 1}ï¼‰: {e}")

            # å„ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
            for i, (model_name, model_template) in enumerate(base_models.items()):
                # ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡è£½ï¼ˆå‰ã®Foldã®å­¦ç¿’çŠ¶æ…‹ã‚’å¼•ãç¶™ãŒãªã„ãŸã‚ï¼‰
                model = self._clone_model(model_template, model_name)

                try:
                    model.fit(X_train_fold, y_train_fold)
                    proba = model.predict_proba(X_val_fold)

                    # OOFäºˆæ¸¬ã‚’æ ¼ç´
                    start_col = i * n_classes
                    end_col = (i + 1) * n_classes
                    oof_preds[val_idx, start_col:end_col] = proba

                except Exception as e:
                    self.logger.warning(f"âš ï¸ {model_name} Fold {fold + 1} äºˆæ¸¬å¤±æ•—: {e}")
                    # å¤±æ•—æ™‚ã¯å‡ä¸€ç¢ºç‡ã§åŸ‹ã‚ã‚‹
                    uniform_proba = 1.0 / n_classes
                    oof_preds[val_idx, i * n_classes : (i + 1) * n_classes] = uniform_proba

        self.logger.info(f"âœ… Phase 59.7: OOFäºˆæ¸¬ç”Ÿæˆå®Œäº† - å½¢çŠ¶: {oof_preds.shape}")
        return oof_preds

    def _clone_model(self, model: Any, model_name: str) -> Any:
        """
        Phase 59.7: ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡è£½ï¼ˆFoldæ¯ã«æ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆï¼‰

        Args:
            model: å…ƒã®ãƒ¢ãƒ‡ãƒ«
            model_name: ãƒ¢ãƒ‡ãƒ«å

        Returns:
            è¤‡è£½ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
        """
        if model_name == "lightgbm":
            params = model.get_params()
            return LGBMClassifier(**params)
        elif model_name == "xgboost":
            params = model.get_params()
            return XGBClassifier(**params)
        elif model_name == "random_forest":
            params = model.get_params()
            return RandomForestClassifier(**params)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: sklearn clone
            from sklearn.base import clone

            return clone(model)

    def _train_meta_learner(
        self,
        oof_preds: np.ndarray,
        y: pd.Series,
        meta_features: Optional[pd.DataFrame] = None,
    ) -> LGBMClassifier:
        """
        Phase 59.7: Meta-Learnerï¼ˆLightGBMï¼‰ã‚’è¨“ç·´

        OOFäºˆæ¸¬ã‚’ãƒ¡ã‚¿ç‰¹å¾´é‡ã¨ã—ã¦Meta-Learnerã‚’è¨“ç·´ã€‚

        Args:
            oof_preds: OOFäºˆæ¸¬ (n_samples, n_models * n_classes)
            y: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿
            meta_features: è¿½åŠ ãƒ¡ã‚¿ç‰¹å¾´é‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            LGBMClassifier: è¨“ç·´æ¸ˆã¿Meta-Learner
        """
        self.logger.info("ğŸ“Š Phase 59.7: Meta-Learnerè¨“ç·´é–‹å§‹")

        # ãƒ¡ã‚¿ç‰¹å¾´é‡ã®DataFrameåŒ–
        n_models = len(self.models)
        n_classes = self.n_classes
        meta_feature_names = []
        for model_name in self.models.keys():
            for cls in range(n_classes):
                meta_feature_names.append(f"{model_name}_class{cls}")

        X_meta = pd.DataFrame(oof_preds, columns=meta_feature_names)

        # è¿½åŠ ãƒ¡ã‚¿ç‰¹å¾´é‡ãŒã‚ã‚‹å ´åˆã¯çµåˆ
        if meta_features is not None:
            X_meta = pd.concat([X_meta, meta_features], axis=1)

        # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯ï¼ˆTimeSeriesSplitåˆæœŸFoldã§æœªã‚«ãƒãƒ¼ã®è¡Œï¼‰
        valid_mask = ~(X_meta.isna().any(axis=1) | (X_meta.sum(axis=1) == 0))
        valid_mask_arr = valid_mask.values  # numpy arrayã«å¤‰æ›
        X_meta_valid = X_meta[valid_mask_arr]
        # yã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ã‹ã‚‰ãƒã‚¹ã‚¯é©ç”¨
        y_reset = y.reset_index(drop=True)
        y_valid = y_reset[valid_mask_arr]

        self.logger.info(
            f"   Meta-Learnerè¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_meta_valid)}ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ{len(X_meta) - len(X_meta_valid)}ã‚µãƒ³ãƒ—ãƒ«é™¤å¤–ï¼‰"
        )

        # Meta-Learnerãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè»½é‡è¨­å®šï¼‰
        meta_params = {
            "n_estimators": 50,
            "learning_rate": 0.05,
            "max_depth": 4,
            "num_leaves": 15,
            "random_state": 42,
            "verbose": -1,
            "class_weight": "balanced",
        }

        if self.n_classes == 3:
            meta_params["objective"] = "multiclass"
            meta_params["num_class"] = 3

        meta_model = LGBMClassifier(**meta_params)

        # Train/Test split for Meta-Learner
        n_train = int(len(X_meta_valid) * 0.85)
        X_meta_train = X_meta_valid.iloc[:n_train]
        y_meta_train = y_valid.iloc[:n_train]
        X_meta_test = X_meta_valid.iloc[n_train:]
        y_meta_test = y_valid.iloc[n_train:]

        # SMOTEé©ç”¨
        if self.use_smote:
            try:
                smote = SMOTE(sampling_strategy="auto", k_neighbors=5, random_state=42)
                X_meta_resampled, y_meta_resampled = smote.fit_resample(X_meta_train, y_meta_train)
                X_meta_train = pd.DataFrame(X_meta_resampled, columns=X_meta_train.columns)
                y_meta_train = pd.Series(y_meta_resampled)
            except Exception as e:
                self.logger.warning(f"âš ï¸ Meta-Learner SMOTEé©ç”¨å¤±æ•—: {e}")

        # è¨“ç·´
        try:
            meta_model.fit(
                X_meta_train,
                y_meta_train,
                eval_set=[(X_meta_test, y_meta_test)],
                callbacks=[
                    __import__("lightgbm").early_stopping(stopping_rounds=10, verbose=False)
                ],
            )
        except Exception as e:
            self.logger.warning(f"âš ï¸ Early Stoppingå¤±æ•—: {e}, é€šå¸¸è¨“ç·´ã«åˆ‡ã‚Šæ›¿ãˆ")
            meta_model.fit(X_meta_train, y_meta_train)

        # è©•ä¾¡
        y_pred = meta_model.predict(X_meta_test)
        meta_f1 = f1_score(y_meta_test, y_pred, average="weighted")
        meta_acc = accuracy_score(y_meta_test, y_pred)

        self.logger.info(f"âœ… Phase 59.7: Meta-Learnerè¨“ç·´å®Œäº†")
        self.logger.info(f"   Meta-Learner F1: {meta_f1:.4f}, Accuracy: {meta_acc:.4f}")

        return meta_model

    def _train_stacking_ensemble(
        self,
        features: pd.DataFrame,
        target: pd.Series,
        trained_base_models: Dict[str, Any],
    ) -> Tuple[Any, Dict[str, Any]]:
        """
        Phase 59.7: Stacking Ensembleè¨“ç·´

        1. OOFäºˆæ¸¬ã‚’ç”Ÿæˆ
        2. Meta-Learnerã‚’è¨“ç·´
        3. StackingEnsembleã‚’ä½œæˆ

        Args:
            features: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            target: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿
            trained_base_models: è¨“ç·´æ¸ˆã¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«

        Returns:
            Tuple[StackingEnsemble, Dict]: (Stackingã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«, ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿)
        """
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“Š Phase 59.7: Stacking Ensembleè¨“ç·´é–‹å§‹")
        self.logger.info("=" * 60)

        # 1. OOFäºˆæ¸¬ç”Ÿæˆï¼ˆæœªè¨“ç·´ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
        # æ³¨æ„: OOFã¯æœªè¨“ç·´ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        untrained_models = {}
        for name, model in self.models.items():
            untrained_models[name] = self._clone_model(model, name)

        oof_preds = self._generate_oof_predictions(features, target, untrained_models)

        # 2. Meta-Learnerè¨“ç·´
        meta_learner = self._train_meta_learner(oof_preds, target)

        # 3. StackingEnsembleä½œæˆ
        stacking_ensemble = StackingEnsemble(
            base_models=trained_base_models,
            meta_model=meta_learner,
        )

        # 4. è©•ä¾¡ï¼ˆTrain/Test splitï¼‰
        n_test = int(len(features) * 0.15)
        X_test = features.iloc[-n_test:]
        y_test = target.iloc[-n_test:]

        y_pred_stacking = stacking_ensemble.predict(X_test)
        stacking_f1 = f1_score(y_test, y_pred_stacking, average="weighted")
        stacking_acc = accuracy_score(y_test, y_pred_stacking)

        self.logger.info(f"ğŸ“Š Stacking Ensembleè©•ä¾¡:")
        self.logger.info(f"   F1 Score: {stacking_f1:.4f}")
        self.logger.info(f"   Accuracy: {stacking_acc:.4f}")

        stacking_metadata = {
            "stacking_f1": float(stacking_f1),
            "stacking_accuracy": float(stacking_acc),
            "meta_features_count": oof_preds.shape[1],
            "base_models": list(trained_base_models.keys()),
        }

        return stacking_ensemble, stacking_metadata

    def save_models(self, training_results: Dict[str, Any]) -> Dict[str, str]:
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆå€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ï¼štrainingã€çµ±åˆãƒ¢ãƒ‡ãƒ«ï¼šproductionï¼‰."""
        self.logger.info("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜é–‹å§‹")

        saved_files = {}
        models = training_results.get("models", {})

        for model_name, model in models.items():
            try:
                if model_name == "production_ensemble":
                    # Phase 51.5-A Fix: feature_order.jsonã‹ã‚‰è¨­å®šé§†å‹•å‹ã§ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«åå–å¾—
                    target_model_type = self.current_model_type
                    model_config = _feature_manager.get_feature_level_info()
                    model_filename = model_config[target_model_type].get(
                        "model_file", "ensemble_full.pkl"
                    )

                    # æœ¬ç•ªç”¨çµ±åˆãƒ¢ãƒ‡ãƒ«ã¯productionãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
                    model_file = self.production_dir / model_filename
                    with open(model_file, "wb") as f:
                        pickle.dump(model, f)

                    # Gitæƒ…å ±å–å¾—
                    try:
                        git_commit = self._get_git_info()
                    except Exception:
                        git_commit = {"commit": "unknown", "branch": "unknown"}

                    # æœ¬ç•ªç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆPhase 50.9å®Œäº†: å¤–éƒ¨APIå®Œå…¨å‰Šé™¤ãƒ»2æ®µéšGraceful Degradationï¼‰
                    production_metadata = {
                        "created_at": datetime.now().isoformat(),
                        "model_type": "ProductionEnsemble",
                        "model_file": str(model_file),
                        "version": "1.0.0",
                        "phase": "Phase 50.9",  # Phase 50.9å®Œäº†: å¤–éƒ¨APIå®Œå…¨å‰Šé™¤ãƒ»ã‚·ãƒ³ãƒ—ãƒ«è¨­è¨ˆå›å¸°
                        "status": "production_ready",
                        "feature_names": training_results.get("feature_names", []),
                        "individual_models": [
                            k for k in model.models.keys() if k != "production_ensemble"
                        ],
                        "model_weights": model.weights,
                        "performance_metrics": training_results.get("results", {}),
                        "training_info": {
                            "samples": training_results.get("training_samples", 0),
                            "feature_count": len(training_results.get("feature_names", [])),
                            "training_duration_seconds": getattr(self, "_training_start_time", 0),
                        },
                        "git_info": git_commit,
                        "notes": "Phase 50.9å®Œäº†ãƒ»å¤–éƒ¨APIå®Œå…¨å‰Šé™¤ãƒ»62ç‰¹å¾´é‡å›ºå®šã‚·ã‚¹ãƒ†ãƒ ãƒ»2æ®µéšGraceful Degradationãƒ»ã‚·ãƒ³ãƒ—ãƒ«è¨­è¨ˆå›å¸°ãƒ»TimeSeriesSplit n_splits=5ãƒ»Early Stoppingãƒ»SMOTEãƒ»Optunaæœ€é©åŒ–",
                    }

                    # Phase 51.5-A Fix: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«åæ±ºå®š
                    # fullãƒ¢ãƒ‡ãƒ«ã¯æ¤œè¨¼ç”¨ã«production_model_metadata.jsonã«ä¿å­˜
                    # basicãƒ¢ãƒ‡ãƒ«ã¯åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                    if self.current_model_type == "full":
                        production_metadata_file = (
                            self.production_dir / "production_model_metadata.json"
                        )
                    else:
                        production_metadata_file = (
                            self.production_dir
                            / f"production_model_metadata_{self.current_model_type}.json"
                        )

                    with open(production_metadata_file, "w", encoding="utf-8") as f:
                        json.dump(
                            production_metadata,
                            f,
                            indent=2,
                            ensure_ascii=False,
                        )

                    self.logger.info(f"âœ… æœ¬ç•ªç”¨çµ±åˆãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_file}")

                elif model_name == "stacking_ensemble":
                    # Phase 59.7: Stacking Ensembleã‚’productionãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
                    stacking_file = self.production_dir / "stacking_ensemble.pkl"
                    with open(stacking_file, "wb") as f:
                        pickle.dump(model, f)

                    # Meta-Learnerã‚‚å€‹åˆ¥ã«ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                    if hasattr(model, "meta_model"):
                        meta_learner_file = self.production_dir / "meta_learner.pkl"
                        with open(meta_learner_file, "wb") as f:
                            pickle.dump(model.meta_model, f)
                        self.logger.info(f"âœ… Meta-Learnerä¿å­˜: {meta_learner_file}")

                    self.logger.info(f"âœ… Phase 59.7: Stacking Ensembleä¿å­˜: {stacking_file}")
                    saved_files[model_name] = str(stacking_file)
                    continue  # saved_filesã¸ã®è¿½åŠ ã¯ä¸Šã§è¡Œã£ãŸã®ã§ã‚¹ã‚­ãƒƒãƒ—

                else:
                    # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã¯trainingãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
                    model_file = self.training_dir / f"{model_name}_model.pkl"
                    with open(model_file, "wb") as f:
                        pickle.dump(model, f)

                    self.logger.info(f"âœ… å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_file}")

                saved_files[model_name] = str(model_file)

            except Exception as e:
                self.logger.error(f"âŒ {model_name} ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        # å­¦ç¿’ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆPhase 51.9å®Œäº†: çœŸã®3ã‚¯ãƒ©ã‚¹åˆ†é¡ãƒ»6æˆ¦ç•¥çµ±åˆãƒ»Strategy-Aware MLãƒ»å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’ï¼‰
        training_metadata = {
            "created_at": datetime.now().isoformat(),
            "feature_names": training_results.get("feature_names", []),
            "training_samples": training_results.get("training_samples", 0),
            "model_metrics": training_results.get("results", {}),
            "model_files": saved_files,
            "config_path": self.config_path,
            "phase": "Phase 51.9",  # Phase 51.9å®Œäº†: çœŸã®3ã‚¯ãƒ©ã‚¹åˆ†é¡å®Ÿè£…
            "notes": "Phase 51.9å®Œäº†ãƒ»çœŸã®3ã‚¯ãƒ©ã‚¹åˆ†é¡å®Ÿè£…ï¼ˆmulticlass paramsè¿½åŠ ï¼‰ãƒ»6æˆ¦ç•¥çµ±åˆï¼ˆATRBased/DonchianChannel/ADXTrendStrength/BBReversal/StochasticReversal/MACDEMACrossoverï¼‰ãƒ»å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’ï¼ˆè¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã®ä¸€è²«æ€§ç¢ºä¿ï¼‰ãƒ»55ç‰¹å¾´é‡ãƒ»é–¾å€¤0.5%ãƒ»CV n_splits=5ãƒ»Early Stoppingãƒ»SMOTEãƒ»Optunaæœ€é©åŒ–ãƒ»å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’çµæœ",
        }

        training_metadata_file = self.training_dir / "training_metadata.json"
        with open(training_metadata_file, "w", encoding="utf-8") as f:
            json.dump(training_metadata, f, indent=2, ensure_ascii=False)

        self.logger.info(f"âœ… å­¦ç¿’ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {training_metadata_file}")
        return saved_files

    def validate_models(self, saved_files: Dict[str, str]) -> bool:
        """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ï¼ˆæœ¬ç•ªç”¨ãƒ¢ãƒ‡ãƒ«é‡ç‚¹ï¼‰."""
        self.logger.info("ğŸ” ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼é–‹å§‹")

        validation_passed = True

        for model_name, model_file in saved_files.items():
            try:
                # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
                with open(model_file, "rb") as f:
                    model = pickle.load(f)

                # åŸºæœ¬å±æ€§ãƒã‚§ãƒƒã‚¯
                if hasattr(model, "predict"):
                    self.logger.info(f"âœ… {model_name}: predict ãƒ¡ã‚½ãƒƒãƒ‰ç¢ºèª")
                else:
                    self.logger.error(f"âŒ {model_name}: predict ãƒ¡ã‚½ãƒƒãƒ‰ãªã—")
                    validation_passed = False
                    continue

                # Phase 50.9: ãƒ¢ãƒ‡ãƒ«åˆ¥ç‰¹å¾´é‡æ•°å¯¾å¿œã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬ãƒ†ã‚¹ãƒˆ
                # ãƒ¢ãƒ‡ãƒ«ã®å®Ÿéš›ã®ç‰¹å¾´é‡æ•°ã‚’å–å¾—ï¼ˆLightGBMãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ï¼‰
                if hasattr(model, "models") and "lightgbm" in model.models:
                    # ProductionEnsembleã®å ´åˆ
                    lgbm_model = model.models["lightgbm"]
                    if hasattr(lgbm_model, "n_features_in_"):
                        n_features = lgbm_model.n_features_in_
                    elif hasattr(lgbm_model, "_n_features"):
                        n_features = lgbm_model._n_features
                    else:
                        n_features = len(self.expected_features)
                elif hasattr(model, "n_features_in_"):
                    n_features = model.n_features_in_
                else:
                    n_features = len(self.expected_features)

                # ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ãŸç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’å–å¾—
                feature_list = self.expected_features[:n_features]
                sample_features_array = np.random.random((5, n_features))
                sample_features = pd.DataFrame(sample_features_array, columns=feature_list)
                prediction = model.predict(sample_features)

                if len(prediction) == 5:
                    self.logger.info(f"âœ… {model_name}: ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬æˆåŠŸ")
                else:
                    self.logger.error(f"âŒ {model_name}: ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬å¤±æ•—")
                    validation_passed = False
                    continue

                # æœ¬ç•ªç”¨ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°æ¤œè¨¼
                if model_name == "production_ensemble":
                    self.logger.info("ğŸ¯ æœ¬ç•ªç”¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è©³ç´°æ¤œè¨¼é–‹å§‹")

                    # predict_proba ãƒ¡ã‚½ãƒƒãƒ‰ç¢ºèªï¼ˆPhase 51.9-6D: 3ã‚¯ãƒ©ã‚¹å¯¾å¿œï¼‰
                    if hasattr(model, "predict_proba"):
                        probabilities = model.predict_proba(sample_features)
                        expected_shape = (5, self.n_classes)
                        if probabilities.shape == expected_shape:
                            self.logger.info(f"âœ… predict_proba ç¢ºèªæˆåŠŸï¼ˆ{self.n_classes}ã‚¯ãƒ©ã‚¹ï¼‰")
                        else:
                            self.logger.error(
                                f"âŒ predict_proba å½¢çŠ¶ä¸æ­£: {probabilities.shape} "
                                f"!= {expected_shape}"
                            )
                            validation_passed = False

                    # Phase 50.9: ãƒ¢ãƒ‡ãƒ«åˆ¥ç‰¹å¾´é‡æ•°å¯¾å¿œ - get_model_infoç¢ºèª
                    if hasattr(model, "get_model_info"):
                        info = model.get_model_info()
                        # ã™ã§ã«å–å¾—æ¸ˆã¿ã®n_featuresï¼ˆãƒ¢ãƒ‡ãƒ«åˆ¥ï¼‰ã‚’ä½¿ç”¨
                        if info.get("n_features") == n_features:
                            self.logger.info(f"âœ… get_model_info ç¢ºèªæˆåŠŸï¼ˆ{n_features}ç‰¹å¾´é‡ï¼‰")
                        else:
                            self.logger.error(
                                f"âŒ get_model_info ç‰¹å¾´é‡æ•°ä¸æ­£: "
                                f"{info.get('n_features')} != {n_features}"
                            )
                            validation_passed = False

                    self.logger.info("ğŸ¯ æœ¬ç•ªç”¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è©³ç´°æ¤œè¨¼å®Œäº†")

            except Exception as e:
                self.logger.error(f"âŒ {model_name} æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
                validation_passed = False

        if validation_passed:
            self.logger.info("ğŸ‰ å…¨ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼æˆåŠŸï¼")
        else:
            self.logger.error("ğŸ’¥ ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼å¤±æ•—")

        return validation_passed

    def _get_git_info(self) -> Dict[str, str]:
        """Gitæƒ…å ±å–å¾—ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ç”¨ï¼‰."""
        import subprocess

        try:
            # Git commit hashå–å¾—
            commit = subprocess.check_output(
                ["git", "rev-parse", "HEAD"], text=True, cwd=project_root
            ).strip()

            # Git branchå–å¾—
            branch = subprocess.check_output(
                ["git", "rev-parse", "--abbrev-ref", "HEAD"],
                text=True,
                cwd=project_root,
            ).strip()

            return {"commit": commit, "commit_short": commit[:8], "branch": branch}
        except Exception as e:
            self.logger.warning(f"Gitæƒ…å ±å–å¾—å¤±æ•—: {e}")
            return {"commit": "unknown", "commit_short": "unknown", "branch": "unknown"}

    def _archive_existing_models(self) -> bool:
        """
        æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆPhase 50.9: 2æ®µéšã‚·ã‚¹ãƒ†ãƒ ï¼‰

        Phase 50.9å¯¾å¿œãƒ¢ãƒ‡ãƒ«ï¼š
        - ensemble_full.pklï¼ˆ62ç‰¹å¾´é‡ï¼‰
        - ensemble_basic.pklï¼ˆ57ç‰¹å¾´é‡ï¼‰
        """
        try:
            # Phase 50.9: 2æ®µéšãƒ¢ãƒ‡ãƒ«ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œ
            level_files = [
                "ensemble_full.pkl",
                "ensemble_basic.pkl",
            ]

            archived_any = False
            for model_filename in level_files:
                production_model = self.production_dir / model_filename

                if production_model.exists():
                    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
                    archive_dir = Path("models/archive")
                    archive_dir.mkdir(exist_ok=True)

                    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    model_name = model_filename.replace(".pkl", "")
                    archive_model = archive_dir / f"{model_name}_{timestamp}.pkl"

                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼
                    import shutil

                    shutil.copy2(production_model, archive_model)

                    self.logger.info(f"âœ… æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å®Œäº†: {archive_model}")
                    archived_any = True

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
            production_metadata = self.production_dir / "production_model_metadata.json"
            if production_metadata.exists():
                archive_dir = Path("models/archive")
                archive_dir.mkdir(exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                archive_metadata = archive_dir / f"production_model_metadata_{timestamp}.json"

                import shutil

                shutil.copy2(production_metadata, archive_metadata)
                self.logger.info(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å®Œäº†: {archive_metadata}")

            if not archived_any:
                self.logger.info("ğŸ“‚ æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ãªã— - ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¹ã‚­ãƒƒãƒ—")

            return True

        except Exception as e:
            self.logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def run(self, dry_run: bool = False, days: int = 180) -> bool:
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œå‡¦ç†ï¼ˆPhase 51.5-A Fix: ä¸€æ‹¬ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼‰."""
        try:
            self.logger.info(f"ğŸš€ Phase 51.9: MLãƒ¢ãƒ‡ãƒ«ä½œæˆé–‹å§‹ - è¨“ç·´å¯¾è±¡: {self.models_to_train}")

            # 0. æ—¢å­˜ãƒ¢ãƒ‡ãƒ«è‡ªå‹•ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆPhase 29: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†å¼·åŒ–ï¼‰
            if not dry_run:
                if not self._archive_existing_models():
                    self.logger.warning("âš ï¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¤±æ•— - å‡¦ç†ç¶šè¡Œ")

            # 1. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆ1å›ã®ã¿ãƒ»å…¨55ç‰¹å¾´é‡ç”Ÿæˆï¼‰
            # æˆ¦ç•¥ä¿¡å·ç”ŸæˆãŒæœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ã€1å›ã ã‘å®Ÿè¡Œ
            self.logger.info("ğŸ“Š Phase 51.9: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«å…±é€šï¼‰")
            features, target = self.prepare_training_data(days)
            self.logger.info("âœ… å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†ï¼ˆå…¨55ç‰¹å¾´é‡ç”Ÿæˆæ¸ˆã¿ï¼‰")

            # 2. å„ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ï¼ˆãƒ«ãƒ¼ãƒ—å‡¦ç†ï¼‰
            all_saved_files = {}
            for model_type in self.models_to_train:
                self.current_model_type = model_type
                model_name = "fullï¼ˆ55ç‰¹å¾´é‡ï¼‰" if model_type == "full" else "basicï¼ˆ49ç‰¹å¾´é‡ï¼‰"

                self.logger.info("")
                self.logger.info("=" * 80)
                self.logger.info(f"ğŸ“Š Phase 51.9: {model_name}ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹")
                self.logger.info("=" * 80)

                # Phase 55.6 Fix: ãƒ«ãƒ¼ãƒ—å†…ã§ç‰¹å¾´é‡é¸æŠã‚’å®Ÿè¡Œ
                # ä»¥å‰ã¯ prepare_training_data() å†…ã§1å›ã ã‘é¸æŠã•ã‚Œã¦ã„ãŸãŸã‚
                # ä¸¡ãƒ¢ãƒ‡ãƒ«ãŒåŒä¸€ç‰¹å¾´é‡ã§è¨“ç·´ã•ã‚Œã‚‹å•é¡ŒãŒã‚ã£ãŸ
                model_features = self._select_features_by_level(features.copy())
                self.logger.info(
                    f"ğŸ“Š Phase 55.6: {model_type}ãƒ¢ãƒ‡ãƒ«ç”¨ç‰¹å¾´é‡é¸æŠ - "
                    f"{len(model_features.columns)}ç‰¹å¾´é‡"
                )

                # Phase 55.6 Fix: ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å†åˆæœŸåŒ–
                # å‰å›ã®è¨“ç·´çŠ¶æ…‹ãŒãƒªãƒ¼ã‚¯ã—ãªã„ã‚ˆã†ã«ã‚¯ãƒªãƒ¼ãƒ³ãªçŠ¶æ…‹ã‹ã‚‰è¨“ç·´
                self._initialize_models()

                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                training_results = self.train_models(model_features, target, dry_run)

                if dry_run:
                    self.logger.info(f"ğŸ” {model_name}ãƒ¢ãƒ‡ãƒ« ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Œäº†")
                    continue

                # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
                saved_files = self.save_models(training_results)
                all_saved_files.update(saved_files)

                self.logger.info(f"âœ… {model_name}ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»ä¿å­˜å®Œäº†")

            if dry_run:
                self.logger.info("ğŸ” å…¨ãƒ¢ãƒ‡ãƒ« ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Œäº†")
                return True

            # 3. æ¤œè¨¼ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«ï¼‰
            self.logger.info("")
            self.logger.info("=" * 80)
            self.logger.info("ğŸ” Phase 51.5-A Fix: å…¨ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼é–‹å§‹")
            self.logger.info("=" * 80)

            validation_passed = self.validate_models(all_saved_files)

            if validation_passed:
                self.logger.info("âœ… Phase 51.5-A Fix: å…¨MLãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº† - å®Ÿå–å¼•æº–å‚™å®Œäº†")
                return True
            else:
                self.logger.error("âŒ MLãƒ¢ãƒ‡ãƒ«ä½œæˆå¤±æ•—")
                return False

        except Exception as e:
            self.logger.error(f"ğŸ’¥ MLãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆPhase 39.1-39.5å®Œäº†ï¼‰"""
    parser = argparse.ArgumentParser(
        description="æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨MLãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆPhase 39.1-39.5å®Œäº†ãƒ»MLä¿¡é ¼åº¦å‘ä¸ŠæœŸï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆå®Ÿéš›ã®å­¦ç¿’ãƒ»ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰",
    )
    parser.add_argument("--verbose", action="store_true", help="è©³ç´°ãƒ­ã‚°å‡ºåŠ›")
    parser.add_argument(
        "--days",
        type=int,
        default=180,
        help="å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æœŸé–“ï¼ˆæ—¥æ•°ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 180æ—¥ï¼‰",
    )
    parser.add_argument("--config", default="config/core/unified.yaml", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")

    # Phase 39.2: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å®šå¼•æ•°
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.0005,
        help="Phase 55.8: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.05%%ãƒ»HOLDç‡é©æ­£åŒ–ï¼‰",
    )
    # Phase 55.6: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’3ã‚¯ãƒ©ã‚¹ã«å¤‰æ›´ï¼ˆ2ã‚¯ãƒ©ã‚¹ã¯éæ¨å¥¨ï¼‰
    parser.add_argument(
        "--n-classes",
        type=int,
        default=3,
        choices=[2, 3],
        help="Phase 55.6: ã‚¯ãƒ©ã‚¹æ•° 3ï¼ˆBUY/HOLD/SELLï¼‰æ¨å¥¨ã€2ã¯å¾Œæ–¹äº’æ›ç”¨",
    )

    # Phase 55.6: SMOTEãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœ‰åŠ¹ï¼ˆ--no-smoteã§ç„¡åŠ¹åŒ–å¯èƒ½ï¼‰
    parser.add_argument(
        "--use-smote",
        action="store_true",
        default=True,
        help="Phase 55.6: SMOTEã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœ‰åŠ¹ï¼‰",
    )
    parser.add_argument(
        "--no-smote",
        action="store_true",
        help="Phase 55.6: SMOTEç„¡åŠ¹åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³",
    )

    # Phase 39.5: Optunaæœ€é©åŒ–è¨­å®šå¼•æ•°
    parser.add_argument(
        "--optimize",
        action="store_true",
        help="Phase 39.5: Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–æœ‰åŠ¹åŒ–",
    )
    parser.add_argument(
        "--n-trials",
        type=int,
        default=20,
        help="Phase 39.5: Optunaæœ€é©åŒ–è©¦è¡Œå›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰",
    )

    # Phase 51.5-B: ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆä¸€æ‹¬ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼‰
    parser.add_argument(
        "--model",
        type=str,
        default="both",
        choices=["both", "full", "basic"],
        help="Phase 51.5-B: è¨“ç·´ã™ã‚‹ãƒ¢ãƒ‡ãƒ« both=ä¸¡æ–¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨å¥¨ï¼‰/full=fullã®ã¿/basic=basicã®ã¿",
    )

    # Phase 59.7: Stacking Meta-Learner
    parser.add_argument(
        "--stacking",
        action="store_true",
        help="Phase 59.7: Stacking Meta-Learneræœ‰åŠ¹åŒ–ï¼ˆOOFäºˆæ¸¬ + Meta-Learnerè¨“ç·´ï¼‰",
    )

    args = parser.parse_args()

    # ãƒ¢ãƒ‡ãƒ«é¸æŠã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
    if args.model == "both":
        models_to_train = ["full", "basic"]
    elif args.model == "full":
        models_to_train = ["full"]
    elif args.model == "basic":
        models_to_train = ["basic"]
    else:
        models_to_train = ["full", "basic"]

    # Phase 55.6: --no-smoteãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯SMOTEç„¡åŠ¹åŒ–
    use_smote = args.use_smote and not args.no_smote

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Ÿè¡Œï¼ˆPhase 51.5-Bå¯¾å¿œï¼‰
    creator = NewSystemMLModelCreator(
        config_path=args.config,
        models_to_train=models_to_train,
        verbose=args.verbose,
        target_threshold=args.threshold,
        n_classes=args.n_classes,
        use_smote=use_smote,
        optimize=args.optimize,
        n_trials=args.n_trials,
        stacking=args.stacking,  # Phase 59.7: Stacking Meta-Learner
    )

    success = creator.run(dry_run=args.dry_run, days=args.days)

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
