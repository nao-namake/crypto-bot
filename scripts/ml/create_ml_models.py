#!/usr/bin/env python3
"""
æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨MLãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ - Phase 41.8å®Œäº†ç‰ˆï¼ˆStrategy-Aware MLãƒ»å®Œå…¨å®Ÿè£…ï¼‰

Phase 41.8å¯¾å¿œ: å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’ï¼ˆè¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã®ä¸€è²«æ€§ç¢ºä¿ï¼‰
Phase 41å¯¾å¿œ: æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ç‰¹å¾´é‡çµ±åˆï¼ˆ50â†’55ç‰¹å¾´é‡ï¼‰
Phase 39å¯¾å¿œ: å®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’ãƒ»é–¾å€¤æœ€é©åŒ–ãƒ»CVå¼·åŒ–ãƒ»SMOTEãƒ»Optunaæœ€é©åŒ–

æ©Ÿèƒ½:
- 55ç‰¹å¾´é‡ã§ã® LightGBMãƒ»XGBoostãƒ»RandomForest ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’
- Phase 41.8: å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’ - éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿéš›ã«5æˆ¦ç•¥ã‚’å®Ÿè¡Œã—ã¦å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- Phase 41: Strategy-Aware ML - æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«5ç‰¹å¾´é‡è¿½åŠ 
- Phase 40.6: Feature Engineeringæ‹¡å¼µ - 15â†’50ç‰¹å¾´é‡
- Phase 39.1: å®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’ï¼ˆCSVèª­ã¿è¾¼ã¿ãƒ»éå»180æ—¥åˆ†15åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ï¼‰
- Phase 39.2: é–¾å€¤æœ€é©åŒ–ï¼ˆ0.3% â†’ 0.5%ï¼‰ãƒ»3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆBUY/HOLD/SELLï¼‰
- Phase 39.3: TimeSeriesSplit n_splits=5ãƒ»Early Stopping rounds=20ãƒ»Train/Val/Test 70/15/15
- Phase 39.4: SMOTE oversamplingãƒ»class_weight='balanced'
- Phase 39.5: Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼ˆTPESamplerï¼‰
- æ–°ã‚·ã‚¹ãƒ†ãƒ  src/ æ§‹é€ ã«å¯¾å¿œ
- models/production/ ã«ãƒ¢ãƒ‡ãƒ«ä¿å­˜
- å®Ÿå–å¼•å‰ã®å“è³ªä¿è¨¼ãƒ»æ€§èƒ½æ¤œè¨¼

Phase 41.8å®Œäº†æˆæœ: 55ç‰¹å¾´é‡å®Œå…¨çµ±åˆãƒ»è¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã®ä¸€è²«æ€§ç¢ºä¿ãƒ»å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’

ä½¿ç”¨æ–¹æ³•:
    # Phase 41.8: 55ç‰¹å¾´é‡ã§å­¦ç¿’ï¼ˆå®Ÿæˆ¦ç•¥ä¿¡å·ãƒ»æ¨å¥¨ï¼‰
    python scripts/ml/create_ml_models.py --n-classes 3 --threshold 0.005 --optimize --n-trials 50 --verbose

    # åŸºæœ¬å®Ÿè¡Œï¼ˆPhase 39.1-39.4ï¼‰
    python scripts/ml/create_ml_models.py [--dry-run] [--verbose]

    # Phase 39.4: SMOTE oversamplingæœ‰åŠ¹åŒ–
    python scripts/ml/create_ml_models.py --use-smote
"""

import argparse
import asyncio
import json
import logging
import pickle
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, Tuple

import numpy as np
import optuna
import pandas as pd
from imblearn.over_sampling import SMOTE
from lightgbm import LGBMClassifier
from optuna.samplers import TPESampler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import TimeSeriesSplit
from xgboost import XGBClassifier

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆscripts/ml -> botï¼‰
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from src.backtest.scripts.collect_historical_csv import HistoricalDataCollector
    from src.core.config import load_config
    from src.core.logger import get_logger
    from src.data.data_pipeline import DataPipeline, DataRequest, TimeFrame
    from src.features.feature_generator import FeatureGenerator
    from src.ml.ensemble import ProductionEnsemble
    from src.strategies.base.strategy_manager import StrategyManager  # Phase 41.8
except ImportError as e:
    print(f"âŒ æ–°ã‚·ã‚¹ãƒ†ãƒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    print("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)


class NewSystemMLModelCreator:
    """æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨MLãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ»å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ."""

    def __init__(
        self,
        config_path: str = "config/core/unified.yaml",
        verbose: bool = False,
        target_threshold: float = 0.005,
        n_classes: int = 2,
        use_smote: bool = False,
        optimize: bool = False,
        n_trials: int = 20,
    ):
        """
        åˆæœŸåŒ–ï¼ˆPhase 39.2-39.5å¯¾å¿œï¼‰

        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            verbose: è©³ç´°ãƒ­ã‚°å‡ºåŠ›
            target_threshold: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé–¾å€¤ï¼ˆPhase 39.2ï¼‰
            n_classes: ã‚¯ãƒ©ã‚¹æ•° 2 or 3ï¼ˆPhase 39.2ï¼‰
            use_smote: SMOTEã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä½¿ç”¨ï¼ˆPhase 39.4ï¼‰
            optimize: Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ä½¿ç”¨ï¼ˆPhase 39.5ï¼‰
            n_trials: Optunaè©¦è¡Œå›æ•°ï¼ˆPhase 39.5ï¼‰
        """
        self.config_path = config_path
        self.verbose = verbose
        self.target_threshold = target_threshold  # Phase 39.2
        self.n_classes = n_classes  # Phase 39.2
        self.use_smote = use_smote  # Phase 39.4
        self.optimize = optimize  # Phase 39.5
        self.n_trials = n_trials  # Phase 39.5

        # ãƒ­ã‚°è¨­å®š
        self.logger = get_logger()
        if verbose:
            logging.getLogger().setLevel(logging.DEBUG)

        # è¨­å®šèª­ã¿è¾¼ã¿
        try:
            self.config = load_config(config_path)
            self.logger.info(f"âœ… è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†: {config_path}")
        except Exception as e:
            self.logger.error(f"âŒ è¨­å®šèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            raise

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.training_dir = Path("models/training")
        self.production_dir = Path("models/production")
        self.optuna_dir = Path("models/optuna")  # Phase 39.5
        self.training_dir.mkdir(parents=True, exist_ok=True)
        self.production_dir.mkdir(parents=True, exist_ok=True)
        self.optuna_dir.mkdir(parents=True, exist_ok=True)  # Phase 39.5

        # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
        try:
            self.data_pipeline = DataPipeline()
            self.logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å¤±æ•—: {e}")
            raise

        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        self.feature_generator = FeatureGenerator()

        # Phase 29: ç‰¹å¾´é‡å®šç¾©ä¸€å…ƒåŒ–å¯¾å¿œï¼ˆfeature_managerã‹ã‚‰å–å¾—ï¼‰
        from src.core.config.feature_manager import get_feature_names

        self.expected_features = get_feature_names()

        self.logger.info(f"ğŸ¯ å¯¾è±¡ç‰¹å¾´é‡: {len(self.expected_features)}å€‹ï¼ˆæ–°ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–æ¸ˆã¿ï¼‰")
        self.logger.info(
            f"ğŸ¯ Phase 39.2 ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å®š: é–¾å€¤={target_threshold:.1%}, ã‚¯ãƒ©ã‚¹æ•°={n_classes}"
        )

        # MLãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆPhase 39.3-39.4å¯¾å¿œï¼‰
        self.models = {
            "lightgbm": LGBMClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=8,
                num_leaves=31,
                random_state=42,
                verbose=-1,
                class_weight="balanced",  # Phase 39.4
            ),
            "xgboost": XGBClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=8,
                random_state=42,
                eval_metric="logloss",
                verbosity=0,
                # Phase 39.4: scale_pos_weightã¯å­¦ç¿’æ™‚ã«å‹•çš„è¨­å®š
            ),
            "random_forest": RandomForestClassifier(
                n_estimators=200,
                max_depth=12,
                random_state=42,
                n_jobs=-1,
                class_weight="balanced",  # Phase 39.4
            ),
        }

    async def _load_real_historical_data(self, days: int) -> pd.DataFrame:
        """
        å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆPhase 39.1: MLå®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼‰

        Args:
            days: åé›†æ—¥æ•°

        Returns:
            pd.DataFrame: OHLCV ãƒ‡ãƒ¼ã‚¿
        """
        self.logger.info(f"ğŸ“Š Phase 39.1: å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹ï¼ˆéå»{days}æ—¥åˆ†ï¼‰")

        csv_path = Path("src/backtest/data/historical/BTC_JPY_4h.csv")

        # ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆå­˜åœ¨ã—ãªã„ã€ã¾ãŸã¯å¤ã„å ´åˆï¼‰
        if not csv_path.exists():
            self.logger.info("ğŸ’¾ å±¥æ­´ãƒ‡ãƒ¼ã‚¿æœªå­˜åœ¨ - è‡ªå‹•åé›†é–‹å§‹")
            try:
                collector = HistoricalDataCollector()
                await collector.collect_data(symbol="BTC/JPY", days=days, timeframes=["4h"])
                self.logger.info("âœ… ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†")
            except Exception as e:
                self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿åé›†å¤±æ•—: {e}")
                raise

        # CSVèª­ã¿è¾¼ã¿
        try:
            df = pd.read_csv(csv_path)

            # timestamp ã‚’datetimeã«å¤‰æ›
            df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
            df.set_index("timestamp", inplace=True)

            # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿
            cutoff_date = datetime.now() - timedelta(days=days)
            df = df[df.index >= cutoff_date]

            # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
            if df.isnull().any().any():
                self.logger.warning("âš ï¸ æ¬ æå€¤ã‚’æ¤œå‡º - ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ")
                df = df.dropna()

            self.logger.info(
                f"âœ… å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ã‚µãƒ³ãƒ—ãƒ« "
                f"({df.index.min().strftime('%Y-%m-%d')} ã€œ {df.index.max().strftime('%Y-%m-%d')})"
            )

            # OHLCV ã‚«ãƒ©ãƒ ã®ã¿è¿”å´
            return df[["open", "high", "low", "close", "volume"]].copy()

        except Exception as e:
            self.logger.error(f"âŒ CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    async def prepare_training_data_async(self, days: int = 180) -> Tuple[pd.DataFrame, pd.Series]:
        """å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆPhase 50.3: 70ç‰¹å¾´é‡ãƒ»å¤–éƒ¨APIãƒ»å®Ÿæˆ¦ç•¥ä¿¡å·çµ±åˆï¼‰"""
        self.logger.info(
            f"ğŸ“Š Phase 50.3: å®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’é–‹å§‹ï¼ˆéå»{days}æ—¥åˆ†ãƒ»70ç‰¹å¾´é‡ãƒ»å¤–éƒ¨APIãƒ»å®Ÿæˆ¦ç•¥ä¿¡å·ï¼‰"
        )

        try:
            # Phase 39.1: å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = await self._load_real_historical_data(days)

            self.logger.info(f"âœ… åŸºæœ¬ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(df)}è¡Œ")

            # Phase 50.3: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆ70ç‰¹å¾´é‡: 62åŸºæœ¬ + 8å¤–éƒ¨APIï¼‰
            features_df = await self.feature_generator.generate_features(df)

            # Phase 50.3: æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ç‰¹å¾´é‡ã‚’å‰Šé™¤ï¼ˆå¾Œã§å®Ÿæˆ¦ç•¥ä¿¡å·ã§ç½®ãæ›ãˆã‚‹ï¼‰
            # generate_features() ã¯æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ã‚’0.0ã§è‡ªå‹•ç”Ÿæˆã™ã‚‹ãŒã€Phase 41.8ã§ã¯å®Ÿæˆ¦ç•¥ä¿¡å·ã‚’ä½¿ç”¨
            strategy_signal_cols = [
                col for col in features_df.columns if col.startswith("strategy_signal_")
            ]
            if strategy_signal_cols:
                features_df = features_df.drop(columns=strategy_signal_cols)
                self.logger.info(
                    f"âœ… æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ç‰¹å¾´é‡å‰Šé™¤: {len(strategy_signal_cols)}å€‹ï¼ˆå®Ÿæˆ¦ç•¥ä¿¡å·ã§ç½®ãæ›ãˆï¼‰"
                )

            # Phase 41.8: å®Ÿæˆ¦ç•¥ä¿¡å·ç”Ÿæˆï¼ˆ62â†’70ç‰¹å¾´é‡ or 65â†’70ç‰¹å¾´é‡ï¼‰
            # Note: éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿéš›ã«5æˆ¦ç•¥ã‚’å®Ÿè¡Œã—ã€æœ¬ç‰©ã®æˆ¦ç•¥ä¿¡å·ã‚’ç”Ÿæˆ
            #       ã“ã‚Œã«ã‚ˆã‚Šè¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã®ä¸€è²«æ€§ã‚’ç¢ºä¿
            strategy_signals_df = await self._generate_real_strategy_signals_for_training(df)

            # Phase 50.3: åŸºæœ¬ç‰¹å¾´é‡ + å¤–éƒ¨APIç‰¹å¾´é‡ + å®Ÿæˆ¦ç•¥ä¿¡å· = 70ç‰¹å¾´é‡ã‚’çµåˆ
            features_df = pd.concat([features_df, strategy_signals_df], axis=1)

            # Phase 50.3: ç‰¹å¾´é‡æ•´åˆæ€§ç¢ºä¿ï¼ˆ70ç‰¹å¾´é‡ï¼‰
            features_df = self._ensure_feature_consistency(features_df)

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆPhase 39.2: é–¾å€¤ãƒ»ã‚¯ãƒ©ã‚¹æ•°å¯¾å¿œï¼‰
            target = self._generate_target(df, self.target_threshold, self.n_classes)

            # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
            features_df, target = self._clean_data(features_df, target)

            self.logger.info(
                f"âœ… Phase 50.3 å®Ÿãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(features_df)}ã‚µãƒ³ãƒ—ãƒ«ã€"
                f"{len(features_df.columns)}ç‰¹å¾´é‡ï¼ˆ70ç‰¹å¾´é‡: 62åŸºæœ¬+8å¤–éƒ¨APIãƒ»å®Ÿæˆ¦ç•¥ä¿¡å·çµ±åˆå®Œäº†ï¼‰"
            )
            return features_df, target

        except Exception as e:
            self.logger.error(f"âŒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def prepare_training_data(self, days: int = 180) -> Tuple[pd.DataFrame, pd.Series]:
        """å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆåŒæœŸãƒ©ãƒƒãƒ‘ãƒ¼ãƒ»å¾Œæ–¹äº’æ›æ€§ï¼‰"""
        return asyncio.run(self.prepare_training_data_async(days))

    def _generate_sample_data(self, samples: int) -> pd.DataFrame:
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰."""
        self.logger.info(f"ğŸ”§ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: {samples}ã‚µãƒ³ãƒ—ãƒ«")

        # æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        dates = pd.date_range(end=datetime.now(), periods=samples, freq="h")

        # ãƒªã‚¢ãƒ«ãªBTCä¾¡æ ¼å‹•å‘ã‚’æ¨¡æ“¬
        np.random.seed(42)
        base_price = 5000000  # 5M JPY
        prices = []
        current_price = base_price

        for i in range(samples):
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼ˆæ™‚é–“å¸¯ã«ã‚ˆã‚‹å¤‰å‹•å¹…èª¿æ•´ï¼‰
            hour = dates[i].hour
            volatility = 0.015 if 8 <= hour <= 20 else 0.008  # æ—¥ä¸­é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            change = np.random.normal(0, volatility)
            current_price *= 1 + change
            prices.append(current_price)

        # OHLCV ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        df = pd.DataFrame(
            {
                "timestamp": dates,
                "open": prices,
                "high": [p * (1 + abs(np.random.normal(0, 0.005))) for p in prices],
                "low": [p * (1 - abs(np.random.normal(0, 0.005))) for p in prices],
                "close": prices,
                "volume": np.random.lognormal(8, 1, samples),  # ãƒªã‚¢ãƒ«ãªå‡ºæ¥é«˜åˆ†å¸ƒ
            }
        )

        df.set_index("timestamp", inplace=True)
        return df

    async def _generate_real_strategy_signals_for_training(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Phase 41.8: å®Ÿéš›ã®æˆ¦ç•¥ä¿¡å·ã‚’ç”Ÿæˆï¼ˆéå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰

        å„æ™‚ç‚¹ã§5æˆ¦ç•¥ã‚’å®Ÿè¡Œã—ã€æœ¬ç‰©ã®æˆ¦ç•¥ä¿¡å·ã‚’è¨ˆç®—ã€‚
        ã“ã‚Œã«ã‚ˆã‚Šè¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã®ä¸€è²«æ€§ã‚’ç¢ºä¿ã€‚

        Args:
            df: OHLCVä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿

        Returns:
            pd.DataFrame: æˆ¦ç•¥ä¿¡å·5åˆ—ã®DataFrame (index aligned with df)
        """
        self.logger.info("ğŸ“Š Phase 41.8: å®Ÿæˆ¦ç•¥ä¿¡å·ç”Ÿæˆé–‹å§‹ï¼ˆéå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰5æˆ¦ç•¥å®Ÿè¡Œï¼‰")

        # æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ç‰¹å¾´é‡å
        strategy_names = [
            "ATRBased",
            "MochipoyAlert",
            "MultiTimeframe",
            "DonchianChannel",
            "ADXTrendStrength",
        ]

        # çµæœæ ¼ç´ç”¨DataFrame
        strategy_signals = pd.DataFrame(index=df.index)

        try:
            # StrategyManageråˆæœŸåŒ–
            strategy_manager = StrategyManager()
            self.logger.info("âœ… StrategyManageråˆæœŸåŒ–å®Œäº†")

            # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–
            self.data_pipeline.set_backtest_data({"4h": df.copy()})
            self.logger.info("âœ… DataPipelineãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰è¨­å®šå®Œäº†")

            # å„æ™‚ç‚¹ã§æˆ¦ç•¥å®Ÿè¡Œï¼ˆlook-ahead biaså›é¿ã®ãŸã‚é †æ¬¡å‡¦ç†ï¼‰
            total_points = len(df)
            processed = 0

            for i in range(len(df)):
                # ç¾åœ¨æ™‚ç‚¹ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼ˆæœªæ¥ãƒ‡ãƒ¼ã‚¿æ¼æ´©é˜²æ­¢ï¼‰
                current_data = df.iloc[: i + 1]

                # æœ€ä½é™ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒå¿…è¦ï¼ˆç‰¹å¾´é‡è¨ˆç®—ã®ãŸã‚ï¼‰
                if len(current_data) < 50:
                    # ãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚ã¯0ã§åŸ‹ã‚ã‚‹
                    for strategy_name in strategy_names:
                        strategy_signals.loc[
                            current_data.index[-1], f"strategy_signal_{strategy_name}"
                        ] = 0.0
                    continue

                try:
                    # DataPipelineæ›´æ–°
                    self.data_pipeline.set_backtest_data({"4h": current_data.copy()})

                    # å€‹åˆ¥æˆ¦ç•¥ä¿¡å·å–å¾—
                    signals = strategy_manager.get_individual_strategy_signals({"4h": current_data})

                    # action Ã— confidence ã‚’è¨ˆç®—ã—ã¦æ ¼ç´
                    current_timestamp = current_data.index[-1]
                    for strategy_name in strategy_names:
                        if strategy_name in signals:
                            action = signals[strategy_name]["action"]
                            confidence = signals[strategy_name]["confidence"]

                            # action ã‚’æ•°å€¤åŒ–: buy=+1, hold=0, sell=-1
                            action_value = {"buy": 1.0, "hold": 0.0, "sell": -1.0}.get(action, 0.0)

                            # æˆ¦ç•¥ä¿¡å· = action Ã— confidence
                            signal_value = action_value * confidence

                            strategy_signals.loc[
                                current_timestamp, f"strategy_signal_{strategy_name}"
                            ] = signal_value
                        else:
                            # æˆ¦ç•¥ä¿¡å·ãŒå¾—ã‚‰ã‚Œãªã„å ´åˆã¯0
                            strategy_signals.loc[
                                current_timestamp, f"strategy_signal_{strategy_name}"
                            ] = 0.0

                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯0ã§åŸ‹ã‚ã‚‹ï¼ˆå­¦ç¿’ç¶™ç¶šï¼‰
                    self.logger.warning(f"âš ï¸ æ™‚ç‚¹{i}ã§æˆ¦ç•¥å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}, 0ã§åŸ‹ã‚ã¾ã™")
                    for strategy_name in strategy_names:
                        strategy_signals.loc[
                            current_data.index[-1], f"strategy_signal_{strategy_name}"
                        ] = 0.0

                # é€²æ—è¡¨ç¤ºï¼ˆ10%ã”ã¨ï¼‰
                processed += 1
                if processed % max(1, total_points // 10) == 0:
                    progress = (processed / total_points) * 100
                    self.logger.info(
                        f"ğŸ“Š Phase 41.8: æˆ¦ç•¥ä¿¡å·ç”Ÿæˆé€²æ— {processed}/{total_points} ({progress:.1f}%)"
                    )

            # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹
            strategy_signals.fillna(0.0, inplace=True)

            self.logger.info(
                f"âœ… Phase 41.8: å®Ÿæˆ¦ç•¥ä¿¡å·ç”Ÿæˆå®Œäº† - {len(strategy_signals)}è¡Œ Ã— 5æˆ¦ç•¥"
            )
            self.logger.info(
                f"ğŸ“Š Phase 41.8: æˆ¦ç•¥ä¿¡å·çµ±è¨ˆ - "
                f"éã‚¼ãƒ­ç‡: {(strategy_signals != 0).sum().sum() / (len(strategy_signals) * 5) * 100:.1f}%"
            )

            return strategy_signals

        except Exception as e:
            self.logger.error(f"âŒ Phase 41.8: å®Ÿæˆ¦ç•¥ä¿¡å·ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆ0åŸ‹ã‚ï¼‰
            self.logger.warning("âš ï¸ Phase 41.8: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ - æˆ¦ç•¥ä¿¡å·ã‚’0åŸ‹ã‚")
            for strategy_name in strategy_names:
                strategy_signals[f"strategy_signal_{strategy_name}"] = 0.0
            return strategy_signals

    def _add_strategy_signal_features_for_training(self, features_df: pd.DataFrame) -> pd.DataFrame:
        """
        Phase 41: MLå­¦ç¿’ç”¨ã«æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆ5å€‹ãƒ»0åŸ‹ã‚ï¼‰

        Deprecated: Phase 41.8ã§_generate_real_strategy_signals_for_training()ã«ç½®ãæ›ãˆ
        å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã«æ®‹ç½®

        Note:
            MLå­¦ç¿’æ™‚ã¯éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æˆ¦ç•¥ã‚’å®Ÿè¡Œã—ã¦ã„ãªã„ãŸã‚ã€
            æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ç‰¹å¾´é‡ã‚’0åŸ‹ã‚ã§è¿½åŠ ã—ã¾ã™ã€‚
            å®Ÿé‹ç”¨æ™‚ã«ã¯æœ¬ç‰©ã®æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

        Returns:
            pd.DataFrame: æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ç‰¹å¾´é‡ãŒè¿½åŠ ã•ã‚ŒãŸDataFrame
        """
        self.logger.info("ğŸ“Š Phase 41: æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ç‰¹å¾´é‡è¿½åŠ ï¼ˆMLå­¦ç¿’ç”¨ãƒ»0åŸ‹ã‚ï¼‰")

        # æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ç‰¹å¾´é‡å
        strategy_signal_features = [
            "strategy_signal_ATRBased",
            "strategy_signal_MochipoyAlert",
            "strategy_signal_MultiTimeframe",
            "strategy_signal_DonchianChannel",
            "strategy_signal_ADXTrendStrength",
        ]

        # 0åŸ‹ã‚ã§è¿½åŠ 
        for feature_name in strategy_signal_features:
            features_df[feature_name] = 0.0

        self.logger.info(
            f"âœ… Phase 41: æˆ¦ç•¥ã‚·ã‚°ãƒŠãƒ«ç‰¹å¾´é‡5å€‹è¿½åŠ å®Œäº† "
            f"({len(features_df.columns)}ç‰¹å¾´é‡ - MLå­¦ç¿’ç”¨0åŸ‹ã‚)"
        )

        return features_df

    def _ensure_feature_consistency(self, features_df: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾´é‡æ•´åˆæ€§ç¢ºä¿ï¼ˆPhase 41: 55ç‰¹å¾´é‡å¯¾å¿œï¼‰"""
        # ä¸è¶³ç‰¹å¾´é‡ã®0åŸ‹ã‚
        for feature in self.expected_features:
            if feature not in features_df.columns:
                features_df[feature] = 0.0
                self.logger.warning(f"âš ï¸ ä¸è¶³ç‰¹å¾´é‡ã‚’0åŸ‹ã‚: {feature}")

        # ç‰¹å¾´é‡ã®ã¿é¸æŠ - Phase 41: 55ç‰¹å¾´é‡å¯¾å¿œ
        features_df = features_df[self.expected_features]

        expected_count = len(self.expected_features)
        if len(features_df.columns) != expected_count:
            self.logger.warning(f"âš ï¸ ç‰¹å¾´é‡æ•°ä¸ä¸€è‡´: {len(features_df.columns)} != {expected_count}")

        return features_df

    def _generate_target(
        self,
        df: pd.DataFrame,
        threshold: float = 0.005,
        n_classes: int = 2,
    ) -> pd.Series:
        """
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆPhase 39.2: é–¾å€¤æœ€é©åŒ–ãƒ»3ã‚¯ãƒ©ã‚¹åˆ†é¡å¯¾å¿œï¼‰

        Args:
            df: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
            threshold: BUYé–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.5%ï¼‰
            n_classes: ã‚¯ãƒ©ã‚¹æ•°ï¼ˆ2ã¾ãŸã¯3ï¼‰

        Returns:
            pd.Series: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ©ãƒ™ãƒ«
                2ã‚¯ãƒ©ã‚¹: 0=HOLD/SELL, 1=BUY
                3ã‚¯ãƒ©ã‚¹: 0=SELL, 1=HOLD, 2=BUY
        """
        # 1æ™‚é–“å¾Œã®ä¾¡æ ¼å¤‰å‹•ç‡ï¼ˆ4æ™‚é–“è¶³ãªã®ã§4æ™‚é–“å¾Œï¼‰
        price_change = df["close"].pct_change(periods=1).shift(-1)

        if n_classes == 2:
            # Phase 39.2: é–¾å€¤0.3%â†’0.5%ã«å¤‰æ›´ï¼ˆãƒã‚¤ã‚ºå‰Šæ¸›ï¼‰
            target = (price_change > threshold).astype(int)

            buy_ratio = target.mean()
            self.logger.info(
                f"ğŸ“Š Phase 39.2 ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒï¼ˆé–¾å€¤{threshold:.1%}ï¼‰: "
                f"BUY {buy_ratio:.1%}, OTHER {1 - buy_ratio:.1%}"
            )

        elif n_classes == 3:
            # Phase 39.2: 3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆBUY/HOLD/SELLï¼‰
            sell_threshold = -threshold

            # 0: SELL, 1: HOLD, 2: BUY
            target = pd.Series(1, index=df.index, dtype=int)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆHOLD
            target[price_change > threshold] = 2  # BUY
            target[price_change < sell_threshold] = 0  # SELL

            distribution = target.value_counts(normalize=True).sort_index()
            self.logger.info(
                f"ğŸ“Š Phase 39.2 3ã‚¯ãƒ©ã‚¹åˆ†å¸ƒï¼ˆé–¾å€¤Â±{threshold:.1%}ï¼‰: "
                f"SELL {distribution.get(0, 0):.1%}, "
                f"HOLD {distribution.get(1, 0):.1%}, "
                f"BUY {distribution.get(2, 0):.1%}"
            )

        else:
            raise ValueError(f"Unsupported n_classes: {n_classes} (must be 2 or 3)")

        return target

    def _clean_data(
        self, features_df: pd.DataFrame, target: pd.Series
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°."""
        # NaNé™¤å»
        valid_mask = ~(features_df.isna().any(axis=1) | target.isna())
        features_clean = features_df[valid_mask].copy()
        target_clean = target[valid_mask].copy()

        # ç„¡é™å€¤é™¤å»
        features_clean.replace([np.inf, -np.inf], np.nan, inplace=True)
        features_clean.fillna(0, inplace=True)

        removed_samples = len(features_df) - len(features_clean)
        if removed_samples > 0:
            self.logger.info(f"ğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°: {removed_samples}ã‚µãƒ³ãƒ—ãƒ«é™¤å»")

        return features_clean, target_clean

    def _objective_lightgbm(
        self, trial: optuna.Trial, X_train: pd.DataFrame, y_train: pd.Series
    ) -> float:
        """Phase 39.5: LightGBMæœ€é©åŒ–objectiveé–¢æ•°"""
        params = {
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
            "max_depth": trial.suggest_int("max_depth", 3, 15),
            "n_estimators": trial.suggest_int("n_estimators", 50, 300),
            "num_leaves": trial.suggest_int("num_leaves", 20, 100),
            "random_state": 42,
            "verbose": -1,
            "class_weight": "balanced",
        }

        model = LGBMClassifier(**params)

        # TimeSeriesSplit CV
        tscv = TimeSeriesSplit(n_splits=3)
        scores = []
        for train_idx, val_idx in tscv.split(X_train):
            X_cv_train = X_train.iloc[train_idx]
            y_cv_train = y_train.iloc[train_idx]
            X_cv_val = X_train.iloc[val_idx]
            y_cv_val = y_train.iloc[val_idx]

            model.fit(X_cv_train, y_cv_train)
            y_pred = model.predict(X_cv_val)
            score = f1_score(y_cv_val, y_pred, average="weighted")
            scores.append(score)

        return np.mean(scores)

    def _objective_xgboost(
        self, trial: optuna.Trial, X_train: pd.DataFrame, y_train: pd.Series
    ) -> float:
        """Phase 39.5: XGBoostæœ€é©åŒ–objectiveé–¢æ•°"""
        params = {
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
            "max_depth": trial.suggest_int("max_depth", 3, 15),
            "n_estimators": trial.suggest_int("n_estimators", 50, 300),
            "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
            "random_state": 42,
            "eval_metric": "logloss",
            "verbosity": 0,
        }

        # scale_pos_weightå‹•çš„è¨­å®šï¼ˆ2ã‚¯ãƒ©ã‚¹åˆ†é¡ã®ã¿ï¼‰
        if self.n_classes == 2:
            pos_count = y_train.sum()
            neg_count = len(y_train) - pos_count
            if pos_count > 0:
                params["scale_pos_weight"] = neg_count / pos_count

        model = XGBClassifier(**params)

        # TimeSeriesSplit CV
        tscv = TimeSeriesSplit(n_splits=3)
        scores = []
        for train_idx, val_idx in tscv.split(X_train):
            X_cv_train = X_train.iloc[train_idx]
            y_cv_train = y_train.iloc[train_idx]
            X_cv_val = X_train.iloc[val_idx]
            y_cv_val = y_train.iloc[val_idx]

            model.fit(X_cv_train, y_cv_train)
            y_pred = model.predict(X_cv_val)
            score = f1_score(y_cv_val, y_pred, average="weighted")
            scores.append(score)

        return np.mean(scores)

    def _objective_random_forest(
        self, trial: optuna.Trial, X_train: pd.DataFrame, y_train: pd.Series
    ) -> float:
        """Phase 39.5: RandomForestæœ€é©åŒ–objectiveé–¢æ•°"""
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 50, 300),
            "max_depth": trial.suggest_int("max_depth", 5, 20),
            "min_samples_split": trial.suggest_int("min_samples_split", 2, 20),
            "random_state": 42,
            "n_jobs": -1,
            "class_weight": "balanced",
        }

        model = RandomForestClassifier(**params)

        # TimeSeriesSplit CV
        tscv = TimeSeriesSplit(n_splits=3)
        scores = []
        for train_idx, val_idx in tscv.split(X_train):
            X_cv_train = X_train.iloc[train_idx]
            y_cv_train = y_train.iloc[train_idx]
            X_cv_val = X_train.iloc[val_idx]
            y_cv_val = y_train.iloc[val_idx]

            model.fit(X_cv_train, y_cv_train)
            y_pred = model.predict(X_cv_val)
            score = f1_score(y_cv_val, y_pred, average="weighted")
            scores.append(score)

        return np.mean(scores)

    def optimize_hyperparameters(
        self, features: pd.DataFrame, target: pd.Series, n_trials: int = 20
    ) -> Dict[str, Dict[str, Any]]:
        """
        Phase 39.5: Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–

        Args:
            features: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡
            target: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            n_trials: è©¦è¡Œå›æ•°

        Returns:
            Dict: å„ãƒ¢ãƒ‡ãƒ«ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        self.logger.info(f"ğŸ”¬ Phase 39.5: Optunaæœ€é©åŒ–é–‹å§‹ï¼ˆ{n_trials}è©¦è¡Œï¼‰")

        # Optunaãƒ­ã‚°æŠ‘åˆ¶
        optuna.logging.set_verbosity(optuna.logging.WARNING)

        optimal_params = {}
        optimization_results = {
            "created_at": datetime.now().isoformat(),
            "n_trials": n_trials,
            "models": {},
        }

        for model_name in ["lightgbm", "xgboost", "random_forest"]:
            self.logger.info(f"ğŸ“Š {model_name} æœ€é©åŒ–é–‹å§‹")

            try:
                # Objectiveé–¢æ•°é¸æŠï¼ˆE731: flake8 lambdaå›é¿ï¼‰
                def objective_func(trial: optuna.Trial) -> float:
                    if model_name == "lightgbm":
                        return self._objective_lightgbm(trial, features, target)
                    elif model_name == "xgboost":
                        return self._objective_xgboost(trial, features, target)
                    else:  # random_forest
                        return self._objective_random_forest(trial, features, target)

                # Optuna Studyä½œæˆãƒ»æœ€é©åŒ–å®Ÿè¡Œ
                study = optuna.create_study(direction="maximize", sampler=TPESampler(seed=42))
                study.optimize(objective_func, n_trials=n_trials, show_progress_bar=False)

                # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
                best_params = study.best_params
                best_score = study.best_value

                optimal_params[model_name] = best_params
                optimization_results["models"][model_name] = {
                    "best_params": best_params,
                    "best_score": float(best_score),
                    "n_trials": n_trials,
                }

                self.logger.info(
                    f"âœ… {model_name} æœ€é©åŒ–å®Œäº† - Best F1: {best_score:.4f}, "
                    f"Best params: {best_params}"
                )

            except Exception as e:
                self.logger.error(f"âŒ {model_name} æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                optimization_results["models"][model_name] = {"error": str(e)}

        # çµæœä¿å­˜
        try:
            results_file = self.optuna_dir / "phase39_5_results.json"
            with open(results_file, "w", encoding="utf-8") as f:
                json.dump(optimization_results, f, indent=2, ensure_ascii=False)
            self.logger.info(f"ğŸ’¾ æœ€é©åŒ–çµæœä¿å­˜: {results_file}")
        except Exception as e:
            self.logger.error(f"âŒ æœ€é©åŒ–çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        return optimal_params

    def train_models(
        self, features: pd.DataFrame, target: pd.Series, dry_run: bool = False
    ) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Ÿè¡Œï¼ˆPhase 39.3-39.4å¯¾å¿œï¼‰"""
        self.logger.info("ğŸ¤– Phase 39.3-39.4 MLãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹")

        if dry_run:
            self.logger.info("ğŸ” ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³: å®Ÿéš›ã®å­¦ç¿’ã¯ã‚¹ã‚­ãƒƒãƒ—")
            return {"dry_run": True, "models": list(self.models.keys())}

        results = {}
        trained_models = {}

        # Phase 39.3: Train/Val/Test split (70/15/15)
        n_samples = len(features)
        train_size = int(n_samples * 0.70)
        val_size = int(n_samples * 0.15)

        X_train = features.iloc[:train_size]
        y_train = target.iloc[:train_size]
        X_val = features.iloc[train_size : train_size + val_size]
        y_val = target.iloc[train_size : train_size + val_size]
        X_test = features.iloc[train_size + val_size :]
        y_test = target.iloc[train_size + val_size :]

        self.logger.info(
            f"ğŸ“Š Phase 39.3: Train/Val/Test split - "
            f"Train: {len(X_train)} ({len(X_train) / n_samples:.1%}), "
            f"Val: {len(X_val)} ({len(X_val) / n_samples:.1%}), "
            f"Test: {len(X_test)} ({len(X_test) / n_samples:.1%})"
        )

        # Phase 39.5: Optuna hyperparameter optimization
        if self.optimize:
            self.logger.info("ğŸ”¬ Phase 39.5: Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–é–‹å§‹")
            optimal_params = self.optimize_hyperparameters(
                pd.concat([X_train, X_val]),
                pd.concat([y_train, y_val]),
                self.n_trials,
            )

            # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨
            for model_name in self.models.keys():
                if model_name in optimal_params:
                    self.models[model_name].set_params(**optimal_params[model_name])
                    self.logger.info(f"âœ… {model_name}: æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨å®Œäº†")

        # Phase 39.3: TimeSeriesSplit n_splits=5 for Cross Validation
        tscv = TimeSeriesSplit(n_splits=5)
        self.logger.info("ğŸ“Š Phase 39.3: TimeSeriesSplit n_splits=5 for CV")

        # Phase 39.4: XGBoost scale_pos_weightå‹•çš„è¨­å®š
        if self.n_classes == 2:
            pos_count = y_train.sum()
            neg_count = len(y_train) - pos_count
            if pos_count > 0:
                scale_pos_weight = neg_count / pos_count
                self.models["xgboost"].set_params(scale_pos_weight=scale_pos_weight)
                self.logger.info(f"ğŸ“Š Phase 39.4: XGBoost scale_pos_weight={scale_pos_weight:.2f}")

        for model_name, model in self.models.items():
            self.logger.info(f"ğŸ“ˆ {model_name} å­¦ç¿’é–‹å§‹")

            try:
                # Phase 39.3: Cross Validation with Early Stopping
                cv_scores = []

                for train_idx, val_idx in tscv.split(X_train):
                    X_cv_train = X_train.iloc[train_idx]
                    y_cv_train = y_train.iloc[train_idx]
                    X_cv_val = X_train.iloc[val_idx]
                    y_cv_val = y_train.iloc[val_idx]

                    # Phase 39.4: SMOTE Oversampling (CV fold)
                    if self.use_smote and self.n_classes == 2:
                        try:
                            smote = SMOTE(random_state=42)
                            X_cv_train_resampled, y_cv_train_resampled = smote.fit_resample(
                                X_cv_train, y_cv_train
                            )
                            # Convert back to DataFrame to preserve feature names
                            X_cv_train = pd.DataFrame(
                                X_cv_train_resampled, columns=X_cv_train.columns
                            )
                            y_cv_train = pd.Series(y_cv_train_resampled)
                            if len(X_cv_train_resampled) > len(X_cv_train):
                                self.logger.debug(
                                    f"ğŸ“Š Phase 39.4: SMOTEé©ç”¨ - CV fold "
                                    f"{len(train_idx)}â†’{len(X_cv_train_resampled)}ã‚µãƒ³ãƒ—ãƒ«"
                                )
                        except Exception as e:
                            self.logger.warning(
                                f"âš ï¸ SMOTEé©ç”¨å¤±æ•—ï¼ˆCV foldï¼‰: {e}, å…ƒãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ç¶™ç¶š"
                            )

                    # Phase 39.3: Early Stopping for LightGBM and XGBoost
                    if model_name == "lightgbm":
                        try:
                            model.fit(
                                X_cv_train,
                                y_cv_train,
                                eval_set=[(X_cv_val, y_cv_val)],
                                callbacks=[
                                    # LightGBM 4.0+ uses callbacks instead of early_stopping_rounds
                                    __import__("lightgbm").early_stopping(
                                        stopping_rounds=20, verbose=False
                                    )
                                ],
                            )
                        except ValueError as e:
                            # Handle unseen labels in CV folds (small datasets)
                            if "previously unseen labels" in str(e):
                                model.fit(X_cv_train, y_cv_train)
                            else:
                                raise
                    elif model_name == "xgboost":
                        # XGBoost 2.0+ uses callbacks for early stopping
                        try:
                            from xgboost import callback as xgb_callback

                            model.fit(
                                X_cv_train,
                                y_cv_train,
                                eval_set=[(X_cv_val, y_cv_val)],
                                callbacks=[xgb_callback.EarlyStopping(rounds=20)],
                                verbose=False,
                            )
                        except Exception:
                            # Fallback: train without early stopping
                            model.fit(X_cv_train, y_cv_train)
                    else:
                        # RandomForest doesn't support early stopping
                        model.fit(X_cv_train, y_cv_train)

                    # äºˆæ¸¬ãƒ»è©•ä¾¡
                    y_pred = model.predict(X_cv_val)
                    score = f1_score(y_cv_val, y_pred, average="weighted")
                    cv_scores.append(score)

                # Phase 39.3: Final model training on Train+Val with Early Stopping
                X_train_val = pd.concat([X_train, X_val])
                y_train_val = pd.concat([y_train, y_val])

                # Phase 39.4: SMOTE Oversampling (Final training)
                if self.use_smote and self.n_classes == 2:
                    try:
                        smote = SMOTE(random_state=42)
                        X_train_val_resampled, y_train_val_resampled = smote.fit_resample(
                            X_train_val, y_train_val
                        )
                        # Convert back to DataFrame to preserve feature names
                        X_train_val = pd.DataFrame(
                            X_train_val_resampled, columns=X_train_val.columns
                        )
                        y_train_val = pd.Series(y_train_val_resampled)
                        self.logger.info(
                            f"ğŸ“Š Phase 39.4: SMOTEé©ç”¨ï¼ˆFinal trainingï¼‰ - "
                            f"{len(X_train) + len(X_val)}â†’{len(X_train_val_resampled)}ã‚µãƒ³ãƒ—ãƒ«"
                        )
                    except Exception as e:
                        self.logger.warning(
                            f"âš ï¸ SMOTEé©ç”¨å¤±æ•—ï¼ˆFinal trainingï¼‰: {e}, å…ƒãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ç¶™ç¶š"
                        )

                if model_name == "lightgbm":
                    model.fit(
                        X_train_val,
                        y_train_val,
                        eval_set=[(X_test, y_test)],
                        callbacks=[
                            __import__("lightgbm").early_stopping(stopping_rounds=20, verbose=False)
                        ],
                    )
                    self.logger.info(
                        f"ğŸ“Š Phase 39.3: {model_name} Early Stopping enabled (rounds=20)"
                    )
                elif model_name == "xgboost":
                    # XGBoost 2.0+ uses callbacks for early stopping
                    try:
                        from xgboost import callback as xgb_callback

                        model.fit(
                            X_train_val,
                            y_train_val,
                            eval_set=[(X_test, y_test)],
                            callbacks=[xgb_callback.EarlyStopping(rounds=20)],
                            verbose=False,
                        )
                        self.logger.info(
                            f"ğŸ“Š Phase 39.3: {model_name} Early Stopping enabled (rounds=20)"
                        )
                    except Exception as e:
                        # Fallback: train without early stopping
                        self.logger.warning(
                            f"âš ï¸ XGBoost Early Stopping failed: {e}, training without it"
                        )
                        model.fit(X_train_val, y_train_val)
                else:
                    # RandomForest: Train on Train+Val without early stopping
                    model.fit(X_train_val, y_train_val)

                # Test set evaluation
                y_test_pred = model.predict(X_test)
                test_metrics = {
                    "accuracy": accuracy_score(y_test, y_test_pred),
                    "f1_score": f1_score(y_test, y_test_pred, average="weighted"),
                    "precision": precision_score(
                        y_test, y_test_pred, average="weighted", zero_division=0
                    ),
                    "recall": recall_score(
                        y_test, y_test_pred, average="weighted", zero_division=0
                    ),
                    "cv_f1_mean": np.mean(cv_scores),
                    "cv_f1_std": np.std(cv_scores),
                }

                results[model_name] = test_metrics
                trained_models[model_name] = model

                self.logger.info(
                    f"âœ… {model_name} å­¦ç¿’å®Œäº† - Test F1: {test_metrics['f1_score']:.3f}, "
                    f"CV F1: {test_metrics['cv_f1_mean']:.3f}Â±{test_metrics['cv_f1_std']:.3f}"
                )

            except Exception as e:
                self.logger.error(f"âŒ {model_name} å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
                results[model_name] = {"error": str(e)}

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆindividual_modelsã®ã¿ã‚’ä½¿ç”¨ãƒ»å¾ªç’°å‚ç…§é˜²æ­¢ï¼‰
        if len(trained_models) >= 2:
            try:
                # ProductionEnsembleè‡ªä½“ã‚’å«ã¾ãªã„ã‚ˆã†ã«å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®ã¿æ¸¡ã™
                individual_models_only = {
                    k: v for k, v in trained_models.items() if k != "production_ensemble"
                }
                ensemble_model = self._create_ensemble(individual_models_only)
                trained_models["production_ensemble"] = ensemble_model
                self.logger.info("âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†ï¼ˆå¾ªç’°å‚ç…§é˜²æ­¢å¯¾å¿œï¼‰")
            except Exception as e:
                self.logger.error(f"âŒ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

        return {
            "results": results,
            "models": trained_models,
            "feature_names": list(features.columns),
            "training_samples": len(features),
        }

    def _create_ensemble(self, models: Dict) -> ProductionEnsemble:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆProductionEnsembleã‚¯ãƒ©ã‚¹ä½¿ç”¨ï¼‰."""
        try:
            self.logger.info("ğŸ”§ ProductionEnsembleã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
            ensemble_model = ProductionEnsemble(models)
            self.logger.info("âœ… ProductionEnsembleã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†")
            return ensemble_model
        except Exception as e:
            self.logger.error(f"âŒ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def save_models(self, training_results: Dict[str, Any]) -> Dict[str, str]:
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆå€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ï¼štrainingã€çµ±åˆãƒ¢ãƒ‡ãƒ«ï¼šproductionï¼‰."""
        self.logger.info("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜é–‹å§‹")

        saved_files = {}
        models = training_results.get("models", {})

        for model_name, model in models.items():
            try:
                if model_name == "production_ensemble":
                    # æœ¬ç•ªç”¨çµ±åˆãƒ¢ãƒ‡ãƒ«ã¯productionãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
                    model_file = self.production_dir / "production_ensemble.pkl"
                    with open(model_file, "wb") as f:
                        pickle.dump(model, f)

                    # Gitæƒ…å ±å–å¾—
                    try:
                        git_commit = self._get_git_info()
                    except Exception:
                        git_commit = {"commit": "unknown", "branch": "unknown"}

                    # æœ¬ç•ªç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆPhase 41.8å®Œäº†: Strategy-Aware MLãƒ»å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’ï¼‰
                    production_metadata = {
                        "created_at": datetime.now().isoformat(),
                        "model_type": "ProductionEnsemble",
                        "model_file": str(model_file),
                        "version": "1.0.0",
                        "phase": "Phase 41.8",  # Phase 41.8å®Œäº†: å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’
                        "status": "production_ready",
                        "feature_names": training_results.get("feature_names", []),
                        "individual_models": [
                            k for k in model.models.keys() if k != "production_ensemble"
                        ],
                        "model_weights": model.weights,
                        "performance_metrics": training_results.get("results", {}),
                        "training_info": {
                            "samples": training_results.get("training_samples", 0),
                            "feature_count": len(training_results.get("feature_names", [])),
                            "training_duration_seconds": getattr(self, "_training_start_time", 0),
                        },
                        "git_info": git_commit,
                        "notes": "Phase 41.8å®Œäº†ãƒ»å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’ï¼ˆè¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã®ä¸€è²«æ€§ç¢ºä¿ï¼‰ãƒ»55ç‰¹å¾´é‡ãƒ»é–¾å€¤0.5%ãƒ»TimeSeriesSplit n_splits=5ãƒ»Early Stoppingãƒ»SMOTEãƒ»Optunaæœ€é©åŒ–",
                    }

                    production_metadata_file = (
                        self.production_dir / "production_model_metadata.json"
                    )
                    with open(production_metadata_file, "w", encoding="utf-8") as f:
                        json.dump(
                            production_metadata,
                            f,
                            indent=2,
                            ensure_ascii=False,
                        )

                    self.logger.info(f"âœ… æœ¬ç•ªç”¨çµ±åˆãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_file}")
                else:
                    # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã¯trainingãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
                    model_file = self.training_dir / f"{model_name}_model.pkl"
                    with open(model_file, "wb") as f:
                        pickle.dump(model, f)

                    self.logger.info(f"âœ… å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_file}")

                saved_files[model_name] = str(model_file)

            except Exception as e:
                self.logger.error(f"âŒ {model_name} ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        # å­¦ç¿’ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆPhase 41.8å®Œäº†: Strategy-Aware MLãƒ»å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’ï¼‰
        training_metadata = {
            "created_at": datetime.now().isoformat(),
            "feature_names": training_results.get("feature_names", []),
            "training_samples": training_results.get("training_samples", 0),
            "model_metrics": training_results.get("results", {}),
            "model_files": saved_files,
            "config_path": self.config_path,
            "phase": "Phase 41.8",  # Phase 41.8å®Œäº†: å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’
            "notes": "Phase 41.8å®Œäº†ãƒ»å®Ÿæˆ¦ç•¥ä¿¡å·å­¦ç¿’ï¼ˆè¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã®ä¸€è²«æ€§ç¢ºä¿ï¼‰ãƒ»55ç‰¹å¾´é‡ãƒ»é–¾å€¤0.5%ãƒ»CV n_splits=5ãƒ»Early Stoppingãƒ»SMOTEãƒ»Optunaæœ€é©åŒ–ãƒ»å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’çµæœ",
        }

        training_metadata_file = self.training_dir / "training_metadata.json"
        with open(training_metadata_file, "w", encoding="utf-8") as f:
            json.dump(training_metadata, f, indent=2, ensure_ascii=False)

        self.logger.info(f"âœ… å­¦ç¿’ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {training_metadata_file}")
        return saved_files

    def validate_models(self, saved_files: Dict[str, str]) -> bool:
        """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ï¼ˆæœ¬ç•ªç”¨ãƒ¢ãƒ‡ãƒ«é‡ç‚¹ï¼‰."""
        self.logger.info("ğŸ” ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼é–‹å§‹")

        validation_passed = True

        for model_name, model_file in saved_files.items():
            try:
                # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
                with open(model_file, "rb") as f:
                    model = pickle.load(f)

                # åŸºæœ¬å±æ€§ãƒã‚§ãƒƒã‚¯
                if hasattr(model, "predict"):
                    self.logger.info(f"âœ… {model_name}: predict ãƒ¡ã‚½ãƒƒãƒ‰ç¢ºèª")
                else:
                    self.logger.error(f"âŒ {model_name}: predict ãƒ¡ã‚½ãƒƒãƒ‰ãªã—")
                    validation_passed = False
                    continue

                # ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬ãƒ†ã‚¹ãƒˆï¼ˆDataFrameã§sklearnè­¦å‘Šå›é¿ï¼‰- Phase 40.6: å‹•çš„ç‰¹å¾´é‡æ•°å¯¾å¿œ
                n_features = len(self.expected_features)
                sample_features_array = np.random.random((5, n_features))
                sample_features = pd.DataFrame(
                    sample_features_array, columns=self.expected_features
                )
                prediction = model.predict(sample_features)

                if len(prediction) == 5:
                    self.logger.info(f"âœ… {model_name}: ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬æˆåŠŸ")
                else:
                    self.logger.error(f"âŒ {model_name}: ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬å¤±æ•—")
                    validation_passed = False
                    continue

                # æœ¬ç•ªç”¨ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°æ¤œè¨¼
                if model_name == "production_ensemble":
                    self.logger.info("ğŸ¯ æœ¬ç•ªç”¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è©³ç´°æ¤œè¨¼é–‹å§‹")

                    # predict_proba ãƒ¡ã‚½ãƒƒãƒ‰ç¢ºèª
                    if hasattr(model, "predict_proba"):
                        probabilities = model.predict_proba(sample_features)
                        if probabilities.shape == (5, 2):
                            self.logger.info("âœ… predict_proba ç¢ºèªæˆåŠŸ")
                        else:
                            self.logger.error(f"âŒ predict_proba å½¢çŠ¶ä¸æ­£: {probabilities.shape}")
                            validation_passed = False

                    # get_model_info ãƒ¡ã‚½ãƒƒãƒ‰ç¢ºèª - Phase 40.6: å‹•çš„ç‰¹å¾´é‡æ•°å¯¾å¿œ
                    if hasattr(model, "get_model_info"):
                        info = model.get_model_info()
                        expected_count = len(self.expected_features)
                        if info.get("n_features") == expected_count:
                            self.logger.info("âœ… get_model_info ç¢ºèªæˆåŠŸ")
                        else:
                            self.logger.error(
                                f"âŒ get_model_info ç‰¹å¾´é‡æ•°ä¸æ­£: "
                                f"{info.get('n_features')} != {expected_count}"
                            )
                            validation_passed = False

                    self.logger.info("ğŸ¯ æœ¬ç•ªç”¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è©³ç´°æ¤œè¨¼å®Œäº†")

            except Exception as e:
                self.logger.error(f"âŒ {model_name} æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
                validation_passed = False

        if validation_passed:
            self.logger.info("ğŸ‰ å…¨ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼æˆåŠŸï¼")
        else:
            self.logger.error("ğŸ’¥ ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼å¤±æ•—")

        return validation_passed

    def _get_git_info(self) -> Dict[str, str]:
        """Gitæƒ…å ±å–å¾—ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ç”¨ï¼‰."""
        import subprocess

        try:
            # Git commit hashå–å¾—
            commit = subprocess.check_output(
                ["git", "rev-parse", "HEAD"], text=True, cwd=project_root
            ).strip()

            # Git branchå–å¾—
            branch = subprocess.check_output(
                ["git", "rev-parse", "--abbrev-ref", "HEAD"],
                text=True,
                cwd=project_root,
            ).strip()

            return {"commit": commit, "commit_short": commit[:8], "branch": branch}
        except Exception as e:
            self.logger.warning(f"Gitæƒ…å ±å–å¾—å¤±æ•—: {e}")
            return {"commit": "unknown", "commit_short": "unknown", "branch": "unknown"}

    def _archive_existing_models(self) -> bool:
        """æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆPhase 29: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†å¼·åŒ–ï¼‰."""
        try:
            production_model = self.production_dir / "production_ensemble.pkl"
            production_metadata = self.production_dir / "production_model_metadata.json"

            if production_model.exists():
                # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
                archive_dir = Path("models/archive")
                archive_dir.mkdir(exist_ok=True)

                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                archive_model = archive_dir / f"production_ensemble_{timestamp}.pkl"
                archive_metadata = archive_dir / f"production_model_metadata_{timestamp}.json"

                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼
                import shutil

                shutil.copy2(production_model, archive_model)
                if production_metadata.exists():
                    shutil.copy2(production_metadata, archive_metadata)

                self.logger.info(f"âœ… æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å®Œäº†: {archive_model}")
                return True
            else:
                self.logger.info("ğŸ“‚ æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ãªã— - ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¹ã‚­ãƒƒãƒ—")
                return True

        except Exception as e:
            self.logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def run(self, dry_run: bool = False, days: int = 180) -> bool:
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œå‡¦ç†."""
        try:
            self.logger.info("ğŸš€ æ–°ã‚·ã‚¹ãƒ†ãƒ MLãƒ¢ãƒ‡ãƒ«ä½œæˆé–‹å§‹")

            # 0. æ—¢å­˜ãƒ¢ãƒ‡ãƒ«è‡ªå‹•ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆPhase 29: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†å¼·åŒ–ï¼‰
            if not dry_run:
                if not self._archive_existing_models():
                    self.logger.warning("âš ï¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¤±æ•— - å‡¦ç†ç¶šè¡Œ")

            # 1. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™
            features, target = self.prepare_training_data(days)

            # 2. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            training_results = self.train_models(features, target, dry_run)

            if dry_run:
                self.logger.info("ğŸ” ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Œäº†")
                return True

            # 3. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            saved_files = self.save_models(training_results)

            # 4. æ¤œè¨¼
            validation_passed = self.validate_models(saved_files)

            if validation_passed:
                self.logger.info("âœ… MLãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº† - å®Ÿå–å¼•æº–å‚™å®Œäº†")
                return True
            else:
                self.logger.error("âŒ MLãƒ¢ãƒ‡ãƒ«ä½œæˆå¤±æ•—")
                return False

        except Exception as e:
            self.logger.error(f"ğŸ’¥ MLãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆPhase 39.1-39.5å®Œäº†ï¼‰"""
    parser = argparse.ArgumentParser(
        description="æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨MLãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆPhase 39.1-39.5å®Œäº†ãƒ»MLä¿¡é ¼åº¦å‘ä¸ŠæœŸï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆå®Ÿéš›ã®å­¦ç¿’ãƒ»ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰",
    )
    parser.add_argument("--verbose", action="store_true", help="è©³ç´°ãƒ­ã‚°å‡ºåŠ›")
    parser.add_argument(
        "--days",
        type=int,
        default=180,
        help="å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æœŸé–“ï¼ˆæ—¥æ•°ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 180æ—¥ï¼‰",
    )
    parser.add_argument("--config", default="config/core/unified.yaml", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")

    # Phase 39.2: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å®šå¼•æ•°
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.005,
        help="Phase 39.2: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.5%%ï¼‰",
    )
    parser.add_argument(
        "--n-classes",
        type=int,
        default=2,
        choices=[2, 3],
        help="Phase 39.2: ã‚¯ãƒ©ã‚¹æ•° 2ï¼ˆBUY/OTHERï¼‰ or 3ï¼ˆBUY/HOLD/SELLï¼‰",
    )

    # Phase 39.4: SMOTEè¨­å®šå¼•æ•°
    parser.add_argument(
        "--use-smote",
        action="store_true",
        help="Phase 39.4: SMOTEã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æœ‰åŠ¹åŒ–ï¼ˆã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–ï¼‰",
    )

    # Phase 39.5: Optunaæœ€é©åŒ–è¨­å®šå¼•æ•°
    parser.add_argument(
        "--optimize",
        action="store_true",
        help="Phase 39.5: Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–æœ‰åŠ¹åŒ–",
    )
    parser.add_argument(
        "--n-trials",
        type=int,
        default=20,
        help="Phase 39.5: Optunaæœ€é©åŒ–è©¦è¡Œå›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰",
    )

    args = parser.parse_args()

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Ÿè¡Œï¼ˆPhase 39.2-39.5å¯¾å¿œï¼‰
    creator = NewSystemMLModelCreator(
        config_path=args.config,
        verbose=args.verbose,
        target_threshold=args.threshold,
        n_classes=args.n_classes,
        use_smote=args.use_smote,
        optimize=args.optimize,
        n_trials=args.n_trials,
    )

    success = creator.run(dry_run=args.dry_run, days=args.days)

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
