#!/usr/bin/env python3
"""
Phase 3.1: feature_names mismatchå®Œå…¨è§£æ±º
ç‰¹å¾´é‡é †åºçµ±ä¸€ãƒ»MLäºˆæ¸¬ç²¾åº¦ä¿®å¾©ã‚·ã‚¹ãƒ†ãƒ 

è§£æ±ºã™ã‚‹å•é¡Œ:
- XGBoost/RandomForestãŒå¸¸ã«0.500ã‚’è¿”ã™å•é¡Œ
- å­¦ç¿’æ™‚ã¨äºˆæ¸¬æ™‚ã®ç‰¹å¾´é‡é †åºä¸ä¸€è‡´
- feature_names mismatch ã‚¨ãƒ©ãƒ¼
- MLäºˆæ¸¬ã®ä¿¡é ¼æ€§ç¢ºä¿

å®Ÿè£…æ©Ÿèƒ½:
1. ç‰¹å¾´é‡é †åºæ¤œè¨¼ãƒ»ä¿®æ­£ã‚·ã‚¹ãƒ†ãƒ 
2. MLãƒ¢ãƒ‡ãƒ«äº’æ›æ€§ãƒã‚§ãƒƒã‚¯
3. feature_order.jsonæ•´åˆæ€§ç¢ºä¿
4. 97ç‰¹å¾´é‡ã‚·ã‚¹ãƒ†ãƒ å®Œå…¨å¯¾å¿œ
"""

import json
import logging
import pickle
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import yaml

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# FeatureOrderManager ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from crypto_bot.ml.feature_order_manager import (
        FeatureOrderManager,
        get_feature_order_manager,
    )

    logger.info("âœ… FeatureOrderManager successfully imported")
except ImportError as e:
    logger.error(f"âŒ FeatureOrderManager import failed: {e}")
    sys.exit(1)


class FeatureNamesMismatchFixer:
    """
    Phase 3.1: feature_names mismatchå®Œå…¨è§£æ±ºã‚·ã‚¹ãƒ†ãƒ 

    æ©Ÿèƒ½:
    - ç‰¹å¾´é‡é †åºçµ±ä¸€
    - MLãƒ¢ãƒ‡ãƒ«äº’æ›æ€§ä¿è¨¼
    - å­¦ç¿’ãƒ»äºˆæ¸¬é–“ä¸€è‡´ç¢ºä¿
    - 97ç‰¹å¾´é‡ã‚·ã‚¹ãƒ†ãƒ å®Œå…¨å¯¾å¿œ
    """

    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path
        self.config = None
        self.feature_manager = get_feature_order_manager()

        # 97ç‰¹å¾´é‡ã‚·ã‚¹ãƒ†ãƒ æ¨™æº–é †åº
        self.standard_97_features = self.feature_manager.FEATURE_ORDER_97

        logger.info("ğŸ”§ FeatureNamesMismatchFixer initialized")

        if config_path:
            self.load_config()

    def load_config(self) -> bool:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                self.config = yaml.safe_load(f)
            logger.info(f"âœ… Config loaded: {self.config_path}")
            return True
        except Exception as e:
            logger.error(f"âŒ Config load failed: {e}")
            return False

    def diagnose_feature_mismatch(
        self, df: pd.DataFrame, model_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ç‰¹å¾´é‡ãƒŸã‚¹ãƒãƒƒãƒã®åŒ…æ‹¬çš„è¨ºæ–­

        Args:
            df: è¨ºæ–­å¯¾è±¡DataFrame
            model_path: MLãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            è¨ºæ–­çµæœã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
        """
        logger.info("ğŸ” Starting comprehensive feature mismatch diagnosis...")

        diagnosis = {
            "timestamp": datetime.now().isoformat(),
            "input_features": {"count": len(df.columns), "features": list(df.columns)},
            "standard_features": {
                "count": len(self.standard_97_features),
                "features": self.standard_97_features,
            },
            "analysis": {},
            "issues": [],
            "recommendations": [],
        }

        # 1. ç‰¹å¾´é‡æ•°ã®æ¤œè¨¼
        input_count = len(df.columns)
        if input_count != 97:
            issue = f"Feature count mismatch: {input_count} instead of 97"
            diagnosis["issues"].append(issue)
            logger.warning(f"âš ï¸ {issue}")

        # 2. ç‰¹å¾´é‡é †åºã®æ¤œè¨¼
        input_features = list(df.columns)
        if input_features != self.standard_97_features:
            issue = "Feature order mismatch detected"
            diagnosis["issues"].append(issue)
            logger.warning(f"âš ï¸ {issue}")

            # è©³ç´°åˆ†æ
            input_set = set(input_features)
            standard_set = set(self.standard_97_features)

            missing_features = standard_set - input_set
            extra_features = input_set - standard_set
            common_features = input_set & standard_set

            diagnosis["analysis"] = {
                "missing_features": list(missing_features),
                "extra_features": list(extra_features),
                "common_features": list(common_features),
                "missing_count": len(missing_features),
                "extra_count": len(extra_features),
                "common_count": len(common_features),
                "overlap_ratio": len(common_features) / 97,
            }

            logger.info(
                f"ğŸ“Š Analysis: {len(missing_features)} missing, {len(extra_features)} extra, {len(common_features)} common"
            )

        # 3. MLãƒ¢ãƒ‡ãƒ«äº’æ›æ€§æ¤œè¨¼
        if model_path and Path(model_path).exists():
            model_diagnosis = self._diagnose_model_compatibility(
                model_path, input_features
            )
            diagnosis["model_compatibility"] = model_diagnosis

        # 4. feature_order.jsonæ•´åˆæ€§æ¤œè¨¼
        feature_order_diagnosis = self._diagnose_feature_order_file(input_features)
        diagnosis["feature_order_file"] = feature_order_diagnosis

        # 5. æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
        diagnosis["recommendations"] = self._generate_recommendations(diagnosis)

        logger.info(f"âœ… Diagnosis completed: {len(diagnosis['issues'])} issues found")
        return diagnosis

    def _diagnose_model_compatibility(
        self, model_path: str, input_features: List[str]
    ) -> Dict[str, Any]:
        """MLãƒ¢ãƒ‡ãƒ«äº’æ›æ€§è¨ºæ–­"""
        logger.info(f"ğŸ¤– Diagnosing ML model compatibility: {model_path}")

        try:
            with open(model_path, "rb") as f:
                model = pickle.load(f)

            model_info = {
                "model_type": type(model).__name__,
                "model_loaded": True,
                "feature_compatibility": {},
                "issues": [],
            }

            # XGBoost/LightGBM ã®å ´åˆã€ç‰¹å¾´é‡åã‚’ç¢ºèª
            if hasattr(model, "feature_names_in_"):
                model_features = list(model.feature_names_in_)
                model_info["model_features"] = model_features
                model_info["model_feature_count"] = len(model_features)

                # ç‰¹å¾´é‡ã®æ¯”è¼ƒ
                if model_features != input_features:
                    model_info["issues"].append(
                        "Feature names/order mismatch with model"
                    )
                    model_info["feature_compatibility"] = {
                        "perfect_match": False,
                        "model_features": model_features,
                        "input_features": input_features,
                    }
                else:
                    model_info["feature_compatibility"] = {"perfect_match": True}

            elif hasattr(model, "get_booster"):  # XGBoost specific
                try:
                    booster = model.get_booster()
                    model_features = booster.feature_names
                    model_info["model_features"] = model_features
                    model_info["model_feature_count"] = len(model_features)

                    if model_features != input_features:
                        model_info["issues"].append("XGBoost feature names mismatch")
                except Exception as e:
                    model_info["issues"].append(
                        f"XGBoost feature extraction failed: {e}"
                    )

            else:
                model_info["issues"].append("Cannot extract feature names from model")

            logger.info(f"ğŸ¤– Model compatibility diagnosis completed")
            return model_info

        except Exception as e:
            logger.error(f"âŒ Model compatibility diagnosis failed: {e}")
            return {
                "model_loaded": False,
                "error": str(e),
                "issues": [f"Model loading failed: {e}"],
            }

    def _diagnose_feature_order_file(self, input_features: List[str]) -> Dict[str, Any]:
        """feature_order.jsonæ•´åˆæ€§è¨ºæ–­"""
        logger.info("ğŸ“‹ Diagnosing feature_order.json consistency...")

        feature_order_path = Path("config/core/feature_order.json")

        diagnosis = {"file_exists": feature_order_path.exists(), "issues": []}

        if not feature_order_path.exists():
            diagnosis["issues"].append("feature_order.json file not found")
            return diagnosis

        try:
            with open(feature_order_path, "r") as f:
                data = json.load(f)

            stored_features = data.get("feature_order", [])
            stored_count = data.get("num_features", 0)

            diagnosis.update(
                {
                    "stored_features": stored_features,
                    "stored_count": stored_count,
                    "data_loaded": True,
                }
            )

            # æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            if stored_count != 97:
                diagnosis["issues"].append(f"Stored feature count {stored_count} != 97")

            if stored_features != self.standard_97_features:
                diagnosis["issues"].append("Stored features != standard 97 features")

            if stored_features != input_features:
                diagnosis["issues"].append("Stored features != input features")

            if not diagnosis["issues"]:
                logger.info("âœ… feature_order.json is perfectly consistent")

            return diagnosis

        except Exception as e:
            diagnosis["issues"].append(f"Failed to load feature_order.json: {e}")
            return diagnosis

    def _generate_recommendations(self, diagnosis: Dict[str, Any]) -> List[str]:
        """è¨ºæ–­çµæœã«åŸºã¥ãæ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        if diagnosis["issues"]:
            recommendations.append("ğŸ”§ Execute fix_feature_order() to resolve issues")

        # ç‰¹å¾´é‡æ•°ã®å•é¡Œ
        input_count = diagnosis["input_features"]["count"]
        if input_count != 97:
            if input_count < 97:
                recommendations.append(
                    f"ğŸ“ˆ Add {97 - input_count} missing features using generate_missing_features()"
                )
            else:
                recommendations.append(
                    f"ğŸ“‰ Remove {input_count - 97} excess features using trim_to_97_features()"
                )

        # ç‰¹å¾´é‡é †åºã®å•é¡Œ
        if (
            "analysis" in diagnosis
            and diagnosis["analysis"].get("overlap_ratio", 1.0) < 0.8
        ):
            recommendations.append(
                "ğŸ”„ Regenerate features using HybridBacktestEngine.add_97_features()"
            )

        # MLãƒ¢ãƒ‡ãƒ«ã®å•é¡Œ
        if "model_compatibility" in diagnosis:
            model_issues = diagnosis["model_compatibility"].get("issues", [])
            if model_issues:
                recommendations.append(
                    "ğŸ¤– Retrain ML model with corrected 97-feature dataset"
                )

        # feature_order.jsonã®å•é¡Œ
        if "feature_order_file" in diagnosis:
            file_issues = diagnosis["feature_order_file"].get("issues", [])
            if file_issues:
                recommendations.append(
                    "ğŸ“‹ Update feature_order.json using save_corrected_feature_order()"
                )

        if not recommendations:
            recommendations.append(
                "âœ… No issues detected - system is properly configured"
            )

        return recommendations

    def fix_feature_order(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç‰¹å¾´é‡é †åºã®å®Œå…¨ä¿®æ­£

        Args:
            df: ä¿®æ­£å¯¾è±¡DataFrame

        Returns:
            97ç‰¹å¾´é‡é †åºã§ä¿®æ­£ã•ã‚ŒãŸDataFrame
        """
        logger.info("ğŸ”§ Starting comprehensive feature order fix...")
        start_time = pd.Timestamp.now()

        try:
            # Step 1: ç¾çŠ¶è¨ºæ–­
            original_count = len(df.columns)
            logger.info(f"ğŸ“Š Input: {original_count} features")

            # Step 2: FeatureOrderManagerã‚’ä½¿ç”¨ã—ãŸé †åºçµ±ä¸€
            df_ordered = self.feature_manager.ensure_column_order(df.copy())

            # Step 3: 97ç‰¹å¾´é‡å®Œå…¨æ€§ä¿è¨¼
            df_complete = self.feature_manager.ensure_97_features_completeness(
                df_ordered
            )

            # Step 4: æœ€çµ‚æ¤œè¨¼
            final_features = list(df_complete.columns)
            if final_features != self.standard_97_features:
                logger.warning("âš ï¸ Final validation failed - applying emergency fix")
                df_complete = self._apply_emergency_97_fix(df_complete)

            # Step 5: å“è³ªãƒã‚§ãƒƒã‚¯
            df_final = self._ensure_data_quality(df_complete)

            execution_time = pd.Timestamp.now() - start_time
            logger.info(
                f"âœ… Feature order fix completed in {execution_time.total_seconds():.2f}s"
            )
            logger.info(f"ğŸ“Š Result: {len(df_final.columns)} features (target: 97)")

            return df_final

        except Exception as e:
            logger.error(f"âŒ Feature order fix failed: {e}")
            return self._create_emergency_97_dataframe(df)

    def _apply_emergency_97_fix(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç·Šæ€¥97ç‰¹å¾´é‡ä¿®æ­£"""
        logger.warning("ğŸš¨ Applying emergency 97-feature fix...")

        # 97ç‰¹å¾´é‡ã®æ¨™æº–DataFrameã‚’ä½œæˆ
        result_df = pd.DataFrame(0.0, index=df.index, columns=self.standard_97_features)

        # åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ã‚’ã‚³ãƒ”ãƒ¼
        for col in df.columns:
            if col in result_df.columns:
                result_df[col] = df[col]

        logger.warning(f"ğŸš¨ Emergency fix applied: {len(result_df.columns)} features")
        return result_df

    def _ensure_data_quality(self, df: pd.DataFrame) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºä¿"""
        df_clean = df.copy()

        # NaNå€¤å‡¦ç†
        nan_cols = df_clean.columns[df_clean.isna().any()].tolist()
        if nan_cols:
            logger.info(f"ğŸ§¹ Cleaning NaN values in {len(nan_cols)} columns")
            df_clean = (
                df_clean.fillna(method="ffill").fillna(method="bfill").fillna(0.0)
            )

        # Infå€¤å‡¦ç†
        inf_cols = df_clean.columns[np.isinf(df_clean).any()].tolist()
        if inf_cols:
            logger.info(f"ğŸ§¹ Cleaning Inf values in {len(inf_cols)} columns")
            df_clean = df_clean.replace([np.inf, -np.inf], 0.0)

        return df_clean

    def _create_emergency_97_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç·Šæ€¥97ç‰¹å¾´é‡DataFrameä½œæˆ"""
        logger.error("ğŸš¨ Creating emergency 97-feature DataFrame")
        return pd.DataFrame(
            0.0,
            index=df.index if not df.empty else [0],
            columns=self.standard_97_features,
        )

    def validate_ml_prediction_compatibility(
        self, df: pd.DataFrame, model_path: str
    ) -> Tuple[bool, Dict[str, Any]]:
        """
        MLäºˆæ¸¬äº’æ›æ€§ã®æ¤œè¨¼

        Args:
            df: äºˆæ¸¬ç”¨DataFrame
            model_path: MLãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹

        Returns:
            (äº’æ›æ€§OK, è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ)
        """
        logger.info("ğŸ¤– Validating ML prediction compatibility...")

        if not Path(model_path).exists():
            return False, {"error": "Model file not found"}

        try:
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            with open(model_path, "rb") as f:
                model = pickle.load(f)

            # ç‰¹å¾´é‡æ¤œè¨¼
            df_features = list(df.columns)

            # ãƒ†ã‚¹ãƒˆäºˆæ¸¬å®Ÿè¡Œ
            test_data = df.head(5).fillna(0.0)  # æœ€åˆã®5è¡Œã§ãƒ†ã‚¹ãƒˆ

            if hasattr(model, "predict_proba"):
                predictions = model.predict_proba(test_data)
                pred_shape = predictions.shape
                pred_sample = predictions[0] if len(predictions) > 0 else []
            else:
                predictions = model.predict(test_data)
                pred_shape = predictions.shape
                pred_sample = predictions[:3] if len(predictions) > 0 else []

            # å›ºå®šå€¤ãƒã‚§ãƒƒã‚¯ï¼ˆ0.500å•é¡Œã®æ¤œå‡ºï¼‰
            if hasattr(model, "predict_proba") and len(predictions) > 1:
                # å…¨ã¦ã®äºˆæ¸¬ãŒåŒã˜å€¤ã‹ãƒã‚§ãƒƒã‚¯
                if len(predictions.shape) > 1 and predictions.shape[1] > 1:
                    first_pred = predictions[0, 1]  # Buy probability
                    all_same = np.all(np.abs(predictions[:, 1] - first_pred) < 1e-6)

                    if all_same:
                        logger.warning(
                            f"âš ï¸ Model returning fixed predictions: {first_pred}"
                        )
                        return False, {
                            "fixed_prediction_detected": True,
                            "fixed_value": float(first_pred),
                            "prediction_shape": pred_shape,
                            "sample_predictions": (
                                pred_sample.tolist()
                                if hasattr(pred_sample, "tolist")
                                else pred_sample
                            ),
                        }

            logger.info("âœ… ML prediction compatibility validated")
            return True, {
                "model_type": type(model).__name__,
                "prediction_shape": pred_shape,
                "sample_predictions": (
                    pred_sample.tolist()
                    if hasattr(pred_sample, "tolist")
                    else pred_sample
                ),
                "feature_count": len(df_features),
            }

        except Exception as e:
            logger.error(f"âŒ ML prediction compatibility validation failed: {e}")
            return False, {"error": str(e)}

    def save_corrected_feature_order(self, df: pd.DataFrame):
        """ä¿®æ­£ã•ã‚ŒãŸç‰¹å¾´é‡é †åºã‚’ä¿å­˜"""
        logger.info("ğŸ’¾ Saving corrected feature order...")

        features = list(df.columns)
        if len(features) == 97 and features == self.standard_97_features:
            self.feature_manager.save_feature_order(features)
            logger.info("âœ… Corrected feature order saved")
        else:
            logger.warning(
                f"âš ï¸ Cannot save non-standard feature order: {len(features)} features"
            )

    def comprehensive_fix_and_validate(
        self, df: pd.DataFrame, model_path: Optional[str] = None
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        åŒ…æ‹¬çš„ä¿®æ­£ãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 

        Args:
            df: å‡¦ç†å¯¾è±¡DataFrame
            model_path: MLãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            (ä¿®æ­£æ¸ˆã¿DataFrame, æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ)
        """
        logger.info("ğŸš€ Starting comprehensive fix and validation...")

        # 1. åˆæœŸè¨ºæ–­
        initial_diagnosis = self.diagnose_feature_mismatch(df, model_path)

        # 2. ç‰¹å¾´é‡é †åºä¿®æ­£
        df_fixed = self.fix_feature_order(df)

        # 3. MLäº’æ›æ€§æ¤œè¨¼
        ml_compatibility = {}
        if model_path:
            is_compatible, ml_report = self.validate_ml_prediction_compatibility(
                df_fixed, model_path
            )
            ml_compatibility = {"compatible": is_compatible, "report": ml_report}

        # 4. æœ€çµ‚è¨ºæ–­
        final_diagnosis = self.diagnose_feature_mismatch(df_fixed, model_path)

        # 5. ä¿®æ­£ã•ã‚ŒãŸç‰¹å¾´é‡é †åºã‚’ä¿å­˜
        if not final_diagnosis["issues"]:
            self.save_corrected_feature_order(df_fixed)

        # 6. åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆ
        comprehensive_report = {
            "timestamp": datetime.now().isoformat(),
            "initial_diagnosis": initial_diagnosis,
            "final_diagnosis": final_diagnosis,
            "ml_compatibility": ml_compatibility,
            "fix_successful": len(final_diagnosis["issues"]) == 0,
            "original_feature_count": len(df.columns),
            "final_feature_count": len(df_fixed.columns),
        }

        if comprehensive_report["fix_successful"]:
            logger.info("ğŸŠ Comprehensive fix and validation completed successfully!")
        else:
            logger.warning(
                f"âš ï¸ Fix completed with {len(final_diagnosis['issues'])} remaining issues"
            )

        return df_fixed, comprehensive_report


def run_feature_mismatch_diagnosis():
    """Phase 3.1è¨ºæ–­å®Ÿè¡Œ"""
    print("ğŸ” Phase 3.1: Feature Names Mismatch Diagnosis")
    print("=" * 70)

    fixer = FeatureNamesMismatchFixer()

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§è¨ºæ–­å®Ÿè¡Œ
    logger.info("ğŸ“Š Creating sample data for diagnosis...")
    sample_data = pd.DataFrame(
        np.random.randn(100, 95),  # 97ç‰¹å¾´é‡æœªæº€ã§ãƒ†ã‚¹ãƒˆ
        columns=[f"feature_{i:03d}" for i in range(95)],
    )

    # è¨ºæ–­å®Ÿè¡Œ
    diagnosis = fixer.diagnose_feature_mismatch(sample_data)

    # çµæœè¡¨ç¤º
    print(f"\nğŸ“‹ Diagnosis Results:")
    print(f"   Input features: {diagnosis['input_features']['count']}")
    print(f"   Issues found: {len(diagnosis['issues'])}")

    for issue in diagnosis["issues"]:
        print(f"   âŒ {issue}")

    print(f"\nğŸ’¡ Recommendations:")
    for rec in diagnosis["recommendations"]:
        print(f"   {rec}")

    return diagnosis


def run_comprehensive_fix_demo():
    """åŒ…æ‹¬çš„ä¿®æ­£ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    print("\nğŸ”§ Phase 3.1: Comprehensive Fix Demo")
    print("=" * 70)

    fixer = FeatureNamesMismatchFixer()

    # HybridBacktestEngineã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¢ãƒ«ãª97ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    try:
        from scripts.hybrid_backtest_approach import HybridBacktest

        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        dates = pd.date_range("2024-01-01", periods=200, freq="H")
        np.random.seed(42)
        price = 7500000  # BTC/JPY
        sample_ohlcv = []

        for _ in dates:
            price += np.random.normal(0, 50000)
            high = price + np.random.uniform(0, 30000)
            low = price - np.random.uniform(0, 30000)
            volume = np.random.uniform(50, 500)

            sample_ohlcv.append(
                {
                    "open": price,
                    "high": high,
                    "low": low,
                    "close": price,
                    "volume": volume,
                }
            )

        df_ohlcv = pd.DataFrame(sample_ohlcv, index=dates)

        # 97ç‰¹å¾´é‡ç”Ÿæˆ
        backtest_engine = HybridBacktest(phase="B")
        df_features = backtest_engine.add_97_features(df_ohlcv)

        logger.info(
            f"ğŸ“Š Generated {len(df_features.columns)} features for comprehensive fix demo"
        )

        # åŒ…æ‹¬çš„ä¿®æ­£ãƒ»æ¤œè¨¼å®Ÿè¡Œ
        df_fixed, report = fixer.comprehensive_fix_and_validate(df_features)

        # çµæœè¡¨ç¤º
        print(f"\nâœ… Comprehensive Fix Results:")
        print(f"   Original features: {report['original_feature_count']}")
        print(f"   Final features: {report['final_feature_count']}")
        print(f"   Fix successful: {report['fix_successful']}")

        if report["fix_successful"]:
            print("   ğŸŠ All feature_names mismatches resolved!")

        return df_fixed, report

    except ImportError:
        logger.error("âŒ HybridBacktestEngine not available for demo")
        return None, None


def main():
    """Phase 3.1ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš€ Phase 3.1: Feature Names Mismatch Complete Solution")
    print("=" * 80)
    print("ç‰¹å¾´é‡é †åºçµ±ä¸€ãƒ»MLäºˆæ¸¬ç²¾åº¦ä¿®å¾©ãƒ»97ç‰¹å¾´é‡ã‚·ã‚¹ãƒ†ãƒ å®Œå…¨å¯¾å¿œ")
    print("=" * 80)

    # 1. è¨ºæ–­å®Ÿè¡Œ
    diagnosis = run_feature_mismatch_diagnosis()

    # 2. åŒ…æ‹¬çš„ä¿®æ­£ãƒ‡ãƒ¢
    df_fixed, report = run_comprehensive_fix_demo()

    # 3. å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ
    print(f"\n" + "ğŸŠ" * 80)
    print("Phase 3.1: Feature Names Mismatch Solution - å®Œäº†!")
    print("ğŸŠ" * 80)

    if report and report.get("fix_successful", False):
        print("âœ… ã™ã¹ã¦ã®feature_names mismatchå•é¡Œã‚’è§£æ±ºã—ã¾ã—ãŸ")
        print("ğŸ¤– MLäºˆæ¸¬ç²¾åº¦ã®æ”¹å–„ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        print("ğŸ“Š 97ç‰¹å¾´é‡ã‚·ã‚¹ãƒ†ãƒ ã®æ•´åˆæ€§ã‚’ç¢ºä¿ã—ã¾ã—ãŸ")
    else:
        print("âš ï¸ ä¸€éƒ¨ã®å•é¡ŒãŒæ®‹å­˜ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")

    print("ğŸ”„ Next: Phase 4 - å®Ÿè¡Œãƒ»æ¤œè¨¼ãƒ»æœ€é©åŒ–")

    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
