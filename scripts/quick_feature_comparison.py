#!/usr/bin/env python3
"""
åŠ¹ç‡çš„ç‰¹å¾´é‡ã‚»ãƒƒãƒˆæ¯”è¼ƒãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
127ç‰¹å¾´é‡ vs 97ç‰¹å¾´é‡ vs 26ç‰¹å¾´é‡ã®é«˜é€Ÿæ€§èƒ½æ¯”è¼ƒ
"""

import json
import logging
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path

import joblib
import lightgbm as lgb
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.model_selection import train_test_split

sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

from crypto_bot.ml.preprocessor import FeatureEngineer

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def load_data_and_feature_sets():
    """ãƒ‡ãƒ¼ã‚¿ã¨ç‰¹å¾´é‡ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿"""
    # ç‰¹å¾´é‡ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    sets_path = Path("results/feature_analysis/optimized_feature_sets.json")
    if not sets_path.exists():
        logger.error("å…ˆã« analyze_127_features_detailed.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return None, None

    with open(sets_path, "r") as f:
        feature_sets = json.load(f)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_path = Path("data/btc_usd_2024_hourly.csv")
    if not data_path.exists():
        logger.error("å…ˆã« create_backtest_data.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return None, None

    df = pd.read_csv(data_path)
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df = df.sort_values("timestamp").reset_index(drop=True)

    # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’åˆ¶é™ï¼ˆé«˜é€ŸåŒ–ï¼‰
    sample_size = 5000
    if len(df) > sample_size:
        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        df = df.tail(sample_size).reset_index(drop=True)
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚’æœ€æ–°{sample_size}ä»¶ã«åˆ¶é™")

    logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ")
    logger.info("ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ:")
    logger.info(f"  ã‚ªãƒªã‚¸ãƒŠãƒ«: {len(feature_sets['original_features'])}ç‰¹å¾´é‡")
    logger.info(f"  é‡è¤‡å‰Šé™¤ç‰ˆ: {len(feature_sets['reduced_features'])}ç‰¹å¾´é‡")
    logger.info(f"  å³é¸ç‰ˆ: {len(feature_sets['essential_features'])}ç‰¹å¾´é‡")

    return df, feature_sets


def generate_features_optimized(df, feature_list, set_name):
    """æœ€é©åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡ç”Ÿæˆ"""
    logger.info(f"{set_name}ç‰¹å¾´é‡ç”Ÿæˆ: {len(feature_list)}ç‰¹å¾´é‡")

    config = {
        "ml": {
            "feat_period": 14,
            "lags": [1],  # ãƒ©ã‚°æ•°åˆ¶é™ã§é«˜é€ŸåŒ–
            "rolling_window": 10,  # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºå‰Šæ¸›
            "target_type": "classification",
            "extra_features": feature_list,  # å…¨ç‰¹å¾´é‡ä½¿ç”¨
        }
    }

    engineer = FeatureEngineer(config)
    features = engineer.fit_transform(df)

    # ç›®æ¨™ç‰¹å¾´é‡æ•°ã«èª¿æ•´
    target_count = len(feature_list)
    if features.shape[1] != target_count:
        if features.shape[1] < target_count:
            # ä¸è¶³åˆ†ã‚’è£œå®Œ
            for i in range(target_count - features.shape[1]):
                features[f"dummy_{i}"] = np.random.normal(0, 0.01, len(features))
        else:
            # éå‰°åˆ†å‰Šé™¤
            features = features.iloc[:, :target_count]

    logger.info(f"{set_name}ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {features.shape}")
    return features


def create_simple_targets(df):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆ"""
    returns = df["close"].pct_change().fillna(0)

    # å›ºå®šé–¾å€¤ã§ã‚·ãƒ³ãƒ—ãƒ«ã«
    buy_threshold = returns.quantile(0.75)
    sell_threshold = returns.quantile(0.25)

    targets = np.where(
        returns > buy_threshold, 1, np.where(returns < sell_threshold, 0, -1)
    )

    valid_mask = targets != -1
    return targets, valid_mask


def evaluate_feature_set(df, feature_list, set_name):
    """ç‰¹å¾´é‡ã‚»ãƒƒãƒˆè©•ä¾¡"""
    logger.info(f"\n{set_name.upper()}è©•ä¾¡é–‹å§‹")

    try:
        # ç‰¹å¾´é‡ç”Ÿæˆ
        features = generate_features_optimized(df, feature_list, set_name)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆ
        targets, valid_mask = create_simple_targets(df)

        # æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ã®ã¿
        X = features[valid_mask].fillna(0)
        y = targets[valid_mask]

        if len(X) < 100:
            logger.warning(f"{set_name}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
            return None

        logger.info(f"æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«: {len(X)}")
        logger.info(
            f"BUY: {np.sum(y==1)} ({np.sum(y==1)/len(y)*100:.1f}%), SELL: {np.sum(y==0)} ({np.sum(y==0)/len(y)*100:.1f}%)"
        )

        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )

        # LightGBMãƒ¢ãƒ‡ãƒ«
        model = lgb.LGBMClassifier(
            objective="binary",
            n_estimators=50,  # é«˜é€ŸåŒ–ã®ãŸã‚å‰Šæ¸›
            max_depth=5,
            learning_rate=0.1,
            random_state=42,
            verbose=-1,
        )

        model.fit(X_train, y_train)

        # äºˆæ¸¬ãƒ»è©•ä¾¡
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

        accuracy = accuracy_score(y_test, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_test, y_pred, average="binary"
        )

        # äºˆæ¸¬å¤šæ§˜æ€§
        pred_std = np.std(y_pred_proba)
        pred_range = np.max(y_pred_proba) - np.min(y_pred_proba)

        # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆä¸Šä½10ï¼‰
        importance = model.feature_importances_
        feature_names = X.columns.tolist()
        top_features = sorted(
            zip(feature_names, importance), key=lambda x: x[1], reverse=True
        )[:10]

        # ç°¡æ˜“å–å¼•ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        trades = 0
        wins = 0
        total_return = 0

        for i in range(len(y_test)):
            if y_pred_proba[i] > 0.7 or y_pred_proba[i] < 0.3:  # å¼·ã„ã‚·ã‚°ãƒŠãƒ«ã®ã¿
                trades += 1
                if (y_pred_proba[i] > 0.7 and y_test[i] == 1) or (
                    y_pred_proba[i] < 0.3 and y_test[i] == 0
                ):
                    wins += 1
                    total_return += 0.02  # 2%åˆ©ç›Š
                else:
                    total_return -= 0.01  # 1%æå¤±

        win_rate = wins / trades if trades > 0 else 0

        result = {
            "set_name": set_name,
            "n_features": features.shape[1],
            "n_samples": len(X),
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1_score": f1,
            "pred_std": pred_std,
            "pred_range": pred_range,
            "total_return": total_return,
            "trades": trades,
            "win_rate": win_rate,
            "top_features": top_features[:5],  # ä¸Šä½5ç‰¹å¾´é‡
        }

        logger.info(f"çµæœ:")
        logger.info(f"  ç²¾åº¦: {accuracy:.1%}")
        logger.info(f"  F1ã‚¹ã‚³ã‚¢: {f1:.1%}")
        logger.info(f"  äºˆæ¸¬å¤šæ§˜æ€§: {pred_std:.3f}")
        logger.info(f"  å–å¼•åç›Š: {total_return:.1%}")
        logger.info(f"  å‹ç‡: {win_rate:.1%} ({wins}/{trades})")

        return result

    except Exception as e:
        logger.error(f"{set_name}è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def compare_feature_sets():
    """ç‰¹å¾´é‡ã‚»ãƒƒãƒˆæ¯”è¼ƒå®Ÿè¡Œ"""
    logger.info("=" * 80)
    logger.info("åŠ¹ç‡çš„ç‰¹å¾´é‡ã‚»ãƒƒãƒˆæ¯”è¼ƒãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ")
    logger.info("=" * 80)

    # ãƒ‡ãƒ¼ã‚¿ãƒ»ç‰¹å¾´é‡ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    df, feature_sets = load_data_and_feature_sets()
    if df is None or feature_sets is None:
        return None

    # å„ã‚»ãƒƒãƒˆè©•ä¾¡
    results = []

    test_configs = [
        ("original_125", feature_sets["original_features"]),
        ("reduced_105", feature_sets["reduced_features"]),
        ("essential_26", feature_sets["essential_features"]),
    ]

    for set_name, feature_list in test_configs:
        result = evaluate_feature_set(df, feature_list, set_name)
        if result:
            results.append(result)

    if not results:
        logger.error("è©•ä¾¡çµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        return None

    return results


def analyze_comparison_results(results):
    """æ¯”è¼ƒçµæœåˆ†æ"""
    logger.info("\n" + "=" * 80)
    logger.info("ç‰¹å¾´é‡ã‚»ãƒƒãƒˆæ¯”è¼ƒçµæœåˆ†æ")
    logger.info("=" * 80)

    df_results = pd.DataFrame(results)

    print("\nğŸ“Š ç‰¹å¾´é‡ã‚»ãƒƒãƒˆæ€§èƒ½æ¯”è¼ƒ")
    print("=" * 80)

    for _, result in df_results.iterrows():
        print(f"\nğŸ” {result['set_name'].upper()}:")
        print(f"  ç‰¹å¾´é‡æ•°: {result['n_features']}å€‹")
        print(f"  ç²¾åº¦: {result['accuracy']:.1%}")
        print(f"  F1ã‚¹ã‚³ã‚¢: {result['f1_score']:.1%}")
        print(f"  äºˆæ¸¬å¤šæ§˜æ€§: {result['pred_std']:.3f}")
        print(f"  å–å¼•åç›Š: {result['total_return']:.1%}")
        print(f"  å‹ç‡: {result['win_rate']:.1%}")
        print(f"  å–å¼•æ•°: {result['trades']}å›")
        print(f"  é‡è¦ç‰¹å¾´é‡: {', '.join([f[0] for f in result['top_features']])}")

    # æœ€é©ã‚»ãƒƒãƒˆæ±ºå®š
    # ç·åˆã‚¹ã‚³ã‚¢ = F1ã‚¹ã‚³ã‚¢ * 0.4 + å–å¼•åç›Š * 0.4 + äºˆæ¸¬å¤šæ§˜æ€§ * 0.2
    df_results["composite_score"] = (
        df_results["f1_score"] * 0.4
        + df_results["total_return"] * 0.4
        + df_results["pred_std"] * 0.2
    )

    best_set = df_results.loc[df_results["composite_score"].idxmax()]

    print(f"\nğŸ† æ¨å¥¨ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ: {best_set['set_name'].upper()}")
    print(f"   ç‰¹å¾´é‡æ•°: {best_set['n_features']}å€‹")
    print(f"   ç·åˆã‚¹ã‚³ã‚¢: {best_set['composite_score']:.3f}")
    print(f"   F1ã‚¹ã‚³ã‚¢: {best_set['f1_score']:.1%}")
    print(f"   å–å¼•åç›Š: {best_set['total_return']:.1%}")
    print(f"   äºˆæ¸¬å¤šæ§˜æ€§: {best_set['pred_std']:.3f}")

    return df_results, best_set


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    # æ¯”è¼ƒå®Ÿè¡Œ
    results = compare_feature_sets()
    if results is None:
        return

    # çµæœåˆ†æ
    df_results, best_set = analyze_comparison_results(results)

    # çµæœä¿å­˜
    results_dir = Path("results/feature_comparison")
    results_dir.mkdir(parents=True, exist_ok=True)

    comparison_path = results_dir / "quick_comparison_results.json"
    with open(comparison_path, "w") as f:
        json.dump(results, f, indent=2, default=str)

    best_set_path = results_dir / "recommended_feature_set.json"
    with open(best_set_path, "w") as f:
        json.dump(best_set.to_dict(), f, indent=2, default=str)

    logger.info(f"\nçµæœä¿å­˜:")
    logger.info(f"  æ¯”è¼ƒçµæœ: {comparison_path}")
    logger.info(f"  æ¨å¥¨ã‚»ãƒƒãƒˆ: {best_set_path}")

    print("\n" + "=" * 80)
    print("ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—")
    print("=" * 80)
    print(f"1. æ¨å¥¨ã‚»ãƒƒãƒˆï¼ˆ{best_set['set_name']}ï¼‰ã§è©³ç´°ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("2. æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ»ä¿å­˜")
    print("3. ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆâ†’bitbankå®Ÿå–å¼•ãƒ†ã‚¹ãƒˆ")

    # æ¨å¥¨ç‰¹å¾´é‡ãƒªã‚¹ãƒˆè¡¨ç¤º
    if best_set["set_name"] == "essential_26":
        print("\nğŸ“‹ æ¨å¥¨ç‰¹å¾´é‡ãƒªã‚¹ãƒˆï¼ˆ26å€‹ï¼‰:")
        feature_sets = None
        sets_path = Path("results/feature_analysis/optimized_feature_sets.json")
        if sets_path.exists():
            with open(sets_path, "r") as f:
                feature_sets = json.load(f)
            essential_features = feature_sets["essential_features"]
            for i, feature in enumerate(essential_features, 1):
                print(f"  {i:2d}. {feature}")


if __name__ == "__main__":
    main()
