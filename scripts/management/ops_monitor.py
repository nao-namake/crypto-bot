#!/usr/bin/env python3
"""
crypto-bot æ–°ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒçŠ¶æ³ç¢ºèªã‚·ã‚¹ãƒ†ãƒ ï¼ˆPhase 12-3ç‰ˆãƒ»BaseAnalyzerç¶™æ‰¿ï¼‰.

CIå¾Œ/æœ¬ç•ªé‹ç”¨æ™‚ã®è©³ç´°è¨ºæ–­ãƒ„ãƒ¼ãƒ«ã€‚BaseAnalyzerç¶™æ‰¿ã«ã‚ˆã‚Šé‡è¤‡ã‚³ãƒ¼ãƒ‰å‰Šé™¤ã€‚
4ãƒ•ã‚§ãƒ¼ã‚ºã§ã®åŒ…æ‹¬çš„ç¨¼åƒçŠ¶æ³ç¢ºèªãƒ»å•é¡Œæ¤œå‡ºãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

ä½¿ç”¨æ–¹æ³•:
    python scripts/management/operational_status_checker.py [--verbose] [--save-report]
    python scripts/management/dev_check.py operational  # çµ±åˆCLIçµŒç”±ï¼ˆæ¨å¥¨ï¼‰

ãƒ•ã‚§ãƒ¼ã‚ºæ§‹æˆ:
    Phase 1: ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»åŸºç›¤ç¢ºèªï¼ˆGCPãƒ»APIãƒ»ãƒªã‚½ãƒ¼ã‚¹ï¼‰
    Phase 2: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‹•ä½œç¢ºèªï¼ˆãƒ­ã‚°ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»MLï¼‰
    Phase 3: éš ã‚ŒãŸå•é¡Œæ¤œå‡ºï¼ˆã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»ç•°å¸¸æ¤œçŸ¥ï¼‰
    Phase 4: ç·åˆåˆ¤å®šãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ.
"""

import json
import logging
import re
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

# base_analyzer.pyæ´»ç”¨
sys.path.append(str(Path(__file__).parent.parent))
from analytics.base_analyzer import BaseAnalyzer

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(PROJECT_ROOT / "logs" / "operational_status.log", encoding="utf-8"),
    ],
)
logger = logging.getLogger(__name__)


class NewSystemOperationalStatusChecker(BaseAnalyzer):
    """æ–°ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒçŠ¶æ³ç¢ºèªã‚·ã‚¹ãƒ†ãƒ ï¼ˆPhase 12-3ç‰ˆãƒ»BaseAnalyzerç¶™æ‰¿ï¼‰."""

    def __init__(self, config_path: str = None):
        """åˆæœŸåŒ–å‡¦ç†ï¼ˆBaseAnalyzerç¶™æ‰¿ç‰ˆï¼‰."""
        # BaseAnalyzeråˆæœŸåŒ–
        super().__init__(output_dir="logs/operational_status")
        self.config_path = config_path or str(
            PROJECT_ROOT / "scripts" / "management" / "status_config.json"
        )
        self.config = self.load_config()
        self.report_dir = PROJECT_ROOT / "logs" / "operational_reports"
        self.report_dir.mkdir(exist_ok=True, parents=True)

        # å®Ÿè¡Œçµæœä¿å­˜
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "system_version": "Phase 10 - æ–°ã‚·ã‚¹ãƒ†ãƒ ",
            "phases": {},
            "overall_status": "UNKNOWN",
            "overall_score": 0,
            "critical_issues": [],
            "recommendations": [],
        }

        # æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨è¨­å®š
        self.new_system_paths = {
            "core": PROJECT_ROOT / "src" / "core",
            "data": PROJECT_ROOT / "src" / "data",
            "features": PROJECT_ROOT / "src" / "features",
            "strategies": PROJECT_ROOT / "src" / "strategies",
            "ml": PROJECT_ROOT / "src" / "ml",
            "trading": PROJECT_ROOT / "src" / "trading",
            "scripts": PROJECT_ROOT / "scripts",
            "models": PROJECT_ROOT / "models",
            "tests": PROJECT_ROOT / "tests",
        }

    def load_config(self) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆæ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨ï¼‰."""
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                config = json.load(f)
                logger.info(f"âœ… Configuration loaded from {self.config_path}")
                return config
        except Exception as e:
            logger.warning(f"âš ï¸ Config load failed, using defaults: {e}")
            return self._get_default_config()

    def _get_default_config(self) -> Dict:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆæ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨ï¼‰."""
        return {
            "check_phases": {
                "phase1": {"name": "ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»åŸºç›¤ç¢ºèª", "weight": 25},
                "phase2": {"name": "ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‹•ä½œç¢ºèª", "weight": 30},
                "phase3": {"name": "éš ã‚ŒãŸå•é¡Œæ¤œå‡º", "weight": 30},
                "phase4": {"name": "ç·åˆåˆ¤å®šãƒ»ãƒ¬ãƒãƒ¼ãƒˆ", "weight": 15},
            },
            "thresholds": {
                "critical_score": 70,
                "warning_score": 85,
                "max_response_time_seconds": 5.0,
                "min_test_success_rate": 0.95,
            },
            "new_system_patterns": {
                "import_errors": ["ModuleNotFoundError", "ImportError"],
                "test_failures": ["FAILED", "ERROR"],
                "ml_issues": ["prediction.*failed", "model.*error"],
            },
        }

    # ===== Phase 1: ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»åŸºç›¤ç¢ºèªã‚·ã‚¹ãƒ†ãƒ  =====

    def check_project_structure(self) -> Dict[str, Any]:
        """æ–°ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ç¢ºèª."""
        logger.info("ğŸ” Checking new system project structure...")

        try:
            _ = []  # structure_issues - å¿…è¦æ™‚ã«ä½¿ç”¨
            required_paths = [
                "src/core",
                "src/data",
                "src/features",
                "src/strategies",
                "src/ml",
                "src/trading",
                "scripts/management",
                "models/production",
                "tests/unit",
                "config",
            ]

            missing_paths = []
            for path_str in required_paths:
                path = PROJECT_ROOT / path_str
                if not path.exists():
                    missing_paths.append(path_str)

            if missing_paths:
                return {
                    "status": "critical",
                    "missing_paths": missing_paths,
                    "details": f"Missing {len(missing_paths)} required directories",
                }

            # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
            critical_files = [
                "src/core/config.py",
                "src/data/data_pipeline.py",
                "scripts/management/dev_check.py",
                "models/production/production_ensemble.pkl",
            ]

            missing_files = []
            for file_str in critical_files:
                file_path = PROJECT_ROOT / file_str
                if not file_path.exists():
                    missing_files.append(file_str)

            if missing_files:
                return {
                    "status": "warning",
                    "missing_files": missing_files,
                    "details": f"Missing {len(missing_files)} critical files",
                }

            return {
                "status": "healthy",
                "paths_checked": len(required_paths),
                "files_checked": len(critical_files),
                "details": "Project structure intact",
            }

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "details": f"Structure check failed: {type(e).__name__}",
            }

    def check_system_imports(self) -> Dict[str, Any]:
        """æ–°ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª."""
        logger.info("ğŸ” Checking new system imports...")

        try:
            import_tests = [
                ("src.core.config", "Config"),
                ("src.data.data_pipeline", "DataPipeline"),
                ("src.features.technical", "TechnicalIndicators"),
                ("src.strategies.base.strategy_manager", "StrategyManager"),
                ("src.ml.ensemble.ensemble_model", "EnsembleModel"),
                ("src.trading.risk", "RiskManager"),
            ]

            failed_imports = []
            for module_name, class_name in import_tests:
                try:
                    module = __import__(module_name, fromlist=[class_name])
                    getattr(module, class_name)
                except Exception as e:
                    failed_imports.append(
                        {
                            "module": module_name,
                            "class": class_name,
                            "error": str(e)[:100],
                        }
                    )

            if failed_imports:
                return {
                    "status": "critical",
                    "failed_imports": failed_imports,
                    "success_rate": (len(import_tests) - len(failed_imports)) / len(import_tests),
                    "details": f"{len(failed_imports)} import failures detected",
                }

            return {
                "status": "healthy",
                "imports_tested": len(import_tests),
                "success_rate": 1.0,
                "details": "All system imports successful",
            }

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "details": f"Import check failed: {type(e).__name__}",
            }

    def check_ml_models_availability(self) -> Dict[str, Any]:
        """MLãƒ¢ãƒ‡ãƒ«å¯ç”¨æ€§ç¢ºèª."""
        logger.info("ğŸ” Checking ML models availability...")

        try:
            models_path = PROJECT_ROOT / "models" / "production"

            # é‡è¦ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
            required_models = [
                "production_ensemble.pkl",
                "production_model_metadata.json",
            ]

            missing_models = []
            for model_file in required_models:
                model_path = models_path / model_file
                if not model_path.exists():
                    missing_models.append(model_file)

            if missing_models:
                return {
                    "status": "critical",
                    "missing_models": missing_models,
                    "details": f"Missing {len(missing_models)} production models",
                }

            # ProductionEnsembleèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
            try:
                import pickle

                with open(models_path / "production_ensemble.pkl", "rb") as f:
                    model = pickle.load(f)

                # åŸºæœ¬çš„ãªå‹•ä½œç¢ºèª
                import numpy as np

                test_features = np.random.random((1, 12))  # 12ç‰¹å¾´é‡
                _ = model.predict(test_features)  # predictions - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®ã¿

                return {
                    "status": "healthy",
                    "models_checked": len(required_models),
                    "model_type": type(model).__name__,
                    "prediction_test": "success",
                    "details": "ML models operational",
                }

            except Exception as model_error:
                return {
                    "status": "warning",
                    "model_load_error": str(model_error)[:100],
                    "details": "Model files exist but loading failed",
                }

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "details": f"Model check failed: {type(e).__name__}",
            }

    def check_test_system_health(self) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ç¢ºèª."""
        logger.info("ğŸ” Checking test system health...")

        try:
            # è»½é‡ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
            cmd = [
                "python",
                "-m",
                "pytest",
                "tests/unit/",
                "--tb=no",
                "-q",
                "--maxfail=5",
            ]

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=60,
                cwd=PROJECT_ROOT,
            )

            if result.returncode == 0:
                # æˆåŠŸæ™‚ã®è§£æ
                output = result.stdout
                passed_match = re.search(r"(\d+) passed", output)
                passed_count = int(passed_match.group(1)) if passed_match else 0

                return {
                    "status": "healthy",
                    "tests_passed": passed_count,
                    "exit_code": result.returncode,
                    "details": f"{passed_count} tests passed successfully",
                }

            else:
                # å¤±æ•—æ™‚ã®è§£æ
                output = result.stdout + result.stderr
                failed_match = re.search(r"(\d+) failed", output)
                failed_count = int(failed_match.group(1)) if failed_match else 0

                if failed_count <= 5:  # è»½å¾®ãªå¤±æ•—
                    status = "warning"
                else:  # é‡å¤§ãªå¤±æ•—
                    status = "critical"

                return {
                    "status": status,
                    "tests_failed": failed_count,
                    "exit_code": result.returncode,
                    "details": f"{failed_count} test failures detected",
                }

        except subprocess.TimeoutExpired:
            return {
                "status": "warning",
                "error": "Test execution timeout",
                "details": "Tests took too long to execute",
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "details": f"Test system check failed: {type(e).__name__}",
            }

    def run_phase1_infrastructure_checks(self) -> Dict[str, Any]:
        """Phase 1: ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»åŸºç›¤ç¢ºèªã‚’å®Ÿè¡Œ."""
        logger.info("ğŸš€ === Phase 1: ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»åŸºç›¤ç¢ºèªï¼ˆæ–°ã‚·ã‚¹ãƒ†ãƒ ï¼‰ ===")

        phase1_results = {
            "phase_name": "ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»åŸºç›¤ç¢ºèª",
            "weight": self.config["check_phases"]["phase1"]["weight"],
            "checks": {},
            "score": 0,
            "status": "unknown",
            "issues": [],
        }

        # å„ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        checks = {
            "project_structure": self.check_project_structure(),
            "system_imports": self.check_system_imports(),
            "ml_models_availability": self.check_ml_models_availability(),
            "test_system_health": self.check_test_system_health(),
        }

        phase1_results["checks"] = checks

        # Phase 1ã‚¹ã‚³ã‚¢è¨ˆç®—
        check_scores = []
        for check_name, check_result in checks.items():
            status = check_result.get("status", "error")

            if status == "healthy":
                score = 100
            elif status == "warning":
                score = 70
            elif status == "critical":
                score = 30
            else:  # error
                score = 0

            check_scores.append(score)

            # é‡å¤§ãªå•é¡Œã‚’è¨˜éŒ²
            if score < 70:
                phase1_results["issues"].append(
                    {
                        "check": check_name,
                        "severity": "CRITICAL" if score <= 30 else "HIGH",
                        "message": check_result.get(
                            "error",
                            check_result.get("details", "Unknown issue"),
                        ),
                        "score": score,
                    }
                )

        # Phase 1ç·åˆã‚¹ã‚³ã‚¢
        phase1_results["score"] = sum(check_scores) / len(check_scores) if check_scores else 0

        # Phase 1ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
        if phase1_results["score"] >= 80:
            phase1_results["status"] = "healthy"
        elif phase1_results["score"] >= 60:
            phase1_results["status"] = "warning"
        else:
            phase1_results["status"] = "critical"

        logger.info(
            f"âœ… Phase 1 å®Œäº†: ã‚¹ã‚³ã‚¢ {phase1_results['score']:.1f}/100 ({phase1_results['status']})"
        )

        self.results["phases"]["phase1"] = phase1_results
        return phase1_results

    # ===== Phase 2: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‹•ä½œç¢ºèªã‚·ã‚¹ãƒ†ãƒ  =====

    def check_data_pipeline_health(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¥å…¨æ€§ç¢ºèª."""
        logger.info("ğŸ” Checking data pipeline health...")

        try:
            # DataPipelineåŸºæœ¬å‹•ä½œç¢ºèª
            from src.core.config import Config
            from src.data.data_pipeline import DataPipeline

            config = Config()
            _ = DataPipeline(config)  # pipeline - ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ãƒ†ã‚¹ãƒˆã®ã¿

            # åŸºæœ¬çš„ãªæ¥ç¶šãƒ†ã‚¹ãƒˆï¼ˆè»½é‡ï¼‰
            # æ³¨æ„: å®Ÿéš›ã®APIå‘¼ã³å‡ºã—ã¯é¿ã‘ã€ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã®ã¿

            return {
                "status": "healthy",
                "pipeline_initialized": True,
                "config_loaded": True,
                "details": "Data pipeline components operational",
            }

        except Exception as e:
            return {
                "status": "critical",
                "error": str(e),
                "details": f"Data pipeline check failed: {type(e).__name__}",
            }

    def check_ml_prediction_system(self) -> Dict[str, Any]:
        """MLäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ç¢ºèª."""
        logger.info("ğŸ” Checking ML prediction system...")

        try:
            # EnsembleModelåŸºæœ¬å‹•ä½œç¢ºèª
            from src.ml.ensemble.ensemble_model import EnsembleModel

            ensemble = EnsembleModel()

            # 12ç‰¹å¾´é‡ã§ã®ãƒ†ã‚¹ãƒˆäºˆæ¸¬
            import numpy as np

            test_features = np.random.random((5, 12))

            # äºˆæ¸¬å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
            predictions = ensemble.predict(test_features)
            probabilities = ensemble.predict_proba(test_features)

            # äºˆæ¸¬çµæœã®å¦¥å½“æ€§ç¢ºèª
            if len(predictions) == 5 and len(probabilities) == 5:
                return {
                    "status": "healthy",
                    "predictions_generated": len(predictions),
                    "feature_count": 12,
                    "model_type": type(ensemble).__name__,
                    "details": "ML prediction system operational",
                }
            else:
                return {
                    "status": "warning",
                    "predictions_generated": len(predictions),
                    "details": "ML predictions inconsistent",
                }

        except Exception as e:
            return {
                "status": "critical",
                "error": str(e),
                "details": f"ML prediction check failed: {type(e).__name__}",
            }

    def check_strategy_system_health(self) -> Dict[str, Any]:
        """æˆ¦ç•¥ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ç¢ºèª."""
        logger.info("ğŸ” Checking strategy system health...")

        try:
            # StrategyManageråŸºæœ¬å‹•ä½œç¢ºèª
            from src.core.config import Config
            from src.strategies.base.strategy_manager import StrategyManager

            config = Config()
            strategy_manager = StrategyManager(config)

            # æˆ¦ç•¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ç¢ºèª
            strategies_count = (
                len(strategy_manager.strategies) if hasattr(strategy_manager, "strategies") else 0
            )

            return {
                "status": "healthy",
                "manager_initialized": True,
                "strategies_loaded": strategies_count,
                "details": f"Strategy system operational with {strategies_count} strategies",
            }

        except Exception as e:
            return {
                "status": "critical",
                "error": str(e),
                "details": f"Strategy system check failed: {type(e).__name__}",
            }

    def check_trading_risk_system(self) -> Dict[str, Any]:
        """å–å¼•ãƒ»ãƒªã‚¹ã‚¯ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ç¢ºèª."""
        logger.info("ğŸ” Checking trading risk system...")

        try:
            # RiskManageråŸºæœ¬å‹•ä½œç¢ºèª
            from src.core.config import Config
            from src.trading.risk import RiskManager

            config = Config()
            risk_manager = RiskManager(config)

            # KellyåŸºæº–ãƒ†ã‚¹ãƒˆï¼ˆãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼‰
            test_data = {
                "win_rate": 0.6,
                "avg_win": 1.5,
                "avg_loss": 1.0,
                "current_balance": 10000,
            }

            kelly_fraction = risk_manager.calculate_kelly_criterion(
                test_data["win_rate"],
                test_data["avg_win"],
                test_data["avg_loss"],
            )

            if 0 <= kelly_fraction <= 1:
                return {
                    "status": "healthy",
                    "kelly_calculation": "success",
                    "kelly_fraction": kelly_fraction,
                    "details": "Risk management system operational",
                }
            else:
                return {
                    "status": "warning",
                    "kelly_calculation": "anomaly",
                    "kelly_fraction": kelly_fraction,
                    "details": "Risk calculations producing unusual values",
                }

        except Exception as e:
            return {
                "status": "critical",
                "error": str(e),
                "details": f"Risk system check failed: {type(e).__name__}",
            }

    def run_phase2_application_checks(self) -> Dict[str, Any]:
        """Phase 2: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‹•ä½œç¢ºèªã‚’å®Ÿè¡Œ."""
        logger.info("ğŸš€ === Phase 2: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‹•ä½œç¢ºèªï¼ˆæ–°ã‚·ã‚¹ãƒ†ãƒ ï¼‰ ===")

        phase2_results = {
            "phase_name": "ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‹•ä½œç¢ºèª",
            "weight": self.config["check_phases"]["phase2"]["weight"],
            "checks": {},
            "score": 0,
            "status": "unknown",
            "issues": [],
        }

        # å„ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        checks = {
            "data_pipeline_health": self.check_data_pipeline_health(),
            "ml_prediction_system": self.check_ml_prediction_system(),
            "strategy_system_health": self.check_strategy_system_health(),
            "trading_risk_system": self.check_trading_risk_system(),
        }

        phase2_results["checks"] = checks

        # Phase 2ã‚¹ã‚³ã‚¢è¨ˆç®—
        check_scores = []
        for check_name, check_result in checks.items():
            status = check_result.get("status", "error")

            if status == "healthy":
                score = 100
            elif status == "warning":
                score = 60
            elif status == "critical":
                score = 20
            else:  # error
                score = 0

            check_scores.append(score)

            # é‡å¤§ãªå•é¡Œã‚’è¨˜éŒ²
            if score < 60:
                phase2_results["issues"].append(
                    {
                        "check": check_name,
                        "severity": "CRITICAL" if score <= 20 else "HIGH",
                        "message": check_result.get(
                            "error",
                            check_result.get("details", "Unknown issue"),
                        ),
                        "score": score,
                    }
                )

        # Phase 2ç·åˆã‚¹ã‚³ã‚¢
        phase2_results["score"] = sum(check_scores) / len(check_scores) if check_scores else 0

        # Phase 2ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
        if phase2_results["score"] >= 80:
            phase2_results["status"] = "healthy"
        elif phase2_results["score"] >= 60:
            phase2_results["status"] = "warning"
        else:
            phase2_results["status"] = "critical"

        logger.info(
            f"âœ… Phase 2 å®Œäº†: ã‚¹ã‚³ã‚¢ {phase2_results['score']:.1f}/100 ({phase2_results['status']})"
        )

        self.results["phases"]["phase2"] = phase2_results
        return phase2_results

    # ===== Phase 3: éš ã‚ŒãŸå•é¡Œæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ  =====

    def check_critical_error_patterns(self) -> Dict[str, Any]:
        """é‡è¦ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼ˆæ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨ï¼‰."""
        logger.info("ğŸ” Checking critical error patterns...")

        detected_patterns = []

        try:
            # æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨é‡è¦ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ5å€‹ã«çµã‚‹ï¼‰
            patterns_to_check = [
                {
                    "id": "import_errors",
                    "description": "ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ãƒ»ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸åœ¨",
                    "severity": "CRITICAL",
                    "check_method": self._check_import_error_pattern,
                },
                {
                    "id": "ml_model_failures",
                    "description": "MLãƒ¢ãƒ‡ãƒ«äºˆæ¸¬å¤±æ•—",
                    "severity": "HIGH",
                    "check_method": self._check_ml_failure_pattern,
                },
                {
                    "id": "test_system_degradation",
                    "description": "ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ åŠ£åŒ–",
                    "severity": "HIGH",
                    "check_method": self._check_test_degradation_pattern,
                },
                {
                    "id": "config_inconsistency",
                    "description": "è¨­å®šä¸æ•´åˆ",
                    "severity": "MEDIUM",
                    "check_method": self._check_config_inconsistency_pattern,
                },
                {
                    "id": "performance_anomaly",
                    "description": "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç•°å¸¸",
                    "severity": "MEDIUM",
                    "check_method": self._check_performance_anomaly_pattern,
                },
            ]

            for pattern in patterns_to_check:
                try:
                    detection_result = pattern["check_method"]()
                    if detection_result:
                        detected_patterns.append(
                            {
                                "id": pattern["id"],
                                "description": pattern["description"],
                                "severity": pattern["severity"],
                                "detection_details": detection_result,
                            }
                        )
                except Exception as e:
                    logger.warning(f"Pattern check failed for {pattern['id']}: {e}")

            if detected_patterns:
                critical_count = len([p for p in detected_patterns if p["severity"] == "CRITICAL"])
                high_count = len([p for p in detected_patterns if p["severity"] == "HIGH"])

                if critical_count > 0:
                    status = "critical"
                elif high_count > 0:
                    status = "warning"
                else:
                    status = "info"

                return {
                    "status": status,
                    "patterns_detected": len(detected_patterns),
                    "critical_patterns": critical_count,
                    "high_patterns": high_count,
                    "detected_patterns": detected_patterns,
                    "details": f"{len(detected_patterns)} error patterns detected",
                }
            else:
                return {
                    "status": "healthy",
                    "patterns_detected": 0,
                    "detected_patterns": [],
                    "details": "No critical error patterns detected",
                }

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "details": f"Pattern detection failed: {type(e).__name__}",
            }

    def _check_import_error_pattern(self) -> Optional[Dict]:
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º."""
        try:
            # æœ€è¿‘ã®ãƒ­ã‚°ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ã‚’æ¤œç´¢
            log_file = PROJECT_ROOT / "logs" / "operational_status.log"
            if log_file.exists():
                with open(log_file, "r", encoding="utf-8") as f:
                    recent_logs = f.readlines()[-100:]  # æœ€æ–°100è¡Œ

                import_errors = []
                for line in recent_logs:
                    if any(
                        pattern in line
                        for pattern in [
                            "ModuleNotFoundError",
                            "ImportError",
                            "cannot import",
                        ]
                    ):
                        import_errors.append(line.strip())

                if import_errors:
                    return {
                        "detected": True,
                        "error_count": len(import_errors),
                        "sample_errors": import_errors[:3],
                    }
        except Exception:
            pass
        return None

    def _check_ml_failure_pattern(self) -> Optional[Dict]:
        """MLãƒ¢ãƒ‡ãƒ«å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º."""
        try:
            # MLãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬å‹•ä½œãƒ†ã‚¹ãƒˆ
            import numpy as np

            from src.ml.ensemble.ensemble_model import EnsembleModel

            ensemble = EnsembleModel()
            test_features = np.random.random((3, 12))

            try:
                predictions = ensemble.predict(test_features)
                probabilities = ensemble.predict_proba(test_features)

                # äºˆæ¸¬å€¤ã®ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                if len(set(predictions)) == 1:  # å…¨ã¦åŒã˜äºˆæ¸¬
                    return {
                        "detected": True,
                        "anomaly_type": "static_predictions",
                        "evidence": f"All predictions identical: {predictions[0]}",
                    }

                # ç¢ºç‡å€¤ã®ç•°å¸¸ãƒã‚§ãƒƒã‚¯
                prob_values = probabilities[:, 1] if probabilities.shape[1] > 1 else probabilities
                if all(p < 0.1 for p in prob_values) or all(p > 0.9 for p in prob_values):
                    return {
                        "detected": True,
                        "anomaly_type": "extreme_probabilities",
                        "evidence": f"Extreme probability values: {prob_values.tolist()}",
                    }

            except Exception as prediction_error:
                return {
                    "detected": True,
                    "anomaly_type": "prediction_failure",
                    "evidence": f"Prediction failed: {str(prediction_error)[:100]}",
                }

        except Exception:
            pass
        return None

    def _check_test_degradation_pattern(self) -> Optional[Dict]:
        """ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ åŠ£åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º."""
        try:
            # ç°¡æ˜“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã§å¤±æ•—ç‡ç¢ºèª
            cmd = [
                "python",
                "-m",
                "pytest",
                "tests/unit/",
                "--tb=no",
                "-q",
                "--maxfail=10",
            ]
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=30,
                cwd=PROJECT_ROOT,
            )

            if result.returncode != 0:
                output = result.stdout + result.stderr
                failed_match = re.search(r"(\d+) failed", output)
                failed_count = int(failed_match.group(1)) if failed_match else 0

                if failed_count >= 10:  # 10å€‹ä»¥ä¸Šã®å¤±æ•—
                    return {
                        "detected": True,
                        "failed_tests": failed_count,
                        "evidence": f"{failed_count} test failures detected",
                    }
        except Exception:
            pass
        return None

    def _check_config_inconsistency_pattern(self) -> Optional[Dict]:
        """è¨­å®šä¸æ•´åˆãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º."""
        try:
            # é‡è¦è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
            config_files = [
                PROJECT_ROOT / "config" / "base.yaml",
                PROJECT_ROOT / "config" / "production.yaml",
            ]

            missing_configs = []
            for config_file in config_files:
                if not config_file.exists():
                    missing_configs.append(config_file.name)

            if missing_configs:
                return {
                    "detected": True,
                    "missing_configs": missing_configs,
                    "evidence": f"Missing config files: {missing_configs}",
                }
        except Exception:
            pass
        return None

    def _check_performance_anomaly_pattern(self) -> Optional[Dict]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º."""
        try:
            # ç°¡æ˜“çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            import time

            start_time = time.time()

            # è»½é‡ãªå‡¦ç†æ™‚é–“æ¸¬å®š
            import numpy as np
            import pandas as pd

            from src.features.technical import TechnicalIndicators

            # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—
            dummy_data = pd.DataFrame(
                {
                    "close": np.random.random(100),
                    "high": np.random.random(100),
                    "low": np.random.random(100),
                    "volume": np.random.random(100),
                }
            )

            tech_indicators = TechnicalIndicators()
            _ = tech_indicators.calculate_rsi(dummy_data["close"])

            processing_time = time.time() - start_time

            if processing_time > 2.0:  # 2ç§’ä»¥ä¸Šã¯ç•°å¸¸
                return {
                    "detected": True,
                    "processing_time": processing_time,
                    "evidence": f"Slow processing detected: {processing_time:.2f}s",
                }

        except Exception:
            pass
        return None

    def check_file_system_integrity(self) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ æ•´åˆæ€§ç¢ºèª."""
        logger.info("ğŸ” Checking file system integrity...")

        try:
            integrity_issues = []

            # é‡è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚µã‚¤ã‚ºç¢ºèª
            important_dirs = ["src", "scripts", "models", "tests", "config"]

            for dir_name in important_dirs:
                dir_path = PROJECT_ROOT / dir_name
                if dir_path.exists():
                    file_count = len(list(dir_path.rglob("*.py")))
                    if file_count == 0:
                        integrity_issues.append(f"{dir_name}: No Python files found")
                else:
                    integrity_issues.append(f"{dir_name}: Directory missing")

            # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª
            logs_dir = PROJECT_ROOT / "logs"
            if not logs_dir.exists():
                integrity_issues.append("logs: Directory missing")

            if integrity_issues:
                return {
                    "status": "warning",
                    "integrity_issues": integrity_issues,
                    "issues_count": len(integrity_issues),
                    "details": f"{len(integrity_issues)} file system issues found",
                }

            return {
                "status": "healthy",
                "directories_checked": len(important_dirs),
                "integrity_issues": [],
                "details": "File system integrity OK",
            }

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "details": f"File system check failed: {type(e).__name__}",
            }

    def run_phase3_hidden_issues_detection(self) -> Dict[str, Any]:
        """Phase 3: éš ã‚ŒãŸå•é¡Œæ¤œå‡ºã‚’å®Ÿè¡Œ."""
        logger.info("ğŸš€ === Phase 3: éš ã‚ŒãŸå•é¡Œæ¤œå‡ºï¼ˆæ–°ã‚·ã‚¹ãƒ†ãƒ ï¼‰ ===")

        phase3_results = {
            "phase_name": "éš ã‚ŒãŸå•é¡Œæ¤œå‡º",
            "weight": self.config["check_phases"]["phase3"]["weight"],
            "checks": {},
            "score": 0,
            "status": "unknown",
            "issues": [],
        }

        # å„ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        checks = {
            "critical_error_patterns": self.check_critical_error_patterns(),
            "file_system_integrity": self.check_file_system_integrity(),
        }

        phase3_results["checks"] = checks

        # Phase 3ã‚¹ã‚³ã‚¢è¨ˆç®—
        check_scores = []
        for check_name, check_result in checks.items():
            status = check_result.get("status", "error")

            if status == "healthy":
                score = 100
            elif status == "info":
                score = 85
            elif status == "warning":
                score = 50
            elif status == "critical":
                score = 10
            else:  # error
                score = 0

            check_scores.append(score)

            # é‡å¤§ãªå•é¡Œã‚’è¨˜éŒ²
            if score < 70:
                severity = "CRITICAL" if score <= 20 else "HIGH" if score <= 50 else "MEDIUM"
                phase3_results["issues"].append(
                    {
                        "check": check_name,
                        "severity": severity,
                        "message": check_result.get(
                            "error",
                            check_result.get("details", "Unknown issue"),
                        ),
                        "score": score,
                    }
                )

        # Phase 3ç·åˆã‚¹ã‚³ã‚¢
        phase3_results["score"] = sum(check_scores) / len(check_scores) if check_scores else 0

        # Phase 3ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
        if phase3_results["score"] >= 80:
            phase3_results["status"] = "healthy"
        elif phase3_results["score"] >= 60:
            phase3_results["status"] = "warning"
        else:
            phase3_results["status"] = "critical"

        logger.info(
            f"âœ… Phase 3 å®Œäº†: ã‚¹ã‚³ã‚¢ {phase3_results['score']:.1f}/100 ({phase3_results['status']})"
        )

        self.results["phases"]["phase3"] = phase3_results
        return phase3_results

    # ===== Phase 4: ç·åˆåˆ¤å®šãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ  =====

    def calculate_overall_assessment(self) -> Dict[str, Any]:
        """ç·åˆè©•ä¾¡è¨ˆç®—."""
        logger.info("ğŸ” Calculating overall assessment...")

        try:
            phase_scores = {}
            total_score = 0
            total_weight = 0

            # å„ãƒ•ã‚§ãƒ¼ã‚ºã®ã‚¹ã‚³ã‚¢åé›†
            for phase_name, phase_data in self.results["phases"].items():
                if phase_name != "phase4":
                    weight = phase_data.get("weight", 25)
                    score = phase_data.get("score", 0)
                    status = phase_data.get("status", "unknown")

                    phase_scores[phase_name] = {
                        "score": score,
                        "weight": weight,
                        "status": status,
                        "weighted_score": score * weight,
                    }

                    total_score += score * weight
                    total_weight += weight

            overall_score = total_score / total_weight if total_weight > 0 else 0

            # ç·åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
            if overall_score >= 90:
                overall_status = "excellent"
            elif overall_score >= 80:
                overall_status = "healthy"
            elif overall_score >= 60:
                overall_status = "warning"
            elif overall_score >= 40:
                overall_status = "degraded"
            else:
                overall_status = "critical"

            # ã‚°ãƒ­ãƒ¼ãƒãƒ«çµæœã«åæ˜ 
            self.results["overall_score"] = overall_score
            self.results["overall_status"] = overall_status

            return {
                "status": "healthy",
                "overall_score": overall_score,
                "overall_status": overall_status,
                "phase_scores": phase_scores,
                "total_weight": total_weight,
                "details": f"Overall score: {overall_score:.1f}/100 ({overall_status})",
            }

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "details": f"Assessment calculation failed: {type(e).__name__}",
            }

    def generate_recommendations(self) -> Dict[str, Any]:
        """æ”¹å–„ææ¡ˆç”Ÿæˆ."""
        logger.info("ğŸ” Generating recommendations...")

        try:
            recommendations = []
            urgent_actions = []

            # å…¨ãƒ•ã‚§ãƒ¼ã‚ºã‹ã‚‰å•é¡Œã‚’åé›†
            all_issues = []
            for phase_name, phase_data in self.results["phases"].items():
                if phase_name != "phase4":
                    phase_issues = phase_data.get("issues", [])
                    for issue in phase_issues:
                        issue["phase"] = phase_name
                        all_issues.append(issue)

            # å•é¡Œã«å¿œã˜ãŸæ¨å¥¨äº‹é …ç”Ÿæˆ
            for issue in all_issues:
                check = issue.get("check", "")
                severity = issue.get("severity", "")

                actions = self._generate_actions_for_issue(
                    check, severity, issue.get("message", "")
                )

                for action in actions:
                    if severity == "CRITICAL":
                        action["priority"] = "IMMEDIATE"
                        urgent_actions.append(action)
                    else:
                        action["priority"] = severity
                        recommendations.append(action)

            # é‡è¤‡é™¤å»
            unique_urgent = self._deduplicate_actions(urgent_actions)
            unique_recommendations = self._deduplicate_actions(recommendations)

            return {
                "status": "healthy",
                "urgent_actions": unique_urgent,
                "recommendations": unique_recommendations,
                "total_actions": len(unique_urgent) + len(unique_recommendations),
                "details": (
                    f"{len(unique_urgent)} urgent actions, "
                    f"{len(unique_recommendations)} recommendations"
                ),
            }

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "details": (f"Recommendation generation failed: {type(e).__name__}"),
            }

    def _generate_actions_for_issue(self, check: str, severity: str, message: str) -> List[Dict]:
        """å•é¡Œã«å¯¾ã™ã‚‹å…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ."""
        actions = []

        # æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®šç¾©  # noqa: F841
        if "import" in check or "structure" in check:
            actions.append(
                {
                    "action": "Check Python path and module imports",
                    "command": 'python -c "import sys; print(sys.path)"',
                    "category": "System",
                }
            )
        elif "ml" in check or "model" in check:
            actions.append(
                {
                    "action": "Verify ML models and retrain if necessary",
                    "command": "python scripts/management/dev_check.py ml-models",
                    "category": "ML",
                }
            )
        elif "test" in check:
            actions.append(
                {
                    "action": "Run comprehensive test suite",
                    "command": "python scripts/management/dev_check.py validate",
                    "category": "Testing",
                }
            )
        elif "config" in check:
            actions.append(
                {
                    "action": "Check configuration files",
                    "command": "python scripts/management/dev_check.py phase-check",
                    "category": "Configuration",
                }
            )
        else:
            actions.append(
                {
                    "action": f"Investigate {check} issue",
                    "command": f"Review: {message[:50]}...",
                    "category": "General",
                }
            )

        return actions

    def _deduplicate_actions(self, actions: List[Dict]) -> List[Dict]:
        """é‡è¤‡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é™¤å»."""
        seen = set()
        unique = []

        for action in actions:
            key = (action.get("action", ""), action.get("category", ""))
            if key not in seen:
                seen.add(key)
                unique.append(action)

        return unique

    def generate_summary_report(self) -> Dict[str, Any]:
        """è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ."""
        logger.info("ğŸ” Generating summary report...")

        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # JSONè¦ç´„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            summary = {
                "execution_timestamp": self.results["timestamp"],
                "system_version": self.results["system_version"],
                "overall_score": self.results["overall_score"],
                "overall_status": self.results["overall_status"],
                "phase_summary": {},
                "total_issues": 0,
                "critical_issues": 0,
                "recommendations_count": 0,
            }

            # ãƒ•ã‚§ãƒ¼ã‚ºè¦ç´„
            total_issues = 0
            critical_issues = 0

            for phase_name, phase_data in self.results["phases"].items():
                if phase_name != "phase4":
                    phase_issues = phase_data.get("issues", [])
                    phase_critical = len(
                        [i for i in phase_issues if i.get("severity") == "CRITICAL"]
                    )

                    summary["phase_summary"][phase_name] = {
                        "score": phase_data.get("score", 0),
                        "status": phase_data.get("status", "unknown"),
                        "issues_count": len(phase_issues),
                        "critical_count": phase_critical,
                    }

                    total_issues += len(phase_issues)
                    critical_issues += phase_critical

            summary["total_issues"] = total_issues
            summary["critical_issues"] = critical_issues

            # Phase 4çµæœã‹ã‚‰æ¨å¥¨äº‹é …æ•°å–å¾—
            phase4_data = self.results["phases"].get("phase4", {})
            recommendations_data = phase4_data.get("checks", {}).get("recommendations", {})
            urgent_count = len(recommendations_data.get("urgent_actions", []))
            rec_count = len(recommendations_data.get("recommendations", []))
            summary["recommendations_count"] = urgent_count + rec_count

            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            report_path = self.report_dir / f"summary_report_{timestamp}.json"
            with open(report_path, "w", encoding="utf-8") as f:
                json.dump(summary, f, indent=2, ensure_ascii=False, default=str)

            return {
                "status": "healthy",
                "report_path": str(report_path),
                "summary": summary,
                "details": f"Summary report saved: {report_path}",
            }

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "details": f"Summary report generation failed: {type(e).__name__}",
            }

    def run_phase4_overall_assessment(self) -> Dict[str, Any]:
        """Phase 4: ç·åˆåˆ¤å®šãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œ."""
        logger.info("ğŸš€ === Phase 4: ç·åˆåˆ¤å®šãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆæ–°ã‚·ã‚¹ãƒ†ãƒ ï¼‰ ===")

        phase4_results = {
            "phase_name": "ç·åˆåˆ¤å®šãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ",
            "weight": self.config["check_phases"]["phase4"]["weight"],
            "checks": {},
            "score": 0,
            "status": "unknown",
            "issues": [],
        }

        # å„ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        checks = {
            "overall_assessment": self.calculate_overall_assessment(),
            "recommendations": self.generate_recommendations(),
            "summary_report": self.generate_summary_report(),
        }

        phase4_results["checks"] = checks

        # Phase 4ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆåˆ†æå“è³ªãƒ™ãƒ¼ã‚¹ï¼‰
        check_scores = []
        for check_name, check_result in checks.items():
            status = check_result.get("status", "error")

            if status == "healthy":
                score = 100
            elif status == "warning":
                score = 70
            else:  # error
                score = 50  # åˆ†æã‚¨ãƒ©ãƒ¼ã§ã‚‚éƒ¨åˆ†çš„ã«ã‚¹ã‚³ã‚¢ä»˜ä¸

            check_scores.append(score)

            # åˆ†æã«å•é¡ŒãŒã‚ã‚‹å ´åˆã®ã¿issuesã«è¿½åŠ 
            if score < 80:
                phase4_results["issues"].append(
                    {
                        "check": check_name,
                        "severity": "MEDIUM",
                        "message": check_result.get(
                            "error",
                            check_result.get("details", "Analysis issue"),
                        ),
                        "score": score,
                    }
                )

        # Phase 4ç·åˆã‚¹ã‚³ã‚¢
        phase4_results["score"] = sum(check_scores) / len(check_scores) if check_scores else 0

        # Phase 4ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
        if phase4_results["score"] >= 90:
            phase4_results["status"] = "healthy"
        elif phase4_results["score"] >= 70:
            phase4_results["status"] = "warning"
        else:
            phase4_results["status"] = "degraded"

        logger.info(
            f"âœ… Phase 4 å®Œäº†: ã‚¹ã‚³ã‚¢ {phase4_results['score']:.1f}/100 ({phase4_results['status']})"
        )

        self.results["phases"]["phase4"] = phase4_results
        return phase4_results

    # ===== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ =====

    def run_comprehensive_check(self, target_phases: List[str] = None) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ç¨¼åƒçŠ¶æ³ç¢ºèªã‚’å®Ÿè¡Œï¼ˆæ–°ã‚·ã‚¹ãƒ†ãƒ ç‰ˆï¼‰."""
        logger.info("ğŸ¯ === æ–°ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒçŠ¶æ³ç¢ºèªé–‹å§‹ ===")
        logger.info(f"ğŸ“… å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S JST')}")
        logger.info(f"ğŸ¨ ã‚·ã‚¹ãƒ†ãƒ : {self.results['system_version']}")

        # Phase 1å®Ÿè¡Œ
        if not target_phases or "phase1" in target_phases:
            self.run_phase1_infrastructure_checks()

        # Phase 2å®Ÿè¡Œ
        if not target_phases or "phase2" in target_phases:
            self.run_phase2_application_checks()

        # Phase 3å®Ÿè¡Œ
        if not target_phases or "phase3" in target_phases:
            self.run_phase3_hidden_issues_detection()

        # Phase 4å®Ÿè¡Œ
        if not target_phases or "phase4" in target_phases:
            self.run_phase4_overall_assessment()

        self._generate_final_console_report()

        logger.info("ğŸŠ === æ–°ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒçŠ¶æ³ç¢ºèªå®Œäº† ===")
        return self.results

    def _generate_final_console_report(self):
        """æœ€çµ‚ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ."""
        print("\n" + "=" * 60)
        print("ğŸ“Š crypto-bot æ–°ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒçŠ¶æ³ç¢ºèªçµæœ")
        print("=" * 60)
        print(f"ğŸ• å®Ÿè¡Œæ™‚åˆ»: {self.results['timestamp']}")
        print(f"ğŸ¨ ã‚·ã‚¹ãƒ†ãƒ : {self.results['system_version']}")
        print(f"ğŸ¯ ç·åˆã‚¹ã‚³ã‚¢: {self.results['overall_score']:.1f}/100")
        print(f"ğŸ” ç·åˆçŠ¶æ…‹: {self.results['overall_status']}")

        # å„ãƒ•ã‚§ãƒ¼ã‚ºçµæœè¡¨ç¤º
        for phase_name, phase_data in self.results["phases"].items():
            phase_title = phase_data.get("phase_name", phase_name)
            score = phase_data.get("score", 0)
            status = phase_data.get("status", "unknown")

            print(f"\nğŸ“‹ {phase_title}: {score:.1f}/100 ({status})")

            # å„ãƒã‚§ãƒƒã‚¯çµæœè¡¨ç¤º
            checks = phase_data.get("checks", {})
            for check_name, check_result in checks.items():
                check_status = check_result.get("status", "unknown")
                status_icon = (
                    "âœ…"
                    if check_status == "healthy"
                    else "âš ï¸" if check_status in ["warning", "info"] else "âŒ"
                )
                details = check_result.get("details", "No details")
                print(f"  {status_icon} {check_name}: {details}")

        # å•é¡ŒãŒã‚ã‚Œã°è¡¨ç¤º
        all_issues = []
        for _, phase_data in self.results["phases"].items():
            all_issues.extend(phase_data.get("issues", []))

        if all_issues:
            print(f"\nâš ï¸ æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ ({len(all_issues)}ä»¶):")
            for issue in all_issues[:10]:  # æœ€åˆã®10ä»¶
                print(f"  ğŸ”¸ [{issue['severity']}] {issue['check']}: {issue['message']}")
        else:
            print("\nâœ… é‡å¤§ãªå•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

        # æ¨å¥¨äº‹é …è¡¨ç¤º
        phase4_data = self.results["phases"].get("phase4", {})
        recommendations_data = phase4_data.get("checks", {}).get("recommendations", {})
        urgent_actions = recommendations_data.get("urgent_actions", [])

        if urgent_actions:
            print(f"\nğŸš¨ ç·Šæ€¥å¯¾å¿œãŒå¿…è¦ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ({len(urgent_actions)}ä»¶):")
            for action in urgent_actions[:5]:
                print(f"  âš¡ {action.get('action', 'Unknown action')}")
                print(f"     {action.get('command', '')}")

        print("=" * 60)

        # JSONãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_path = self.report_dir / f"operational_status_{timestamp}.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"ğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {json_path}")

    # ===== BaseAnalyzeræŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè£… =====

    def run_analysis(self, target_phases: List[str] = None) -> Dict:
        """é‹ç”¨è¨ºæ–­åˆ†æå®Ÿè¡Œï¼ˆBaseAnalyzerè¦æ±‚ï¼‰"""
        results = self.run_comprehensive_check(target_phases)
        return {
            "timestamp": datetime.now().isoformat(),
            "analysis_type": "operational_diagnosis",
            "overall_status": results.get("overall_status", "unknown"),
            "overall_score": results.get("overall_score", 0),
            "phases_count": len(results.get("phases", {})),
            "critical_issues": len(results.get("critical_issues", [])),
            "recommendations": len(results.get("recommendations", [])),
            "results": results,
        }

    def generate_report(self, analysis_result: Dict) -> str:
        """é‹ç”¨è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆBaseAnalyzerè¦æ±‚ï¼‰"""
        results = analysis_result.get("results", {})
        return f"""
=== æœ¬ç•ªé‹ç”¨è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆ ===
å®Ÿè¡Œæ—¥æ™‚: {analysis_result.get('timestamp', '')}
ç·åˆçŠ¶æ…‹: {analysis_result.get('overall_status', 'unknown')}
ç·åˆã‚¹ã‚³ã‚¢: {analysis_result.get('overall_score', 0)}/100
è¨ºæ–­ãƒ•ã‚§ãƒ¼ã‚ºæ•°: {analysis_result.get('phases_count', 0)}
é‡å¤§å•é¡Œæ•°: {analysis_result.get('critical_issues', 0)}
æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°: {analysis_result.get('recommendations', 0)}
=============================="""


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°."""
    import argparse

    parser = argparse.ArgumentParser(description="crypto-bot æ–°ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒçŠ¶æ³ç¢ºèªã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--verbose", "-v", action="store_true", help="è©³ç´°ãƒ­ã‚°å‡ºåŠ›")
    parser.add_argument("--save-report", "-s", action="store_true", help="ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜")
    parser.add_argument(
        "--phase",
        choices=["phase1", "phase2", "phase3", "phase4"],
        help="ç‰¹å®šãƒ•ã‚§ãƒ¼ã‚ºã®ã¿å®Ÿè¡Œ",
    )
    parser.add_argument("--config", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
    checker = NewSystemOperationalStatusChecker(config_path=args.config)

    target_phases = [args.phase] if args.phase else None
    results = checker.run_comprehensive_check(target_phases)

    # çµ‚äº†ã‚³ãƒ¼ãƒ‰è¨­å®š
    if results["overall_status"] == "critical":
        sys.exit(2)  # Critical issues
    elif results["overall_status"] in ["warning", "degraded"]:
        sys.exit(1)  # Warning issues
    else:
        sys.exit(0)  # Success


if __name__ == "__main__":
    main()
