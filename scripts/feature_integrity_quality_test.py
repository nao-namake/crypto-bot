#!/usr/bin/env python3
"""
Phase B2.6.2: æ©Ÿèƒ½ãƒ»å“è³ªãƒ†ã‚¹ãƒˆ - 151ç‰¹å¾´é‡æ•´åˆæ€§ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ¤œè¨¼

åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå†…å®¹:
1. 151ç‰¹å¾´é‡å®Œå…¨ç”Ÿæˆæ¤œè¨¼
2. ãƒãƒƒãƒ vs ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ç‰¹å¾´é‡æ•´åˆæ€§
3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½æ¤œè¨¼
4. å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆå“è³ªãƒ†ã‚¹ãƒˆ
5. ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ»ç•°å¸¸ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ†ã‚¹ãƒˆ
6. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ»ä¸€è²«æ€§æ¤œè¨¼

æœŸå¾…çµæœ:
- 151ç‰¹å¾´é‡ã™ã¹ã¦ç”ŸæˆæˆåŠŸ
- ãƒãƒƒãƒãƒ»ãƒ¬ã‚¬ã‚·ãƒ¼é–“ã®å®Œå…¨ä¸€è‡´
- å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿éšœå®³æ™‚ã®é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
"""

import json
import logging
import os
import sys
import time
import traceback
from typing import Any, Dict, List, Optional, Set, Tuple
from unittest.mock import Mock, patch

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import numpy as np
import pandas as pd
import yaml

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ 
from crypto_bot.ml.feature_engines.batch_calculator import BatchFeatureCalculator
from crypto_bot.ml.feature_engines.technical_engine import TechnicalFeatureEngine
from crypto_bot.ml.feature_engines.external_data_engine import ExternalDataIntegrator
from crypto_bot.ml.preprocessor import FeatureEngineer

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class FeatureIntegrityTester:
    """ç‰¹å¾´é‡æ•´åˆæ€§ãƒ»å“è³ªãƒ†ã‚¹ã‚¿ãƒ¼"""
    
    def __init__(self):
        self.results = {
            "test_metadata": {
                "timestamp": time.time(),
                "python_version": sys.version,
                "test_phase": "B2.6.2"
            },
            "feature_integrity": {},
            "error_handling": {},
            "external_data_quality": {},
            "edge_case_handling": {},
            "overall_assessment": {}
        }
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        self.config = self._load_full_config()
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
        self.test_data = self._prepare_comprehensive_test_data()
        
        # æœŸå¾…ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
        self.expected_features = self._get_expected_151_features()
        
        logger.info("ğŸ§ª FeatureIntegrityTester initialized for Phase B2.6.2")
    
    def _load_full_config(self) -> Dict[str, Any]:
        """å®Œå…¨ç‰ˆ151ç‰¹å¾´é‡è¨­å®šèª­ã¿è¾¼ã¿"""
        config_path = "/Users/nao/Desktop/bot/config/production/production.yml"
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                
            logger.info(f"ğŸ“‹ Full production config loaded: {len(config.get('ml', {}).get('extra_features', []))} features")
            return config
        except Exception as e:
            logger.error(f"âŒ Failed to load production config: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šï¼ˆ151ç‰¹å¾´é‡å¯¾å¿œï¼‰
            return {
                "ml": {
                    "extra_features": [
                        # åŸºæœ¬ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«
                        "rsi_14", "rsi_7", "rsi_21", "sma_20", "sma_50", "sma_100", "sma_200",
                        "ema_12", "ema_26", "ema_50", "ema_100", "macd", "atr_14", "stoch", "adx",
                        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿
                        "vix", "macro", "dxy", "treasury", "fear_greed", "funding",
                        # Phase 3.2 æ–°ç‰¹å¾´é‡ (65ç‰¹å¾´é‡)
                        "volume_profile_poc", "volume_profile_vah", "volume_profile_val",
                        "volatility_regime", "momentum_signals", "liquidity_indicators",
                        "market_microstructure", "cross_asset_correlation", "sentiment_analysis"
                    ],
                    "feat_period": 14,
                    "lags": [1, 2, 3],
                    "rolling_window": 14
                }
            }
    
    def _prepare_comprehensive_test_data(self) -> pd.DataFrame:
        """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        try:
            # CSVãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
            csv_path = "/Users/nao/Desktop/bot/data/btc_usd_2024_hourly.csv"
            
            if os.path.exists(csv_path):
                logger.info("ğŸ“Š Loading comprehensive test data from CSV...")
                df = pd.read_csv(csv_path)
                
                # timestampã‚«ãƒ©ãƒ ã‚’DatetimeIndexã«å¤‰æ›
                if 'timestamp' in df.columns:
                    df['timestamp'] = pd.to_datetime(df['timestamp'])
                    df.set_index('timestamp', inplace=True)
                elif df.index.name != 'timestamp':
                    # æœ€åˆã®åˆ—ã‚’timestampã¨ã—ã¦ä½¿ç”¨
                    df.index = pd.to_datetime(df.iloc[:, 0])
                    df.index.name = 'timestamp'
                
                # å¤§è¦æ¨¡ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆ500è¡Œã§å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ†ã‚¹ãƒˆï¼‰
                test_size = min(500, len(df))
                test_df = df.tail(test_size).copy()
                
                logger.info(f"âœ… Comprehensive test data prepared: {len(test_df)} rows Ã— {len(test_df.columns)} columns")
                return test_df
            else:
                # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå¤§è¦æ¨¡ï¼‰
                logger.warning("âš ï¸ CSV not found, generating comprehensive mock data...")
                return self._generate_comprehensive_mock_data()
                
        except Exception as e:
            logger.error(f"âŒ Failed to prepare comprehensive test data: {e}")
            return self._generate_comprehensive_mock_data()
    
    def _generate_comprehensive_mock_data(self, n_rows: int = 500) -> pd.DataFrame:
        """åŒ…æ‹¬çš„ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        dates = pd.date_range('2024-01-01', periods=n_rows, freq='H')
        
        # ãƒªã‚¢ãƒ«ãªä¾¡æ ¼å‹•ä½œï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å«ã‚€ï¼‰
        base_price = 50000.0
        returns = []
        vol = 0.02
        
        for i in range(n_rows):
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åŠ¹æœ
            vol = 0.95 * vol + 0.05 * np.abs(np.random.normal(0, 0.01))
            vol = max(0.005, min(0.05, vol))  # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£åˆ¶é™
            
            ret = np.random.normal(0, vol)
            returns.append(ret)
        
        returns = np.array(returns)
        prices = base_price * np.exp(returns.cumsum())
        
        # é«˜ãƒ»ä½ä¾¡æ ¼ã®ç¾å®Ÿçš„ãªè¨ˆç®—
        highs = prices * (1 + np.abs(np.random.normal(0, 0.003, n_rows)))
        lows = prices * (1 - np.abs(np.random.normal(0, 0.003, n_rows)))
        
        # OHLCV DataFrameä½œæˆ
        df = pd.DataFrame({
            'timestamp': dates,
            'open': prices * (1 + np.random.normal(0, 0.0005, n_rows)),
            'high': highs,
            'low': lows,
            'close': prices,
            'volume': np.random.lognormal(10, 1.5, n_rows)  # å¤§ããªãƒœãƒªãƒ¥ãƒ¼ãƒ å¤‰å‹•
        })
        
        df.set_index('timestamp', inplace=True)
        
        logger.info(f"ğŸ“Š Comprehensive mock data generated: {len(df)} rows with realistic price dynamics")
        return df
    
    def _get_expected_151_features(self) -> Set[str]:
        """æœŸå¾…ã•ã‚Œã‚‹151ç‰¹å¾´é‡ãƒªã‚¹ãƒˆä½œæˆ"""
        expected = set()
        
        # åŸºæœ¬ã‚«ãƒ©ãƒ ï¼ˆOHLCV: 6å€‹ï¼‰
        base_columns = {"open", "high", "low", "close", "volume"}
        
        # MLè¨­å®šã‹ã‚‰ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’å‹•çš„ç”Ÿæˆ
        extra_features = self.config.get("ml", {}).get("extra_features", [])
        feat_period = self.config.get("ml", {}).get("feat_period", 14)
        lags = self.config.get("ml", {}).get("lags", [1, 2, 3])
        rolling_window = self.config.get("ml", {}).get("rolling_window", 14)
        
        # åŸºæœ¬çµ±è¨ˆç‰¹å¾´é‡
        expected.add(f"ATR_{feat_period}")
        for lag in lags:
            expected.add(f"close_lag_{lag}")
        expected.add(f"close_mean_{rolling_window}")
        expected.add(f"close_std_{rolling_window}")
        
        # extra_featuresã‹ã‚‰ç‰¹å¾´é‡ã‚’å±•é–‹
        for feat in extra_features:
            feat_lc = feat.lower()
            
            # RSIç³»
            if feat_lc.startswith("rsi"):
                if "_" in feat_lc:
                    expected.add(feat_lc)
                else:
                    expected.update([f"rsi_{p}" for p in [7, 14, 21]])
            
            # SMAç³»
            elif feat_lc.startswith("sma"):
                if "_" in feat_lc:
                    expected.add(feat_lc)
                else:
                    expected.update([f"sma_{p}" for p in [10, 20, 50, 100, 200]])
            
            # EMAç³»
            elif feat_lc.startswith("ema"):
                if "_" in feat_lc:
                    expected.add(feat_lc)
                else:
                    expected.update([f"ema_{p}" for p in [10, 12, 26, 50, 100]])
            
            # ATRç³»
            elif feat_lc.startswith("atr"):
                if "_" in feat_lc:
                    expected.add(feat_lc)
                expected.add(f"atr_{feat_period}")
            
            # è¤‡åˆæŒ‡æ¨™
            elif feat_lc == "macd":
                expected.update(["macd", "macd_signal", "macd_hist"])
            elif feat_lc == "stoch":
                expected.update(["stoch_k", "stoch_d"])
            elif feat_lc == "adx":
                expected.add("adx")
            
            # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¾¼ã¿ï¼‰
            elif feat_lc == "vix":
                expected.update([
                    "vix_level", "vix_change", "vix_zscore", "fear_level", 
                    "vix_spike", "vix_regime_numeric"
                ])
            elif feat_lc in ["macro", "dxy", "treasury"]:
                expected.update([
                    "dxy_level", "dxy_change", "dxy_zscore", "dxy_strength",
                    "treasury_10y_level", "treasury_10y_change", "treasury_10y_zscore", 
                    "treasury_regime", "yield_curve_spread", "risk_sentiment",
                    "usdjpy_level", "usdjpy_change", "usdjpy_volatility", 
                    "usdjpy_zscore", "usdjpy_trend", "usdjpy_strength"
                ])
            elif feat_lc == "fear_greed":
                expected.update([
                    "fg_index", "fg_change_1d", "fg_change_7d", "fg_ma_7", "fg_ma_30",
                    "fg_extreme_fear", "fg_fear", "fg_neutral", "fg_greed", "fg_extreme_greed",
                    "fg_volatility", "fg_momentum", "fg_reversal_signal"
                ])
            elif feat_lc == "funding":
                expected.update([
                    "funding_rate", "funding_rate_change", "funding_rate_ma", 
                    "funding_rate_zscore", "funding_trend", "open_interest",
                    "oi_change", "oi_trend"
                ])
        
        logger.info(f"ğŸ¯ Expected features compiled: {len(expected)} features")
        return expected
    
    def test_151_feature_complete_generation(self) -> Dict[str, Any]:
        """151ç‰¹å¾´é‡å®Œå…¨ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ§ª Testing 151 Feature Complete Generation...")
        
        try:
            # ãƒãƒƒãƒã‚·ã‚¹ãƒ†ãƒ ã§ã®ç‰¹å¾´é‡ç”Ÿæˆ
            fe_batch = FeatureEngineer(self.config)
            if hasattr(fe_batch, 'batch_engines_enabled'):
                fe_batch.batch_engines_enabled = True
            
            result_df = fe_batch.transform(self.test_data.copy())
            
            if result_df is not None:
                generated_features = set(result_df.columns) - set(self.test_data.columns)
                total_features = len(generated_features)
                
                # ç‰¹å¾´é‡åˆ†æ
                missing_features = self.expected_features - generated_features
                extra_features = generated_features - self.expected_features
                
                # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
                technical_features = {f for f in generated_features if any(
                    f.startswith(prefix) for prefix in ['rsi_', 'sma_', 'ema_', 'atr_', 'macd', 'stoch', 'adx']
                )}
                external_features = {f for f in generated_features if any(
                    f.startswith(prefix) for prefix in ['vix_', 'dxy_', 'treasury_', 'fg_', 'funding_', 'usdjpy_', 'oi_']
                )}
                basic_features = {f for f in generated_features if any(
                    f.startswith(prefix) for prefix in ['close_lag_', 'close_mean_', 'close_std_', 'ATR_']
                )}
                
                feature_test_results = {
                    "total_features_generated": total_features,
                    "expected_features_count": len(self.expected_features),
                    "target_151_achieved": total_features >= 151,
                    "feature_coverage_percent": (len(generated_features & self.expected_features) / len(self.expected_features)) * 100,
                    "missing_features": list(missing_features),
                    "extra_features": list(extra_features),
                    "missing_count": len(missing_features),
                    "extra_count": len(extra_features),
                    "category_breakdown": {
                        "technical_indicators": len(technical_features),
                        "external_data": len(external_features), 
                        "basic_statistics": len(basic_features),
                        "other_features": total_features - len(technical_features) - len(external_features) - len(basic_features)
                    },
                    "success": total_features >= 151 and len(missing_features) <= 5  # è¨±å®¹èª¤å·®5ç‰¹å¾´é‡
                }
                
                self.results["feature_integrity"]["complete_generation"] = feature_test_results
                
                logger.info(f"ğŸ“Š Feature Generation Results:")
                logger.info(f"  â€¢ Total Features: {total_features} (Target: 151)")
                logger.info(f"  â€¢ Missing Features: {len(missing_features)}")
                logger.info(f"  â€¢ Extra Features: {len(extra_features)}")
                logger.info(f"  â€¢ Technical Indicators: {len(technical_features)}")
                logger.info(f"  â€¢ External Data: {len(external_features)}")
                logger.info(f"  â€¢ Success: {'âœ…' if feature_test_results['success'] else 'âŒ'}")
                
                return feature_test_results
            else:
                error_result = {"error": "Feature generation returned None", "success": False}
                self.results["feature_integrity"]["complete_generation"] = error_result
                return error_result
                
        except Exception as e:
            error_result = {
                "error": str(e),
                "traceback": traceback.format_exc(),
                "success": False
            }
            self.results["feature_integrity"]["complete_generation"] = error_result
            logger.error(f"âŒ 151 feature generation test failed: {e}")
            return error_result
    
    def test_batch_vs_legacy_consistency(self) -> Dict[str, Any]:
        """ãƒãƒƒãƒ vs ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ”„ Testing Batch vs Legacy System Consistency...")
        
        try:
            # ãƒãƒƒãƒã‚·ã‚¹ãƒ†ãƒ 
            fe_batch = FeatureEngineer(self.config)
            if hasattr(fe_batch, 'batch_engines_enabled'):
                fe_batch.batch_engines_enabled = True
            
            batch_result = fe_batch.transform(self.test_data.copy())
            
            # ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ 
            fe_legacy = FeatureEngineer(self.config)
            if hasattr(fe_legacy, 'batch_engines_enabled'):
                fe_legacy.batch_engines_enabled = False
            
            legacy_result = fe_legacy.transform(self.test_data.copy())
            
            if batch_result is not None and legacy_result is not None:
                # å…±é€šç‰¹å¾´é‡ã®ç‰¹å®š
                batch_features = set(batch_result.columns) - set(self.test_data.columns)
                legacy_features = set(legacy_result.columns) - set(self.test_data.columns)
                
                common_features = batch_features & legacy_features
                batch_only = batch_features - legacy_features
                legacy_only = legacy_features - batch_features
                
                # æ•°å€¤ä¸€è‡´åº¦ãƒã‚§ãƒƒã‚¯
                consistency_issues = []
                feature_correlations = {}
                
                for feature in common_features:
                    if feature in batch_result.columns and feature in legacy_result.columns:
                        batch_series = batch_result[feature].dropna()
                        legacy_series = legacy_result[feature].dropna()
                        
                        if len(batch_series) > 0 and len(legacy_series) > 0:
                            # ç›¸é–¢ä¿‚æ•°è¨ˆç®—
                            correlation = batch_series.corr(legacy_series)
                            feature_correlations[feature] = correlation
                            
                            # è¨±å®¹èª¤å·®ãƒã‚§ãƒƒã‚¯ï¼ˆç›¸é–¢ä¿‚æ•°0.99ä»¥ä¸Šã‚’æœŸå¾…ï¼‰
                            if correlation < 0.99:
                                mean_diff = abs(batch_series.mean() - legacy_series.mean())
                                consistency_issues.append({
                                    "feature": feature,
                                    "correlation": correlation,
                                    "mean_difference": mean_diff
                                })
                
                avg_correlation = np.mean(list(feature_correlations.values())) if feature_correlations else 0
                
                consistency_results = {
                    "batch_features_count": len(batch_features),
                    "legacy_features_count": len(legacy_features),
                    "common_features_count": len(common_features),
                    "batch_only_features": list(batch_only),
                    "legacy_only_features": list(legacy_only),
                    "batch_only_count": len(batch_only),
                    "legacy_only_count": len(legacy_only),
                    "average_correlation": avg_correlation,
                    "consistency_issues_count": len(consistency_issues),
                    "consistency_issues": consistency_issues[:10],  # æœ€åˆã®10ä»¶ã®ã¿ä¿å­˜
                    "high_consistency_features": len([c for c in feature_correlations.values() if c >= 0.99]),
                    "success": len(consistency_issues) <= 5 and avg_correlation >= 0.95  # è¨±å®¹åŸºæº–
                }
                
                self.results["feature_integrity"]["batch_vs_legacy"] = consistency_results
                
                logger.info(f"ğŸ”„ Consistency Test Results:")
                logger.info(f"  â€¢ Common Features: {len(common_features)}")
                logger.info(f"  â€¢ Avg Correlation: {avg_correlation:.4f}")
                logger.info(f"  â€¢ Consistency Issues: {len(consistency_issues)}")
                logger.info(f"  â€¢ High Consistency: {consistency_results['high_consistency_features']}")
                logger.info(f"  â€¢ Success: {'âœ…' if consistency_results['success'] else 'âŒ'}")
                
                return consistency_results
            else:
                error_result = {"error": "One or both systems failed to generate features", "success": False}
                self.results["feature_integrity"]["batch_vs_legacy"] = error_result
                return error_result
                
        except Exception as e:
            error_result = {
                "error": str(e),
                "traceback": traceback.format_exc(),
                "success": False
            }
            self.results["feature_integrity"]["batch_vs_legacy"] = error_result
            logger.error(f"âŒ Batch vs legacy consistency test failed: {e}")
            return error_result
    
    def test_error_handling_robustness(self) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å …ç‰¢æ€§ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ›¡ï¸ Testing Error Handling Robustness...")
        
        error_scenarios = []
        
        try:
            # ã‚·ãƒŠãƒªã‚ª1: ç©ºãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            try:
                empty_df = pd.DataFrame()
                fe = FeatureEngineer(self.config)
                if hasattr(fe, 'batch_engines_enabled'):
                    fe.batch_engines_enabled = True
                result = fe.transform(empty_df)
                error_scenarios.append({
                    "scenario": "empty_dataframe",
                    "success": result is not None,
                    "error": None
                })
            except Exception as e:
                error_scenarios.append({
                    "scenario": "empty_dataframe",
                    "success": False,
                    "error": str(e)
                })
            
            # ã‚·ãƒŠãƒªã‚ª2: ä¸è¶³ã‚«ãƒ©ãƒ ï¼ˆcloseãªã—ï¼‰
            try:
                incomplete_df = self.test_data[['open', 'high', 'low', 'volume']].copy()
                fe = FeatureEngineer(self.config)
                if hasattr(fe, 'batch_engines_enabled'):
                    fe.batch_engines_enabled = True
                result = fe.transform(incomplete_df)
                error_scenarios.append({
                    "scenario": "missing_close_column",
                    "success": result is not None,
                    "error": None
                })
            except Exception as e:
                error_scenarios.append({
                    "scenario": "missing_close_column",
                    "success": False,
                    "error": str(e)
                })
            
            # ã‚·ãƒŠãƒªã‚ª3: NaNå€¤å¤§é‡ãƒ‡ãƒ¼ã‚¿
            try:
                nan_df = self.test_data.copy()
                # 50%ã®ãƒ‡ãƒ¼ã‚¿ã‚’NaNã«
                mask = np.random.rand(*nan_df.shape) < 0.5
                nan_df = nan_df.mask(mask)
                
                fe = FeatureEngineer(self.config)
                if hasattr(fe, 'batch_engines_enabled'):
                    fe.batch_engines_enabled = True
                result = fe.transform(nan_df)
                error_scenarios.append({
                    "scenario": "heavy_nan_data",
                    "success": result is not None and not result.isnull().all().all(),
                    "error": None
                })
            except Exception as e:
                error_scenarios.append({
                    "scenario": "heavy_nan_data",
                    "success": False,
                    "error": str(e)
                })
            
            # ã‚·ãƒŠãƒªã‚ª4: å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼éšœå®³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            try:
                with patch('crypto_bot.data.vix_fetcher.VIXDataFetcher') as mock_vix:
                    mock_vix.side_effect = Exception("Simulated VIX API failure")
                    
                    fe = FeatureEngineer(self.config)
                    if hasattr(fe, 'batch_engines_enabled'):
                        fe.batch_engines_enabled = True
                    result = fe.transform(self.test_data.copy())
                    error_scenarios.append({
                        "scenario": "external_data_failure",
                        "success": result is not None,
                        "error": None
                    })
            except Exception as e:
                error_scenarios.append({
                    "scenario": "external_data_failure",
                    "success": False,
                    "error": str(e)
                })
            
            # çµæœé›†è¨ˆ
            successful_scenarios = [s for s in error_scenarios if s["success"]]
            failed_scenarios = [s for s in error_scenarios if not s["success"]]
            
            error_handling_results = {
                "total_scenarios_tested": len(error_scenarios),
                "successful_scenarios": len(successful_scenarios),
                "failed_scenarios": len(failed_scenarios),
                "success_rate": len(successful_scenarios) / len(error_scenarios) if error_scenarios else 0,
                "scenarios_detail": error_scenarios,
                "robustness_score": len(successful_scenarios) / len(error_scenarios) if error_scenarios else 0,
                "success": len(successful_scenarios) >= len(error_scenarios) * 0.75  # 75%æˆåŠŸç‡æœŸå¾…
            }
            
            self.results["error_handling"]["robustness"] = error_handling_results
            
            logger.info(f"ğŸ›¡ï¸ Error Handling Test Results:")
            logger.info(f"  â€¢ Scenarios Tested: {len(error_scenarios)}")
            logger.info(f"  â€¢ Successful: {len(successful_scenarios)}")
            logger.info(f"  â€¢ Failed: {len(failed_scenarios)}")
            logger.info(f"  â€¢ Success Rate: {error_handling_results['success_rate']:.2%}")
            logger.info(f"  â€¢ Success: {'âœ…' if error_handling_results['success'] else 'âŒ'}")
            
            return error_handling_results
            
        except Exception as e:
            error_result = {
                "error": str(e),
                "traceback": traceback.format_exc(),
                "success": False
            }
            self.results["error_handling"]["robustness"] = error_result
            logger.error(f"âŒ Error handling robustness test failed: {e}")
            return error_result
    
    def test_external_data_quality(self) -> Dict[str, Any]:
        """å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ“¡ Testing External Data Quality...")
        
        try:
            # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
            batch_calc = BatchFeatureCalculator(self.config)
            external_integrator = ExternalDataIntegrator(self.config, batch_calc)
            
            # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ†ã‚¹ãƒˆ
            external_batches = external_integrator.create_external_data_batches(self.test_data.index)
            
            # å„ã‚½ãƒ¼ã‚¹åˆ¥å“è³ªè©•ä¾¡
            source_quality = {}
            total_external_features = 0
            
            for batch in external_batches:
                source_name = batch.batch_name.replace('_batch', '')
                features_count = len(batch)
                
                # ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡
                if len(batch) > 0:
                    # NaNç‡ãƒã‚§ãƒƒã‚¯
                    nan_rates = {}
                    for feature_name, feature_series in batch.data.items():
                        if isinstance(feature_series, pd.Series):
                            nan_rate = feature_series.isnull().sum() / len(feature_series)
                            nan_rates[feature_name] = nan_rate
                    
                    avg_nan_rate = np.mean(list(nan_rates.values())) if nan_rates else 0
                    
                    source_quality[source_name] = {
                        "features_count": features_count,
                        "avg_nan_rate": avg_nan_rate,
                        "high_quality_features": len([r for r in nan_rates.values() if r <= 0.1]),
                        "poor_quality_features": len([r for r in nan_rates.values() if r > 0.5]),
                        "quality_score": max(0, 1 - avg_nan_rate),
                        "success": avg_nan_rate <= 0.3  # 30%ä»¥ä¸‹ã®NaNç‡ã‚’æœŸå¾…
                    }
                else:
                    source_quality[source_name] = {
                        "features_count": 0,
                        "avg_nan_rate": 1.0,
                        "high_quality_features": 0,
                        "poor_quality_features": 0,
                        "quality_score": 0,
                        "success": False
                    }
                
                total_external_features += features_count
            
            # çµ±åˆçµ±è¨ˆå–å¾—
            integration_stats = external_integrator.get_integration_stats()
            
            external_data_results = {
                "total_external_features": total_external_features,
                "sources_tested": len(source_quality),
                "successful_sources": len([s for s in source_quality.values() if s["success"]]),
                "source_quality_detail": source_quality,
                "integration_stats": integration_stats,
                "overall_quality_score": np.mean([s["quality_score"] for s in source_quality.values()]) if source_quality else 0,
                "success": len([s for s in source_quality.values() if s["success"]]) >= len(source_quality) * 0.5  # 50%ã®ã‚½ãƒ¼ã‚¹æˆåŠŸæœŸå¾…
            }
            
            self.results["external_data_quality"]["integration_test"] = external_data_results
            
            logger.info(f"ğŸ“¡ External Data Quality Results:")
            logger.info(f"  â€¢ Total External Features: {total_external_features}")
            logger.info(f"  â€¢ Sources Tested: {len(source_quality)}")
            logger.info(f"  â€¢ Successful Sources: {external_data_results['successful_sources']}")
            logger.info(f"  â€¢ Overall Quality Score: {external_data_results['overall_quality_score']:.3f}")
            logger.info(f"  â€¢ Success: {'âœ…' if external_data_results['success'] else 'âŒ'}")
            
            return external_data_results
            
        except Exception as e:
            error_result = {
                "error": str(e),
                "traceback": traceback.format_exc(),
                "success": False
            }
            self.results["external_data_quality"]["integration_test"] = error_result
            logger.error(f"âŒ External data quality test failed: {e}")
            return error_result
    
    def test_edge_case_handling(self) -> Dict[str, Any]:
        """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ”¬ Testing Edge Case Handling...")
        
        edge_cases = []
        
        try:
            # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹1: æ¥µå°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ10è¡Œï¼‰
            try:
                tiny_df = self.test_data.head(10).copy()
                fe = FeatureEngineer(self.config)
                if hasattr(fe, 'batch_engines_enabled'):
                    fe.batch_engines_enabled = True
                result = fe.transform(tiny_df)
                edge_cases.append({
                    "case": "tiny_dataset_10_rows",
                    "success": result is not None and len(result) > 0,
                    "features_generated": len(result.columns) - len(tiny_df.columns) if result is not None else 0,
                    "error": None
                })
            except Exception as e:
                edge_cases.append({
                    "case": "tiny_dataset_10_rows",
                    "success": False,
                    "features_generated": 0,
                    "error": str(e)
                })
            
            # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹2: ä¾¡æ ¼ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¥µå¤§ï¼ˆ500%æ—¥æ¬¡å¤‰å‹•ï¼‰
            try:
                extreme_vol_df = self.test_data.copy()
                # æ¥µç«¯ãªãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ä½œæˆ
                extreme_returns = np.random.normal(0, 5, len(extreme_vol_df))  # 500%å¹´ç‡ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                extreme_vol_df['close'] = extreme_vol_df['close'].iloc[0] * np.exp(extreme_returns.cumsum())
                extreme_vol_df['high'] = extreme_vol_df['close'] * 1.1
                extreme_vol_df['low'] = extreme_vol_df['close'] * 0.9
                
                fe = FeatureEngineer(self.config)
                if hasattr(fe, 'batch_engines_enabled'):
                    fe.batch_engines_enabled = True
                result = fe.transform(extreme_vol_df)
                
                # ç„¡é™å¤§ãƒ»NaNå€¤ãƒã‚§ãƒƒã‚¯
                has_inf = np.isinf(result.select_dtypes(include=[np.number])).any().any() if result is not None else True
                has_extreme_nan = result.isnull().sum().sum() > len(result) * 0.8 if result is not None else True
                
                edge_cases.append({
                    "case": "extreme_volatility",
                    "success": result is not None and not has_inf and not has_extreme_nan,
                    "features_generated": len(result.columns) - len(extreme_vol_df.columns) if result is not None else 0,
                    "has_infinity": has_inf,
                    "extreme_nan_rate": result.isnull().sum().sum() / result.size if result is not None else 1,
                    "error": None
                })
            except Exception as e:
                edge_cases.append({
                    "case": "extreme_volatility",
                    "success": False,
                    "features_generated": 0,
                    "error": str(e)
                })
            
            # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹3: ä¾¡æ ¼ãƒ•ãƒ©ãƒƒãƒˆï¼ˆå¤‰å‹•ãªã—ï¼‰
            try:
                flat_df = self.test_data.copy()
                flat_price = flat_df['close'].mean()
                flat_df['open'] = flat_price
                flat_df['high'] = flat_price
                flat_df['low'] = flat_price
                flat_df['close'] = flat_price
                flat_df['volume'] = flat_df['volume'].mean()  # ãƒœãƒªãƒ¥ãƒ¼ãƒ ã¯æ­£å¸¸ç¶­æŒ
                
                fe = FeatureEngineer(self.config)
                if hasattr(fe, 'batch_engines_enabled'):
                    fe.batch_engines_enabled = True
                result = fe.transform(flat_df)
                edge_cases.append({
                    "case": "flat_prices_no_volatility",
                    "success": result is not None and len(result.columns) > len(flat_df.columns),
                    "features_generated": len(result.columns) - len(flat_df.columns) if result is not None else 0,
                    "error": None
                })
            except Exception as e:
                edge_cases.append({
                    "case": "flat_prices_no_volatility",
                    "success": False,
                    "features_generated": 0,
                    "error": str(e)
                })
            
            # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹4: æ··åˆãƒ‡ãƒ¼ã‚¿å‹ï¼ˆæ–‡å­—åˆ—æ··åœ¨ï¼‰
            try:
                mixed_df = self.test_data.copy()
                # ä¸€éƒ¨ã®æ•°å€¤ã‚’æ–‡å­—åˆ—ã«å¤‰æ›´ï¼ˆç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
                mixed_df.loc[mixed_df.index[::10], 'close'] = 'invalid_price'
                mixed_df.loc[mixed_df.index[5::15], 'volume'] = 'NaN'
                
                fe = FeatureEngineer(self.config)
                if hasattr(fe, 'batch_engines_enabled'):
                    fe.batch_engines_enabled = True
                result = fe.transform(mixed_df)
                edge_cases.append({
                    "case": "mixed_data_types",
                    "success": result is not None,
                    "features_generated": len(result.columns) - len(self.test_data.columns) if result is not None else 0,
                    "error": None
                })
            except Exception as e:
                edge_cases.append({
                    "case": "mixed_data_types",
                    "success": False,
                    "features_generated": 0,
                    "error": str(e)
                })
            
            # çµæœé›†è¨ˆ
            successful_cases = [c for c in edge_cases if c["success"]]
            failed_cases = [c for c in edge_cases if not c["success"]]
            
            edge_case_results = {
                "total_edge_cases_tested": len(edge_cases),
                "successful_cases": len(successful_cases),
                "failed_cases": len(failed_cases),
                "success_rate": len(successful_cases) / len(edge_cases) if edge_cases else 0,
                "cases_detail": edge_cases,
                "avg_features_generated": np.mean([c.get("features_generated", 0) for c in successful_cases]) if successful_cases else 0,
                "robustness_score": len(successful_cases) / len(edge_cases) if edge_cases else 0,
                "success": len(successful_cases) >= len(edge_cases) * 0.75  # 75%æˆåŠŸç‡æœŸå¾…
            }
            
            self.results["edge_case_handling"]["comprehensive_test"] = edge_case_results
            
            logger.info(f"ğŸ”¬ Edge Case Handling Results:")
            logger.info(f"  â€¢ Cases Tested: {len(edge_cases)}")
            logger.info(f"  â€¢ Successful: {len(successful_cases)}")
            logger.info(f"  â€¢ Failed: {len(failed_cases)}")
            logger.info(f"  â€¢ Success Rate: {edge_case_results['success_rate']:.2%}")
            logger.info(f"  â€¢ Avg Features Generated: {edge_case_results['avg_features_generated']:.1f}")
            logger.info(f"  â€¢ Success: {'âœ…' if edge_case_results['success'] else 'âŒ'}")
            
            return edge_case_results
            
        except Exception as e:
            error_result = {
                "error": str(e),
                "traceback": traceback.format_exc(),
                "success": False
            }
            self.results["edge_case_handling"]["comprehensive_test"] = error_result
            logger.error(f"âŒ Edge case handling test failed: {e}")
            return error_result
    
    def calculate_overall_assessment(self):
        """ç·åˆè©•ä¾¡è¨ˆç®—"""
        assessments = {
            "feature_integrity": self.results.get("feature_integrity", {}),
            "error_handling": self.results.get("error_handling", {}),
            "external_data_quality": self.results.get("external_data_quality", {}),
            "edge_case_handling": self.results.get("edge_case_handling", {})
        }
        
        # å„ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒªã®æˆåŠŸçŠ¶æ³
        category_scores = {}
        for category, tests in assessments.items():
            if tests:
                successes = [test.get("success", False) for test in tests.values() if isinstance(test, dict)]
                category_scores[category] = sum(successes) / len(successes) if successes else 0
            else:
                category_scores[category] = 0
        
        # é‡ã¿ä»˜ã‘è©•ä¾¡
        weights = {
            "feature_integrity": 0.4,  # ç‰¹å¾´é‡æ•´åˆæ€§ãŒæœ€é‡è¦
            "error_handling": 0.2,
            "external_data_quality": 0.2,
            "edge_case_handling": 0.2
        }
        
        overall_score = sum(category_scores[cat] * weights[cat] for cat in weights)
        
        # å“è³ªãƒ¬ãƒ™ãƒ«åˆ¤å®š
        if overall_score >= 0.9:
            quality_level = "Excellent"
        elif overall_score >= 0.75:
            quality_level = "Good"
        elif overall_score >= 0.6:
            quality_level = "Acceptable"
        else:
            quality_level = "Needs Improvement"
        
        overall_assessment = {
            "overall_score": overall_score,
            "quality_level": quality_level,
            "category_scores": category_scores,
            "total_tests_run": sum(len(tests) for tests in assessments.values() if tests),
            "total_successful_tests": sum(
                sum(test.get("success", False) for test in tests.values() if isinstance(test, dict))
                for tests in assessments.values() if tests
            ),
            "recommendation": self._generate_recommendation(overall_score, category_scores),
            "ready_for_production": overall_score >= 0.75
        }
        
        self.results["overall_assessment"] = overall_assessment
        
        logger.info(f"ğŸ¯ Overall Assessment:")
        logger.info(f"  â€¢ Overall Score: {overall_score:.3f}")
        logger.info(f"  â€¢ Quality Level: {quality_level}")
        logger.info(f"  â€¢ Production Ready: {'âœ…' if overall_assessment['ready_for_production'] else 'âŒ'}")
        
        return overall_assessment
    
    def _generate_recommendation(self, overall_score: float, category_scores: Dict[str, float]) -> str:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        if category_scores.get("feature_integrity", 0) < 0.8:
            recommendations.append("ç‰¹å¾´é‡æ•´åˆæ€§ã®å‘ä¸ŠãŒå¿…è¦")
        
        if category_scores.get("error_handling", 0) < 0.7:
            recommendations.append("ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®å¼·åŒ–ãŒå¿…è¦")
        
        if category_scores.get("external_data_quality", 0) < 0.6:
            recommendations.append("å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆã®å®‰å®šåŒ–ãŒå¿…è¦")
        
        if category_scores.get("edge_case_handling", 0) < 0.7:
            recommendations.append("ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹å¯¾å¿œã®æ”¹å–„ãŒå¿…è¦")
        
        if overall_score >= 0.9:
            recommendations.append("ã‚·ã‚¹ãƒ†ãƒ ã¯æœ¬ç•ªç’°å¢ƒã§ã‚‚å®‰å®šå‹•ä½œã™ã‚‹è¦‹è¾¼ã¿")
        elif overall_score >= 0.75:
            recommendations.append("è»½å¾®ãªèª¿æ•´å¾Œã«æœ¬ç•ªç§»è¡ŒãŒå¯èƒ½")
        else:
            recommendations.append("é‡è¦ãªä¿®æ­£ãŒå¿…è¦ - æœ¬ç•ªç§»è¡Œã¯è¦æ¤œè¨")
        
        return "; ".join(recommendations)
    
    def save_results(self, output_path: str = "/Users/nao/Desktop/bot/test_results/feature_integrity_quality_test.json"):
        """çµæœä¿å­˜"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"ğŸ’¾ Feature integrity test results saved to: {output_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to save results: {e}")
    
    def run_comprehensive_quality_test(self):
        """åŒ…æ‹¬çš„å“è³ªãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("ğŸ§ª Starting Phase B2.6.2: Feature Integrity & Quality Test...")
        
        # å„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        logger.info("=" * 80)
        self.test_151_feature_complete_generation()
        
        logger.info("=" * 80)
        self.test_batch_vs_legacy_consistency()
        
        logger.info("=" * 80)
        self.test_error_handling_robustness()
        
        logger.info("=" * 80)
        self.test_external_data_quality()
        
        logger.info("=" * 80)
        self.test_edge_case_handling()
        
        # ç·åˆè©•ä¾¡
        logger.info("=" * 80)
        self.calculate_overall_assessment()
        
        # çµæœä¿å­˜
        self.save_results()
        
        logger.info("ğŸ‰ Phase B2.6.2: Feature Integrity & Quality Test Completed!")
        
        return self.results


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        tester = FeatureIntegrityTester()
        results = tester.run_comprehensive_quality_test()
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\n" + "=" * 80)
        print("ğŸ¯ PHASE B2.6.2: FEATURE INTEGRITY & QUALITY TEST RESULTS")
        print("=" * 80)
        
        overall = results.get("overall_assessment", {})
        if overall:
            print(f"ğŸ¯ Overall Score: {overall.get('overall_score', 0):.3f}")
            print(f"ğŸ“Š Quality Level: {overall.get('quality_level', 'Unknown')}")
            print(f"ğŸš€ Production Ready: {'âœ… YES' if overall.get('ready_for_production', False) else 'âŒ NO'}")
            print(f"ğŸ’¡ Recommendation: {overall.get('recommendation', 'No recommendation')}")
            
            print("\nğŸ“ˆ Category Scores:")
            for category, score in overall.get("category_scores", {}).items():
                print(f"  â€¢ {category.replace('_', ' ').title()}: {score:.3f}")
        else:
            print("âŒ TEST FAILED: Could not complete comprehensive assessment")
        
        print("=" * 80)
        return results
        
    except Exception as e:
        logger.error(f"âŒ Test execution failed: {e}")
        return None


if __name__ == "__main__":
    main()