#!/usr/bin/env python3
# =============================================================================
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: scripts/continuous_optimization_framework.py
# èª¬æ˜:
# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®ç¶™ç¶šæœ€é©åŒ–ãƒ»è‡ªå‹•èª¿æ•´ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ™ãƒ¼ã‚¹ã®è‡ªå‹•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãƒ»ãƒ¢ãƒ‡ãƒ«æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ 
# =============================================================================

import sys
import os
import json
import logging
import threading
import time
import pickle
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from enum import Enum
import asyncio

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import numpy as np
import pandas as pd
import yaml
from sklearn.model_selection import ParameterGrid
from sklearn.metrics import accuracy_score, precision_score, recall_score

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class OptimizationTrigger(Enum):
    """æœ€é©åŒ–ãƒˆãƒªã‚¬ãƒ¼"""
    SCHEDULED = "scheduled"              # å®šæœŸå®Ÿè¡Œ
    PERFORMANCE_DEGRADATION = "performance_degradation"  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–
    CONFIDENCE_DROP = "confidence_drop"  # ä¿¡é ¼åº¦ä½ä¸‹
    MARKET_REGIME_CHANGE = "market_regime_change"  # å¸‚å ´ç’°å¢ƒå¤‰åŒ–
    ERROR_RATE_SPIKE = "error_rate_spike"  # ã‚¨ãƒ©ãƒ¼ç‡ä¸Šæ˜‡
    MANUAL = "manual"                    # æ‰‹å‹•å®Ÿè¡Œ


class OptimizationStatus(Enum):
    """æœ€é©åŒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    IDLE = "idle"
    ANALYZING = "analyzing"
    OPTIMIZING = "optimizing"
    TESTING = "testing"
    DEPLOYING = "deploying"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class PerformanceTarget:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™"""
    target_win_rate: float = 0.6
    target_sharpe_ratio: float = 1.5
    max_drawdown_limit: float = -0.08
    min_confidence: float = 0.65
    target_return: float = 0.03
    
    # æ”¹å–„é–¾å€¤
    min_improvement_threshold: float = 0.02
    significance_threshold: float = 0.05


@dataclass
class OptimizationTask:
    """æœ€é©åŒ–ã‚¿ã‚¹ã‚¯"""
    task_id: str
    trigger: OptimizationTrigger
    priority: int
    created_time: datetime
    target_component: str
    optimization_type: str
    parameters: Dict[str, Any]
    status: OptimizationStatus = OptimizationStatus.IDLE
    progress: float = 0.0
    results: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    completion_time: Optional[datetime] = None


@dataclass
class OptimizationResult:
    """æœ€é©åŒ–çµæœ"""
    task_id: str
    component: str
    optimization_type: str
    original_params: Dict[str, Any]
    optimized_params: Dict[str, Any]
    performance_improvement: Dict[str, float]
    validation_metrics: Dict[str, float]
    confidence_score: float
    deployment_recommendation: str
    optimization_time: float


class ParameterOptimizer:
    """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"""
    
    def __init__(self, performance_targets: PerformanceTarget):
        self.performance_targets = performance_targets
        self.optimization_history = []
    
    def optimize_ensemble_parameters(self, current_params: Dict[str, Any], 
                                   performance_data: List[Dict[str, Any]]) -> OptimizationResult:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"""
        logger.info("Starting ensemble parameter optimization...")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ç©ºé–“å®šç¾©
        param_grid = {
            'confidence_threshold': np.arange(0.55, 0.85, 0.05),
            'ensemble_weights': self._generate_weight_combinations(),
            'dynamic_threshold_factor': np.arange(0.8, 1.2, 0.1),
            'risk_adjustment_factor': np.arange(0.9, 1.1, 0.05)
        }
        
        best_params = current_params.copy()
        best_score = self._calculate_performance_score(performance_data, current_params)
        optimization_results = []
        
        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒå®Ÿè¡Œ
        total_combinations = len(list(ParameterGrid(param_grid)))
        tested_combinations = 0
        
        for params in ParameterGrid(param_grid):
            tested_combinations += 1
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨ã§ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            simulated_performance = self._simulate_performance_with_params(
                performance_data, params
            )
            
            # ã‚¹ã‚³ã‚¢è¨ˆç®—
            score = self._calculate_performance_score(simulated_performance, params)
            
            optimization_results.append({
                'params': params,
                'score': score,
                'simulated_performance': simulated_performance
            })
            
            # ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
            if score > best_score:
                best_score = score
                best_params.update(params)
            
            # é€²æ—ãƒ­ã‚°
            if tested_combinations % 10 == 0:
                logger.info(f"Optimization progress: {tested_combinations}/{total_combinations}")
        
        # æ”¹å–„åŠ¹æœè¨ˆç®—
        performance_improvement = self._calculate_improvement(
            performance_data, optimization_results
        )
        
        # æ¤œè¨¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        validation_metrics = self._validate_optimization(
            best_params, performance_data
        )
        
        result = OptimizationResult(
            task_id=f"ensemble_opt_{int(time.time())}",
            component="ensemble_classifier",
            optimization_type="parameter_tuning",
            original_params=current_params,
            optimized_params=best_params,
            performance_improvement=performance_improvement,
            validation_metrics=validation_metrics,
            confidence_score=self._calculate_confidence_score(optimization_results),
            deployment_recommendation=self._generate_deployment_recommendation(
                performance_improvement, validation_metrics
            ),
            optimization_time=time.time()
        )
        
        self.optimization_history.append(result)
        return result
    
    def optimize_threshold_strategy(self, current_thresholds: Dict[str, float],
                                  signal_history: List[Dict[str, Any]]) -> OptimizationResult:
        """å‹•çš„é–¾å€¤æˆ¦ç•¥æœ€é©åŒ–"""
        logger.info("Starting threshold strategy optimization...")
        
        # VIXåˆ¥æœ€é©é–¾å€¤æ¢ç´¢
        vix_ranges = [(0, 20), (20, 30), (30, 40), (40, 100)]
        optimized_thresholds = {}
        
        for vix_min, vix_max in vix_ranges:
            # VIXç¯„å›²ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿
            range_data = [
                signal for signal in signal_history
                if vix_min <= signal.get('vix_level', 25) < vix_max
            ]
            
            if not range_data:
                continue
            
            # ã“ã®ç¯„å›²ã§ã®æœ€é©é–¾å€¤æ¢ç´¢
            threshold_range = np.arange(0.5, 0.9, 0.02)
            best_threshold = current_thresholds.get(f'vix_{vix_min}_{vix_max}', 0.65)
            best_performance = 0
            
            for threshold in threshold_range:
                # é–¾å€¤é©ç”¨ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆç®—
                performance = self._calculate_threshold_performance(
                    range_data, threshold
                )
                
                if performance > best_performance:
                    best_performance = performance
                    best_threshold = threshold
            
            optimized_thresholds[f'vix_{vix_min}_{vix_max}'] = best_threshold
        
        # çµæœç”Ÿæˆ
        performance_improvement = {
            'threshold_accuracy_improvement': self._calculate_threshold_accuracy_improvement(
                current_thresholds, optimized_thresholds, signal_history
            )
        }
        
        validation_metrics = {
            'cross_validation_score': self._cross_validate_thresholds(
                optimized_thresholds, signal_history
            ),
            'stability_score': self._calculate_threshold_stability(optimized_thresholds)
        }
        
        result = OptimizationResult(
            task_id=f"threshold_opt_{int(time.time())}",
            component="dynamic_threshold",
            optimization_type="threshold_optimization",
            original_params=current_thresholds,
            optimized_params=optimized_thresholds,
            performance_improvement=performance_improvement,
            validation_metrics=validation_metrics,
            confidence_score=validation_metrics['cross_validation_score'],
            deployment_recommendation="RECOMMENDED" if validation_metrics['cross_validation_score'] > 0.7 else "CAUTION",
            optimization_time=time.time()
        )
        
        return result
    
    def _generate_weight_combinations(self) -> List[List[float]]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿çµ„ã¿åˆã‚ã›ç”Ÿæˆ"""
        combinations = []
        
        # ç­‰é‡ã¿
        combinations.append([1/3, 1/3, 1/3])
        
        # åé‡çµ„ã¿åˆã‚ã›
        for i in range(3):
            weights = [0.2, 0.2, 0.2]
            weights[i] = 0.6
            combinations.append(weights)
        
        # ã‚«ã‚¹ã‚¿ãƒ çµ„ã¿åˆã‚ã›
        combinations.extend([
            [0.5, 0.3, 0.2],
            [0.4, 0.4, 0.2],
            [0.6, 0.2, 0.2]
        ])
        
        return combinations
    
    def _simulate_performance_with_params(self, performance_data: List[Dict[str, Any]], 
                                        params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        simulated_data = []
        
        for data_point in performance_data:
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹æœã‚’æ¨¡æ“¬
            confidence_boost = (params.get('confidence_threshold', 0.65) - 0.65) * 0.1
            weight_effect = self._calculate_weight_effect(params.get('ensemble_weights', [1/3, 1/3, 1/3]))
            
            simulated_win_rate = data_point.get('win_rate', 0.58) + confidence_boost + weight_effect
            simulated_confidence = data_point.get('avg_confidence', 0.7) + confidence_boost * 0.5
            
            simulated_data.append({
                'win_rate': np.clip(simulated_win_rate, 0, 1),
                'avg_confidence': np.clip(simulated_confidence, 0, 1),
                'total_return': data_point.get('total_return', 0.02) + confidence_boost * 0.5,
                'sharpe_ratio': data_point.get('sharpe_ratio', 1.2) + weight_effect
            })
        
        return simulated_data
    
    def _calculate_weight_effect(self, weights: List[float]) -> float:
        """é‡ã¿åŠ¹æœè¨ˆç®—"""
        # é‡ã¿ã®åˆ†æ•£ãŒå°ã•ã„ã»ã©å®‰å®šæ€§å‘ä¸Šï¼ˆç°¡æ˜“è¨ˆç®—ï¼‰
        weight_variance = np.var(weights)
        return 0.02 * (1 - weight_variance * 3)  # æ­£è¦åŒ–ã•ã‚ŒãŸåŠ¹æœ
    
    def _calculate_performance_score(self, performance_data: List[Dict[str, Any]], 
                                   params: Dict[str, Any]) -> float:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if not performance_data:
            return 0
        
        avg_win_rate = np.mean([d.get('win_rate', 0) for d in performance_data])
        avg_sharpe = np.mean([d.get('sharpe_ratio', 0) for d in performance_data])
        avg_confidence = np.mean([d.get('avg_confidence', 0) for d in performance_data])
        
        # ç›®æ¨™ã¨ã®æ¯”è¼ƒã§ã‚¹ã‚³ã‚¢è¨ˆç®—
        win_rate_score = avg_win_rate / self.performance_targets.target_win_rate
        sharpe_score = avg_sharpe / self.performance_targets.target_sharpe_ratio
        confidence_score = avg_confidence / self.performance_targets.min_confidence
        
        # é‡ã¿ä»˜ãç·åˆã‚¹ã‚³ã‚¢
        total_score = (win_rate_score * 0.4 + sharpe_score * 0.35 + confidence_score * 0.25)
        return total_score
    
    def _calculate_improvement(self, original_data: List[Dict[str, Any]], 
                             optimization_results: List[Dict[str, Any]]) -> Dict[str, float]:
        """æ”¹å–„åŠ¹æœè¨ˆç®—"""
        original_score = self._calculate_performance_score(original_data, {})
        
        best_result = max(optimization_results, key=lambda x: x['score'])
        improvement = best_result['score'] - original_score
        
        return {
            'total_improvement': improvement,
            'win_rate_improvement': np.mean([d.get('win_rate', 0) for d in best_result['simulated_performance']]) - 
                                  np.mean([d.get('win_rate', 0) for d in original_data]),
            'sharpe_improvement': np.mean([d.get('sharpe_ratio', 0) for d in best_result['simulated_performance']]) - 
                                np.mean([d.get('sharpe_ratio', 0) for d in original_data])
        }
    
    def _validate_optimization(self, optimized_params: Dict[str, Any], 
                             performance_data: List[Dict[str, Any]]) -> Dict[str, float]:
        """æœ€é©åŒ–æ¤œè¨¼"""
        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ¨¡æ“¬
        validation_scores = []
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¦ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        for i in range(5):  # 5-fold CV
            start_idx = i * len(performance_data) // 5
            end_idx = (i + 1) * len(performance_data) // 5
            
            test_data = performance_data[start_idx:end_idx]
            simulated_data = self._simulate_performance_with_params(test_data, optimized_params)
            score = self._calculate_performance_score(simulated_data, optimized_params)
            validation_scores.append(score)
        
        return {
            'cross_validation_mean': np.mean(validation_scores),
            'cross_validation_std': np.std(validation_scores),
            'overfitting_risk': np.std(validation_scores) / np.mean(validation_scores) if np.mean(validation_scores) > 0 else 1
        }
    
    def _calculate_confidence_score(self, optimization_results: List[Dict[str, Any]]) -> float:
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        scores = [result['score'] for result in optimization_results]
        
        if not scores:
            return 0.5
        
        # ä¸Šä½çµæœã®ä¸€è²«æ€§ã‚’ä¿¡é ¼åº¦ã¨ã™ã‚‹
        top_10_percent = sorted(scores, reverse=True)[:max(1, len(scores) // 10)]
        confidence = 1 - (np.std(top_10_percent) / np.mean(top_10_percent)) if np.mean(top_10_percent) > 0 else 0.5
        
        return np.clip(confidence, 0, 1)
    
    def _generate_deployment_recommendation(self, performance_improvement: Dict[str, float], 
                                          validation_metrics: Dict[str, float]) -> str:
        """ãƒ‡ãƒ—ãƒ­ã‚¤æ¨å¥¨åˆ¤å®š"""
        total_improvement = performance_improvement.get('total_improvement', 0)
        cv_score = validation_metrics.get('cross_validation_mean', 0)
        overfitting_risk = validation_metrics.get('overfitting_risk', 1)
        
        if total_improvement > 0.1 and cv_score > 0.8 and overfitting_risk < 0.2:
            return "STRONGLY_RECOMMENDED"
        elif total_improvement > 0.05 and cv_score > 0.7 and overfitting_risk < 0.3:
            return "RECOMMENDED"
        elif total_improvement > 0.02 and cv_score > 0.6:
            return "CAUTION"
        else:
            return "NOT_RECOMMENDED"
    
    def _calculate_threshold_performance(self, signal_data: List[Dict[str, Any]], 
                                       threshold: float) -> float:
        """é–¾å€¤ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆç®—"""
        if not signal_data:
            return 0
        
        correct_predictions = 0
        total_predictions = 0
        
        for signal in signal_data:
            confidence = signal.get('confidence', 0.7)
            actual_result = signal.get('actual_result', np.random.choice([0, 1]))
            
            if confidence >= threshold:
                prediction = 1 if confidence > 0.5 else 0
                if prediction == actual_result:
                    correct_predictions += 1
                total_predictions += 1
        
        return correct_predictions / total_predictions if total_predictions > 0 else 0
    
    def _calculate_threshold_accuracy_improvement(self, current_thresholds: Dict[str, float],
                                                optimized_thresholds: Dict[str, float],
                                                signal_history: List[Dict[str, Any]]) -> float:
        """é–¾å€¤ç²¾åº¦æ”¹å–„è¨ˆç®—"""
        current_accuracy = 0.65  # ç¾åœ¨ã®ç²¾åº¦ï¼ˆæ¨¡æ“¬ï¼‰
        optimized_accuracy = 0.68  # æœ€é©åŒ–å¾Œç²¾åº¦ï¼ˆæ¨¡æ“¬ï¼‰
        
        return optimized_accuracy - current_accuracy
    
    def _cross_validate_thresholds(self, thresholds: Dict[str, float], 
                                 signal_history: List[Dict[str, Any]]) -> float:
        """é–¾å€¤ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        # ç°¡æ˜“çš„ãªã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        return np.random.uniform(0.6, 0.9)  # æ¨¡æ“¬ã‚¹ã‚³ã‚¢
    
    def _calculate_threshold_stability(self, thresholds: Dict[str, float]) -> float:
        """é–¾å€¤å®‰å®šæ€§è¨ˆç®—"""
        threshold_values = list(thresholds.values())
        if not threshold_values:
            return 0
        
        # é–¾å€¤ã®ã°ã‚‰ã¤ããŒå°ã•ã„ã»ã©å®‰å®š
        stability = 1 - (np.std(threshold_values) / np.mean(threshold_values))
        return np.clip(stability, 0, 1)


class ModelUpdater:
    """ãƒ¢ãƒ‡ãƒ«æ›´æ–°ç®¡ç†"""
    
    def __init__(self, model_path: str = None):
        self.model_path = model_path or str(project_root / "model")
        self.backup_path = Path(self.model_path) / "backups"
        self.backup_path.mkdir(parents=True, exist_ok=True)
        
    def should_retrain_model(self, performance_metrics: List[Dict[str, Any]]) -> Tuple[bool, str]:
        """ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’åˆ¤å®š"""
        if not performance_metrics:
            return False, "ãƒ‡ãƒ¼ã‚¿ä¸è¶³"
        
        recent_metrics = performance_metrics[-20:]  # æœ€æ–°20ä»¶
        
        # å¹³å‡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        avg_accuracy = np.mean([m.get('prediction_accuracy', 0.6) for m in recent_metrics])
        avg_confidence = np.mean([m.get('avg_confidence', 0.7) for m in recent_metrics])
        
        # å†å­¦ç¿’æ¡ä»¶
        retrain_reasons = []
        
        if avg_accuracy < 0.55:
            retrain_reasons.append("äºˆæ¸¬ç²¾åº¦ä½ä¸‹")
        
        if avg_confidence < 0.6:
            retrain_reasons.append("ä¿¡é ¼åº¦ä½ä¸‹")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰
        if len(recent_metrics) >= 10:
            accuracy_trend = np.polyfit(range(10), [m.get('prediction_accuracy', 0.6) for m in recent_metrics[-10:]], 1)[0]
            if accuracy_trend < -0.01:  # 1%/æœŸé–“ã®ä¸‹é™
                retrain_reasons.append("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰")
        
        should_retrain = len(retrain_reasons) > 0
        reason = "; ".join(retrain_reasons) if retrain_reasons else ""
        
        return should_retrain, reason
    
    def backup_current_model(self) -> str:
        """ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = self.backup_path / f"ensemble_model_backup_{timestamp}.pkl"
        
        try:
            # æ¨¡æ“¬çš„ãªãƒ¢ãƒ‡ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            backup_data = {
                'timestamp': timestamp,
                'model_type': 'ensemble_classifier',
                'backup_reason': 'scheduled_backup'
            }
            
            with open(backup_file, 'wb') as f:
                pickle.dump(backup_data, f)
            
            logger.info(f"Model backed up to: {backup_file}")
            return str(backup_file)
            
        except Exception as e:
            logger.error(f"Failed to backup model: {e}")
            return ""
    
    def update_model_incrementally(self, new_data: pd.DataFrame, 
                                 new_targets: pd.Series) -> Dict[str, Any]:
        """å¢—åˆ†ãƒ¢ãƒ‡ãƒ«æ›´æ–°"""
        logger.info("Starting incremental model update...")
        
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            backup_path = self.backup_current_model()
            
            # å¢—åˆ†å­¦ç¿’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            update_results = {
                'update_type': 'incremental',
                'new_samples': len(new_data),
                'backup_path': backup_path,
                'accuracy_before': 0.68,
                'accuracy_after': 0.70,
                'update_time': time.time(),
                'success': True
            }
            
            logger.info("Incremental model update completed successfully")
            return update_results
            
        except Exception as e:
            logger.error(f"Incremental model update failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def retrain_model_full(self, training_data: pd.DataFrame,
                          training_targets: pd.Series) -> Dict[str, Any]:
        """ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’"""
        logger.info("Starting full model retraining...")
        
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            backup_path = self.backup_current_model()
            
            # ãƒ•ãƒ«å†å­¦ç¿’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            retrain_results = {
                'retrain_type': 'full',
                'training_samples': len(training_data),
                'backup_path': backup_path,
                'accuracy_before': 0.65,
                'accuracy_after': 0.72,
                'training_time': 1800,  # 30åˆ†
                'cross_validation_score': 0.71,
                'success': True
            }
            
            logger.info("Full model retraining completed successfully")
            return retrain_results
            
        except Exception as e:
            logger.error(f"Full model retraining failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def rollback_model(self, backup_path: str) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        logger.info(f"Rolling back model from: {backup_path}")
        
        try:
            if not os.path.exists(backup_path):
                raise FileNotFoundError(f"Backup file not found: {backup_path}")
            
            # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œï¼ˆæ¨¡æ“¬ï¼‰
            rollback_results = {
                'rollback_successful': True,
                'backup_path': backup_path,
                'rollback_time': time.time(),
                'restored_version': backup_path.split('_')[-1].replace('.pkl', '')
            }
            
            logger.info("Model rollback completed successfully")
            return rollback_results
            
        except Exception as e:
            logger.error(f"Model rollback failed: {e}")
            return {'rollback_successful': False, 'error': str(e)}


class ContinuousOptimizationFramework:
    """ç¶™ç¶šæœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯"""
    
    def __init__(self, config_path: str = None):
        """
        ç¶™ç¶šæœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–
        
        Parameters:
        -----------
        config_path : str
            è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.config = self._load_config(config_path)
        self.performance_targets = PerformanceTarget(**self.config.get('targets', {}))
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.parameter_optimizer = ParameterOptimizer(self.performance_targets)
        self.model_updater = ModelUpdater(self.config.get('model_path'))
        
        # ã‚¿ã‚¹ã‚¯ç®¡ç†
        self.optimization_queue = []
        self.active_tasks = {}
        self.completed_tasks = []
        
        # å®Ÿè¡ŒçŠ¶æ…‹
        self.is_running = False
        self.optimization_thread = None
        
        # å®Ÿè¡Œå±¥æ­´
        self.optimization_history = []
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«
        self.status_file = project_root / "status_optimization.json"
        
        logger.info("Continuous Optimization Framework initialized")
    
    def _load_config(self, config_path: str = None) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        if config_path is None:
            config_path = project_root / "config" / "continuous_optimization.yml"
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"Optimization config loaded from: {config_path}")
            return config
        except Exception as e:
            logger.error(f"Failed to load optimization config: {e}")
            return self._get_default_optimization_config()
    
    def _get_default_optimization_config(self) -> Dict:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœ€é©åŒ–è¨­å®š"""
        return {
            'targets': {
                'target_win_rate': 0.6,
                'target_sharpe_ratio': 1.5,
                'max_drawdown_limit': -0.08,
                'min_confidence': 0.65,
                'target_return': 0.03,
                'min_improvement_threshold': 0.02,
                'significance_threshold': 0.05
            },
            'scheduling': {
                'parameter_optimization_interval': 7,  # æ—¥
                'model_evaluation_interval': 3,       # æ—¥
                'full_retrain_interval': 30,         # æ—¥
                'max_concurrent_tasks': 2
            },
            'model_path': str(project_root / "model"),
            'performance_degradation_threshold': 0.05,
            'confidence_drop_threshold': 0.1
        }
    
    def start_optimization_framework(self):
        """æœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é–‹å§‹"""
        if self.is_running:
            logger.warning("Optimization framework already running")
            return
        
        self.is_running = True
        self.optimization_thread = threading.Thread(
            target=self._optimization_loop, daemon=True
        )
        self.optimization_thread.start()
        
        logger.info("Continuous optimization framework started")
    
    def stop_optimization_framework(self):
        """æœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åœæ­¢"""
        self.is_running = False
        
        if self.optimization_thread and self.optimization_thread.is_alive():
            self.optimization_thread.join(timeout=10)
        
        logger.info("Continuous optimization framework stopped")
    
    def trigger_optimization(self, trigger: OptimizationTrigger, 
                           component: str = "ensemble", 
                           optimization_type: str = "parameter_tuning",
                           parameters: Dict[str, Any] = None) -> str:
        """æœ€é©åŒ–ãƒˆãƒªã‚¬ãƒ¼"""
        task_id = f"opt_{trigger.value}_{int(time.time())}"
        
        task = OptimizationTask(
            task_id=task_id,
            trigger=trigger,
            priority=self._get_priority_for_trigger(trigger),
            created_time=datetime.now(),
            target_component=component,
            optimization_type=optimization_type,
            parameters=parameters or {}
        )
        
        self.optimization_queue.append(task)
        self.optimization_queue.sort(key=lambda x: x.priority, reverse=True)
        
        logger.info(f"Optimization task queued: {task_id} ({trigger.value})")
        return task_id
    
    def _optimization_loop(self):
        """æœ€é©åŒ–ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        while self.is_running:
            try:
                # å®šæœŸæœ€é©åŒ–ãƒã‚§ãƒƒã‚¯
                self._check_scheduled_optimizations()
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ãƒã‚§ãƒƒã‚¯
                self._check_performance_triggers()
                
                # ã‚­ãƒ¥ãƒ¼å‡¦ç†
                self._process_optimization_queue()
                
                # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
                self._update_optimization_status()
                
                time.sleep(300)  # 5åˆ†é–“éš”
                
            except Exception as e:
                logger.error(f"Optimization loop error: {e}")
                time.sleep(60)
    
    def _check_scheduled_optimizations(self):
        """å®šæœŸæœ€é©åŒ–ãƒã‚§ãƒƒã‚¯"""
        now = datetime.now()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼ˆé€±æ¬¡ï¼‰
        if self._should_run_scheduled_optimization('parameter_optimization', 7):
            self.trigger_optimization(
                OptimizationTrigger.SCHEDULED,
                "ensemble",
                "parameter_tuning"
            )
        
        # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼ˆ3æ—¥é–“éš”ï¼‰
        if self._should_run_scheduled_optimization('model_evaluation', 3):
            self.trigger_optimization(
                OptimizationTrigger.SCHEDULED,
                "model",
                "evaluation"
            )
        
        # ãƒ•ãƒ«å†å­¦ç¿’ï¼ˆæœˆæ¬¡ï¼‰
        if self._should_run_scheduled_optimization('full_retrain', 30):
            self.trigger_optimization(
                OptimizationTrigger.SCHEDULED,
                "model",
                "full_retrain"
            )
    
    def _should_run_scheduled_optimization(self, optimization_type: str, 
                                         interval_days: int) -> bool:
        """å®šæœŸæœ€é©åŒ–å®Ÿè¡Œåˆ¤å®š"""
        last_run = self._get_last_optimization_time(optimization_type)
        if not last_run:
            return True
        
        time_since_last = datetime.now() - last_run
        return time_since_last.days >= interval_days
    
    def _get_last_optimization_time(self, optimization_type: str) -> Optional[datetime]:
        """æœ€çµ‚æœ€é©åŒ–æ™‚åˆ»å–å¾—"""
        for task in reversed(self.completed_tasks):
            if task.optimization_type == optimization_type and task.status == OptimizationStatus.COMPLETED:
                return task.completion_time
        return None
    
    def _check_performance_triggers(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ãƒã‚§ãƒƒã‚¯"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
        current_metrics = self._get_current_performance_metrics()
        
        if not current_metrics:
            return
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ãƒã‚§ãƒƒã‚¯
        degradation_threshold = self.config.get('performance_degradation_threshold', 0.05)
        if current_metrics.get('win_rate_decline', 0) > degradation_threshold:
            self.trigger_optimization(
                OptimizationTrigger.PERFORMANCE_DEGRADATION,
                "ensemble",
                "parameter_tuning"
            )
        
        # ä¿¡é ¼åº¦ä½ä¸‹ãƒã‚§ãƒƒã‚¯
        confidence_threshold = self.config.get('confidence_drop_threshold', 0.1)
        if current_metrics.get('confidence_drop', 0) > confidence_threshold:
            self.trigger_optimization(
                OptimizationTrigger.CONFIDENCE_DROP,
                "ensemble",
                "threshold_optimization"
            )
    
    def _get_current_performance_metrics(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰å–å¾—
        # ã“ã“ã§ã¯æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿è¿”å´
        return {
            'current_win_rate': 0.58,
            'win_rate_decline': 0.03,
            'current_confidence': 0.68,
            'confidence_drop': 0.05,
            'current_sharpe': 1.15
        }
    
    def _process_optimization_queue(self):
        """æœ€é©åŒ–ã‚­ãƒ¥ãƒ¼å‡¦ç†"""
        max_concurrent = self.config.get('scheduling', {}).get('max_concurrent_tasks', 2)
        
        while (len(self.active_tasks) < max_concurrent and 
               self.optimization_queue and 
               self.is_running):
            
            task = self.optimization_queue.pop(0)
            task.status = OptimizationStatus.ANALYZING
            self.active_tasks[task.task_id] = task
            
            # æœ€é©åŒ–å®Ÿè¡Œï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰
            optimization_thread = threading.Thread(
                target=self._execute_optimization_task,
                args=(task,),
                daemon=True
            )
            optimization_thread.start()
    
    def _execute_optimization_task(self, task: OptimizationTask):
        """æœ€é©åŒ–ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ"""
        try:
            logger.info(f"Executing optimization task: {task.task_id}")
            
            task.status = OptimizationStatus.OPTIMIZING
            task.progress = 0.1
            
            if task.optimization_type == "parameter_tuning":
                result = self._execute_parameter_optimization(task)
            elif task.optimization_type == "threshold_optimization":
                result = self._execute_threshold_optimization(task)
            elif task.optimization_type == "model_retrain":
                result = self._execute_model_retrain(task)
            elif task.optimization_type == "evaluation":
                result = self._execute_model_evaluation(task)
            else:
                raise ValueError(f"Unknown optimization type: {task.optimization_type}")
            
            task.progress = 0.8
            task.status = OptimizationStatus.TESTING
            
            # çµæœæ¤œè¨¼
            validation_result = self._validate_optimization_result(result)
            
            task.progress = 0.9
            task.status = OptimizationStatus.DEPLOYING
            
            # è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤åˆ¤å®š
            if self._should_auto_deploy(result, validation_result):
                deploy_result = self._deploy_optimization(result)
                result['deployment'] = deploy_result
            
            task.results = result
            task.status = OptimizationStatus.COMPLETED
            task.progress = 1.0
            task.completion_time = datetime.now()
            
            logger.info(f"Optimization task completed: {task.task_id}")
            
        except Exception as e:
            task.status = OptimizationStatus.FAILED
            task.error_message = str(e)
            task.completion_time = datetime.now()
            logger.error(f"Optimization task failed: {task.task_id} - {e}")
        
        finally:
            # ã‚¿ã‚¹ã‚¯ç§»å‹•
            if task.task_id in self.active_tasks:
                del self.active_tasks[task.task_id]
            self.completed_tasks.append(task)
            self.optimization_history.append(task)
    
    def _execute_parameter_optimization(self, task: OptimizationTask) -> Dict[str, Any]:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å®Ÿè¡Œ"""
        # ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
        current_params = self._get_current_parameters(task.target_component)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—
        performance_data = self._get_performance_data()
        
        # æœ€é©åŒ–å®Ÿè¡Œ
        optimization_result = self.parameter_optimizer.optimize_ensemble_parameters(
            current_params, performance_data
        )
        
        return asdict(optimization_result)
    
    def _execute_threshold_optimization(self, task: OptimizationTask) -> Dict[str, Any]:
        """é–¾å€¤æœ€é©åŒ–å®Ÿè¡Œ"""
        current_thresholds = self._get_current_thresholds()
        signal_history = self._get_signal_history()
        
        optimization_result = self.parameter_optimizer.optimize_threshold_strategy(
            current_thresholds, signal_history
        )
        
        return asdict(optimization_result)
    
    def _execute_model_retrain(self, task: OptimizationTask) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’å®Ÿè¡Œ"""
        training_data, training_targets = self._get_training_data()
        
        retrain_result = self.model_updater.retrain_model_full(
            training_data, training_targets
        )
        
        return retrain_result
    
    def _execute_model_evaluation(self, task: OptimizationTask) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡å®Ÿè¡Œ"""
        performance_metrics = self._get_performance_data()
        
        should_retrain, reason = self.model_updater.should_retrain_model(performance_metrics)
        
        return {
            'evaluation_type': 'model_health_check',
            'should_retrain': should_retrain,
            'retrain_reason': reason,
            'evaluation_time': time.time(),
            'performance_summary': {
                'avg_accuracy': np.mean([m.get('prediction_accuracy', 0.6) for m in performance_metrics]),
                'avg_confidence': np.mean([m.get('avg_confidence', 0.7) for m in performance_metrics])
            }
        }
    
    def _get_current_parameters(self, component: str) -> Dict[str, Any]:
        """ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚„DBã‹ã‚‰å–å¾—
        return {
            'confidence_threshold': 0.65,
            'ensemble_weights': [1/3, 1/3, 1/3],
            'dynamic_threshold_factor': 1.0,
            'risk_adjustment_factor': 1.0
        }
    
    def _get_current_thresholds(self) -> Dict[str, float]:
        """ç¾åœ¨ã®é–¾å€¤å–å¾—"""
        return {
            'vix_0_20': 0.6,
            'vix_20_30': 0.65,
            'vix_30_40': 0.7,
            'vix_40_100': 0.75
        }
    
    def _get_performance_data(self) -> List[Dict[str, Any]]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰å–å¾—
        # ã“ã“ã§ã¯æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        performance_data = []
        for i in range(50):
            performance_data.append({
                'win_rate': 0.58 + np.random.normal(0, 0.02),
                'total_return': 0.02 + np.random.normal(0, 0.005),
                'sharpe_ratio': 1.2 + np.random.normal(0, 0.1),
                'avg_confidence': 0.7 + np.random.normal(0, 0.05),
                'prediction_accuracy': 0.65 + np.random.normal(0, 0.03)
            })
        return performance_data
    
    def _get_signal_history(self) -> List[Dict[str, Any]]:
        """ã‚·ã‚°ãƒŠãƒ«å±¥æ­´å–å¾—"""
        signal_history = []
        for i in range(100):
            signal_history.append({
                'confidence': np.random.uniform(0.5, 0.9),
                'vix_level': np.random.uniform(15, 40),
                'actual_result': np.random.choice([0, 1])
            })
        return signal_history
    
    def _get_training_data(self) -> Tuple[pd.DataFrame, pd.Series]:
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        # æ¨¡æ“¬å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
        n_samples = 1000
        n_features = 10
        
        X = pd.DataFrame(np.random.randn(n_samples, n_features))
        y = pd.Series(np.random.choice([0, 1], n_samples))
        
        return X, y
    
    def _validate_optimization_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """æœ€é©åŒ–çµæœæ¤œè¨¼"""
        validation = {
            'performance_improvement_significant': True,
            'validation_score': 0.75,
            'risk_assessment': 'LOW',
            'deployment_safe': True
        }
        
        # æ”¹å–„åŠ¹æœæ¤œè¨¼
        total_improvement = result.get('performance_improvement', {}).get('total_improvement', 0)
        if total_improvement < self.performance_targets.min_improvement_threshold:
            validation['performance_improvement_significant'] = False
        
        # ãƒªã‚¹ã‚¯è©•ä¾¡
        overfitting_risk = result.get('validation_metrics', {}).get('overfitting_risk', 0)
        if overfitting_risk > 0.3:
            validation['risk_assessment'] = 'HIGH'
            validation['deployment_safe'] = False
        
        return validation
    
    def _should_auto_deploy(self, optimization_result: Dict[str, Any], 
                          validation_result: Dict[str, Any]) -> bool:
        """è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤åˆ¤å®š"""
        deployment_recommendation = optimization_result.get('deployment_recommendation', 'NOT_RECOMMENDED')
        validation_safe = validation_result.get('deployment_safe', False)
        
        return (deployment_recommendation in ['STRONGLY_RECOMMENDED', 'RECOMMENDED'] and 
                validation_safe)
    
    def _deploy_optimization(self, optimization_result: Dict[str, Any]) -> Dict[str, Any]:
        """æœ€é©åŒ–çµæœãƒ‡ãƒ—ãƒ­ã‚¤"""
        logger.info("Deploying optimization results...")
        
        try:
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ï¼ˆæ¨¡æ“¬ï¼‰
            optimized_params = optimization_result.get('optimized_params', {})
            
            deployment_result = {
                'deployment_successful': True,
                'deployed_params': optimized_params,
                'deployment_time': time.time(),
                'backup_created': True,
                'rollback_available': True
            }
            
            logger.info("Optimization deployment completed successfully")
            return deployment_result
            
        except Exception as e:
            logger.error(f"Optimization deployment failed: {e}")
            return {
                'deployment_successful': False,
                'error': str(e),
                'deployment_time': time.time()
            }
    
    def _get_priority_for_trigger(self, trigger: OptimizationTrigger) -> int:
        """ãƒˆãƒªã‚¬ãƒ¼å„ªå…ˆåº¦å–å¾—"""
        priority_map = {
            OptimizationTrigger.EMERGENCY: 10,
            OptimizationTrigger.PERFORMANCE_DEGRADATION: 8,
            OptimizationTrigger.ERROR_RATE_SPIKE: 7,
            OptimizationTrigger.CONFIDENCE_DROP: 6,
            OptimizationTrigger.MARKET_REGIME_CHANGE: 5,
            OptimizationTrigger.SCHEDULED: 3,
            OptimizationTrigger.MANUAL: 2
        }
        return priority_map.get(trigger, 1)
    
    def _update_optimization_status(self):
        """æœ€é©åŒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°"""
        try:
            status = {
                'timestamp': datetime.now().isoformat(),
                'is_running': self.is_running,
                'queue_size': len(self.optimization_queue),
                'active_tasks': len(self.active_tasks),
                'completed_tasks': len(self.completed_tasks),
                'optimization_history_size': len(self.optimization_history)
            }
            
            with open(self.status_file, 'w', encoding='utf-8') as f:
                json.dump(status, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            logger.error(f"Failed to update optimization status: {e}")
    
    def get_optimization_status(self) -> Dict[str, Any]:
        """æœ€é©åŒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—"""
        return {
            'framework_running': self.is_running,
            'queue_length': len(self.optimization_queue),
            'active_tasks': len(self.active_tasks),
            'completed_tasks_count': len(self.completed_tasks),
            'recent_optimizations': [
                {
                    'task_id': task.task_id,
                    'status': task.status.value,
                    'progress': task.progress,
                    'trigger': task.trigger.value,
                    'component': task.target_component
                }
                for task in list(self.active_tasks.values()) + self.completed_tasks[-5:]
            ]
        }
    
    def get_optimization_history_summary(self) -> Dict[str, Any]:
        """æœ€é©åŒ–å±¥æ­´ã‚µãƒãƒªãƒ¼"""
        if not self.optimization_history:
            return {'total_optimizations': 0}
        
        successful_tasks = [task for task in self.optimization_history 
                          if task.status == OptimizationStatus.COMPLETED]
        
        return {
            'total_optimizations': len(self.optimization_history),
            'successful_optimizations': len(successful_tasks),
            'success_rate': len(successful_tasks) / len(self.optimization_history),
            'optimization_by_trigger': {
                trigger.value: len([task for task in self.optimization_history 
                                  if task.trigger == trigger])
                for trigger in OptimizationTrigger
            },
            'avg_optimization_time': np.mean([
                (task.completion_time - task.created_time).total_seconds()
                for task in successful_tasks
                if task.completion_time
            ]) if successful_tasks else 0
        }


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ”„ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  ç¶™ç¶šæœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯")
    print("=" * 70)
    
    try:
        # æœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–
        optimization_framework = ContinuousOptimizationFramework()
        
        # å¯¾è©±å¼ãƒ¡ãƒ‹ãƒ¥ãƒ¼
        while True:
            print("\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰:")
            print("1. æœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é–‹å§‹")
            print("2. æœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åœæ­¢")
            print("3. æ‰‹å‹•æœ€é©åŒ–ãƒˆãƒªã‚¬ãƒ¼")
            print("4. æœ€é©åŒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª")
            print("5. æœ€é©åŒ–å±¥æ­´è¡¨ç¤º")
            print("6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ç¢ºèª")
            print("0. çµ‚äº†")
            
            choice = input("\nã‚³ãƒãƒ³ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„ (0-6): ").strip()
            
            if choice == '1':
                optimization_framework.start_optimization_framework()
                print("âœ… ç¶™ç¶šæœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é–‹å§‹ã—ã¾ã—ãŸ")
                
            elif choice == '2':
                optimization_framework.stop_optimization_framework()
                print("âœ… ç¶™ç¶šæœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’åœæ­¢ã—ã¾ã—ãŸ")
                
            elif choice == '3':
                print("\næœ€é©åŒ–ã‚¿ã‚¤ãƒ—ã‚’é¸æŠ:")
                print("1. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´")
                print("2. é–¾å€¤æœ€é©åŒ–") 
                print("3. ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’")
                
                opt_choice = input("é¸æŠ (1-3): ").strip()
                
                if opt_choice == '1':
                    task_id = optimization_framework.trigger_optimization(
                        OptimizationTrigger.MANUAL, "ensemble", "parameter_tuning"
                    )
                    print(f"âœ… ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’ãƒˆãƒªã‚¬ãƒ¼ã—ã¾ã—ãŸ: {task_id}")
                elif opt_choice == '2':
                    task_id = optimization_framework.trigger_optimization(
                        OptimizationTrigger.MANUAL, "ensemble", "threshold_optimization"
                    )
                    print(f"âœ… é–¾å€¤æœ€é©åŒ–ã‚’ãƒˆãƒªã‚¬ãƒ¼ã—ã¾ã—ãŸ: {task_id}")
                elif opt_choice == '3':
                    task_id = optimization_framework.trigger_optimization(
                        OptimizationTrigger.MANUAL, "model", "model_retrain"
                    )
                    print(f"âœ… ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’ã‚’ãƒˆãƒªã‚¬ãƒ¼ã—ã¾ã—ãŸ: {task_id}")
                
            elif choice == '4':
                status = optimization_framework.get_optimization_status()
                print(f"\nğŸ“Š æœ€é©åŒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:")
                print(f"  ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å®Ÿè¡Œä¸­: {status['framework_running']}")
                print(f"  ã‚­ãƒ¥ãƒ¼é•·: {status['queue_length']}")
                print(f"  å®Ÿè¡Œä¸­ã‚¿ã‚¹ã‚¯: {status['active_tasks']}")
                print(f"  å®Œäº†ã‚¿ã‚¹ã‚¯æ•°: {status['completed_tasks_count']}")
                
                if status['recent_optimizations']:
                    print(f"\nğŸ“ æœ€è¿‘ã®æœ€é©åŒ–:")
                    for opt in status['recent_optimizations'][-3:]:
                        print(f"    {opt['task_id']}: {opt['status']} ({opt['progress']:.1%})")
                
            elif choice == '5':
                history = optimization_framework.get_optimization_history_summary()
                print(f"\nğŸ“ˆ æœ€é©åŒ–å±¥æ­´ã‚µãƒãƒªãƒ¼:")
                print(f"  ç·æœ€é©åŒ–å›æ•°: {history['total_optimizations']}")
                print(f"  æˆåŠŸç‡: {history.get('success_rate', 0):.1%}")
                print(f"  å¹³å‡å®Ÿè¡Œæ™‚é–“: {history.get('avg_optimization_time', 0):.0f}ç§’")
                
                if 'optimization_by_trigger' in history:
                    print(f"\nğŸ“Š ãƒˆãƒªã‚¬ãƒ¼åˆ¥å®Ÿè¡Œå›æ•°:")
                    for trigger, count in history['optimization_by_trigger'].items():
                        if count > 0:
                            print(f"    {trigger}: {count}å›")
                
            elif choice == '6':
                targets = optimization_framework.performance_targets
                print(f"\nğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™:")
                print(f"  ç›®æ¨™å‹ç‡: {targets.target_win_rate:.1%}")
                print(f"  ç›®æ¨™ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª: {targets.target_sharpe_ratio:.2f}")
                print(f"  æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³é™ç•Œ: {targets.max_drawdown_limit:.1%}")
                print(f"  æœ€å°ä¿¡é ¼åº¦: {targets.min_confidence:.2f}")
                print(f"  ç›®æ¨™ãƒªã‚¿ãƒ¼ãƒ³: {targets.target_return:.1%}")
                
            elif choice == '0':
                if optimization_framework.is_running:
                    optimization_framework.stop_optimization_framework()
                print("ğŸ‘‹ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
                break
                
            else:
                print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
        
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"Main execution failed: {e}")
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


if __name__ == "__main__":
    main()