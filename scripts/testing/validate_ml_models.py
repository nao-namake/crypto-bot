#!/usr/bin/env python3
"""
Phase 61: MLæ¤œè¨¼çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

çµ±åˆå…ƒ:
- Phase 54.7: validate_model_performance.py
- Phase 54.8: validate_ml_prediction_distribution.py
- Phase 51.5-A/55.7: validate_model_consistency.py
- Phase 59.8: Stackingæ¤œè¨¼è¿½åŠ 

æ¤œè¨¼é …ç›®:
1. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ•´åˆæ€§
2. ç‰¹å¾´é‡æ•°æ•´åˆæ€§
3. full/basicãƒ¢ãƒ‡ãƒ«å·®ç•°ï¼ˆMD5æ¯”è¼ƒï¼‰
4. 3ã‚¯ãƒ©ã‚¹åˆ†é¡ç¢ºèª
5. äºˆæ¸¬åˆ†å¸ƒæ¤œè¨¼
6. ä¿¡é ¼åº¦çµ±è¨ˆ
7. å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«æ€§èƒ½
8. Stackingãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ï¼ˆPhase 59.8è¿½åŠ ï¼‰

ä½¿ç”¨æ–¹æ³•:
    # å…¨æ¤œè¨¼
    python scripts/testing/validate_ml_models.py

    # ç‰¹å®šæ¤œè¨¼ã®ã¿
    python scripts/testing/validate_ml_models.py --check consistency
    python scripts/testing/validate_ml_models.py --check distribution
    python scripts/testing/validate_ml_models.py --check performance

    # è»½é‡ãƒ¢ãƒ¼ãƒ‰ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãªã—ãƒ»é«˜é€Ÿï¼‰
    python scripts/testing/validate_ml_models.py --quick
"""

import argparse
import hashlib
import json
import pickle
import sys
from pathlib import Path
from typing import Dict, List, Optional

import numpy as np
import pandas as pd

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
PROJECT_ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / "src"))


class MLModelValidator:
    """MLãƒ¢ãƒ‡ãƒ«çµ±åˆæ¤œè¨¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.errors: List[str] = []
        self.warnings: List[str] = []
        self.model = None
        self.metadata = None
        self.feature_order_data = None

    # ========================================
    # æ•´åˆæ€§æ¤œè¨¼ï¼ˆconsistencyï¼‰
    # ========================================

    def _load_feature_order(self) -> Optional[Dict]:
        """feature_order.jsonèª­ã¿è¾¼ã¿"""
        path = self.project_root / "config/core/feature_order.json"
        if not path.exists():
            self.errors.append(f"âŒ {path} not found")
            return None

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            print(f"âœ… feature_order.jsonèª­ã¿è¾¼ã¿æˆåŠŸ")
            print(f"   Phase: {data.get('phase', 'unknown')}")
            print(f"   Total features: {data.get('total_features', 'unknown')}")
            return data
        except Exception as e:
            self.errors.append(f"âŒ feature_order.jsonèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _load_model_metadata(self) -> Optional[Dict]:
        """production_model_metadata.jsonèª­ã¿è¾¼ã¿"""
        path = self.project_root / "models/production/production_model_metadata.json"
        if not path.exists():
            self.warnings.append(
                "âš ï¸  production_model_metadata.json not found - ãƒ¢ãƒ‡ãƒ«æœªè¨“ç·´ã®å¯èƒ½æ€§"
            )
            return None

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            print(f"\nâœ… production_model_metadata.jsonèª­ã¿è¾¼ã¿æˆåŠŸ")
            print(f"   Phase: {data.get('phase', 'unknown')}")
            print(
                f"   Feature count: {data.get('training_info', {}).get('feature_count', 'unknown')}"
            )
            return data
        except Exception as e:
            self.warnings.append(f"âš ï¸  production_model_metadata.jsonèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _count_active_strategies(self) -> int:
        """strategies.yamlã‹ã‚‰æœ‰åŠ¹æˆ¦ç•¥æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        path = self.project_root / "config/strategies.yaml"
        if not path.exists():
            self.warnings.append(f"âš ï¸  {path} not found - æˆ¦ç•¥æ•°æ¤œè¨¼ã‚¹ã‚­ãƒƒãƒ—")
            return 0

        try:
            import yaml

            with open(path, "r", encoding="utf-8") as f:
                strategies_config = yaml.safe_load(f)

            strategies = strategies_config.get("strategies", {})
            if isinstance(strategies, dict):
                active = [
                    (name, cfg) for name, cfg in strategies.items() if cfg.get("enabled", False)
                ]
                count = len(active)
                print(f"\nâœ… strategies.yamlèª­ã¿è¾¼ã¿æˆåŠŸ")
                print(f"   æœ‰åŠ¹æˆ¦ç•¥æ•°: {count}")
                for name, _ in active:
                    print(f"     - {name}")
            else:
                active = [s for s in strategies if s.get("enabled", False)]
                count = len(active)
            return count
        except Exception as e:
            self.warnings.append(f"âš ï¸  strategies.yamlèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return 0

    def validate_feature_counts(self) -> None:
        """ç‰¹å¾´é‡æ•°ã®æ•´åˆæ€§æ¤œè¨¼"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ç‰¹å¾´é‡æ•°æ•´åˆæ€§æ¤œè¨¼")
        print("=" * 60)

        if not self.feature_order_data:
            return

        expected_full = (
            self.feature_order_data.get("feature_levels", {}).get("full", {}).get("count")
        )
        expected_basic = (
            self.feature_order_data.get("feature_levels", {}).get("basic", {}).get("count")
        )

        print(f"\nğŸ¯ æœŸå¾…å€¤ (feature_order.json):")
        print(f"   Full model: {expected_full} features")
        print(f"   Basic model: {expected_basic} features")

        if not self.metadata:
            self.warnings.append("âš ï¸  ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã— - ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãŒå¿…è¦ã§ã™")
            return

        actual_feature_count = self.metadata.get("training_info", {}).get("feature_count")
        actual_feature_names_count = len(self.metadata.get("feature_names", []))

        print(f"\nğŸ“¦ å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«:")
        print(f"   training_info.feature_count: {actual_feature_count}")
        print(f"   len(feature_names): {actual_feature_names_count}")

        if actual_feature_count != expected_full:
            self.errors.append(
                f"âŒ ç‰¹å¾´é‡æ•°ä¸ä¸€è‡´: ãƒ¢ãƒ‡ãƒ«={actual_feature_count}, æœŸå¾…å€¤={expected_full}"
            )
        else:
            print(f"\nâœ… ç‰¹å¾´é‡æ•°ä¸€è‡´: {actual_feature_count} == {expected_full}")

    def validate_strategy_signals(self) -> None:
        """æˆ¦ç•¥ä¿¡å·ç‰¹å¾´é‡ã®æ•´åˆæ€§æ¤œè¨¼"""
        print("\n" + "=" * 60)
        print("ğŸ¯ æˆ¦ç•¥ä¿¡å·ç‰¹å¾´é‡æ•´åˆæ€§æ¤œè¨¼")
        print("=" * 60)

        if not self.feature_order_data:
            return

        active_strategies = self._count_active_strategies()
        strategy_signals = self.feature_order_data.get("feature_categories", {}).get(
            "strategy_signals", {}
        )
        expected_signals = len(strategy_signals.get("features", []))

        print(f"\nğŸ¯ æœŸå¾…å€¤:")
        print(f"   æœ‰åŠ¹æˆ¦ç•¥æ•°: {active_strategies}")
        print(f"   æˆ¦ç•¥ä¿¡å·ç‰¹å¾´é‡æ•°: {expected_signals}")

        if active_strategies > 0 and active_strategies != expected_signals:
            self.errors.append(
                f"âŒ æˆ¦ç•¥ä¿¡å·æ•°ä¸ä¸€è‡´: æœ‰åŠ¹æˆ¦ç•¥={active_strategies}, æˆ¦ç•¥ä¿¡å·ç‰¹å¾´é‡={expected_signals}"
            )
        else:
            print(f"\nâœ… æˆ¦ç•¥ä¿¡å·æ•°ä¸€è‡´: {active_strategies} == {expected_signals}")

    def validate_model_files(self) -> None:
        """ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª"""
        print("\n" + "=" * 60)
        print("ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª")
        print("=" * 60)

        if not self.feature_order_data:
            return

        full_model_file = (
            self.feature_order_data.get("feature_levels", {}).get("full", {}).get("model_file")
        )
        basic_model_file = (
            self.feature_order_data.get("feature_levels", {}).get("basic", {}).get("model_file")
        )

        print(f"\nğŸ¯ æœŸå¾…ã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"   Full: {full_model_file}")
        print(f"   Basic: {basic_model_file}")

        full_path = self.project_root / f"models/production/{full_model_file}"
        basic_path = self.project_root / f"models/production/{basic_model_file}"

        if full_path.exists():
            print(f"\nâœ… {full_model_file} å­˜åœ¨ç¢ºèª")
            print(f"   ã‚µã‚¤ã‚º: {full_path.stat().st_size / 1024 / 1024:.2f} MB")
        else:
            self.warnings.append(f"âš ï¸  {full_model_file} not found")

        if basic_path.exists():
            print(f"âœ… {basic_model_file} å­˜åœ¨ç¢ºèª")
            print(f"   ã‚µã‚¤ã‚º: {basic_path.stat().st_size / 1024 / 1024:.2f} MB")
        else:
            self.warnings.append(f"âš ï¸  {basic_model_file} not found")

    def validate_model_difference(self) -> None:
        """Phase 55.7: full/basicãƒ¢ãƒ‡ãƒ«ãŒç•°ãªã‚‹ã‹æ¤œè¨¼ï¼ˆMD5æ¯”è¼ƒï¼‰"""
        print("\n" + "=" * 60)
        print("ğŸ”¬ full/basicãƒ¢ãƒ‡ãƒ«å·®ç•°æ¤œè¨¼")
        print("=" * 60)

        if not self.feature_order_data:
            return

        full_model_file = (
            self.feature_order_data.get("feature_levels", {}).get("full", {}).get("model_file")
        )
        basic_model_file = (
            self.feature_order_data.get("feature_levels", {}).get("basic", {}).get("model_file")
        )

        full_path = self.project_root / f"models/production/{full_model_file}"
        basic_path = self.project_root / f"models/production/{basic_model_file}"

        if not full_path.exists() or not basic_path.exists():
            self.warnings.append("âš ï¸  ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸è¶³ - MD5æ¯”è¼ƒã‚¹ã‚­ãƒƒãƒ—")
            return

        def get_md5(path: Path) -> str:
            hash_md5 = hashlib.md5()
            with open(path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()

        full_md5 = get_md5(full_path)
        basic_md5 = get_md5(basic_path)

        print(f"\nğŸ¯ MD5ãƒãƒƒã‚·ãƒ¥:")
        print(f"   Full:  {full_md5}")
        print(f"   Basic: {basic_md5}")

        if full_md5 == basic_md5:
            self.errors.append(
                "âŒ full/basicãƒ¢ãƒ‡ãƒ«ãŒåŒä¸€ï¼ˆMD5ä¸€è‡´ï¼‰- create_ml_models.pyã®ãƒã‚°ã®å¯èƒ½æ€§"
            )
        else:
            print(f"\nâœ… full/basicãƒ¢ãƒ‡ãƒ«ã¯ç•°ãªã‚‹ï¼ˆMD5ä¸ä¸€è‡´ï¼‰")

        full_size = full_path.stat().st_size
        basic_size = basic_path.stat().st_size
        print(f"\nğŸ¯ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º:")
        print(f"   Full:  {full_size / 1024 / 1024:.2f} MB")
        print(f"   Basic: {basic_size / 1024 / 1024:.2f} MB")

        if full_size <= basic_size:
            self.warnings.append(f"âš ï¸  fullãƒ¢ãƒ‡ãƒ« <= basicãƒ¢ãƒ‡ãƒ« - é€šå¸¸ã¯full > basic")

    def validate_n_classes(self) -> None:
        """Phase 55.7: ãƒ¢ãƒ‡ãƒ«ãŒ3ã‚¯ãƒ©ã‚¹åˆ†é¡ã‹æ¤œè¨¼"""
        print("\n" + "=" * 60)
        print("ğŸ”¬ 3ã‚¯ãƒ©ã‚¹åˆ†é¡æ¤œè¨¼")
        print("=" * 60)

        n_classes = None

        if self.metadata:
            n_classes = self.metadata.get("training_info", {}).get("n_classes")

        if n_classes is None and self.model:
            if hasattr(self.model, "n_classes"):
                n_classes = self.model.n_classes
            elif hasattr(self.model, "models"):
                for m in self.model.models.values():
                    if hasattr(m, "classes_"):
                        n_classes = len(m.classes_)
                        break

        if n_classes is None:
            full_path = self.project_root / "models/production/ensemble_full.pkl"
            if full_path.exists():
                try:
                    with open(full_path, "rb") as f:
                        model = pickle.load(f)
                    if hasattr(model, "n_classes"):
                        n_classes = model.n_classes
                    elif hasattr(model, "models"):
                        for m in model.models.values():
                            if hasattr(m, "classes_"):
                                n_classes = len(m.classes_)
                                break
                except Exception as e:
                    self.warnings.append(f"âš ï¸  ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                    return

        print(f"\nğŸ¯ æ¤œå‡ºã•ã‚ŒãŸã‚¯ãƒ©ã‚¹æ•°: {n_classes}")

        if n_classes is None:
            self.warnings.append("âš ï¸  ã‚¯ãƒ©ã‚¹æ•°ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸ")
        elif n_classes == 2:
            self.errors.append("âŒ ãƒ¢ãƒ‡ãƒ«ãŒ2ã‚¯ãƒ©ã‚¹åˆ†é¡ - 3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆBUY/HOLD/SELLï¼‰ãŒå¿…è¦")
        elif n_classes == 3:
            print(f"\nâœ… 3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆBUY/HOLD/SELLï¼‰ç¢ºèª")
        else:
            self.warnings.append(f"âš ï¸  äºˆæœŸã—ãªã„ã‚¯ãƒ©ã‚¹æ•°: {n_classes}")

    def validate_stacking_model(self) -> None:
        """Phase 59.8: Stackingãƒ¢ãƒ‡ãƒ«æ¤œè¨¼"""
        print("\n" + "=" * 60)
        print("ğŸ”¬ Phase 59.8: Stackingãƒ¢ãƒ‡ãƒ«æ¤œè¨¼")
        print("=" * 60)

        if not self.feature_order_data:
            return

        # feature_order.jsonã‹ã‚‰Stackingå®šç¾©ç¢ºèª
        stacking_info = self.feature_order_data.get("feature_levels", {}).get("stacking", {})

        if not stacking_info:
            print("\nâ„¹ï¸  Stackingãƒ¬ãƒ™ãƒ«å®šç¾©ãŒfeature_order.jsonã«ã‚ã‚Šã¾ã›ã‚“")
            print("   â†’ stacking_enabled=falseæ™‚ã¯æ­£å¸¸")
            return

        print(f"\nğŸ¯ Stackingå®šç¾© (feature_order.json):")
        print(f"   ç‰¹å¾´é‡æ•°: {stacking_info.get('count', 'unknown')}")
        print(f"   ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {stacking_info.get('model_file', 'unknown')}")
        print(f"   Meta-Learnerãƒ•ã‚¡ã‚¤ãƒ«: {stacking_info.get('meta_learner_file', 'unknown')}")
        print(f"   Phase: {stacking_info.get('phase', 'unknown')}")

        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
        stacking_file = stacking_info.get("model_file", "stacking_ensemble.pkl")
        meta_file = stacking_info.get("meta_learner_file", "meta_learner.pkl")

        stacking_path = self.project_root / f"models/production/{stacking_file}"
        meta_path = self.project_root / f"models/production/{meta_file}"

        if stacking_path.exists():
            print(f"\nâœ… {stacking_file} å­˜åœ¨ç¢ºèª")
            print(f"   ã‚µã‚¤ã‚º: {stacking_path.stat().st_size / 1024 / 1024:.2f} MB")

            # Stackingãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ
            try:
                with open(stacking_path, "rb") as f:
                    stacking_model = pickle.load(f)

                if hasattr(stacking_model, "predict") and hasattr(stacking_model, "predict_proba"):
                    print(f"   predict/predict_proba: âœ… å­˜åœ¨")
                else:
                    self.errors.append("âŒ Stackingãƒ¢ãƒ‡ãƒ«ã«å¿…é ˆãƒ¡ã‚½ãƒƒãƒ‰ãŒä¸è¶³")

                if hasattr(stacking_model, "meta_model"):
                    print(f"   meta_model: âœ… å­˜åœ¨")
                else:
                    print(f"   meta_model: âš ï¸  å†…è”µMeta-Learnerä¸å­˜åœ¨ï¼ˆå¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨ï¼‰")

                if hasattr(stacking_model, "stacking_enabled"):
                    print(f"   stacking_enabled: {stacking_model.stacking_enabled}")

                # Phase 59.8: ç‰¹å¾´é‡æ•°æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¦ï¼‰
                stacking_n_features = None
                # StackingEnsembleã¯n_features_å±æ€§ã‚’ç›´æ¥æŒã¤
                if hasattr(stacking_model, "n_features_"):
                    stacking_n_features = stacking_model.n_features_
                elif hasattr(stacking_model, "models"):
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å–å¾—
                    for model_name, base_model in stacking_model.models.items():
                        if hasattr(base_model, "n_features_in_"):
                            stacking_n_features = base_model.n_features_in_
                            break

                if stacking_n_features:
                    print(f"   n_features_in_: {stacking_n_features}")
                    expected_features = stacking_info.get("count", 55)

                    # ensemble_full.pklã¨ã®æ¯”è¼ƒ
                    full_path = self.project_root / "models/production/ensemble_full.pkl"
                    if full_path.exists():
                        with open(full_path, "rb") as f:
                            full_model = pickle.load(f)
                        full_n_features = None
                        if hasattr(full_model, "models"):
                            for _, base_model in full_model.models.items():
                                if hasattr(base_model, "n_features_in_"):
                                    full_n_features = base_model.n_features_in_
                                    break

                        if full_n_features and stacking_n_features != full_n_features:
                            self.errors.append(
                                f"âŒ ç‰¹å¾´é‡æ•°ä¸ä¸€è‡´: Stacking({stacking_n_features}) != Full({full_n_features}) "
                                f"â†’ Stackingãƒ¢ãƒ‡ãƒ«å†è¨“ç·´ãŒå¿…è¦"
                            )
                            print(f"   â›” ç‰¹å¾´é‡ä¸ä¸€è‡´: {stacking_n_features} != {full_n_features}")
                        elif full_n_features:
                            print(
                                f"   âœ… ç‰¹å¾´é‡ä¸€è‡´: Stacking={stacking_n_features}, Full={full_n_features}"
                            )

                    if stacking_n_features != expected_features:
                        self.warnings.append(
                            f"âš ï¸  Stackingç‰¹å¾´é‡æ•°ãŒè¨­å®šã¨ä¸ä¸€è‡´: å®Ÿéš›={stacking_n_features}, æœŸå¾…={expected_features}"
                        )
                else:
                    self.warnings.append("âš ï¸  Stackingãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡æ•°ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

            except Exception as e:
                self.errors.append(f"âŒ Stackingãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            print(f"\nâ„¹ï¸  {stacking_file} æœªæ¤œå‡º")
            print("   â†’ Stackingãƒ¢ãƒ‡ãƒ«æœªè¨“ç·´ï¼ˆstacking_enabled=falseæ™‚ã¯æ­£å¸¸ï¼‰")

        if meta_path.exists():
            print(f"\nâœ… {meta_file} å­˜åœ¨ç¢ºèª")
            print(f"   ã‚µã‚¤ã‚º: {meta_path.stat().st_size / 1024:.2f} KB")
        else:
            print(f"\nâ„¹ï¸  {meta_file} æœªæ¤œå‡º")

        # thresholds.yamlã‹ã‚‰stacking_enabledç¢ºèª
        try:
            import yaml

            thresholds_path = self.project_root / "config/core/thresholds.yaml"
            if thresholds_path.exists():
                with open(thresholds_path, "r", encoding="utf-8") as f:
                    thresholds = yaml.safe_load(f)

                stacking_enabled = thresholds.get("ensemble", {}).get("stacking_enabled", False)
                print(f"\nğŸ¯ thresholds.yamlè¨­å®š:")
                print(f"   ensemble.stacking_enabled: {stacking_enabled}")

                if stacking_enabled and not stacking_path.exists():
                    self.errors.append("âŒ stacking_enabled=trueã ãŒStackingãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        except Exception as e:
            self.warnings.append(f"âš ï¸  thresholds.yamlèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        print(f"\nâœ… Stackingãƒ¢ãƒ‡ãƒ«æ¤œè¨¼å®Œäº†")

    # ========================================
    # äºˆæ¸¬åˆ†å¸ƒæ¤œè¨¼ï¼ˆdistributionï¼‰
    # ========================================

    def _load_real_data(self, n_samples: int = 200) -> Optional[pd.DataFrame]:
        """å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
        data_path = self.project_root / "src/backtest/data/historical/BTC_JPY_15m.csv"
        if not data_path.exists():
            self.errors.append(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
            return None

        try:
            df = pd.read_csv(data_path)
            if "timestamp" in df.columns:
                df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
                df = df.set_index("timestamp")
                df.index = pd.DatetimeIndex(df.index)
            return df.tail(n_samples)
        except Exception as e:
            self.errors.append(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return None

    def _generate_features(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:
        """ç‰¹å¾´é‡ã‚’ç”Ÿæˆ"""
        try:
            from src.features.feature_generator import FeatureGenerator

            generator = FeatureGenerator()
            return generator.generate_features_sync(df)
        except Exception as e:
            self.errors.append(f"âŒ ç‰¹å¾´é‡ç”Ÿæˆå¤±æ•—: {e}")
            return None

    def _load_model(self) -> bool:
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        model_path = self.project_root / "models/production/ensemble_full.pkl"

        if not model_path.exists():
            self.warnings.append(f"âš ï¸  ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
            return False

        try:
            with open(model_path, "rb") as f:
                self.model = pickle.load(f)
            return True
        except Exception as e:
            self.errors.append(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def validate_prediction_distribution(self) -> None:
        """äºˆæ¸¬åˆ†å¸ƒæ¤œè¨¼"""
        print("\n" + "=" * 60)
        print("ğŸ“Š äºˆæ¸¬åˆ†å¸ƒæ¤œè¨¼")
        print("=" * 60)

        print("\n  å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        df = self._load_real_data(n_samples=300)
        if df is None:
            return
        print(f"  ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ")

        print("  ç‰¹å¾´é‡ç”Ÿæˆä¸­...")
        features_df = self._generate_features(df)
        if features_df is None:
            return
        print(f"  ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {len(features_df)}è¡Œ x {len(features_df.columns)}åˆ—")

        if not self._load_model():
            return

        try:
            expected_features = (
                self.model.feature_names if hasattr(self.model, "feature_names") else []
            )

            test_df = features_df.copy()
            for f in expected_features:
                if f not in test_df.columns:
                    test_df[f] = 0.0

            X_test = test_df[expected_features].values
            X_test = np.nan_to_num(X_test, nan=0.0)

            predictions = self.model.predict(X_test)

            unique, counts = np.unique(predictions, return_counts=True)
            distribution = dict(zip(unique, counts))

            total = len(predictions)
            sell_pct = distribution.get(0, 0) / total * 100
            hold_pct = distribution.get(1, 0) / total * 100
            buy_pct = distribution.get(2, 0) / total * 100

            print(f"\nğŸ¯ äºˆæ¸¬åˆ†å¸ƒ:")
            print(f"   SELL: {distribution.get(0, 0)}å› ({sell_pct:.1f}%)")
            print(f"   HOLD: {distribution.get(1, 0)}å› ({hold_pct:.1f}%)")
            print(f"   BUY:  {distribution.get(2, 0)}å› ({buy_pct:.1f}%)")

            max_ratio = max(counts) / total
            print(f"   æœ€å¤§ã‚¯ãƒ©ã‚¹æ¯”ç‡: {max_ratio * 100:.1f}%")

            MAX_CLASS_THRESHOLD = 0.90
            WARN_THRESHOLD = 0.80

            if max_ratio >= MAX_CLASS_THRESHOLD:
                self.errors.append(
                    f"âŒ æ¥µç«¯ãªã‚¯ãƒ©ã‚¹ãƒã‚¤ã‚¢ã‚¹ï¼ˆæœ€å¤§ã‚¯ãƒ©ã‚¹: {max_ratio * 100:.1f}% >= 90%ï¼‰"
                )
            elif max_ratio >= WARN_THRESHOLD:
                self.warnings.append(
                    f"âš ï¸  ã‚¯ãƒ©ã‚¹ãƒãƒ©ãƒ³ã‚¹ãŒã‚„ã‚„åã‚Šï¼ˆæœ€å¤§ã‚¯ãƒ©ã‚¹: {max_ratio * 100:.1f}%ï¼‰"
                )
            else:
                print(f"\nâœ… ã‚¯ãƒ©ã‚¹ãƒãƒ©ãƒ³ã‚¹è‰¯å¥½ï¼ˆæœ€å¤§ã‚¯ãƒ©ã‚¹: {max_ratio * 100:.1f}% < 80%ï¼‰")

            if min(sell_pct, buy_pct) < 5:
                self.warnings.append(
                    f"âš ï¸  BUY/SELLã®ä¸€æ–¹ãŒ5%æœªæº€ï¼ˆSELL:{sell_pct:.1f}%, BUY:{buy_pct:.1f}%ï¼‰"
                )

        except Exception as e:
            self.errors.append(f"âŒ äºˆæ¸¬åˆ†å¸ƒãƒã‚§ãƒƒã‚¯å¤±æ•—: {e}")

    # ========================================
    # æ€§èƒ½æ¤œè¨¼ï¼ˆperformanceï¼‰
    # ========================================

    def validate_confidence_stats(self) -> None:
        """ä¿¡é ¼åº¦çµ±è¨ˆæ¤œè¨¼"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ä¿¡é ¼åº¦çµ±è¨ˆæ¤œè¨¼")
        print("=" * 60)

        if self.model is None:
            if not self._load_model():
                return

        df = self._load_real_data(n_samples=100)
        if df is None:
            return

        features_df = self._generate_features(df)
        if features_df is None:
            return

        try:
            expected_features = (
                self.model.feature_names if hasattr(self.model, "feature_names") else []
            )

            test_df = features_df.copy()
            for f in expected_features:
                if f not in test_df.columns:
                    test_df[f] = 0.0

            X_test = test_df[expected_features].values
            X_test = np.nan_to_num(X_test, nan=0.0)

            probabilities = self.model.predict_proba(X_test)
            max_probs = np.max(probabilities, axis=1)

            print(f"\nğŸ¯ ä¿¡é ¼åº¦çµ±è¨ˆ:")
            print(f"   å¹³å‡: {np.mean(max_probs):.3f}")
            print(f"   æœ€å°: {np.min(max_probs):.3f}")
            print(f"   æœ€å¤§: {np.max(max_probs):.3f}")
            print(f"   æ¨™æº–åå·®: {np.std(max_probs):.3f}")

            high_conf = np.sum(max_probs > 0.6) / len(max_probs) * 100
            print(f"   é«˜ä¿¡é ¼åº¦(>60%): {high_conf:.1f}%")

            if high_conf < 5:
                self.warnings.append(f"âš ï¸  é«˜ä¿¡é ¼åº¦äºˆæ¸¬ãŒå°‘ãªã„ï¼ˆ{high_conf:.1f}%ï¼‰")
            else:
                print(f"\nâœ… ä¿¡é ¼åº¦çµ±è¨ˆæ­£å¸¸")

        except Exception as e:
            self.errors.append(f"âŒ ä¿¡é ¼åº¦çµ±è¨ˆè¨ˆç®—å¤±æ•—: {e}")

    def validate_individual_models(self) -> None:
        """å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼"""
        print("\n" + "=" * 60)
        print("ğŸ” å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼")
        print("=" * 60)

        if self.model is None:
            if not self._load_model():
                return

        if not hasattr(self.model, "models"):
            print("âš ï¸ å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯")
            return

        for name, model in self.model.models.items():
            print(f"\nğŸ“Š {name}:")
            print(f"   ã‚¿ã‚¤ãƒ—: {type(model).__name__}")

            if hasattr(model, "n_estimators"):
                print(f"   n_estimators: {model.n_estimators}")
            if hasattr(model, "n_features_in_"):
                print(f"   n_features_in_: {model.n_features_in_}")
            if hasattr(model, "classes_"):
                print(f"   classes_: {model.classes_}")

            if self.metadata:
                perf = self.metadata.get("performance_metrics", {}).get(name, {})
                if perf:
                    print(f"   è¨“ç·´æ™‚æ€§èƒ½:")
                    if "accuracy" in perf:
                        print(f"     Accuracy: {perf['accuracy']:.3f}")
                    if "f1_score" in perf:
                        print(f"     F1 Score: {perf['f1_score']:.3f}")
                    if "cv_f1_mean" in perf:
                        print(f"     CV F1 Mean: {perf['cv_f1_mean']:.3f}")

        print(f"\nâœ… å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼å®Œäº†")

    # ========================================
    # çµ±åˆå®Ÿè¡Œ
    # ========================================

    def run_consistency(self) -> None:
        """æ•´åˆæ€§æ¤œè¨¼ã‚’å®Ÿè¡Œ"""
        print("\n" + "=" * 60)
        print("ğŸ” æ•´åˆæ€§æ¤œè¨¼é–‹å§‹")
        print("=" * 60)

        self.feature_order_data = self._load_feature_order()
        self.metadata = self._load_model_metadata()

        self.validate_feature_counts()
        self.validate_strategy_signals()
        self.validate_model_files()
        self.validate_model_difference()
        self.validate_n_classes()
        self.validate_stacking_model()  # Phase 59.8è¿½åŠ 

    def run_distribution(self) -> None:
        """äºˆæ¸¬åˆ†å¸ƒæ¤œè¨¼ã‚’å®Ÿè¡Œ"""
        self.validate_prediction_distribution()

    def run_performance(self) -> None:
        """æ€§èƒ½æ¤œè¨¼ã‚’å®Ÿè¡Œ"""
        self.validate_confidence_stats()
        self.validate_individual_models()

    def run_all(self, quick: bool = False) -> bool:
        """å…¨æ¤œè¨¼ã‚’å®Ÿè¡Œ"""
        print("\n" + "=" * 60)
        print("ğŸš€ Phase 61: MLæ¤œè¨¼çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹")
        print("=" * 60)

        # æ•´åˆæ€§æ¤œè¨¼ï¼ˆå¸¸ã«å®Ÿè¡Œï¼‰
        self.run_consistency()

        # å®Ÿãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ï¼ˆquickãƒ¢ãƒ¼ãƒ‰ã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        if not quick:
            self.run_distribution()
            self.run_performance()
        else:
            print("\nâ­ï¸  --quick ãƒ¢ãƒ¼ãƒ‰: å®Ÿãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¹ã‚­ãƒƒãƒ—")

        return self._print_results()

    def _print_results(self) -> bool:
        """æ¤œè¨¼çµæœã‚’å‡ºåŠ›"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 60)

        if self.errors:
            print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {len(self.errors)}ä»¶")
            for error in self.errors:
                print(f"   {error}")

        if self.warnings:
            print(f"\nâš ï¸  è­¦å‘Š: {len(self.warnings)}ä»¶")
            for warning in self.warnings:
                print(f"   {warning}")

        if not self.errors and not self.warnings:
            print("\nâœ… ã™ã¹ã¦ã®æ¤œè¨¼ã«åˆæ ¼ã—ã¾ã—ãŸï¼")
            return True
        elif not self.errors:
            print("\nâœ… ã‚¨ãƒ©ãƒ¼ãªã—ï¼ˆè­¦å‘Šã®ã¿ï¼‰")
            return True
        else:
            print("\nâŒ æ¤œè¨¼å¤±æ•— - ä¸Šè¨˜ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„")
            return False


def main() -> int:
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(description="Phase 61: MLæ¤œè¨¼çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument(
        "--check",
        choices=["all", "consistency", "distribution", "performance"],
        default="all",
        help="æ¤œè¨¼ã‚¿ã‚¤ãƒ—ã‚’æŒ‡å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: allï¼‰",
    )
    parser.add_argument(
        "--quick", action="store_true", help="è»½é‡ãƒ¢ãƒ¼ãƒ‰ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãªã—ãƒ»é«˜é€Ÿï¼‰"
    )

    args = parser.parse_args()

    validator = MLModelValidator(PROJECT_ROOT)

    if args.check == "all":
        success = validator.run_all(quick=args.quick)
    elif args.check == "consistency":
        validator.feature_order_data = validator._load_feature_order()
        validator.metadata = validator._load_model_metadata()
        validator.run_consistency()
        success = validator._print_results()
    elif args.check == "distribution":
        validator.run_distribution()
        success = validator._print_results()
    elif args.check == "performance":
        validator.metadata = validator._load_model_metadata()
        validator.run_performance()
        success = validator._print_results()
    else:
        success = False

    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
