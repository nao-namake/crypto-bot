#!/usr/bin/env python3
"""
127ç‰¹å¾´é‡è©³ç´°åˆ†æãƒ»é‡è¤‡ç‰¹å¾´é‡ç‰¹å®šãƒ»å³é¸ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ®µéšçš„ç‰¹å¾´é‡æœ€é©åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
"""

import json
import logging
import os
import re
import sys
from collections import defaultdict
from pathlib import Path

import numpy as np
import pandas as pd

sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def get_current_127_features():
    """ç¾åœ¨ã®127ç‰¹å¾´é‡ãƒªã‚¹ãƒˆå–å¾—"""
    features_127 = [
        # åŸºæœ¬OHLCV + ãƒ©ã‚° (13ç‰¹å¾´é‡)
        "open",
        "high",
        "low",
        "close",
        "volume",
        "close_lag_1",
        "close_lag_2",
        "close_lag_3",
        "close_lag_4",
        "close_lag_5",
        "volume_lag_1",
        "volume_lag_2",
        "volume_lag_3",
        # ãƒªã‚¿ãƒ¼ãƒ³ç³» (10ç‰¹å¾´é‡)
        "returns_1",
        "returns_2",
        "returns_3",
        "returns_5",
        "returns_10",
        "log_returns_1",
        "log_returns_2",
        "log_returns_3",
        "log_returns_5",
        "log_returns_10",
        # ç§»å‹•å¹³å‡ç³» (12ç‰¹å¾´é‡)
        "sma_5",
        "sma_10",
        "sma_20",
        "sma_50",
        "sma_100",
        "sma_200",
        "ema_5",
        "ema_10",
        "ema_20",
        "ema_50",
        "ema_100",
        "ema_200",
        # ä¾¡æ ¼ãƒã‚¸ã‚·ãƒ§ãƒ³ (5ç‰¹å¾´é‡)
        "price_position_20",
        "price_position_50",
        "price_vs_sma20",
        "bb_position",
        "intraday_position",
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ (5ç‰¹å¾´é‡)
        "bb_upper",
        "bb_middle",
        "bb_lower",
        "bb_width",
        "bb_squeeze",
        # RSIç³» (5ç‰¹å¾´é‡)
        "rsi_14",
        "rsi_7",
        "rsi_21",
        "rsi_oversold",
        "rsi_overbought",
        # MACDç³» (5ç‰¹å¾´é‡)
        "macd",
        "macd_signal",
        "macd_hist",
        "macd_cross_up",
        "macd_cross_down",
        # ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹ (4ç‰¹å¾´é‡)
        "stoch_k",
        "stoch_d",
        "stoch_oversold",
        "stoch_overbought",
        # ATRãƒ»ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ (8ç‰¹å¾´é‡)
        "atr_14",
        "atr_7",
        "atr_21",
        "volatility_20",
        "volatility_50",
        "high_low_ratio",
        "true_range",
        "volatility_ratio",
        # ãƒœãƒªãƒ¥ãƒ¼ãƒ åˆ†æ (10ç‰¹å¾´é‡)
        "volume_sma_20",
        "volume_ratio",
        "volume_trend",
        "vwap",
        "vwap_distance",
        "obv",
        "obv_sma",
        "cmf",
        "mfi",
        "ad_line",
        # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ  (9ç‰¹å¾´é‡)
        "adx_14",
        "plus_di",
        "minus_di",
        "trend_strength",
        "trend_direction",
        "cci_20",
        "williams_r",
        "ultimate_oscillator",
        "momentum_14",
        # ã‚µãƒãƒ¼ãƒˆãƒ»ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹ (6ç‰¹å¾´é‡)
        "support_distance",
        "resistance_distance",
        "support_strength",
        "volume_breakout",
        "price_breakout_up",
        "price_breakout_down",
        # ãƒ­ãƒ¼ã‚½ã‚¯è¶³ãƒ‘ã‚¿ãƒ¼ãƒ³ (4ç‰¹å¾´é‡)
        "doji",
        "hammer",
        "engulfing",
        "pinbar",
        # çµ±è¨ˆãƒ»é«˜åº¦åˆ†æ (5ç‰¹å¾´é‡)
        "skewness_20",
        "kurtosis_20",
        "zscore",
        "mean_reversion_20",
        "mean_reversion_50",
        # æ™‚é–“ç‰¹å¾´é‡ (6ç‰¹å¾´é‡)
        "hour",
        "day_of_week",
        "is_weekend",
        "is_asian_session",
        "is_european_session",
        "is_us_session",
        # è¿½åŠ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ« (16ç‰¹å¾´é‡)
        "roc_10",
        "roc_20",
        "trix",
        "mass_index",
        "keltner_upper",
        "keltner_lower",
        "donchian_upper",
        "donchian_lower",
        "ichimoku_conv",
        "ichimoku_base",
        "price_efficiency",
        "trend_consistency",
        "volume_price_correlation",
        "volatility_regime",
        "momentum_quality",
        "market_phase",
        # è¿½åŠ çµ±è¨ˆ (2ç‰¹å¾´é‡)
        "close_mean_10",
        "close_std_10",
    ]

    return features_127


def categorize_features(features):
    """ç‰¹å¾´é‡ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡"""
    categories = {
        "basic_ohlcv": [],
        "lag_features": [],
        "returns": [],
        "moving_averages": [],
        "price_position": [],
        "bollinger_bands": [],
        "rsi_family": [],
        "macd_family": [],
        "stochastic": [],
        "volatility_atr": [],
        "volume_analysis": [],
        "trend_momentum": [],
        "support_resistance": [],
        "candlestick_patterns": [],
        "statistical": [],
        "time_features": [],
        "advanced_technical": [],
        "other": [],
    }

    for feature in features:
        categorized = False

        # åŸºæœ¬OHLCV
        if feature in ["open", "high", "low", "close", "volume"]:
            categories["basic_ohlcv"].append(feature)
            categorized = True

        # ãƒ©ã‚°ç‰¹å¾´é‡
        elif "lag_" in feature:
            categories["lag_features"].append(feature)
            categorized = True

        # ãƒªã‚¿ãƒ¼ãƒ³ç³»
        elif "returns_" in feature or "log_returns_" in feature:
            categories["returns"].append(feature)
            categorized = True

        # ç§»å‹•å¹³å‡
        elif feature.startswith(("sma_", "ema_")):
            categories["moving_averages"].append(feature)
            categorized = True

        # ä¾¡æ ¼ãƒã‚¸ã‚·ãƒ§ãƒ³
        elif "position" in feature or "price_vs_" in feature:
            categories["price_position"].append(feature)
            categorized = True

        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        elif feature.startswith("bb_"):
            categories["bollinger_bands"].append(feature)
            categorized = True

        # RSIç³»
        elif feature.startswith("rsi_") or "rsi_" in feature:
            categories["rsi_family"].append(feature)
            categorized = True

        # MACDç³»
        elif feature.startswith("macd"):
            categories["macd_family"].append(feature)
            categorized = True

        # ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹
        elif feature.startswith("stoch_"):
            categories["stochastic"].append(feature)
            categorized = True

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ»ATR
        elif feature.startswith(("atr_", "volatility_")) or feature in [
            "true_range",
            "high_low_ratio",
            "volatility_ratio",
        ]:
            categories["volatility_atr"].append(feature)
            categorized = True

        # ãƒœãƒªãƒ¥ãƒ¼ãƒ åˆ†æ
        elif "volume" in feature or feature in [
            "vwap",
            "vwap_distance",
            "obv",
            "obv_sma",
            "cmf",
            "mfi",
            "ad_line",
        ]:
            categories["volume_analysis"].append(feature)
            categorized = True

        # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
        elif feature in [
            "adx_14",
            "plus_di",
            "minus_di",
            "trend_strength",
            "trend_direction",
            "cci_20",
            "williams_r",
            "ultimate_oscillator",
            "momentum_14",
            "momentum_quality",
        ]:
            categories["trend_momentum"].append(feature)
            categorized = True

        # ã‚µãƒãƒ¼ãƒˆãƒ»ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹
        elif "support" in feature or "resistance" in feature or "breakout" in feature:
            categories["support_resistance"].append(feature)
            categorized = True

        # ãƒ­ãƒ¼ã‚½ã‚¯è¶³ãƒ‘ã‚¿ãƒ¼ãƒ³
        elif feature in ["doji", "hammer", "engulfing", "pinbar"]:
            categories["candlestick_patterns"].append(feature)
            categorized = True

        # çµ±è¨ˆç³»
        elif feature in [
            "skewness_20",
            "kurtosis_20",
            "zscore",
            "mean_reversion_20",
            "mean_reversion_50",
            "close_mean_10",
            "close_std_10",
        ]:
            categories["statistical"].append(feature)
            categorized = True

        # æ™‚é–“ç‰¹å¾´é‡
        elif feature in [
            "hour",
            "day_of_week",
            "is_weekend",
            "is_asian_session",
            "is_european_session",
            "is_us_session",
        ]:
            categories["time_features"].append(feature)
            categorized = True

        # é«˜åº¦ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«
        elif feature in [
            "roc_10",
            "roc_20",
            "trix",
            "mass_index",
            "keltner_upper",
            "keltner_lower",
            "donchian_upper",
            "donchian_lower",
            "ichimoku_conv",
            "ichimoku_base",
            "price_efficiency",
            "trend_consistency",
            "volume_price_correlation",
            "volatility_regime",
            "market_phase",
        ]:
            categories["advanced_technical"].append(feature)
            categorized = True

        # ãã®ä»–
        if not categorized:
            categories["other"].append(feature)

    return categories


def identify_redundant_features(categories):
    """é‡è¤‡ãƒ»å†—é•·ç‰¹å¾´é‡ã®ç‰¹å®š"""
    redundant_groups = []

    # 1. åŒä¸€æœŸé–“ã®ç§»å‹•å¹³å‡ï¼ˆSMA vs EMAï¼‰
    sma_periods = set()
    ema_periods = set()

    for feature in categories["moving_averages"]:
        if feature.startswith("sma_"):
            period = feature.split("_")[1]
            sma_periods.add(period)
        elif feature.startswith("ema_"):
            period = feature.split("_")[1]
            ema_periods.add(period)

    common_periods = sma_periods.intersection(ema_periods)
    if common_periods:
        redundant_groups.append(
            {
                "type": "moving_average_duplication",
                "description": "åŒä¸€æœŸé–“ã®SMAã¨EMAï¼ˆé«˜ç›¸é–¢ï¼‰",
                "features": [f"sma_{p}" for p in common_periods]
                + [f"ema_{p}" for p in common_periods],
                "recommendation": f"å„æœŸé–“ã§SMAã¾ãŸã¯EMAã®ã©ã¡ã‚‰ã‹ä¸€æ–¹ã‚’é¸æŠã€‚æ¨å¥¨ï¼šEMAï¼ˆã‚ˆã‚Šåå¿œãŒæ—©ã„ï¼‰",
                "redundant_count": len(common_periods),
            }
        )

    # 2. é¡ä¼¼ATRè¨ˆç®—ï¼ˆè¤‡æ•°æœŸé–“ï¼‰
    atr_features = [f for f in categories["volatility_atr"] if f.startswith("atr_")]
    if len(atr_features) > 1:
        redundant_groups.append(
            {
                "type": "atr_multiple_periods",
                "description": "è¤‡æ•°æœŸé–“ã®ATRï¼ˆé«˜ç›¸é–¢ï¼‰",
                "features": atr_features,
                "recommendation": "ATR_14ã®ã¿æ®‹ã™ï¼ˆæ¨™æº–çš„ãªæœŸé–“ï¼‰",
                "redundant_count": len(atr_features) - 1,
            }
        )

    # 3. é¡ä¼¼ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™
    volatility_features = [f for f in categories["volatility_atr"] if "volatility" in f]
    volatility_features.extend(["true_range", "high_low_ratio"])
    if len(volatility_features) > 2:
        redundant_groups.append(
            {
                "type": "volatility_indicators",
                "description": "è¤‡æ•°ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™ï¼ˆæ¦‚å¿µçš„ã«é¡ä¼¼ï¼‰",
                "features": volatility_features,
                "recommendation": "volatility_20ã¨atr_14ã‚’æ®‹ã™",
                "redundant_count": len(volatility_features) - 2,
            }
        )

    # 4. é¡ä¼¼RSIæŒ‡æ¨™
    rsi_features = [
        f
        for f in categories["rsi_family"]
        if f.startswith("rsi_") and f not in ["rsi_oversold", "rsi_overbought"]
    ]
    if len(rsi_features) > 1:
        redundant_groups.append(
            {
                "type": "rsi_multiple_periods",
                "description": "è¤‡æ•°æœŸé–“ã®RSIï¼ˆé«˜ç›¸é–¢ï¼‰",
                "features": rsi_features,
                "recommendation": "rsi_14ã®ã¿æ®‹ã™ï¼ˆæ¨™æº–æœŸé–“ï¼‰",
                "redundant_count": len(rsi_features) - 1,
            }
        )

    # 5. é¡ä¼¼ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—
    returns_simple = [f for f in categories["returns"] if f.startswith("returns_")]
    returns_log = [f for f in categories["returns"] if f.startswith("log_returns_")]
    if len(returns_simple) > 0 and len(returns_log) > 0:
        redundant_groups.append(
            {
                "type": "returns_calculation_methods",
                "description": "å˜ç´”ãƒªã‚¿ãƒ¼ãƒ³ã¨å¯¾æ•°ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆé¡ä¼¼æƒ…å ±ï¼‰",
                "features": returns_simple + returns_log,
                "recommendation": "returns_1, returns_5ã®ã¿æ®‹ã™ï¼ˆlog_returnsã¯å‰Šé™¤ï¼‰",
                "redundant_count": len(returns_log),
            }
        )

    # 6. éå‰°ãªãƒ©ã‚°ç‰¹å¾´é‡
    lag_features = categories["lag_features"]
    if len(lag_features) > 3:
        redundant_groups.append(
            {
                "type": "excessive_lag_features",
                "description": "éå‰°ãªãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆçŸ­æœŸç›¸é–¢é«˜ï¼‰",
                "features": lag_features,
                "recommendation": "close_lag_1, close_lag_3, volume_lag_1ã®ã¿æ®‹ã™",
                "redundant_count": len(lag_features) - 3,
            }
        )

    # 7. é¡ä¼¼ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“
    session_features = [f for f in categories["time_features"] if "session" in f]
    if len(session_features) > 1:
        redundant_groups.append(
            {
                "type": "session_time_overlap",
                "description": "è¤‡æ•°ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“ï¼ˆä¸€éƒ¨é‡è¤‡ã‚ã‚Šï¼‰",
                "features": session_features,
                "recommendation": "is_us_session, is_asian_sessionã®ã¿æ®‹ã™",
                "redundant_count": len(session_features) - 2,
            }
        )

    # 8. é¡ä¼¼çµ±è¨ˆæŒ‡æ¨™
    if len(categories["statistical"]) > 3:
        redundant_groups.append(
            {
                "type": "statistical_indicators",
                "description": "è¤‡æ•°ã®çµ±è¨ˆæŒ‡æ¨™ï¼ˆä¸€éƒ¨é‡è¤‡æƒ…å ±ï¼‰",
                "features": categories["statistical"],
                "recommendation": "zscore, close_std_10ã®ã¿æ®‹ã™",
                "redundant_count": len(categories["statistical"]) - 2,
            }
        )

    return redundant_groups


def select_essential_features(categories):
    """å–å¼•ã«å¿…è¦ä¸å¯æ¬ ãªç‰¹å¾´é‡ã®å³é¸"""
    essential_features = {
        "core_price_action": {
            "features": ["close", "volume", "high", "low"],
            "reason": "ã‚³ã‚¢ä¾¡æ ¼æƒ…å ±ãƒ»å¿…é ˆ",
        },
        "trend_identification": {
            "features": ["sma_20", "ema_50", "price_vs_sma20"],
            "reason": "ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šãƒ»æ–¹å‘æ€§ç¢ºèª",
        },
        "momentum_oscillators": {
            "features": ["rsi_14", "macd", "macd_signal"],
            "reason": "ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ»ã‚ªã‚·ãƒ¬ãƒ¼ã‚¿ãƒ¼åˆ†æ",
        },
        "volatility_risk": {
            "features": ["atr_14", "volatility_20", "bb_width"],
            "reason": "ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ»ãƒªã‚¹ã‚¯ç®¡ç†",
        },
        "volume_confirmation": {
            "features": ["volume_ratio", "vwap", "obv"],
            "reason": "ãƒœãƒªãƒ¥ãƒ¼ãƒ ç¢ºèªãƒ»æµå‹•æ€§åˆ†æ",
        },
        "short_term_momentum": {
            "features": ["returns_1", "close_lag_1"],
            "reason": "çŸ­æœŸãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ»ç›´è¿‘å‹•å‘",
        },
        "support_resistance": {
            "features": ["bb_upper", "bb_lower", "support_distance"],
            "reason": "ã‚µãƒãƒ¼ãƒˆãƒ»ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹åˆ¤å®š",
        },
        "market_timing": {
            "features": ["hour", "is_us_session"],
            "reason": "å¸‚å ´æ™‚é–“ãƒ»æµå‹•æ€§ã‚¿ã‚¤ãƒŸãƒ³ã‚°",
        },
        "advanced_signals": {
            "features": ["adx_14", "stoch_k", "williams_r"],
            "reason": "é«˜åº¦ã‚·ã‚°ãƒŠãƒ«ãƒ»ç¢ºèªæŒ‡æ¨™",
        },
    }

    return essential_features


def create_optimized_feature_sets():
    """æœ€é©åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡ã‚»ãƒƒãƒˆä½œæˆ"""
    features_127 = get_current_127_features()
    categories = categorize_features(features_127)
    redundant_groups = identify_redundant_features(categories)
    essential_features = select_essential_features(categories)

    # å³é¸ç‰¹å¾´é‡ãƒªã‚¹ãƒˆï¼ˆEssential Setï¼‰
    essential_list = []
    for category, info in essential_features.items():
        essential_list.extend(info["features"])

    # é‡è¤‡é™¤å»å¾Œç‰¹å¾´é‡ãƒªã‚¹ãƒˆï¼ˆReduced Setï¼‰
    reduced_features = features_127.copy()

    # é‡è¤‡ç‰¹å¾´é‡ã‚’å‰Šé™¤
    for group in redundant_groups:
        if group["type"] == "moving_average_duplication":
            # SMAã‚’å‰Šé™¤ã€EMAã‚’æ®‹ã™
            for feature in group["features"]:
                if (
                    feature.startswith("sma_")
                    and feature.replace("sma_", "ema_") in reduced_features
                ):
                    if feature in reduced_features:
                        reduced_features.remove(feature)

        elif group["type"] == "atr_multiple_periods":
            # atr_14ä»¥å¤–ã‚’å‰Šé™¤
            for feature in group["features"]:
                if feature != "atr_14" and feature in reduced_features:
                    reduced_features.remove(feature)

        elif group["type"] == "rsi_multiple_periods":
            # rsi_14ä»¥å¤–ã‚’å‰Šé™¤
            for feature in group["features"]:
                if feature != "rsi_14" and feature in reduced_features:
                    reduced_features.remove(feature)

        elif group["type"] == "returns_calculation_methods":
            # log_returnsã‚’å‰Šé™¤
            for feature in group["features"]:
                if feature.startswith("log_returns_") and feature in reduced_features:
                    reduced_features.remove(feature)

        elif group["type"] == "excessive_lag_features":
            # å¿…è¦æœ€å°é™ã®ãƒ©ã‚°ã®ã¿æ®‹ã™
            keep_lags = ["close_lag_1", "close_lag_3", "volume_lag_1"]
            for feature in group["features"]:
                if feature not in keep_lags and feature in reduced_features:
                    reduced_features.remove(feature)

    return {
        "original_127": features_127,
        "categories": categories,
        "redundant_groups": redundant_groups,
        "essential_features": essential_features,
        "essential_list": list(set(essential_list)),
        "reduced_features": reduced_features,
    }


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("=" * 80)
    logger.info("127ç‰¹å¾´é‡è©³ç´°åˆ†æãƒ»é‡è¤‡ç‰¹å¾´é‡ç‰¹å®šãƒ»æœ€é©åŒ–")
    logger.info("=" * 80)

    # åˆ†æå®Ÿè¡Œ
    analysis_result = create_optimized_feature_sets()

    # çµæœè¡¨ç¤º
    logger.info(f"å…ƒã®ç‰¹å¾´é‡æ•°: {len(analysis_result['original_127'])}")

    print("\n" + "=" * 80)
    print("ğŸ“Š 127ç‰¹å¾´é‡ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æçµæœ")
    print("=" * 80)

    for category, features in analysis_result["categories"].items():
        if features:
            print(f"\nğŸ” {category.replace('_', ' ').title()}: {len(features)}å€‹")
            for i, feature in enumerate(features, 1):
                print(f"  {i:2d}. {feature}")

    print("\n" + "=" * 80)
    print("âš ï¸  é‡è¤‡ãƒ»å†—é•·ç‰¹å¾´é‡åˆ†æ")
    print("=" * 80)

    total_redundant = 0
    for i, group in enumerate(analysis_result["redundant_groups"], 1):
        print(f"\n{i}. {group['description']}")
        print(f"   å½±éŸ¿ç‰¹å¾´é‡: {len(group['features'])}å€‹")
        print(f"   å‰Šé™¤æ¨å¥¨: {group['redundant_count']}å€‹")
        print(f"   æ¨å¥¨å¯¾å¿œ: {group['recommendation']}")
        print(f"   ç‰¹å¾´é‡: {', '.join(group['features'])}")
        total_redundant += group["redundant_count"]

    print(f"\nğŸ“‰ å‰Šé™¤å¯èƒ½ãªé‡è¤‡ç‰¹å¾´é‡: {total_redundant}å€‹")
    print(f"ğŸ“Š é‡è¤‡å‰Šé™¤å¾Œç‰¹å¾´é‡æ•°: {len(analysis_result['reduced_features'])}å€‹")

    print("\n" + "=" * 80)
    print("ğŸ¯ å–å¼•ã«å¿…è¦ä¸å¯æ¬ ãªç‰¹å¾´é‡ï¼ˆå³é¸ç‰ˆï¼‰")
    print("=" * 80)

    for category, info in analysis_result["essential_features"].items():
        print(f"\nğŸ”¹ {category.replace('_', ' ').title()}: {len(info['features'])}å€‹")
        print(f"   ç†ç”±: {info['reason']}")
        print(f"   ç‰¹å¾´é‡: {', '.join(info['features'])}")

    print(f"\nğŸ¯ å³é¸ç‰¹å¾´é‡åˆè¨ˆ: {len(analysis_result['essential_list'])}å€‹")

    # çµæœä¿å­˜
    results_dir = Path("results/feature_analysis")
    results_dir.mkdir(parents=True, exist_ok=True)

    # è©³ç´°åˆ†æçµæœä¿å­˜
    analysis_path = results_dir / "feature_analysis_detailed.json"
    with open(analysis_path, "w", encoding="utf-8") as f:
        # JSON serializableå½¢å¼ã«å¤‰æ›
        save_data = {
            "original_127": analysis_result["original_127"],
            "categories": analysis_result["categories"],
            "redundant_groups": analysis_result["redundant_groups"],
            "essential_list": analysis_result["essential_list"],
            "reduced_features": analysis_result["reduced_features"],
            "summary": {
                "original_count": len(analysis_result["original_127"]),
                "reduced_count": len(analysis_result["reduced_features"]),
                "essential_count": len(analysis_result["essential_list"]),
                "redundant_count": total_redundant,
            },
        }
        json.dump(save_data, f, indent=2, ensure_ascii=False)

    logger.info(f"è©³ç´°åˆ†æçµæœä¿å­˜: {analysis_path}")

    # æœ€é©åŒ–ç‰¹å¾´é‡ã‚»ãƒƒãƒˆä¿å­˜
    feature_sets = {
        "essential_features": analysis_result["essential_list"],
        "reduced_features": analysis_result["reduced_features"],
        "original_features": analysis_result["original_127"],
    }

    sets_path = results_dir / "optimized_feature_sets.json"
    with open(sets_path, "w") as f:
        json.dump(feature_sets, f, indent=2)

    logger.info(f"æœ€é©åŒ–ç‰¹å¾´é‡ã‚»ãƒƒãƒˆä¿å­˜: {sets_path}")

    print("\n" + "=" * 80)
    print("ğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—æ¨å¥¨")
    print("=" * 80)
    print("1. ğŸ” é‡è¤‡å‰Šé™¤ç‰ˆï¼ˆ~80ç‰¹å¾´é‡ï¼‰ã§ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("2. ğŸ¯ å³é¸ç‰ˆï¼ˆ~30ç‰¹å¾´é‡ï¼‰ã§ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("3. ğŸ“Š 1ãƒ¶æœˆé–“ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã§æ€§èƒ½æ¯”è¼ƒ")
    print("4. ğŸ† æœ€é©ãªç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’é¸å®š")

    return analysis_result


if __name__ == "__main__":
    main()
