#!/usr/bin/env python3
"""
Phase 12-2: å…±é€šåˆ†æåŸºç›¤ã‚¯ãƒ©ã‚¹ï¼ˆé‡è¤‡ã‚³ãƒ¼ãƒ‰çµ±åˆç‰ˆï¼‰

ã‚¹ã‚¯ãƒªãƒ—ãƒˆçµ±åˆã«ã‚ˆã‚Šç´„500è¡Œã®é‡è¤‡ã‚³ãƒ¼ãƒ‰ã‚’å‰Šæ¸›ã€‚
Cloud Runãƒ­ã‚°å–å¾—ãƒ»gcloudã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œãƒ»ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æ©Ÿèƒ½ã‚’çµ±åˆã€‚

é‡è¤‡è§£æ±ºå¯¾è±¡:
- trading_data_collector.py: Cloud Runãƒ­ã‚°å–å¾—ï¼ˆ136-184è¡Œï¼‰
- performance_analyzer.py: gcloudã‚³ãƒãƒ³ãƒ‰ãƒ»ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèªï¼ˆ64-200è¡Œï¼‰
- simple_ab_test.py: ãƒ­ã‚°å–å¾—ãƒ»ãƒ‡ãƒ¼ã‚¿è§£æï¼ˆ106-151è¡Œï¼‰
- trading_dashboard.py: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆ48-122è¡Œï¼‰
"""

import json
import logging
import subprocess
from abc import ABC, abstractmethod
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import pandas as pd

# ãƒ­ã‚°è¨­å®š
logger = logging.getLogger(__name__)


class BaseAnalyzer(ABC):
    """åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆå…±é€šåŸºç›¤ã‚¯ãƒ©ã‚¹"""
    
    def __init__(
        self,
        project_id: str = "my-crypto-bot-project",
        service_name: str = "crypto-bot-service", 
        region: str = "asia-northeast1",
        output_dir: str = "logs"
    ):
        self.project_id = project_id
        self.service_name = service_name
        self.region = region
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        logger.info(f"BaseAnalyzeråˆæœŸåŒ–: {service_name}@{region}")

    # ===== å…±é€šgcloudã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œæ©Ÿèƒ½ =====
    
    def run_gcloud_command(
        self, 
        command: List[str], 
        timeout: int = 60,
        show_output: bool = False
    ) -> Tuple[int, str, str]:
        """gcloudã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã®å…±é€šãƒ©ãƒƒãƒ‘ãƒ¼"""
        if show_output:
            logger.info(f"gcloudã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ: {' '.join(command)}")
        
        try:
            result = subprocess.run(
                command,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            if show_output and result.returncode != 0:
                logger.error(f"gcloudã‚³ãƒãƒ³ãƒ‰ã‚¨ãƒ©ãƒ¼: {result.stderr}")
            
            return result.returncode, result.stdout, result.stderr
            
        except subprocess.TimeoutExpired:
            logger.error(f"gcloudã‚³ãƒãƒ³ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout}ç§’")
            return 1, "", "Command timeout"
        except Exception as e:
            logger.error(f"gcloudã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return 1, "", str(e)

    # ===== å…±é€šCloud Runãƒ­ã‚°å–å¾—æ©Ÿèƒ½ =====
    
    def fetch_cloud_run_logs(
        self,
        hours: int = 24,
        service_suffix: str = "",
        log_filter: str = "",
        limit: int = 500
    ) -> Tuple[bool, List[Dict]]:
        """Cloud Runãƒ­ã‚°å–å¾—ã®å…±é€šæ©Ÿèƒ½"""
        target_service = f"{self.service_name}{service_suffix}"
        start_time = (datetime.utcnow() - timedelta(hours=hours)).strftime('%Y-%m-%dT%H:%M:%SZ')
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚£ãƒ«ã‚¿ï¼šå–å¼•ãƒ»ã‚·ã‚°ãƒŠãƒ«é–¢é€£
        if not log_filter:
            log_filter = (
                f"(jsonPayload.message~\"SIGNAL\" OR jsonPayload.message~\"BUY\" OR "
                f"jsonPayload.message~\"SELL\" OR jsonPayload.message~\"HOLD\" OR "
                f"textPayload~\"SIGNAL\" OR textPayload~\"BUY\" OR "
                f"textPayload~\"SELL\" OR textPayload~\"HOLD\")"
            )
        
        cmd = [
            "gcloud", "logging", "read",
            f"resource.type=\"cloud_run_revision\" AND "
            f"resource.labels.service_name=\"{target_service}\" AND "
            f"{log_filter} AND "
            f"timestamp >= \"{start_time}\"",
            "--limit", str(limit),
            "--format", "json"
        ]
        
        logger.info(f"Cloud Runãƒ­ã‚°å–å¾—é–‹å§‹: {target_service} (éå»{hours}æ™‚é–“)")
        
        returncode, stdout, stderr = self.run_gcloud_command(cmd)
        
        if returncode != 0:
            logger.error(f"ãƒ­ã‚°å–å¾—å¤±æ•—: {stderr}")
            return False, []
        
        try:
            logs = json.loads(stdout) if stdout.strip() else []
            logger.info(f"ãƒ­ã‚°å–å¾—æˆåŠŸ: {len(logs)}ä»¶")
            return True, logs
        except json.JSONDecodeError as e:
            logger.error(f"ãƒ­ã‚°JSONè§£æå¤±æ•—: {e}")
            return False, []

    def fetch_error_logs(self, hours: int = 24, service_suffix: str = "") -> Tuple[bool, List[Dict]]:
        """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°å°‚ç”¨å–å¾—"""
        error_filter = "severity >= ERROR"
        return self.fetch_cloud_run_logs(
            hours=hours,
            service_suffix=service_suffix,
            log_filter=error_filter,
            limit=100
        )

    def fetch_trading_logs(self, hours: int = 24, service_suffix: str = "") -> Tuple[bool, List[Dict]]:
        """å–å¼•ãƒ­ã‚°å°‚ç”¨å–å¾—"""
        trading_filter = (
            f"(jsonPayload.message~\"æ³¨æ–‡\" OR jsonPayload.message~\"å–å¼•\" OR "
            f"jsonPayload.message~\"SIGNAL\" OR jsonPayload.message~\"BUY\" OR "
            f"jsonPayload.message~\"SELL\" OR jsonPayload.message~\"HOLD\" OR "
            f"textPayload~\"SIGNAL\" OR textPayload~\"BUY\" OR "
            f"textPayload~\"SELL\" OR textPayload~\"HOLD\")"
        )
        return self.fetch_cloud_run_logs(
            hours=hours,
            service_suffix=service_suffix,
            log_filter=trading_filter,
            limit=300
        )

    # ===== å…±é€šCloud Runã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª =====
    
    def check_service_health(self, service_suffix: str = "") -> Dict:
        """Cloud Runã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª"""
        target_service = f"{self.service_name}{service_suffix}"
        
        logger.info(f"ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª: {target_service}")
        
        cmd = [
            "gcloud", "run", "services", "describe", target_service,
            "--region", self.region,
            "--format", "json"
        ]
        
        returncode, stdout, stderr = self.run_gcloud_command(cmd)
        
        if returncode == 0:
            try:
                service_info = json.loads(stdout)
                
                health_data = {
                    "service_status": "UP",
                    "service_name": target_service,
                    "latest_revision": service_info.get("status", {}).get("latestReadyRevisionName", "unknown"),
                    "traffic_allocation": service_info.get("status", {}).get("traffic", []),
                    "url": service_info.get("status", {}).get("url", ""),
                    "last_updated": service_info.get("metadata", {}).get("annotations", {}).get("run.googleapis.com/lastModifier", "unknown"),
                    "cpu": service_info.get("spec", {}).get("template", {}).get("spec", {}).get("containerConcurrency", "unknown"),
                    "memory": service_info.get("spec", {}).get("template", {}).get("spec", {}).get("containers", [{}])[0].get("resources", {}).get("limits", {}).get("memory", "unknown")
                }
                
                logger.info(f"âœ… ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹: {health_data['service_status']}")
                return health_data
                
            except json.JSONDecodeError as e:
                logger.error(f"ã‚µãƒ¼ãƒ“ã‚¹æƒ…å ±JSONè§£æå¤±æ•—: {e}")
                return {"service_status": "JSON_ERROR", "error": str(e)}
        else:
            logger.error(f"âŒ ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹å–å¾—å¤±æ•—: {stderr}")
            return {"service_status": "DOWN", "error": stderr}

    def check_service_endpoint(self, service_suffix: str = "") -> Dict:
        """ã‚µãƒ¼ãƒ“ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå¿œç­”ç¢ºèª"""
        health_data = self.check_service_health(service_suffix)
        
        if health_data.get("service_status") != "UP":
            return health_data
        
        service_url = health_data.get("url", "")
        if not service_url:
            return {**health_data, "endpoint_status": "NO_URL"}
        
        try:
            import urllib.request
            import urllib.error
            
            with urllib.request.urlopen(f"{service_url}/health", timeout=10) as response:
                if response.status == 200:
                    logger.info("âœ… ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå¿œç­”OK")
                    return {**health_data, "endpoint_status": "OK", "endpoint_response_code": 200}
                else:
                    logger.warning(f"âš ï¸ ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå¿œç­”ç•°å¸¸: {response.status}")
                    return {**health_data, "endpoint_status": "ERROR", "endpoint_response_code": response.status}
                    
        except urllib.error.URLError as e:
            logger.error(f"âŒ ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæ¥ç¶šå¤±æ•—: {e}")
            return {**health_data, "endpoint_status": "CONNECTION_ERROR", "endpoint_error": str(e)}
        except Exception as e:
            logger.error(f"âŒ ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return {**health_data, "endpoint_status": "UNKNOWN_ERROR", "endpoint_error": str(e)}

    # ===== å…±é€šãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æ©Ÿèƒ½ =====
    
    def load_csv_files(self, pattern: str, directory: str = None) -> List[pd.DataFrame]:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼‰"""
        target_dir = Path(directory) if directory else self.output_dir
        
        csv_files = list(target_dir.glob(pattern))
        if not csv_files:
            logger.warning(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pattern}")
            return []
        
        dataframes = []
        for file in csv_files:
            try:
                df = pd.read_csv(file)
                dataframes.append(df)
                logger.info(f"CSVèª­ã¿è¾¼ã¿æˆåŠŸ: {file.name} ({len(df)}è¡Œ)")
            except Exception as e:
                logger.warning(f"CSVèª­ã¿è¾¼ã¿å¤±æ•— {file}: {e}")
        
        return dataframes

    def load_json_files(self, pattern: str, directory: str = None) -> List[Dict]:
        """JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼‰"""
        target_dir = Path(directory) if directory else self.output_dir
        
        json_files = list(target_dir.glob(pattern))
        if not json_files:
            logger.warning(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pattern}")
            return []
        
        data_list = []
        for file in json_files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    data_list.append(data)
                    logger.info(f"JSONèª­ã¿è¾¼ã¿æˆåŠŸ: {file.name}")
            except Exception as e:
                logger.warning(f"JSONèª­ã¿è¾¼ã¿å¤±æ•— {file}: {e}")
        
        return data_list

    def load_latest_data_collection_results(self, data_dir: str = None) -> Dict:
        """æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿åé›†çµæœã‚’èª­ã¿è¾¼ã¿"""
        target_dir = Path(data_dir) if data_dir else self.output_dir / "data_collection"
        
        if not target_dir.exists():
            logger.warning(f"ãƒ‡ãƒ¼ã‚¿åé›†ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {target_dir}")
            return {"trades": [], "daily_stats": [], "performance_metrics": {}}
        
        # å–å¼•ãƒ‡ãƒ¼ã‚¿
        trade_dfs = self.load_csv_files("trades_*.csv", str(target_dir))
        trades = trade_dfs[-1].to_dict('records') if trade_dfs else []
        
        # æ—¥æ¬¡çµ±è¨ˆ
        stats_dfs = self.load_csv_files("daily_stats_*.csv", str(target_dir))
        daily_stats = stats_dfs[-1].to_dict('records') if stats_dfs else []
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
        perf_data = self.load_json_files("performance_metrics_*.json", str(target_dir))
        performance_metrics = perf_data[-1] if perf_data else {}
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿åé›†çµæœèª­ã¿è¾¼ã¿å®Œäº†: å–å¼•{len(trades)}ä»¶, çµ±è¨ˆ{len(daily_stats)}ä»¶")
        return {
            "trades": trades,
            "daily_stats": daily_stats, 
            "performance_metrics": performance_metrics
        }

    def load_ab_test_results(self, ab_test_dir: str = None) -> List[Dict]:
        """A/Bãƒ†ã‚¹ãƒˆçµæœèª­ã¿è¾¼ã¿"""
        target_dir = Path(ab_test_dir) if ab_test_dir else self.output_dir / "ab_testing"
        
        if not target_dir.exists():
            logger.warning(f"A/Bãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {target_dir}")
            return []
        
        ab_test_results = self.load_json_files("ab_test_*.json", str(target_dir))
        logger.info(f"A/Bãƒ†ã‚¹ãƒˆçµæœèª­ã¿è¾¼ã¿å®Œäº†: {len(ab_test_results)}ä»¶")
        return ab_test_results

    # ===== å…±é€šãƒ­ã‚°è§£ææ©Ÿèƒ½ =====
    
    def parse_log_message(self, log_entry: Dict) -> Dict:
        """ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å…±é€šè§£æ"""
        message = log_entry.get("textPayload") or log_entry.get("jsonPayload", {}).get("message", "")
        timestamp = log_entry.get("timestamp", "")
        severity = log_entry.get("severity", "INFO")
        
        # ã‚·ã‚°ãƒŠãƒ«ç¨®åˆ¥åˆ¤å®š
        signal_type = "unknown"
        if "BUY" in message.upper():
            signal_type = "buy"
        elif "SELL" in message.upper():
            signal_type = "sell"
        elif "HOLD" in message.upper():
            signal_type = "hold"
        
        # ä¿¡é ¼åº¦æŠ½å‡º
        confidence = 0.0
        import re
        confidence_match = re.search(r"confidence[:\s]*([0-9.]+)", message.lower())
        if confidence_match:
            try:
                confidence = float(confidence_match.group(1))
            except ValueError:
                pass
        
        # æˆ¦ç•¥ã‚¿ã‚¤ãƒ—æŠ½å‡º
        strategy_type = "unknown"
        if "atr" in message.lower():
            strategy_type = "atr_based"
        elif "ensemble" in message.lower():
            strategy_type = "ensemble"
        elif "ml" in message.lower():
            strategy_type = "ml_strategy"
        
        return {
            "timestamp": timestamp,
            "message": message,
            "severity": severity,
            "signal_type": signal_type,
            "confidence": confidence,
            "strategy_type": strategy_type
        }

    def analyze_signal_frequency(self, logs: List[Dict], hours: int) -> Dict:
        """ã‚·ã‚°ãƒŠãƒ«é »åº¦åˆ†æ"""
        parsed_logs = [self.parse_log_message(log) for log in logs]
        
        # ã‚·ã‚°ãƒŠãƒ«åˆ†é¡
        signal_counts = {"buy": 0, "sell": 0, "hold": 0, "unknown": 0}
        confidences = []
        
        for parsed in parsed_logs:
            signal_type = parsed["signal_type"]
            if signal_type in signal_counts:
                signal_counts[signal_type] += 1
            
            if parsed["confidence"] > 0:
                confidences.append(parsed["confidence"])
        
        total_signals = sum(signal_counts.values())
        signal_frequency = total_signals / hours if hours > 0 else 0.0
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
        high_confidence_count = len([c for c in confidences if c > 0.7])
        
        return {
            "total_signals": total_signals,
            "signal_breakdown": signal_counts,
            "signal_frequency_per_hour": round(signal_frequency, 2),
            "avg_confidence": round(avg_confidence, 3),
            "high_confidence_count": high_confidence_count,
            "analysis_period_hours": hours
        }

    # ===== å…±é€šãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜æ©Ÿèƒ½ =====
    
    def save_json_report(self, data: Dict, filename: str) -> str:
        """JSONå½¢å¼ã§ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = self.output_dir / f"{filename}_{timestamp}.json"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"JSONãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {file_path}")
        return str(file_path)

    def save_csv_data(self, data: List[Dict], filename: str) -> str:
        """CSVå½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        if not data:
            logger.warning("ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return ""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = self.output_dir / f"{filename}_{timestamp}.csv"
        
        df = pd.DataFrame(data)
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        logger.info(f"CSVãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {file_path} ({len(data)}è¡Œ)")
        return str(file_path)

    # ===== æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆå„ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å®Ÿè£…ï¼‰ =====
    
    @abstractmethod
    def run_analysis(self, **kwargs) -> Dict:
        """åˆ†æå®Ÿè¡Œï¼ˆå„ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å®Ÿè£…å¿…é ˆï¼‰"""
        pass

    @abstractmethod
    def generate_report(self, analysis_result: Dict) -> str:
        """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆå„ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å®Ÿè£…å¿…é ˆï¼‰"""
        pass


class CloudRunAnalyzer(BaseAnalyzer):
    """Cloud Runç‰¹åŒ–åˆ†æã‚¯ãƒ©ã‚¹ï¼ˆä¾‹ç¤ºãƒ»æ‹¡å¼µç”¨ï¼‰"""
    
    def run_analysis(self, hours: int = 24, include_health_check: bool = True) -> Dict:
        """Cloud RunåŒ…æ‹¬åˆ†æ"""
        logger.info(f"Cloud RunåŒ…æ‹¬åˆ†æé–‹å§‹ï¼ˆ{hours}æ™‚é–“ï¼‰")
        
        # ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª
        health_data = {}
        if include_health_check:
            health_data = self.check_service_endpoint()
        
        # ãƒ­ã‚°åˆ†æ
        success, logs = self.fetch_trading_logs(hours)
        
        if success:
            signal_analysis = self.analyze_signal_frequency(logs, hours)
        else:
            signal_analysis = {"total_signals": 0, "error": "ãƒ­ã‚°å–å¾—å¤±æ•—"}
        
        # ã‚¨ãƒ©ãƒ¼åˆ†æ
        error_success, error_logs = self.fetch_error_logs(hours)
        error_analysis = {
            "total_errors": len(error_logs) if error_success else 0,
            "error_rate_per_hour": len(error_logs) / hours if error_success and hours > 0 else 0
        }
        
        return {
            "timestamp": datetime.now().isoformat(),
            "analysis_period_hours": hours,
            "service_health": health_data,
            "signal_analysis": signal_analysis,
            "error_analysis": error_analysis
        }

    def generate_report(self, analysis_result: Dict) -> str:
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = []
        report.append("=" * 60)
        report.append("Cloud Runåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        report.append("=" * 60)
        report.append(f"å®Ÿè¡Œæ—¥æ™‚: {analysis_result['timestamp']}")
        report.append(f"åˆ†ææœŸé–“: {analysis_result['analysis_period_hours']}æ™‚é–“")
        report.append("")
        
        # ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹
        health = analysis_result.get("service_health", {})
        report.append("ğŸ¥ ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹:")
        report.append(f"  çŠ¶æ…‹: {health.get('service_status', 'UNKNOWN')}")
        report.append(f"  URL: {health.get('url', 'Unknown')}")
        report.append("")
        
        # ã‚·ã‚°ãƒŠãƒ«åˆ†æ
        signal = analysis_result.get("signal_analysis", {})
        report.append("ğŸ“Š ã‚·ã‚°ãƒŠãƒ«åˆ†æ:")
        report.append(f"  ç·æ•°: {signal.get('total_signals', 0)}")
        report.append(f"  é »åº¦: {signal.get('signal_frequency_per_hour', 0)}/æ™‚é–“")
        report.append(f"  å¹³å‡ä¿¡é ¼åº¦: {signal.get('avg_confidence', 0)}")
        report.append("")
        
        # ã‚¨ãƒ©ãƒ¼åˆ†æ
        error = analysis_result.get("error_analysis", {})
        report.append("ğŸ” ã‚¨ãƒ©ãƒ¼åˆ†æ:")
        report.append(f"  ç·ã‚¨ãƒ©ãƒ¼æ•°: {error.get('total_errors', 0)}")
        report.append(f"  ã‚¨ãƒ©ãƒ¼ç‡: {error.get('error_rate_per_hour', 0)}/æ™‚é–“")
        report.append("=" * 60)
        
        return "\n".join(report)


if __name__ == "__main__":
    # ä½¿ç”¨ä¾‹
    analyzer = CloudRunAnalyzer()
    result = analyzer.run_analysis(hours=24)
    report = analyzer.generate_report(result)
    print(report)