#!/usr/bin/env python3
"""
ãƒ‡ãƒ—ãƒ­ã‚¤å•é¡Œç‚¹æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
éå»ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã§ç™ºç”Ÿã—ãŸå•é¡Œã‚’ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã§äº‹å‰æ¤œå‡º

æ¤œè¨¼é …ç›®:
1. ATRè¨ˆç®—å•é¡Œ
2. ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å•é¡Œ
3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«å•é¡Œ
4. ãƒ‡ãƒ¼ã‚¿å–å¾—ä»¶æ•°å•é¡Œ
5. ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦å•é¡Œ
"""

import json
import logging
import sys
from datetime import datetime, timedelta
from pathlib import Path

import numpy as np
import pandas as pd

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from crypto_bot.indicator.calculator import IndicatorCalculator
from crypto_bot.ml.feature_order_manager import FeatureOrderManager
from crypto_bot.ml.preprocessor import FeatureEngineer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DeploymentIssueValidator:
    """ãƒ‡ãƒ—ãƒ­ã‚¤å•é¡Œç‚¹æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, config_path: str = "config/production/production.yml"):
        self.config_path = config_path
        self.issues_detected = []
        self.validation_results = {}

        # æ¤œè¨¼é …ç›®å®šç¾©
        self.validation_items = {
            "atr_calculation": "ATRè¨ˆç®—ãƒ»NaNå€¤ãƒ»ãƒ‡ãƒ¼ã‚¿ä¸è¶³å•é¡Œ",
            "multi_timeframe": "ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ»ãƒ‡ãƒ¼ã‚¿åŒæœŸå•é¡Œ",
            "ensemble_model": "ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ãƒ»ç‰¹å¾´é‡ä¸ä¸€è‡´å•é¡Œ",
            "data_acquisition": "ãƒ‡ãƒ¼ã‚¿å–å¾—ä»¶æ•°ãƒ»APIåˆ¶é™å•é¡Œ",
            "data_freshness": "ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦ãƒ»ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å•é¡Œ",
        }

    def load_config(self):
        """è¨­å®šèª­ã¿è¾¼ã¿"""
        import yaml

        with open(self.config_path, "r") as f:
            config = yaml.safe_load(f)

        logger.info(f"âœ… Configuration loaded from {self.config_path}")
        return config

    def validate_atr_calculation(self, data_df):
        """ATRè¨ˆç®—å•é¡Œæ¤œè¨¼"""
        logger.info("ğŸ” Validating ATR calculation issues...")

        issues = []

        try:
            # 1. ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã§ã® ATR è¨ˆç®—
            small_data = data_df.head(10)  # 10ä»¶ã®å°‘é‡ãƒ‡ãƒ¼ã‚¿
            calculator = IndicatorCalculator()

            # ATRæœŸé–“14ã§ã®è¨ˆç®—ãƒ†ã‚¹ãƒˆ
            atr_result = calculator.calculate_atr(small_data, period=14)

            # NaNå€¤ãƒã‚§ãƒƒã‚¯
            nan_count = atr_result.isna().sum()
            total_count = len(atr_result)
            nan_ratio = nan_count / total_count if total_count > 0 else 1.0

            if nan_ratio > 0.8:  # 80%ä»¥ä¸ŠãŒNaN
                issues.append(
                    {
                        "type": "atr_high_nan_ratio",
                        "severity": "HIGH",
                        "description": f"ATRè¨ˆç®—ã§{nan_ratio:.2%}ãŒNaNå€¤",
                        "impact": "ãƒªã‚¹ã‚¯ç®¡ç†ä¸èƒ½ãƒ»ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºè¨ˆç®—ã‚¨ãƒ©ãƒ¼",
                        "recommendation": "ATRæœŸé–“çŸ­ç¸®ã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½å®Ÿè£…",
                    }
                )

            # 2. æ¥µç«¯ãªATRå€¤ãƒã‚§ãƒƒã‚¯
            valid_atr = atr_result.dropna()
            if len(valid_atr) > 0:
                max_atr = valid_atr.max()
                mean_price = data_df["close"].mean()

                if max_atr > mean_price * 0.1:  # ATRãŒä¾¡æ ¼ã®10%ä»¥ä¸Š
                    issues.append(
                        {
                            "type": "atr_extreme_values",
                            "severity": "MEDIUM",
                            "description": f"æ¥µç«¯ãªATRå€¤æ¤œå‡º: {max_atr:.2f} (ä¾¡æ ¼ã®{max_atr/mean_price:.2%})",
                            "impact": "éå¤§ãªã‚¹ãƒˆãƒƒãƒ—ãƒ­ã‚¹ãƒ»èª¤ã£ãŸãƒªã‚¹ã‚¯è¨ˆç®—",
                            "recommendation": "ATRä¸Šé™åˆ¶é™ãƒ»ç•°å¸¸å€¤ãƒ•ã‚£ãƒ«ã‚¿å®Ÿè£…",
                        }
                    )

            # 3. ATRæœŸé–“vs ãƒ‡ãƒ¼ã‚¿é‡ãƒã‚§ãƒƒã‚¯
            required_data = 20  # ATR_14 + ãƒãƒ¼ã‚¸ãƒ³
            available_data = len(data_df)

            if available_data < required_data:
                issues.append(
                    {
                        "type": "atr_insufficient_data",
                        "severity": "HIGH",
                        "description": f"ATRè¨ˆç®—ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ä¸è¶³: {available_data}/{required_data}",
                        "impact": "ATRè¨ˆç®—ä¸å¯ãƒ»ãƒªã‚¹ã‚¯ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ æ©Ÿèƒ½åœæ­¢",
                        "recommendation": "ãƒ‡ãƒ¼ã‚¿å–å¾—ä»¶æ•°å¢—åŠ ãƒ»æœ€å°ãƒ‡ãƒ¼ã‚¿è¦ä»¶ç¢ºèª",
                    }
                )

        except Exception as e:
            issues.append(
                {
                    "type": "atr_calculation_error",
                    "severity": "CRITICAL",
                    "description": f"ATRè¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}",
                    "impact": "ã‚·ã‚¹ãƒ†ãƒ åœæ­¢ãƒ»å–å¼•ä¸å¯",
                    "recommendation": "ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…",
                }
            )

        self.validation_results["atr_calculation"] = {
            "status": "FAIL" if issues else "PASS",
            "issues_count": len(issues),
            "issues": issues,
        }

        logger.info(f"ğŸ“Š ATRæ¤œè¨¼å®Œäº†: {len(issues)}å€‹ã®å•é¡Œæ¤œå‡º")
        return issues

    def validate_multi_timeframe(self, config):
        """ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å•é¡Œæ¤œè¨¼"""
        logger.info("ğŸ” Validating multi-timeframe issues...")

        issues = []

        try:
            # 1. ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ è¨­å®šç¢ºèª
            mtf_config = config.get("multi_timeframe", {})
            timeframes = mtf_config.get("timeframes", [])

            if not timeframes:
                issues.append(
                    {
                        "type": "mtf_no_timeframes",
                        "severity": "HIGH",
                        "description": "ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ è¨­å®šãªã—",
                        "impact": "å˜ä¸€ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æãƒ»äºˆæ¸¬ç²¾åº¦ä½ä¸‹",
                        "recommendation": "15m, 1h, 4hã®ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ è¨­å®šè¿½åŠ ",
                    }
                )

            # 2. é‡ã¿è¨­å®šãƒã‚§ãƒƒã‚¯
            weights = mtf_config.get("weights", [])
            if len(weights) != len(timeframes):
                issues.append(
                    {
                        "type": "mtf_weight_mismatch",
                        "severity": "MEDIUM",
                        "description": f"ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ æ•°{len(timeframes)}ã¨é‡ã¿æ•°{len(weights)}ãŒä¸ä¸€è‡´",
                        "impact": "é‡ã¿è¨ˆç®—ã‚¨ãƒ©ãƒ¼ãƒ»ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬å¤±æ•—",
                        "recommendation": "ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã¨é‡ã¿æ•°ã®ä¸€è‡´ç¢ºèª",
                    }
                )

            # 3. ãƒ‡ãƒ¼ã‚¿å“è³ªé–¾å€¤ç¢ºèª
            quality_threshold = mtf_config.get("data_quality_threshold", 0)
            if quality_threshold < 0.5:
                issues.append(
                    {
                        "type": "mtf_low_quality_threshold",
                        "severity": "MEDIUM",
                        "description": f"ãƒ‡ãƒ¼ã‚¿å“è³ªé–¾å€¤ãŒä½ã„: {quality_threshold}",
                        "impact": "ä½å“è³ªãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ãƒ»å–å¼•ç²¾åº¦ä½ä¸‹",
                        "recommendation": "å“è³ªé–¾å€¤ã‚’0.6ä»¥ä¸Šã«è¨­å®š",
                    }
                )

        except Exception as e:
            issues.append(
                {
                    "type": "mtf_configuration_error",
                    "severity": "HIGH",
                    "description": f"ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ è¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}",
                    "impact": "MTFæ©Ÿèƒ½åœæ­¢ãƒ»å˜ä¸€TFåˆ†æã®ã¿",
                    "recommendation": "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ§‹æ–‡ç¢ºèªãƒ»å¿…é ˆé …ç›®è¿½åŠ ",
                }
            )

        self.validation_results["multi_timeframe"] = {
            "status": "FAIL" if issues else "PASS",
            "issues_count": len(issues),
            "issues": issues,
        }

        logger.info(f"ğŸ“Š MTFæ¤œè¨¼å®Œäº†: {len(issues)}å€‹ã®å•é¡Œæ¤œå‡º")
        return issues

    def validate_ensemble_model(self, config):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«å•é¡Œæ¤œè¨¼"""
        logger.info("ğŸ” Validating ensemble model issues...")

        issues = []

        try:
            # 1. ç‰¹å¾´é‡é †åºç®¡ç†ãƒã‚§ãƒƒã‚¯
            feature_manager = FeatureOrderManager()
            expected_features = feature_manager.FEATURE_ORDER_97

            # 2. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
            model_paths = [
                "models/production/model.pkl",
                "models/validation/lgbm_97_features.pkl",
                "models/validation/xgb_97_features.pkl",
                "models/validation/rf_97_features.pkl",
            ]

            missing_models = []
            for model_path in model_paths:
                if not Path(model_path).exists():
                    missing_models.append(model_path)

            if missing_models:
                issues.append(
                    {
                        "type": "ensemble_missing_models",
                        "severity": "HIGH",
                        "description": f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä¸è¶³: {len(missing_models)}å€‹",
                        "missing_files": missing_models,
                        "impact": "ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ä¸å¯ãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‹•ä½œ",
                        "recommendation": "æ¬ æãƒ¢ãƒ‡ãƒ«ã®å†å­¦ç¿’ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ç¢ºèª",
                    }
                )

            # 3. ç‰¹å¾´é‡æ•°ä¸€è‡´ç¢ºèª
            ml_config = config.get("ml", {})
            extra_features = ml_config.get("extra_features", [])

            # Phase 2: 97ç‰¹å¾´é‡ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œ
            # åŸºæœ¬ç‰¹å¾´é‡æ•° + extra_features
            base_features = (
                5  # OHLCV ã®ã¿ï¼ˆlags ã¨ returns ã¯ extra_features ã«å«ã¾ã‚Œã‚‹ï¼‰
            )
            total_expected = base_features + len(extra_features)

            if total_expected != len(expected_features):
                issues.append(
                    {
                        "type": "ensemble_feature_count_mismatch",
                        "severity": "CRITICAL",
                        "description": f"ç‰¹å¾´é‡æ•°ä¸ä¸€è‡´: è¨­å®š{total_expected} vs æœŸå¾…{len(expected_features)}",
                        "impact": "ç‰¹å¾´é‡æ•°ã‚¨ãƒ©ãƒ¼ãƒ»äºˆæ¸¬å®Ÿè¡Œä¸å¯",
                        "recommendation": "ç‰¹å¾´é‡è¨­å®šã¨é †åºå®šç¾©ã®ä¸€è‡´ç¢ºèª",
                    }
                )

            # 4. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šç¢ºèª
            ensemble_config = config.get("ml", {}).get("ensemble", {})
            if not ensemble_config.get("enabled", False):
                issues.append(
                    {
                        "type": "ensemble_disabled",
                        "severity": "MEDIUM",
                        "description": "ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãŒç„¡åŠ¹åŒ–",
                        "impact": "å˜ä¸€ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ãƒ»äºˆæ¸¬ç²¾åº¦ä½ä¸‹å¯èƒ½æ€§",
                        "recommendation": "ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’æœ‰åŠ¹åŒ–æ¤œè¨",
                    }
                )

        except Exception as e:
            issues.append(
                {
                    "type": "ensemble_validation_error",
                    "severity": "CRITICAL",
                    "description": f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}",
                    "impact": "æ¤œè¨¼ä¸å¯ãƒ»æœªçŸ¥ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å•é¡Œæ®‹å­˜",
                    "recommendation": "æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ä¿®æ­£ãƒ»æ‰‹å‹•ç¢ºèªå®Ÿæ–½",
                }
            )

        self.validation_results["ensemble_model"] = {
            "status": "FAIL" if issues else "PASS",
            "issues_count": len(issues),
            "issues": issues,
        }

        logger.info(f"ğŸ“Š ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œè¨¼å®Œäº†: {len(issues)}å€‹ã®å•é¡Œæ¤œå‡º")
        return issues

    def validate_data_acquisition(self, config):
        """ãƒ‡ãƒ¼ã‚¿å–å¾—ä»¶æ•°å•é¡Œæ¤œè¨¼"""
        logger.info("ğŸ” Validating data acquisition issues...")

        issues = []

        try:
            # 1. ãƒ‡ãƒ¼ã‚¿å–å¾—è¨­å®šç¢ºèª
            data_config = config.get("data", {})
            limit = data_config.get("limit", 0)
            since_hours = data_config.get("since_hours", 0)

            # å¿…è¦ãƒ‡ãƒ¼ã‚¿é‡è¨ˆç®—
            required_for_features = 200  # ç‰¹å¾´é‡è¨ˆç®—ã«å¿…è¦
            required_for_ml = 100  # MLå­¦ç¿’ã«å¿…è¦
            total_required = max(required_for_features, required_for_ml)

            if limit < total_required:
                issues.append(
                    {
                        "type": "data_insufficient_limit",
                        "severity": "HIGH",
                        "description": f"ãƒ‡ãƒ¼ã‚¿å–å¾—åˆ¶é™ä¸è¶³: {limit}/{total_required}",
                        "impact": "ç‰¹å¾´é‡è¨ˆç®—ä¸å¯ãƒ»MLå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä¸è¶³",
                        "recommendation": f"limitå€¤ã‚’{total_required}ä»¥ä¸Šã«è¨­å®š",
                    }
                )

            # 2. APIåˆ¶é™ãƒã‚§ãƒƒã‚¯
            max_attempts = data_config.get("max_attempts", 0)
            if max_attempts < 15:
                issues.append(
                    {
                        "type": "data_low_retry_attempts",
                        "severity": "MEDIUM",
                        "description": f"ãƒªãƒˆãƒ©ã‚¤å›æ•°ä¸è¶³: {max_attempts}",
                        "impact": "APIåˆ¶é™æ™‚ã®ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—ãƒ»ä¸å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿",
                        "recommendation": "max_attempts ã‚’20ä»¥ä¸Šã«è¨­å®š",
                    }
                )

            # 3. ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
            per_page = data_config.get("per_page", 0)
            if per_page < 100:
                issues.append(
                    {
                        "type": "data_small_page_size",
                        "severity": "LOW",
                        "description": f"ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºå°: {per_page}",
                        "impact": "APIå‘¼ã³å‡ºã—å›æ•°å¢—åŠ ãƒ»å–å¾—åŠ¹ç‡ä½ä¸‹",
                        "recommendation": "per_page ã‚’200ã«è¨­å®šï¼ˆåŠ¹ç‡åŒ–ï¼‰",
                    }
                )

            # 4. ãƒ‡ãƒ¼ã‚¿ä¿æŒæœŸé–“
            if since_hours < 72:
                issues.append(
                    {
                        "type": "data_short_retention",
                        "severity": "MEDIUM",
                        "description": f"ãƒ‡ãƒ¼ã‚¿ä¿æŒæœŸé–“çŸ­: {since_hours}æ™‚é–“",
                        "impact": "ç‰¹å¾´é‡è¨ˆç®—ã«å¿…è¦ãªå±¥æ­´ä¸è¶³",
                        "recommendation": "since_hours ã‚’96æ™‚é–“ä»¥ä¸Šã«è¨­å®š",
                    }
                )

        except Exception as e:
            issues.append(
                {
                    "type": "data_config_error",
                    "severity": "HIGH",
                    "description": f"ãƒ‡ãƒ¼ã‚¿å–å¾—è¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}",
                    "impact": "ãƒ‡ãƒ¼ã‚¿å–å¾—æ©Ÿèƒ½åœæ­¢ãƒ»ã‚·ã‚¹ãƒ†ãƒ å‹•ä½œä¸å¯",
                    "recommendation": "ãƒ‡ãƒ¼ã‚¿è¨­å®šæ§‹æ–‡ç¢ºèªãƒ»å¿…é ˆé …ç›®è¿½åŠ ",
                }
            )

        self.validation_results["data_acquisition"] = {
            "status": "FAIL" if issues else "PASS",
            "issues_count": len(issues),
            "issues": issues,
        }

        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿å–å¾—æ¤œè¨¼å®Œäº†: {len(issues)}å€‹ã®å•é¡Œæ¤œå‡º")
        return issues

    def validate_data_freshness(self, data_df):
        """ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦å•é¡Œæ¤œè¨¼"""
        logger.info("ğŸ” Validating data freshness issues...")

        issues = []

        try:
            # 1. ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯
            if "timestamp" in data_df.columns:
                timestamps = pd.to_datetime(data_df["timestamp"])
                time_diffs = timestamps.diff().dropna()

                # 1æ™‚é–“ä»¥ä¸Šã®é–“éš”ãƒã‚§ãƒƒã‚¯
                large_gaps = time_diffs[time_diffs > pd.Timedelta(hours=2)]
                if len(large_gaps) > 0:
                    issues.append(
                        {
                            "type": "data_large_time_gaps",
                            "severity": "MEDIUM",
                            "description": f"å¤§ããªæ™‚é–“é–“éš”æ¤œå‡º: {len(large_gaps)}ç®‡æ‰€",
                            "max_gap": str(large_gaps.max()),
                            "impact": "ãƒ‡ãƒ¼ã‚¿æ¬ æãƒ»ç‰¹å¾´é‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼ãƒ»äºˆæ¸¬ç²¾åº¦ä½ä¸‹",
                            "recommendation": "ãƒ‡ãƒ¼ã‚¿è£œé–“ãƒ»æ¬ æå€¤å‡¦ç†å®Ÿè£…",
                        }
                    )

                # 2. æœªæ¥æ™‚åˆ»ãƒã‚§ãƒƒã‚¯
                now = pd.Timestamp.now("UTC")
                future_data = timestamps[timestamps > now]
                if len(future_data) > 0:
                    issues.append(
                        {
                            "type": "data_future_timestamps",
                            "severity": "HIGH",
                            "description": f"æœªæ¥æ™‚åˆ»ãƒ‡ãƒ¼ã‚¿æ¤œå‡º: {len(future_data)}ä»¶",
                            "impact": "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç•°å¸¸ãƒ»æ™‚ç³»åˆ—åˆ†æã‚¨ãƒ©ãƒ¼",
                            "recommendation": "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ¤œè¨¼ãƒ»ä¿®æ­£æ©Ÿèƒ½å®Ÿè£…",
                        }
                    )

                # 3. ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦ãƒã‚§ãƒƒã‚¯
                latest_time = timestamps.max()
                staleness = now - latest_time

                if staleness > pd.Timedelta(hours=6):
                    issues.append(
                        {
                            "type": "data_stale_data",
                            "severity": "HIGH",
                            "description": f"å¤ã„ãƒ‡ãƒ¼ã‚¿: {staleness}å‰ãŒæœ€æ–°",
                            "impact": "å¤ã„ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ãƒ»å¸‚å ´å¤‰åŒ–æœªåæ˜ ",
                            "recommendation": "ãƒ‡ãƒ¼ã‚¿å–å¾—é »åº¦å‘ä¸Šãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å–å¾—å®Ÿè£…",
                        }
                    )

            # 4. ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§
            if all(col in data_df.columns for col in ["open", "high", "low", "close"]):
                # OHLCæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                invalid_ohlc = (
                    (data_df["high"] < data_df["low"])
                    | (data_df["high"] < data_df["open"])
                    | (data_df["high"] < data_df["close"])
                    | (data_df["low"] > data_df["open"])
                    | (data_df["low"] > data_df["close"])
                )

                invalid_count = invalid_ohlc.sum()
                if invalid_count > 0:
                    issues.append(
                        {
                            "type": "data_invalid_ohlc",
                            "severity": "CRITICAL",
                            "description": f"OHLCæ•´åˆæ€§ã‚¨ãƒ©ãƒ¼: {invalid_count}ä»¶",
                            "impact": "ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ç•°å¸¸ãƒ»ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼",
                            "recommendation": "ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ãƒ»ç•°å¸¸å€¤é™¤å»å®Ÿè£…",
                        }
                    )

        except Exception as e:
            issues.append(
                {
                    "type": "data_freshness_error",
                    "severity": "HIGH",
                    "description": f"ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}",
                    "impact": "æ–°é®®åº¦æ¤œè¨¼ä¸å¯ãƒ»ãƒ‡ãƒ¼ã‚¿å“è³ªä¸æ˜",
                    "recommendation": "æ¤œè¨¼ãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£ãƒ»æ‰‹å‹•ãƒ‡ãƒ¼ã‚¿ç¢ºèª",
                }
            )

        self.validation_results["data_freshness"] = {
            "status": "FAIL" if issues else "PASS",
            "issues_count": len(issues),
            "issues": issues,
        }

        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦æ¤œè¨¼å®Œäº†: {len(issues)}å€‹ã®å•é¡Œæ¤œå‡º")
        return issues

    def run_comprehensive_validation(self):
        """åŒ…æ‹¬çš„æ¤œè¨¼å®Ÿè¡Œ"""
        logger.info("ğŸš€ Starting comprehensive deployment issue validation...")

        # è¨­å®šèª­ã¿è¾¼ã¿
        config = self.load_config()

        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        try:
            data_df = pd.read_csv("data/btc_usd_2024_hourly.csv")
            if "timestamp" not in data_df.columns and data_df.columns[0].lower() in [
                "timestamp",
                "time",
                "date",
            ]:
                data_df.rename(columns={data_df.columns[0]: "timestamp"}, inplace=True)
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            data_df = pd.DataFrame()  # ç©ºã®DataFrame

        # å„æ¤œè¨¼é …ç›®å®Ÿè¡Œ
        all_issues = []

        # 1. ATRè¨ˆç®—å•é¡Œæ¤œè¨¼
        atr_issues = self.validate_atr_calculation(data_df)
        all_issues.extend(atr_issues)

        # 2. ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å•é¡Œæ¤œè¨¼
        mtf_issues = self.validate_multi_timeframe(config)
        all_issues.extend(mtf_issues)

        # 3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«å•é¡Œæ¤œè¨¼
        ensemble_issues = self.validate_ensemble_model(config)
        all_issues.extend(ensemble_issues)

        # 4. ãƒ‡ãƒ¼ã‚¿å–å¾—ä»¶æ•°å•é¡Œæ¤œè¨¼
        data_issues = self.validate_data_acquisition(config)
        all_issues.extend(data_issues)

        # 5. ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦å•é¡Œæ¤œè¨¼
        freshness_issues = self.validate_data_freshness(data_df)
        all_issues.extend(freshness_issues)

        # çµæœé›†è¨ˆ
        self.issues_detected = all_issues

        # é‡è¦åº¦åˆ¥é›†è¨ˆ
        critical_issues = [i for i in all_issues if i["severity"] == "CRITICAL"]
        high_issues = [i for i in all_issues if i["severity"] == "HIGH"]
        medium_issues = [i for i in all_issues if i["severity"] == "MEDIUM"]
        low_issues = [i for i in all_issues if i["severity"] == "LOW"]

        # çµæœä¿å­˜
        results = {
            "validation_timestamp": datetime.now().isoformat(),
            "config_path": self.config_path,
            "summary": {
                "total_issues": len(all_issues),
                "critical_issues": len(critical_issues),
                "high_issues": len(high_issues),
                "medium_issues": len(medium_issues),
                "low_issues": len(low_issues),
            },
            "validation_results": self.validation_results,
            "all_issues": all_issues,
            "recommendations": self.generate_recommendations(),
        }

        # çµæœä¿å­˜
        results_dir = Path("results/deployment_validation")
        results_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = results_dir / f"deployment_validation_{timestamp}.json"

        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        # ã‚µãƒãƒªãƒ¼å‡ºåŠ›
        logger.info("ğŸŠ Deployment validation completed!")
        logger.info(f"ğŸ“Š Results summary:")
        logger.info(f"   - Total issues: {len(all_issues)}")
        logger.info(f"   - Critical: {len(critical_issues)}")
        logger.info(f"   - High: {len(high_issues)}")
        logger.info(f"   - Medium: {len(medium_issues)}")
        logger.info(f"   - Low: {len(low_issues)}")
        logger.info(f"ğŸ“ Results saved: {results_file}")

        return results

    def generate_recommendations(self):
        """æ¨å¥¨å¯¾å¿œç­–ç”Ÿæˆ"""
        recommendations = []

        critical_count = len(
            [i for i in self.issues_detected if i["severity"] == "CRITICAL"]
        )
        high_count = len([i for i in self.issues_detected if i["severity"] == "HIGH"])

        if critical_count > 0:
            recommendations.append(
                {
                    "priority": "IMMEDIATE",
                    "action": "CRITICALå•é¡Œã®å³åº§ä¿®æ­£",
                    "description": f"{critical_count}å€‹ã®CRITICALå•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ãƒ‡ãƒ—ãƒ­ã‚¤å‰ã«å¿…ãšä¿®æ­£ã—ã¦ãã ã•ã„ã€‚",
                }
            )

        if high_count > 0:
            recommendations.append(
                {
                    "priority": "HIGH",
                    "action": "HIGHå•é¡Œã®ä¿®æ­£æ¤œè¨",
                    "description": f"{high_count}å€‹ã®HIGHå•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ãƒ‡ãƒ—ãƒ­ã‚¤å‰ã®ä¿®æ­£ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚",
                }
            )

        # å…·ä½“çš„æ¨å¥¨ç­–
        if any(i["type"].startswith("atr_") for i in self.issues_detected):
            recommendations.append(
                {
                    "priority": "HIGH",
                    "action": "ATRã‚·ã‚¹ãƒ†ãƒ å¼·åŒ–",
                    "description": "ATRè¨ˆç®—ã®å …ç‰¢æ€§å‘ä¸Šãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½å®Ÿè£…ãƒ»ç•°å¸¸å€¤ãƒ•ã‚£ãƒ«ã‚¿è¿½åŠ ",
                }
            )

        if any(i["type"].startswith("ensemble_") for i in self.issues_detected):
            recommendations.append(
                {
                    "priority": "HIGH",
                    "action": "ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚·ã‚¹ãƒ†ãƒ ä¿®æ­£",
                    "description": "ç‰¹å¾´é‡æ•°ä¸€è‡´ãƒ»ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªãƒ»ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šæœ€é©åŒ–",
                }
            )

        return recommendations


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    validator = DeploymentIssueValidator()
    results = validator.run_comprehensive_validation()

    # å•é¡ŒãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã§çµ‚äº†
    critical_count = results["summary"]["critical_issues"]
    high_count = results["summary"]["high_issues"]

    if critical_count > 0:
        print(f"âŒ CRITICALå•é¡Œ {critical_count}å€‹æ¤œå‡º - ãƒ‡ãƒ—ãƒ­ã‚¤é˜»æ­¢")
        sys.exit(2)
    elif high_count > 0:
        print(f"âš ï¸ HIGHå•é¡Œ {high_count}å€‹æ¤œå‡º - ä¿®æ­£æ¨å¥¨")
        sys.exit(1)
    else:
        print("âœ… æ¤œè¨¼å®Œäº† - ãƒ‡ãƒ—ãƒ­ã‚¤å¯èƒ½")
        sys.exit(0)


if __name__ == "__main__":
    main()
