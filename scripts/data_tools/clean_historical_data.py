#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ - btc_usd_2024_hourly.csvã®ç•°å¸¸å€¤ä¿®æ­£

Phase 14.6: ãƒ‡ãƒ¼ã‚¿å“è³ªä¿®æ­£ãƒ»ç•°å¸¸å€¤ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿæ–½
- 1e+88ç­‰ã®ç•°å¸¸å€¤ã‚’æ¤œå‡ºãƒ»ä¿®æ­£
- BTCä¾¡æ ¼ç¯„å›²ã‚’é©åˆ‡ãªå€¤ï¼ˆ10,000-150,000 USDï¼‰ã«åˆ¶é™
- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ•´åˆæ€§ç¢ºèª
- ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå¾Œã«ä¿®æ­£å®Ÿæ–½
"""

import logging
import shutil
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def clean_btc_historical_data():
    """BTCæ­´å²ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿæ–½"""

    data_path = Path("data/btc_usd_2024_hourly.csv")
    backup_path = Path(
        f"data/btc_usd_2024_hourly_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    )

    logger.info("ğŸ§¹ BTCæ­´å²ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    logger.info(f"ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
    shutil.copy2(data_path, backup_path)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    df = pd.read_csv(data_path)
    original_rows = len(df)
    logger.info(f"ğŸ“Š å…ƒãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {original_rows:,}")

    # ç•°å¸¸å€¤æ¤œå‡º
    price_columns = ["open", "high", "low", "close"]

    # BTCä¾¡æ ¼ã®é©åˆ‡ãªç¯„å›²è¨­å®š (2023-2024å¹´è€ƒæ…®)
    btc_min_price = 10000  # $10,000
    btc_max_price = 150000  # $150,000
    volume_max = 100000  # å‡ºæ¥é«˜ä¸Šé™

    # ç•°å¸¸å€¤æ¤œå‡º
    for col in price_columns:
        anomaly_mask = (df[col] > btc_max_price) | (df[col] < btc_min_price)
        anomaly_count = anomaly_mask.sum()
        if anomaly_count > 0:
            logger.warning(f"âš ï¸ {col}åˆ—ã§ç•°å¸¸å€¤æ¤œå‡º: {anomaly_count:,}è¡Œ")

    # å‡ºæ¥é«˜ç•°å¸¸å€¤æ¤œå‡º
    volume_anomaly = (df["volume"] > volume_max) | (df["volume"] < 0)
    volume_anomaly_count = volume_anomaly.sum()
    if volume_anomaly_count > 0:
        logger.warning(f"âš ï¸ volumeåˆ—ã§ç•°å¸¸å€¤æ¤œå‡º: {volume_anomaly_count:,}è¡Œ")

    # ç•°å¸¸å€¤ä¿®æ­£æˆ¦ç•¥
    logger.info("ğŸ”§ ç•°å¸¸å€¤ä¿®æ­£å®Ÿæ–½ä¸­...")

    # 1. æ¥µç«¯ãªç•°å¸¸å€¤ï¼ˆ1e+80ä»¥ä¸Šï¼‰ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤
    extreme_anomaly_mask = (
        (df["open"] > 1e80)
        | (df["high"] > 1e80)
        | (df["low"] > 1e80)
        | (df["close"] > 1e80)
    )
    extreme_anomaly_count = extreme_anomaly_mask.sum()

    if extreme_anomaly_count > 0:
        logger.warning(f"ğŸ—‘ï¸ æ¥µç«¯ç•°å¸¸å€¤è¡Œå‰Šé™¤: {extreme_anomaly_count:,}è¡Œ")
        df = df[~extreme_anomaly_mask].copy()

    # 2. é©åº¦ãªç•°å¸¸å€¤ã¯å‰å¾Œã®å€¤ã§è£œé–“
    for col in price_columns:
        anomaly_mask = (df[col] > btc_max_price) | (df[col] < btc_min_price)
        if anomaly_mask.sum() > 0:
            logger.info(f"ğŸ”„ {col}åˆ—è£œé–“ä¿®æ­£: {anomaly_mask.sum():,}è¡Œ")
            # ç·šå½¢è£œé–“ã§ä¿®æ­£
            df.loc[anomaly_mask, col] = np.nan
            df[col] = df[col].interpolate(method="linear")

    # 3. å‡ºæ¥é«˜ç•°å¸¸å€¤ä¿®æ­£
    volume_anomaly = (df["volume"] > volume_max) | (df["volume"] < 0)
    if volume_anomaly.sum() > 0:
        logger.info(f"ğŸ”„ volumeè£œé–“ä¿®æ­£: {volume_anomaly.sum():,}è¡Œ")
        df.loc[volume_anomaly, "volume"] = np.nan
        df["volume"] = df["volume"].interpolate(method="linear")
        df["volume"] = df["volume"].fillna(
            df["volume"].median()
        )  # æ®‹ã‚Šã¯ä¸­å¤®å€¤ã§åŸ‹ã‚ã‚‹

    # 4. ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é‡è¤‡ãƒ»æ¬ æãƒã‚§ãƒƒã‚¯
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    duplicates = df["timestamp"].duplicated().sum()
    if duplicates > 0:
        logger.warning(f"âš ï¸ ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é‡è¤‡æ¤œå‡º: {duplicates}è¡Œ")
        df = df.drop_duplicates(subset=["timestamp"]).copy()

    # 5. OHLCæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£
    ohlc_errors = 0
    for idx in df.index:
        o, h, l, c = df.loc[idx, ["open", "high", "low", "close"]]

        # high >= max(open, close, low) ã‹ã¤ low <= min(open, close, high)
        if h < max(o, l, c):
            df.loc[idx, "high"] = max(o, l, c)
            ohlc_errors += 1
        if l > min(o, h, c):
            df.loc[idx, "low"] = min(o, h, c)
            ohlc_errors += 1

    if ohlc_errors > 0:
        logger.info(f"ğŸ”§ OHLCæ•´åˆæ€§ä¿®æ­£: {ohlc_errors}ç®‡æ‰€")

    # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª
    final_rows = len(df)
    removed_rows = original_rows - final_rows

    logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
    logger.info(f"ğŸ“Š æœ€çµ‚è¡Œæ•°: {final_rows:,} (å‰Šé™¤: {removed_rows:,}è¡Œ)")

    # ä¾¡æ ¼ç¯„å›²ç¢ºèª
    for col in price_columns:
        min_val = df[col].min()
        max_val = df[col].max()
        logger.info(f"ğŸ“ˆ {col}: ${min_val:,.2f} - ${max_val:,.2f}")

    # å‡ºæ¥é«˜ç¯„å›²ç¢ºèª
    vol_min = df["volume"].min()
    vol_max = df["volume"].max()
    logger.info(f"ğŸ“Š volume: {vol_min:,.2f} - {vol_max:,.2f}")

    # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    logger.info("ğŸ’¾ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­...")
    df.to_csv(data_path, index=False)

    logger.info("ğŸ‰ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ãƒ»å“è³ªå‘ä¸Šå®Ÿç¾!")

    # çµ±è¨ˆã‚µãƒãƒªãƒ¼å‡ºåŠ›
    return {
        "original_rows": original_rows,
        "final_rows": final_rows,
        "removed_rows": removed_rows,
        "backup_path": str(backup_path),
        "price_range": {
            col: {"min": float(df[col].min()), "max": float(df[col].max())}
            for col in price_columns
        },
        "volume_range": {
            "min": float(df["volume"].min()),
            "max": float(df["volume"].max()),
        },
    }


if __name__ == "__main__":
    try:
        result = clean_btc_historical_data()
        print(f"\nğŸŠ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æˆåŠŸ!")
        print(f"ğŸ“Š å‡¦ç†çµæœ: {result['original_rows']:,} â†’ {result['final_rows']:,}è¡Œ")
        print(f"ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {result['backup_path']}")
    except Exception as e:
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¤±æ•—: {e}")
        raise
