#!/usr/bin/env python3
"""
Phase 3: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã‚„ãƒã‚¤ã‚¢ã‚¹ã®åŸå› ã‚’ç‰¹å®šã™ã‚‹

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’åˆ†æã—ã¾ã™ï¼š
1. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒï¼ˆä¸Šæ˜‡/ä¸‹è½ã®æ¯”ç‡ï¼‰
2. æœŸé–“åˆ¥ã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒï¼ˆãƒ™ã‚¢ãƒãƒ¼ã‚±ãƒƒãƒˆã®å½±éŸ¿ï¼‰
3. ç‰¹å¾´é‡ã®ç›¸é–¢ã¨é‡è¦åº¦
4. ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
"""

import json
import logging
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import yaml

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from crypto_bot.data.fetcher import MarketDataFetcher
from crypto_bot.ml.feature_engines import BatchFeatureCalculator, TechnicalFeatureEngine
from crypto_bot.ml.preprocessor import FeatureEngineer
from crypto_bot.ml.target import make_classification_target

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class TrainingDataAnalyzer:
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(self, config_path: str):
        """åˆæœŸåŒ–"""
        with open(config_path, "r") as f:
            self.config = yaml.safe_load(f)

        self.feature_engineer = FeatureEngineer(self.config)
        self.batch_calc = BatchFeatureCalculator(self.config)
        self.tech_engine = TechnicalFeatureEngine(self.config, self.batch_calc)

        # çµæœä¿å­˜ç”¨
        self.results = {
            "class_distribution": {},
            "period_analysis": {},
            "feature_analysis": {},
            "metadata": {},
        }

    def load_training_data(self, days: int = 365) -> pd.DataFrame:
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆéå»1å¹´åˆ†ï¼‰"""
        try:
            # ãƒ‡ãƒ¼ã‚¿å–å¾—
            fetcher = MarketDataFetcher(
                exchange_id="bitbank",
                symbol="BTC/JPY",
                api_key=self.config.get("data", {}).get("api_key"),
                api_secret=self.config.get("data", {}).get("api_secret"),
                testnet=False,
            )

            end_time = datetime.now()
            start_time = end_time - timedelta(days=days)

            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã§å¤§é‡ãƒ‡ãƒ¼ã‚¿å–å¾—
            df = fetcher.get_price_df(
                timeframe="1h",
                since=int(start_time.timestamp() * 1000),
                limit=days * 24,
                paginate=True,
            )

            logger.info(f"ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å–å¾—: {len(df)}ãƒ¬ã‚³ãƒ¼ãƒ‰")
            return df

        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {e}")
            raise

    def analyze_class_distribution(self, df: pd.DataFrame):
        """ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã®åˆ†æ"""
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆ5æœŸé–“å¾Œã®ä¸Šæ˜‡/ä¸‹è½ï¼‰
        target = make_classification_target(df, horizon=5, threshold=0.0)

        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
        class_counts = target.value_counts()
        class_ratio = target.value_counts(normalize=True)

        self.results["class_distribution"] = {
            "up_count": int(class_counts.get(1, 0)),
            "down_count": int(class_counts.get(0, 0)),
            "up_ratio": float(class_ratio.get(1, 0)),
            "down_ratio": float(class_ratio.get(0, 0)),
            "total_samples": len(target),
            "imbalance_ratio": (
                float(class_counts.get(0, 0) / class_counts.get(1, 1))
                if class_counts.get(1, 0) > 0
                else np.inf
            ),
        }

        logger.info(
            f"ğŸ“Š ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: UP={class_counts.get(1, 0)} ({class_ratio.get(1, 0):.1%}), DOWN={class_counts.get(0, 0)} ({class_ratio.get(0, 0):.1%})"
        )

        return target

    def analyze_by_period(self, df: pd.DataFrame, target: pd.Series):
        """æœŸé–“åˆ¥åˆ†æ"""
        # æœˆåˆ¥é›†è¨ˆ
        df_with_target = df.copy()
        df_with_target["target"] = target
        df_with_target["month"] = pd.to_datetime(df_with_target.index).to_period("M")

        monthly_stats = []
        for month, group in df_with_target.groupby("month"):
            if "target" in group.columns and len(group) > 0:
                month_target = group["target"]
                up_ratio = (
                    (month_target == 1).sum() / len(month_target)
                    if len(month_target) > 0
                    else 0
                )
                monthly_stats.append(
                    {
                        "month": str(month),
                        "samples": len(group),
                        "up_ratio": float(up_ratio),
                        "down_ratio": float(1 - up_ratio),
                        "avg_price": float(group["close"].mean()),
                        "price_change": float(
                            (group["close"].iloc[-1] - group["close"].iloc[0])
                            / group["close"].iloc[0]
                            * 100
                        ),
                    }
                )

        self.results["period_analysis"] = {
            "monthly_stats": monthly_stats,
            "bear_market_months": sum(
                1 for stat in monthly_stats if stat["up_ratio"] < 0.4
            ),
            "bull_market_months": sum(
                1 for stat in monthly_stats if stat["up_ratio"] > 0.6
            ),
        }

    def analyze_features(self, df: pd.DataFrame):
        """ç‰¹å¾´é‡åˆ†æ"""
        # ç‰¹å¾´é‡ç”Ÿæˆ
        features_df = self.tech_engine.generate_features(df)

        # æ¬ è½ç‰¹å¾´é‡ã®ç¢ºèª
        expected_features = self.config["ml"]["extra_features"]
        actual_features = list(features_df.columns)

        missing_features = set(expected_features) - set(actual_features)
        extra_features = (
            set(actual_features)
            - set(expected_features)
            - {"open", "high", "low", "close", "volume"}
        )

        # ç‰¹å¾´é‡ã®çµ±è¨ˆ
        feature_stats = {}
        for col in features_df.columns:
            if col not in ["open", "high", "low", "close", "volume"]:
                feature_stats[col] = {
                    "mean": (
                        float(features_df[col].mean())
                        if not features_df[col].isna().all()
                        else None
                    ),
                    "std": (
                        float(features_df[col].std())
                        if not features_df[col].isna().all()
                        else None
                    ),
                    "null_ratio": float(
                        features_df[col].isna().sum() / len(features_df)
                    ),
                }

        self.results["feature_analysis"] = {
            "expected_count": len(expected_features) + 5,  # +5 for OHLCV
            "actual_count": len(actual_features),
            "missing_features": list(missing_features),
            "extra_features": list(extra_features),
            "feature_stats": feature_stats,
        }

    def load_model_metadata(self):
        """ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        metadata_path = project_root / "models/production/model_metadata.yaml"

        if metadata_path.exists():
            with open(metadata_path, "r") as f:
                metadata = yaml.safe_load(f)

            self.results["metadata"] = {
                "phase": metadata.get("phase"),
                "features_count": metadata.get("features_count"),
                "training_timestamp": metadata.get("training_timestamp"),
                "validation_results": metadata.get("validation_results", {}),
                "training_data_period": metadata.get("training_data_period", {}),
            }
        else:
            logger.warning("âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    def visualize_results(self, save_path: str = None):
        """çµæœã®å¯è¦–åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
        ax1 = axes[0, 0]
        class_dist = self.results["class_distribution"]
        labels = ["DOWN (0)", "UP (1)"]
        sizes = [class_dist["down_count"], class_dist["up_count"]]
        colors = ["red", "green"]
        ax1.pie(sizes, labels=labels, colors=colors, autopct="%1.1f%%", startangle=90)
        ax1.set_title("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ")

        # 2. æœˆåˆ¥UPæ¯”ç‡ã®æ¨ç§»
        ax2 = axes[0, 1]
        monthly_stats = self.results["period_analysis"]["monthly_stats"]
        months = [stat["month"] for stat in monthly_stats]
        up_ratios = [stat["up_ratio"] for stat in monthly_stats]

        ax2.plot(range(len(months)), up_ratios, marker="o")
        ax2.axhline(y=0.5, color="gray", linestyle="--", alpha=0.5)
        ax2.set_ylim(0, 1)
        ax2.set_title("æœˆåˆ¥UPæ¯”ç‡ã®æ¨ç§»")
        ax2.set_xlabel("æœˆ")
        ax2.set_ylabel("UPæ¯”ç‡")
        ax2.tick_params(axis="x", rotation=45)

        # 3. ä¾¡æ ¼æ¨ç§»ã¨ç›¸å ´ç’°å¢ƒ
        ax3 = axes[1, 0]
        prices = [stat["avg_price"] for stat in monthly_stats]
        ax3.plot(range(len(months)), prices, marker="o", color="blue")
        ax3.set_title("æœˆåˆ¥å¹³å‡ä¾¡æ ¼æ¨ç§»")
        ax3.set_xlabel("æœˆ")
        ax3.set_ylabel("ä¾¡æ ¼ (JPY)")
        ax3.tick_params(axis="x", rotation=45)

        # 4. ç‰¹å¾´é‡æ¬ è½çŠ¶æ³
        ax4 = axes[1, 1]
        feature_info = self.results["feature_analysis"]
        missing_count = len(feature_info["missing_features"])
        extra_count = len(feature_info["extra_features"])
        expected_count = feature_info["expected_count"]
        actual_count = feature_info["actual_count"]

        categories = ["æœŸå¾…", "å®Ÿéš›", "æ¬ è½", "è¿½åŠ "]
        values = [expected_count, actual_count, missing_count, extra_count]
        colors_bar = ["blue", "green", "red", "orange"]

        ax4.bar(categories, values, color=colors_bar)
        ax4.set_title("ç‰¹å¾´é‡ã‚«ã‚¦ãƒ³ãƒˆæ¯”è¼ƒ")
        ax4.set_ylabel("ç‰¹å¾´é‡æ•°")

        # ãƒ†ã‚­ã‚¹ãƒˆãƒ©ãƒ™ãƒ«è¿½åŠ 
        for i, v in enumerate(values):
            ax4.text(i, v + 1, str(v), ha="center")

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path)
            logger.info(f"ğŸ“Š ã‚°ãƒ©ãƒ•ä¿å­˜: {save_path}")
        else:
            plt.show()

    def print_analysis(self):
        """åˆ†æçµæœã®å‡ºåŠ›"""
        print("\n" + "=" * 60)
        print("ğŸ” å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœ")
        print("=" * 60)

        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
        class_dist = self.results["class_distribution"]
        print("\nğŸ“Š ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ:")
        print(f"  - UP (1): {class_dist['up_count']} ({class_dist['up_ratio']:.1%})")
        print(
            f"  - DOWN (0): {class_dist['down_count']} ({class_dist['down_ratio']:.1%})"
        )
        print(f"  - ä¸å‡è¡¡æ¯”ç‡: {class_dist['imbalance_ratio']:.2f}")

        # æœŸé–“åˆ†æ
        period_info = self.results["period_analysis"]
        print("\nğŸ“Š æœŸé–“åˆ¥åˆ†æ:")
        print(f"  - ãƒ™ã‚¢ç›¸å ´æœˆæ•°: {period_info['bear_market_months']}")
        print(f"  - ãƒ–ãƒ«ç›¸å ´æœˆæ•°: {period_info['bull_market_months']}")
        print(f"  - åˆ†ææœŸé–“: {len(period_info['monthly_stats'])}ãƒ¶æœˆ")

        # ç‰¹å¾´é‡åˆ†æ
        feature_info = self.results["feature_analysis"]
        print("\nğŸ“Š ç‰¹å¾´é‡åˆ†æ:")
        print(f"  - æœŸå¾…ç‰¹å¾´é‡æ•°: {feature_info['expected_count']}")
        print(f"  - å®Ÿéš›ç‰¹å¾´é‡æ•°: {feature_info['actual_count']}")
        print(
            f"  - æ¬ è½ç‰¹å¾´é‡: {', '.join(feature_info['missing_features']) if feature_info['missing_features'] else 'ãªã—'}"
        )

        # ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = self.results.get("metadata", {})
        if metadata:
            print("\nğŸ“Š ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿:")
            print(f"  - Phase: {metadata.get('phase', 'N/A')}")
            print(f"  - å­¦ç¿’æ™‚ç‰¹å¾´é‡æ•°: {metadata.get('features_count', 'N/A')}")
            print(f"  - å­¦ç¿’æ—¥æ™‚: {metadata.get('training_timestamp', 'N/A')}")

        # è¨ºæ–­çµæœ
        print("\nğŸ” è¨ºæ–­çµæœ:")
        if class_dist["down_ratio"] > 0.6:
            print("  âš ï¸ é‡å¤§ãªDOWNãƒã‚¤ã‚¢ã‚¹: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒä¸‹è½ç›¸å ´ã«åã£ã¦ã„ã‚‹")
        elif class_dist["up_ratio"] > 0.6:
            print("  âš ï¸ UPãƒã‚¤ã‚¢ã‚¹: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒä¸Šæ˜‡ç›¸å ´ã«åã£ã¦ã„ã‚‹")
        else:
            print("  âœ… ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã¯æ¯”è¼ƒçš„ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚Œã¦ã„ã‚‹")

        if feature_info["missing_features"]:
            print(
                f"  âŒ ç‰¹å¾´é‡ä¸ä¸€è‡´: {len(feature_info['missing_features'])}å€‹ã®ç‰¹å¾´é‡ãŒæ¬ è½"
            )
        else:
            print("  âœ… ç‰¹å¾´é‡ã¯å®Œå…¨ã«ä¸€è‡´")

        print("\n" + "=" * 60)

    def save_results(self):
        """çµæœã®ä¿å­˜"""
        output_path = project_root / "results/training_data_analysis.json"
        output_path.parent.mkdir(exist_ok=True)

        with open(output_path, "w") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)

        logger.info(f"ğŸ“Š åˆ†æçµæœä¿å­˜: {output_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    config_path = str(project_root / "config/production/production.yml")

    # åˆ†æå®Ÿè¡Œ
    analyzer = TrainingDataAnalyzer(config_path)

    try:
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆç°¡æ˜“ç‰ˆï¼š30æ—¥åˆ†ï¼‰
        logger.info("ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
        df = analyzer.load_training_data(days=30)  # ãƒ‡ãƒ¢ç”¨ã«30æ—¥åˆ†

        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒåˆ†æ
        logger.info("ğŸ” ã‚¯ãƒ©ã‚¹åˆ†å¸ƒåˆ†æä¸­...")
        target = analyzer.analyze_class_distribution(df)

        # æœŸé–“åˆ¥åˆ†æ
        logger.info("ğŸ” æœŸé–“åˆ¥åˆ†æä¸­...")
        analyzer.analyze_by_period(df, target)

        # ç‰¹å¾´é‡åˆ†æ
        logger.info("ğŸ” ç‰¹å¾´é‡åˆ†æä¸­...")
        analyzer.analyze_features(df)

        # ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        logger.info("ğŸ” ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        analyzer.load_model_metadata()

        # çµæœå‡ºåŠ›
        analyzer.print_analysis()

        # çµæœä¿å­˜
        analyzer.save_results()

        # å¯è¦–åŒ–
        output_path = str(project_root / "results/training_data_analysis.png")
        analyzer.visualize_results(save_path=output_path)

        logger.info("âœ… åˆ†æå®Œäº†")

    except Exception as e:
        logger.error(f"âŒ åˆ†æå¤±æ•—: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
