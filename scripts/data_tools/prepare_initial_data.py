#!/usr/bin/env python3
"""
åˆæœŸãƒ‡ãƒ¼ã‚¿äº‹å‰å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤å‰ã«åˆæœŸãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦400ãƒ¬ã‚³ãƒ¼ãƒ‰ã®OHLCVãƒ‡ãƒ¼ã‚¿ã¨97ç‰¹å¾´é‡ã‚’äº‹å‰è¨ˆç®—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜

ä½¿ç”¨æ–¹æ³•:
    python scripts/prepare_initial_data.py
    
å‡ºåŠ›:
    cache/initial_data.pkl - åˆæœŸãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    cache/initial_features.pkl - 97ç‰¹å¾´é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥
"""

import json
import logging
import os
import pickle
import sys
from datetime import datetime, timedelta
from pathlib import Path

import pandas as pd
import yaml

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from crypto_bot.data.fetcher import MarketDataFetcher
from crypto_bot.indicator.calculator import IndicatorCalculator

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
)
logger = logging.getLogger(__name__)


def load_config():
    """æœ¬ç•ªè¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    config_path = Path("config/production/production.yml")
    logger.info(f"ğŸ“‹ Loading configuration from {config_path}")
    with open(config_path, "r") as f:
        return yaml.safe_load(f)


def fetch_initial_data(config):
    """åˆæœŸãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦400ãƒ¬ã‚³ãƒ¼ãƒ‰ã®OHLCVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    logger.info("ğŸ”„ Fetching initial OHLCV data (400 records)...")
    
    dd = config.get("data", {})
    
    # MarketDataFetcherã‚’åˆæœŸåŒ–
    fetcher = MarketDataFetcher(
        exchange_id=dd.get("exchange", "bitbank"),
        symbol=dd.get("symbol", "BTC/JPY"),
        api_key=os.getenv("BITBANK_API_KEY"),
        api_secret=os.getenv("BITBANK_API_SECRET"),
        ccxt_options=dd.get("ccxt_options", {}),
    )
    
    # ç¾åœ¨æ™‚åˆ»ã‹ã‚‰72æ™‚é–“å‰ã‚’since_timeã¨ã—ã¦è¨­å®šï¼ˆBitbank APIåˆ¶é™å†…ï¼‰
    current_time = pd.Timestamp.now(tz="UTC")
    since_time = current_time - pd.Timedelta(hours=72)  # 72æ™‚é–“ã«çŸ­ç¸®
    
    logger.info(f"ğŸ“Š Fetching data from {since_time} to {current_time}")
    
    try:
        # 300ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ç›®æ¨™ã«å–å¾—ï¼ˆ72æ™‚é–“åˆ†ï¼‰
        price_df = fetcher.get_price_df(
            timeframe="1h",
            since=since_time,
            limit=300,
            paginate=True,
            per_page=200,
            max_consecutive_empty=12,
            max_consecutive_no_new=20,
            max_attempts=25,
        )
        
        if price_df.empty:
            logger.error("âŒ Failed to fetch any data")
            return None
            
        logger.info(f"âœ… Successfully fetched {len(price_df)} records")
        logger.info(f"ğŸ“ˆ Data range: {price_df.index.min()} to {price_df.index.max()}")
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        if len(price_df) < 200:
            logger.warning(f"âš ï¸ Only {len(price_df)} records fetched (target was 400)")
        
        return price_df
        
    except Exception as e:
        logger.error(f"âŒ Error fetching data: {e}")
        import traceback
        traceback.print_exc()
        return None


def compute_97_features(data, config):
    """97ç‰¹å¾´é‡ã‚’äº‹å‰è¨ˆç®—"""
    logger.info("ğŸ”§ Computing 97 features...")
    
    try:
        # FeatureMasterImplementationã‚’è©¦ã¿ã‚‹
        from crypto_bot.ml.feature_master_implementation import FeatureMasterImplementation
        
        feature_impl = FeatureMasterImplementation()
        
        # DataFrameã®æº–å‚™
        if not isinstance(data, pd.DataFrame):
            data = pd.DataFrame(data)
        
        # ç‰¹å¾´é‡è¨ˆç®—
        features = feature_impl.generate_all_features(data)
        
        logger.info(f"âœ… Computed {features.shape[1]} features for {features.shape[0]} records")
        logger.info(f"ğŸ“Š Feature columns: {list(features.columns)[:10]}... (showing first 10)")
        
        return features
        
    except ImportError:
        logger.warning("âš ï¸ FeatureMasterImplementation not available, using basic features")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªç‰¹å¾´é‡ã®ã¿è¨ˆç®—
        calculator = IndicatorCalculator()
        
        # åŸºæœ¬çš„ãªãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’è¨ˆç®—
        features = data.copy()
        
        # RSI
        features["rsi_14"] = calculator.rsi(data["close"], period=14)
        
        # MACD
        macd_result = calculator.macd(data["close"])
        features["macd"] = macd_result["macd"]
        features["macd_signal"] = macd_result["signal"]
        features["macd_hist"] = macd_result["histogram"]
        
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        bb_result = calculator.bollinger_bands(data["close"])
        features["bb_upper"] = bb_result["upper"]
        features["bb_middle"] = bb_result["middle"]
        features["bb_lower"] = bb_result["lower"]
        
        # ATR
        features["atr_14"] = calculator.atr(
            data["high"], data["low"], data["close"], period=14
        )
        
        # ãƒœãƒªãƒ¥ãƒ¼ãƒ é–¢é€£
        features["volume_sma_20"] = data["volume"].rolling(window=20).mean()
        
        logger.info(f"âœ… Computed basic features: {features.shape}")
        return features
        
    except Exception as e:
        logger.error(f"âŒ Feature computation failed: {e}")
        return data


def save_cache(data, features, config):
    """ãƒ‡ãƒ¼ã‚¿ã¨ç‰¹å¾´é‡ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    cache_dir = Path("cache")
    cache_dir.mkdir(exist_ok=True)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    metadata = {
        "created_at": datetime.now().isoformat(),
        "data_shape": data.shape if data is not None else None,
        "features_shape": features.shape if features is not None else None,
        "symbol": config.get("data", {}).get("symbol", "BTC/JPY"),
        "timeframe": "1h",
        "records": len(data) if data is not None else 0,
    }
    
    # OHLCVãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    if data is not None:
        data_cache_path = cache_dir / "initial_data.pkl"
        with open(data_cache_path, "wb") as f:
            pickle.dump({"data": data, "metadata": metadata}, f)
        logger.info(f"ğŸ’¾ Saved OHLCV data to {data_cache_path}")
    
    # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    if features is not None:
        features_cache_path = cache_dir / "initial_features.pkl"
        with open(features_cache_path, "wb") as f:
            pickle.dump({"features": features, "metadata": metadata}, f)
        logger.info(f"ğŸ’¾ Saved features to {features_cache_path}")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’åˆ¥é€”JSONå½¢å¼ã§ã‚‚ä¿å­˜ï¼ˆäººé–“ãŒèª­ã‚ã‚‹å½¢å¼ï¼‰
    metadata_path = cache_dir / "initial_data_metadata.json"
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=2, default=str)
    logger.info(f"ğŸ“ Saved metadata to {metadata_path}")
    
    return metadata


def verify_cache():
    """ä¿å­˜ã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œè¨¼"""
    cache_dir = Path("cache")
    
    # OHLCVãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
    data_cache_path = cache_dir / "initial_data.pkl"
    if data_cache_path.exists():
        with open(data_cache_path, "rb") as f:
            cache_data = pickle.load(f)
            data = cache_data["data"]
            logger.info(f"âœ… OHLCV cache verified: {len(data)} records")
    else:
        logger.warning("âš ï¸ OHLCV cache not found")
    
    # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
    features_cache_path = cache_dir / "initial_features.pkl"
    if features_cache_path.exists():
        with open(features_cache_path, "rb") as f:
            cache_data = pickle.load(f)
            features = cache_data["features"]
            logger.info(f"âœ… Features cache verified: shape={features.shape}")
    else:
        logger.warning("âš ï¸ Features cache not found")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
    metadata_path = cache_dir / "initial_data_metadata.json"
    if metadata_path.exists():
        with open(metadata_path, "r") as f:
            metadata = json.load(f)
            logger.info(f"ğŸ“Š Cache metadata:")
            for key, value in metadata.items():
                logger.info(f"  - {key}: {value}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger.info("=" * 60)
    logger.info("ğŸš€ Initial Data Preparation Script")
    logger.info("=" * 60)
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config()
    
    # åˆæœŸãƒ‡ãƒ¼ã‚¿å–å¾—
    data = fetch_initial_data(config)
    if data is None:
        logger.error("âŒ Failed to fetch initial data. Exiting.")
        sys.exit(1)
    
    # 97ç‰¹å¾´é‡è¨ˆç®—
    features = compute_97_features(data, config)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
    metadata = save_cache(data, features, config)
    
    # æ¤œè¨¼
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ” Verifying saved cache...")
    verify_cache()
    
    logger.info("\n" + "=" * 60)
    logger.info("âœ… Initial data preparation completed successfully!")
    logger.info("ğŸ“¦ Cache files are ready for deployment")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()