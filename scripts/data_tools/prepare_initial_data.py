#!/usr/bin/env python3
"""
åˆæœŸãƒ‡ãƒ¼ã‚¿äº‹å‰å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ168æ™‚é–“ç‰ˆï¼‰
æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤å‰ã«168æ™‚é–“åˆ†ã®OHLCVãƒ‡ãƒ¼ã‚¿ã¨97ç‰¹å¾´é‡ã‚’äº‹å‰è¨ˆç®—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
APIåˆ¶é™å›é¿ãƒ»ãƒ‡ãƒ¼ã‚¿å–å¾—å®‰å®šåŒ–ã®ãŸã‚ã€ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§äº‹å‰å–å¾—

ä½¿ç”¨æ–¹æ³•:
    export BITBANK_API_KEY="your-api-key"
    export BITBANK_API_SECRET="your-api-secret"
    python scripts/data_tools/prepare_initial_data.py

å‡ºåŠ›:
    cache/initial_data_168h.pkl - 168æ™‚é–“åˆæœŸãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    cache/initial_features_168h.pkl - 97ç‰¹å¾´é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    
æœ¬ç•ªç’°å¢ƒã§ã®åˆ©ç”¨:
    - Docker imageã«å«ã‚ã¦é…ç½®
    - èµ·å‹•æ™‚ã®é«˜é€Ÿãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    - APIåˆ¶é™å›é¿åŠ¹æœ
"""

import json
import logging
import os
import pickle
import sys
from datetime import datetime, timedelta
from pathlib import Path

import pandas as pd
import yaml

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from crypto_bot.data.fetcher import MarketDataFetcher
from crypto_bot.indicator.calculator import IndicatorCalculator

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
)
logger = logging.getLogger(__name__)


def load_config():
    """æœ¬ç•ªè¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    config_path = Path("config/production/production.yml")
    logger.info(f"ğŸ“‹ Loading configuration from {config_path}")
    with open(config_path, "r") as f:
        return yaml.safe_load(f)


def fetch_initial_data(config):
    """168æ™‚é–“åˆ†ã®OHLCVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    logger.info("ğŸ”„ Fetching 168-hour OHLCV data for production cache...")

    dd = config.get("data", {})

    # MarketDataFetcherã‚’åˆæœŸåŒ–
    fetcher = MarketDataFetcher(
        exchange_id=dd.get("exchange", "bitbank"),
        symbol=dd.get("symbol", "BTC/JPY"),
        api_key=os.getenv("BITBANK_API_KEY"),
        api_secret=os.getenv("BITBANK_API_SECRET"),
        ccxt_options=dd.get("ccxt_options", {}),
    )

    # ç¾åœ¨æ™‚åˆ»ã‹ã‚‰168æ™‚é–“å‰ã‚’since_timeã¨ã—ã¦è¨­å®šï¼ˆ1é€±é–“åˆ†ãƒ‡ãƒ¼ã‚¿ï¼‰
    current_time = pd.Timestamp.now(tz="UTC")
    since_time = current_time - pd.Timedelta(hours=168)  # 168æ™‚é–“ï¼ˆ1é€±é–“ï¼‰

    logger.info(f"ğŸ“Š Fetching 168-hour data from {since_time} to {current_time}")

    try:
        # 168ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ç›®æ¨™ã«å–å¾—ï¼ˆ168æ™‚é–“åˆ†ï¼‰
        price_df = fetcher.get_price_df(
            timeframe="1h",
            since=since_time,
            limit=200,  # ä½™è£•ã‚’æŒã£ã¦200ã«è¨­å®š
            paginate=True,
            per_page=200,
            max_consecutive_empty=12,
            max_consecutive_no_new=20,
            max_attempts=25,
        )

        if price_df.empty:
            logger.error("âŒ Failed to fetch any data")
            return None

        logger.info(f"âœ… Successfully fetched {len(price_df)} records")
        logger.info(f"ğŸ“ˆ Data range: {price_df.index.min()} to {price_df.index.max()}")

        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        if len(price_df) < 200:
            logger.warning(f"âš ï¸ Only {len(price_df)} records fetched (target was 400)")

        return price_df

    except Exception as e:
        logger.error(f"âŒ Error fetching data: {e}")
        import traceback

        traceback.print_exc()
        return None


def compute_97_features(data, config):
    """97ç‰¹å¾´é‡ã‚’äº‹å‰è¨ˆç®—"""
    logger.info("ğŸ”§ Computing 97 features...")

    try:
        # FeatureMasterImplementationã‚’è©¦ã¿ã‚‹
        from crypto_bot.ml.feature_master_implementation import (
            FeatureMasterImplementation,
        )

        feature_impl = FeatureMasterImplementation()

        # DataFrameã®æº–å‚™
        if not isinstance(data, pd.DataFrame):
            data = pd.DataFrame(data)

        # ç‰¹å¾´é‡è¨ˆç®—
        features = feature_impl.generate_all_features(data)

        logger.info(
            f"âœ… Computed {features.shape[1]} features for {features.shape[0]} records"
        )
        logger.info(
            f"ğŸ“Š Feature columns: {list(features.columns)[:10]}... (showing first 10)"
        )

        return features

    except ImportError:
        logger.warning(
            "âš ï¸ FeatureMasterImplementation not available, using basic features"
        )
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªç‰¹å¾´é‡ã®ã¿è¨ˆç®—
        calculator = IndicatorCalculator()

        # åŸºæœ¬çš„ãªãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’è¨ˆç®—
        features = data.copy()

        # RSI
        features["rsi_14"] = calculator.rsi(data["close"], period=14)

        # MACD
        macd_result = calculator.macd(data["close"])
        features["macd"] = macd_result["macd"]
        features["macd_signal"] = macd_result["signal"]
        features["macd_hist"] = macd_result["histogram"]

        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        bb_result = calculator.bollinger_bands(data["close"])
        features["bb_upper"] = bb_result["upper"]
        features["bb_middle"] = bb_result["middle"]
        features["bb_lower"] = bb_result["lower"]

        # ATR
        features["atr_14"] = calculator.atr(
            data["high"], data["low"], data["close"], period=14
        )

        # ãƒœãƒªãƒ¥ãƒ¼ãƒ é–¢é€£
        features["volume_sma_20"] = data["volume"].rolling(window=20).mean()

        logger.info(f"âœ… Computed basic features: {features.shape}")
        return features

    except Exception as e:
        logger.error(f"âŒ Feature computation failed: {e}")
        return data


def save_cache(data, features, config):
    """ãƒ‡ãƒ¼ã‚¿ã¨ç‰¹å¾´é‡ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    cache_dir = Path("cache")
    cache_dir.mkdir(exist_ok=True)

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    metadata = {
        "created_at": datetime.now().isoformat(),
        "data_shape": data.shape if data is not None else None,
        "features_shape": features.shape if features is not None else None,
        "symbol": config.get("data", {}).get("symbol", "BTC/JPY"),
        "timeframe": "1h",
        "records": len(data) if data is not None else 0,
    }

    # OHLCVãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    if data is not None:
        data_cache_path = cache_dir / "initial_data_168h.pkl"
        with open(data_cache_path, "wb") as f:
            pickle.dump({"data": data, "metadata": metadata}, f)
        logger.info(f"ğŸ’¾ Saved OHLCV data to {data_cache_path}")

    # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    if features is not None:
        features_cache_path = cache_dir / "initial_features_168h.pkl"
        with open(features_cache_path, "wb") as f:
            pickle.dump({"features": features, "metadata": metadata}, f)
        logger.info(f"ğŸ’¾ Saved features to {features_cache_path}")

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’åˆ¥é€”JSONå½¢å¼ã§ã‚‚ä¿å­˜ï¼ˆäººé–“ãŒèª­ã‚ã‚‹å½¢å¼ï¼‰
    metadata_path = cache_dir / "initial_data_metadata.json"
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=2, default=str)
    logger.info(f"ğŸ“ Saved metadata to {metadata_path}")

    return metadata


def verify_cache():
    """ä¿å­˜ã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œè¨¼"""
    cache_dir = Path("cache")

    # OHLCVãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
    data_cache_path = cache_dir / "initial_data_168h.pkl"
    if data_cache_path.exists():
        with open(data_cache_path, "rb") as f:
            cache_data = pickle.load(f)
            data = cache_data["data"]
            logger.info(f"âœ… OHLCV cache verified: {len(data)} records")
    else:
        logger.warning("âš ï¸ OHLCV cache not found")

    # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
    features_cache_path = cache_dir / "initial_features_168h.pkl"
    if features_cache_path.exists():
        with open(features_cache_path, "rb") as f:
            cache_data = pickle.load(f)
            features = cache_data["features"]
            logger.info(f"âœ… Features cache verified: shape={features.shape}")
    else:
        logger.warning("âš ï¸ Features cache not found")

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
    metadata_path = cache_dir / "initial_data_metadata.json"
    if metadata_path.exists():
        with open(metadata_path, "r") as f:
            metadata = json.load(f)
            logger.info(f"ğŸ“Š Cache metadata:")
            for key, value in metadata.items():
                logger.info(f"  - {key}: {value}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger.info("=" * 60)
    logger.info("ğŸš€ Initial Data Preparation Script")
    logger.info("=" * 60)

    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config()

    # åˆæœŸãƒ‡ãƒ¼ã‚¿å–å¾—
    data = fetch_initial_data(config)
    if data is None:
        logger.error("âŒ Failed to fetch initial data. Exiting.")
        sys.exit(1)

    # 97ç‰¹å¾´é‡è¨ˆç®—
    features = compute_97_features(data, config)

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
    metadata = save_cache(data, features, config)

    # æ¤œè¨¼
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ” Verifying saved cache...")
    verify_cache()

    logger.info("\n" + "=" * 60)
    logger.info("âœ… Initial data preparation completed successfully!")
    logger.info("ğŸ“¦ Cache files are ready for deployment")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
