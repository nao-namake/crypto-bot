#!/usr/bin/env python3
"""
Phase H.24.6: 155ç‰¹å¾´é‡å®Œå…¨å¯¾å¿œãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç›®çš„:
- æ—¢å­˜ã®154ç‰¹å¾´é‡ï¼ˆenhanced_defaultå«ã‚€ï¼‰ãƒ¢ãƒ‡ãƒ«ã‚’ç½®ãæ›ãˆ
- æ­£ã—ã„155ç‰¹å¾´é‡ã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’
- å„ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆ15m, 1h, 4hï¼‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜

å®Ÿè¡Œæ–¹æ³•:
python retrain_models.py --config config/production/production.yml
"""

import argparse
import json
import logging
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path

import pandas as pd
import yaml
from sklearn.model_selection import train_test_split

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent))

import ccxt  # noqa: E402
from crypto_bot.ml.ensemble import (  # noqa: E402
    TradingEnsembleClassifier,
    create_trading_ensemble,
)
from crypto_bot.ml.preprocessor import FeatureEngineer  # noqa: E402

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def load_config(config_path: str) -> dict:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)

    # ç’°å¢ƒå¤‰æ•°ã‚’é©ç”¨
    if "BITBANK_API_KEY" in os.environ:
        config["data"]["api_key"] = os.environ["BITBANK_API_KEY"]
    if "BITBANK_API_SECRET" in os.environ:
        config["data"]["api_secret"] = os.environ["BITBANK_API_SECRET"]

    return config


def prepare_training_data(config: dict, timeframe: str) -> tuple:
    """
    æŒ‡å®šã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™

    Returns:
        X, y: ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
    """
    logger.info(f"ğŸ”„ Preparing training data for {timeframe}...")

    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    exchange_config = config.get("data", {})
    exchange_id = exchange_config.get("exchange", "bitbank")
    symbol = exchange_config.get("symbol", "BTC/JPY")

    # ccxtã§ç›´æ¥ãƒ‡ãƒ¼ã‚¿å–å¾—
    try:
        exchange = getattr(ccxt, exchange_id)({
            "apiKey": exchange_config.get("api_key"),
            "secret": exchange_config.get("api_secret"),
            "enableRateLimit": True
        })

        # 30æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        since_ms = int((datetime.now() - timedelta(days=30)).timestamp() * 1000)
        ohlcv = exchange.fetch_ohlcv(symbol, timeframe, since=since_ms, limit=1000)

        # DataFrameã«å¤‰æ›
        df = pd.DataFrame(
            ohlcv,
            columns=["timestamp", "open", "high", "low", "close", "volume"]
        )
        df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
        df.set_index("timestamp", inplace=True)

        if df is None or df.empty:
            logger.error(f"No data fetched for {timeframe}")
            return None, None

        logger.info(f"âœ… Fetched {len(df)} records for {timeframe}")

        # ç‰¹å¾´é‡ç”Ÿæˆ
        feature_engineer = FeatureEngineer(config)
        features_df = feature_engineer.create_features(df)

        if features_df is None or features_df.empty:
            logger.error(f"No features generated for {timeframe}")
            return None, None

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆ5æœŸé–“å…ˆã®ä¾¡æ ¼å¤‰åŒ–ï¼‰
        df["target"] = (df["close"].shift(-5) > df["close"]).astype(int)

        # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã¿æŠ½å‡º
        valid_idx = ~(features_df.isna().any(axis=1) | df["target"].isna())
        X = features_df[valid_idx]
        y = df["target"][valid_idx]

        logger.info(
            f"âœ… Prepared {len(X)} samples with {X.shape[1]} features for {timeframe}"
        )

        # 155ç‰¹å¾´é‡ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        if X.shape[1] != 155:
            logger.warning(f"âš ï¸ Expected 155 features, got {X.shape[1]}")

        return X, y

    except Exception as e:
        logger.error(f"âŒ Error preparing data for {timeframe}: {e}")
        return None, None


def train_timeframe_model(
    config: dict, timeframe: str, X: pd.DataFrame, y: pd.Series
) -> TradingEnsembleClassifier:
    """
    å˜ä¸€ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
    """
    logger.info(f"ğŸ¯ Training ensemble model for {timeframe}...")

    # å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    ensemble_model = create_trading_ensemble(config)

    # å­¦ç¿’
    ensemble_model.fit(X_train, y_train)

    # æ¤œè¨¼ã‚¹ã‚³ã‚¢
    val_score = ensemble_model.score(X_val, y_val)
    logger.info(f"âœ… {timeframe} model trained - Validation accuracy: {val_score:.4f}")

    return ensemble_model


def save_models(models: dict, config: dict):
    """
    å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    """
    model_dir = Path("models/production/timeframe_models")
    model_dir.mkdir(parents=True, exist_ok=True)

    for timeframe, model in models.items():
        model_path = model_dir / f"{timeframe}_ensemble_model.pkl"

        import joblib

        joblib.dump(model, model_path)
        logger.info(f"ğŸ’¾ Saved {timeframe} model to {model_path}")

    # ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    metadata = {
        "trained_at": datetime.now().isoformat(),
        "num_features": 155,
        "timeframes": list(models.keys()),
        "feature_order_file": "feature_order.json",
        "config_used": config.get("strategy", {}).get("params", {}),
    }

    metadata_path = model_dir / "model_metadata.json"
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=2)

    logger.info(f"ğŸ“ Saved model metadata to {metadata_path}")


def update_feature_order():
    """
    feature_order.jsonãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
    """
    feature_order_path = Path("feature_order.json")

    if feature_order_path.exists():
        with open(feature_order_path, "r") as f:
            data = json.load(f)

        if data.get("num_features") == 155:
            logger.info(
                "âœ… feature_order.json is correctly configured with 155 features"
            )
        else:
            logger.warning("âš ï¸ feature_order.json has incorrect feature count")
    else:
        logger.warning("âš ï¸ feature_order.json not found")


def main():
    parser = argparse.ArgumentParser(description="Retrain models with 155 features")
    parser.add_argument("--config", required=True, help="Path to configuration file")
    parser.add_argument(
        "--timeframes",
        nargs="+",
        default=["15m", "1h", "4h"],
        help="Timeframes to train models for",
    )
    args = parser.parse_args()

    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config(args.config)

    logger.info("ğŸš€ Starting model retraining with 155 features...")
    logger.info(f"   Config: {args.config}")
    logger.info(f"   Timeframes: {args.timeframes}")

    # feature_order.jsonç¢ºèª
    update_feature_order()

    # å„ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã§ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    trained_models = {}

    for timeframe in args.timeframes:
        logger.info(f"\n{'='*60}")
        logger.info(f"Processing {timeframe}")
        logger.info(f"{'='*60}")

        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        X, y = prepare_training_data(config, timeframe)

        if X is None or y is None:
            logger.error(f"âŒ Skipping {timeframe} due to data preparation failure")
            continue

        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        try:
            model = train_timeframe_model(config, timeframe, X, y)
            trained_models[timeframe] = model
        except Exception as e:
            logger.error(f"âŒ Failed to train {timeframe} model: {e}")
            continue

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    if trained_models:
        logger.info(f"\nğŸ‰ Successfully trained {len(trained_models)} models")
        save_models(trained_models, config)
        logger.info("\nâœ… Model retraining completed successfully!")
    else:
        logger.error("\nâŒ No models were successfully trained")
        sys.exit(1)


if __name__ == "__main__":
    main()
