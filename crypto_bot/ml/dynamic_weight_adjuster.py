# =============================================================================
# ãƒ•ã‚¡ã‚¤ãƒ«å: crypto_bot/ml/dynamic_weight_adjuster.py
# èª¬æ˜:
# Phase C2: å‹•çš„é‡ã¿èª¿æ•´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½è©•ä¾¡ãƒ»äºˆæ¸¬ç²¾åº¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒ»å¼·åŒ–å­¦ç¿’
# CrossTimeframeIntegratorã®é«˜åº¦ãªé‡ã¿èª¿æ•´ã‚·ã‚¹ãƒ†ãƒ 
# =============================================================================

import logging
from collections import defaultdict, deque
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler

from crypto_bot.analysis.market_environment_analyzer import MarketEnvironmentAnalyzer

logger = logging.getLogger(__name__)


class DynamicWeightAdjuster:
    """
    Phase C2: å‹•çš„é‡ã¿èª¿æ•´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

    æ©Ÿèƒ½:
    - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ç²¾åº¦è©•ä¾¡ãƒ»è¿½è·¡
    - æ€§èƒ½ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«ã‚ˆã‚‹é‡ã¿å‹•çš„èª¿æ•´
    - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒ»é©å¿œçš„é‡ã¿æœ€é©åŒ–
    - å¸‚å ´ç’°å¢ƒé€£å‹•é‡ã¿èª¿æ•´
    - å¤šç›®çš„æœ€é©åŒ–ï¼ˆç²¾åº¦ãƒ»åç›Šæ€§ãƒ»ãƒªã‚¹ã‚¯ï¼‰
    - å¼·åŒ–å­¦ç¿’ãƒ™ãƒ¼ã‚¹é‡ã¿èª¿æ•´ï¼ˆQ-learningé¢¨ï¼‰
    """

    def __init__(self, config: Dict[str, Any]):
        """
        å‹•çš„é‡ã¿èª¿æ•´ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–

        Parameters:
        -----------
        config : Dict[str, Any]
            å‹•çš„é‡ã¿èª¿æ•´è¨­å®š
        """
        self.config = config

        # åŸºæœ¬è¨­å®š
        adjustment_config = config.get("dynamic_weight_adjustment", {})
        self.timeframes = adjustment_config.get("timeframes", ["15m", "1h", "4h"])
        self.base_weights = adjustment_config.get("base_weights", [0.3, 0.5, 0.2])
        self.learning_rate = adjustment_config.get("learning_rate", 0.01)
        self.adaptation_speed = adjustment_config.get("adaptation_speed", 0.1)
        self.memory_window = adjustment_config.get("memory_window", 100)

        # æ€§èƒ½è©•ä¾¡è¨­å®š
        performance_config = adjustment_config.get("performance_tracking", {})
        self.accuracy_weight = performance_config.get("accuracy_weight", 0.4)
        self.profitability_weight = performance_config.get("profitability_weight", 0.4)
        self.risk_weight = performance_config.get("risk_weight", 0.2)
        self.min_samples_for_adjustment = performance_config.get("min_samples", 20)

        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’è¨­å®š
        online_config = adjustment_config.get("online_learning", {})
        self.enable_online_learning = online_config.get("enabled", True)
        self.online_learning_method = online_config.get("method", "sgd_regressor")
        self.feature_window = online_config.get("feature_window", 50)
        self.retraining_frequency = online_config.get("retraining_frequency", 20)

        # å¼·åŒ–å­¦ç¿’è¨­å®š
        rl_config = adjustment_config.get("reinforcement_learning", {})
        self.enable_reinforcement_learning = rl_config.get("enabled", True)
        self.epsilon = rl_config.get("epsilon", 0.1)  # æ¢ç´¢ç‡
        self.epsilon_decay = rl_config.get("epsilon_decay", 0.995)
        self.discount_factor = rl_config.get("discount_factor", 0.95)

        # å¸‚å ´ç’°å¢ƒè§£æã‚·ã‚¹ãƒ†ãƒ 
        self.market_analyzer: Optional[MarketEnvironmentAnalyzer] = None
        if "market_analysis" in config:
            self.market_analyzer = MarketEnvironmentAnalyzer(config)

        # æ€§èƒ½å±¥æ­´ç®¡ç†
        self.performance_history = defaultdict(deque)  # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥æ€§èƒ½å±¥æ­´
        self.prediction_outcomes = deque(maxlen=self.memory_window)  # äºˆæ¸¬çµæœå±¥æ­´
        self.weight_history = deque(maxlen=200)  # é‡ã¿å±¥æ­´
        self.market_context_history = deque(maxlen=100)  # å¸‚å ´ç’°å¢ƒå±¥æ­´

        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒ¢ãƒ‡ãƒ«
        self.online_models = {}
        self.scalers = {}
        self.model_update_count = 0

        if self.enable_online_learning:
            self._initialize_online_models()

        # å¼·åŒ–å­¦ç¿’ã®Q-tableï¼ˆç°¡æ˜“ç‰ˆï¼‰
        self.q_table = defaultdict(lambda: defaultdict(float))
        self.action_space = self._generate_weight_actions()
        self.state_space_size = 0

        # ç¾åœ¨ã®å‹•çš„é‡ã¿
        self.current_weights = dict(zip(self.timeframes, self.base_weights))
        self.weight_confidence = {tf: 0.5 for tf in self.timeframes}

        # çµ±è¨ˆè¿½è·¡
        self.adjustment_stats = {
            "total_adjustments": 0,
            "online_learning_updates": 0,
            "reinforcement_learning_updates": 0,
            "market_adaptation_adjustments": 0,
            "performance_feedback_adjustments": 0,
            "weight_change_magnitude": 0.0,
            "improvement_rate": 0.0,
            "successful_adjustments": 0,
        }

        logger.info("ğŸ›ï¸ DynamicWeightAdjuster initialized")
        logger.info(f"   Timeframes: {self.timeframes}")
        logger.info(f"   Learning rate: {self.learning_rate}")
        logger.info(f"   Online learning: {self.enable_online_learning}")
        logger.info(f"   Reinforcement learning: {self.enable_reinforcement_learning}")

    def _initialize_online_models(self):
        """ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–"""
        try:
            for timeframe in self.timeframes:
                if self.online_learning_method == "sgd_regressor":
                    # ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•å›å¸°å™¨
                    model = SGDRegressor(
                        learning_rate="adaptive",
                        eta0=self.learning_rate,
                        alpha=0.01,
                        random_state=42,
                    )
                    self.online_models[timeframe] = model
                    self.scalers[timeframe] = StandardScaler()

            logger.info("âœ… Online learning models initialized")

        except Exception as e:
            logger.error(f"Failed to initialize online models: {e}")
            self.enable_online_learning = False

    def _generate_weight_actions(self) -> List[Dict[str, float]]:
        """é‡ã¿èª¿æ•´ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ç”Ÿæˆ"""
        actions = []

        # åŸºæœ¬ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: å„ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®é‡ã¿Â±10%èª¿æ•´
        for i, timeframe in enumerate(self.timeframes):
            # é‡ã¿å¢—åŠ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            action_increase = {tf: 1.0 for tf in self.timeframes}
            action_increase[timeframe] = 1.1
            actions.append(action_increase)

            # é‡ã¿æ¸›å°‘ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            action_decrease = {tf: 1.0 for tf in self.timeframes}
            action_decrease[timeframe] = 0.9
            actions.append(action_decrease)

        # çµ„ã¿åˆã‚ã›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        actions.extend(
            [
                {"15m": 1.2, "1h": 0.9, "4h": 0.9},  # çŸ­æœŸé‡è¦–
                {"15m": 0.9, "1h": 1.2, "4h": 0.9},  # ä¸­æœŸé‡è¦–
                {"15m": 0.9, "1h": 0.9, "4h": 1.2},  # é•·æœŸé‡è¦–
                {"15m": 1.0, "1h": 1.0, "4h": 1.0},  # ç¶­æŒ
            ]
        )

        return actions

    def adjust_weights_dynamic(
        self,
        timeframe_predictions: Dict[str, Dict[str, Any]],
        market_context: Optional[Dict[str, Any]] = None,
        recent_performance: Optional[Dict[str, float]] = None,
    ) -> Tuple[Dict[str, float], Dict[str, Any]]:
        """
        å‹•çš„é‡ã¿èª¿æ•´ï¼ˆPhase C2ã‚³ã‚¢æ©Ÿèƒ½ï¼‰

        Parameters:
        -----------
        timeframe_predictions : Dict[str, Dict[str, Any]]
            å„ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®äºˆæ¸¬çµæœ
        market_context : Dict[str, Any], optional
            å¸‚å ´ç’°å¢ƒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        recent_performance : Dict[str, float], optional
            æœ€è¿‘ã®æ€§èƒ½æŒ‡æ¨™

        Returns:
        --------
        Tuple[Dict[str, float], Dict[str, Any]]
            (èª¿æ•´æ¸ˆã¿é‡ã¿è¾æ›¸, èª¿æ•´æƒ…å ±)
        """
        self.adjustment_stats["total_adjustments"] += 1

        try:
            adjustment_info = {
                "adjustment_method": "multi_objective_dynamic",
                "timestamp": datetime.now(),
                "original_weights": self.current_weights.copy(),
            }

            # 1. åŸºæœ¬é‡ã¿ã‚³ãƒ”ãƒ¼
            adjusted_weights = self.current_weights.copy()

            # 2. æ€§èƒ½ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯èª¿æ•´
            if recent_performance:
                performance_adjustment = self._calculate_performance_adjustment(
                    recent_performance, timeframe_predictions
                )
                adjusted_weights = self._apply_weight_adjustment(
                    adjusted_weights, performance_adjustment, 0.3
                )
                adjustment_info["performance_adjustment"] = performance_adjustment
                self.adjustment_stats["performance_feedback_adjustments"] += 1

            # 3. å¸‚å ´ç’°å¢ƒé©å¿œèª¿æ•´
            if market_context and self.market_analyzer:
                market_adjustment = self._calculate_market_adaptation_adjustment(
                    market_context, timeframe_predictions
                )
                adjusted_weights = self._apply_weight_adjustment(
                    adjusted_weights, market_adjustment, 0.4
                )
                adjustment_info["market_adjustment"] = market_adjustment
                self.adjustment_stats["market_adaptation_adjustments"] += 1

            # 4. ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’èª¿æ•´
            if self.enable_online_learning:
                online_adjustment = self._calculate_online_learning_adjustment(
                    timeframe_predictions, market_context, recent_performance
                )
                if online_adjustment:
                    adjusted_weights = self._apply_weight_adjustment(
                        adjusted_weights, online_adjustment, 0.2
                    )
                    adjustment_info["online_learning_adjustment"] = online_adjustment
                    self.adjustment_stats["online_learning_updates"] += 1

            # 5. å¼·åŒ–å­¦ç¿’èª¿æ•´
            if self.enable_reinforcement_learning:
                rl_adjustment = self._calculate_reinforcement_learning_adjustment(
                    timeframe_predictions, market_context, recent_performance
                )
                if rl_adjustment:
                    adjusted_weights = self._apply_weight_adjustment(
                        adjusted_weights, rl_adjustment, 0.1
                    )
                    adjustment_info["reinforcement_learning_adjustment"] = rl_adjustment
                    self.adjustment_stats["reinforcement_learning_updates"] += 1

            # 6. é‡ã¿æ­£è¦åŒ–ãƒ»åˆ¶ç´„é©ç”¨
            final_weights = self._normalize_and_constrain_weights(adjusted_weights)

            # 7. èª¿æ•´åŠ¹æœè©•ä¾¡
            weight_change_magnitude = self._calculate_weight_change_magnitude(
                self.current_weights, final_weights
            )
            adjustment_info["weight_change_magnitude"] = weight_change_magnitude
            adjustment_info["final_weights"] = final_weights
            self.adjustment_stats["weight_change_magnitude"] = weight_change_magnitude

            # 8. é‡ã¿æ›´æ–°ãƒ»å±¥æ­´è¨˜éŒ²
            self._update_weights(final_weights, adjustment_info)

            logger.debug(
                f"ğŸ›ï¸ Dynamic weight adjustment: magnitude={weight_change_magnitude:.3f}, "
                f"methods={len([k for k in adjustment_info.keys() if 'adjustment' in k])}"
            )

            return final_weights, adjustment_info

        except Exception as e:
            logger.error(f"âŒ Dynamic weight adjustment failed: {e}")
            return self.current_weights.copy(), {"error": True}

    def _calculate_performance_adjustment(
        self,
        recent_performance: Dict[str, float],
        timeframe_predictions: Dict[str, Dict[str, Any]],
    ) -> Dict[str, float]:
        """æ€§èƒ½ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ™ãƒ¼ã‚¹èª¿æ•´è¨ˆç®—"""
        try:
            adjustments = {}

            for timeframe in self.timeframes:
                if timeframe not in timeframe_predictions:
                    adjustments[timeframe] = 1.0
                    continue

                # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥æ€§èƒ½å±¥æ­´å–å¾—
                tf_performance_history = self.performance_history[timeframe]

                if len(tf_performance_history) < self.min_samples_for_adjustment:
                    adjustments[timeframe] = 1.0
                    continue

                # æœ€è¿‘ã®æ€§èƒ½ã‚¹ã‚³ã‚¢è¨ˆç®—
                recent_scores = list(tf_performance_history)[-10:]  # æœ€è¿‘10ä»¶
                avg_recent_score = np.mean(recent_scores)

                # å…¨ä½“å¹³å‡ã¨ã®æ¯”è¼ƒ
                overall_avg = np.mean(list(tf_performance_history))

                # æ€§èƒ½å‘ä¸Šç‡è¨ˆç®—
                if overall_avg > 0:
                    performance_ratio = avg_recent_score / overall_avg
                else:
                    performance_ratio = 1.0

                # èª¿æ•´ä¿‚æ•°è¨ˆç®—
                if performance_ratio > 1.1:  # 10%ä»¥ä¸Šã®æ”¹å–„
                    adjustments[timeframe] = min(1.2, 1.0 + (performance_ratio - 1.0))
                elif performance_ratio < 0.9:  # 10%ä»¥ä¸Šã®æ‚ªåŒ–
                    adjustments[timeframe] = max(0.8, performance_ratio)
                else:
                    adjustments[timeframe] = 1.0

            return adjustments

        except Exception as e:
            logger.error(f"Performance adjustment calculation failed: {e}")
            return {tf: 1.0 for tf in self.timeframes}

    def _calculate_market_adaptation_adjustment(
        self,
        market_context: Dict[str, Any],
        timeframe_predictions: Dict[str, Dict[str, Any]],
    ) -> Dict[str, float]:
        """å¸‚å ´ç’°å¢ƒé©å¿œèª¿æ•´è¨ˆç®—"""
        try:
            if not self.market_analyzer:
                return {tf: 1.0 for tf in self.timeframes}

            # å¸‚å ´ç’°å¢ƒè§£æå®Ÿè¡Œï¼ˆä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã ãŒã€ã“ã“ã§ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¨å®šï¼‰
            market_regime = market_context.get("market_regime", "normal")
            volatility_score = market_context.get("volatility", 0.02)

            # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥é‡ã¿èª¿æ•´æˆ¦ç•¥
            if market_regime in ["crisis", "extreme_volatile"]:
                # å±æ©Ÿæ™‚ï¼šé•·æœŸå®‰å®šæ€§é‡è¦–
                adjustments = {
                    "15m": 0.7,
                    "1h": 1.0,
                    "4h": 1.3,
                }
            elif market_regime in ["volatile", "high_volatile"]:
                # é«˜ãƒœãƒ©æ™‚ï¼šä¸­æœŸãƒãƒ©ãƒ³ã‚¹é‡è¦–
                adjustments = {
                    "15m": 0.8,
                    "1h": 1.2,
                    "4h": 1.0,
                }
            elif market_regime in ["calm", "extreme_calm"]:
                # å®‰å®šæ™‚ï¼šçŸ­æœŸæ©Ÿæ•æ€§é‡è¦–
                adjustments = {
                    "15m": 1.2,
                    "1h": 1.0,
                    "4h": 0.9,
                }
            elif "bullish" in market_regime or "bearish" in market_regime:
                # ãƒˆãƒ¬ãƒ³ãƒ‰æ™‚ï¼šä¸­é•·æœŸé‡è¦–
                adjustments = {
                    "15m": 0.9,
                    "1h": 1.1,
                    "4h": 1.1,
                }
            else:
                # é€šå¸¸æ™‚ï¼šãƒãƒ©ãƒ³ã‚¹ç¶­æŒ
                adjustments = {
                    "15m": 1.0,
                    "1h": 1.0,
                    "4h": 1.0,
                }

            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã«ã‚ˆã‚‹å¾®èª¿æ•´
            if volatility_score > 0.05:  # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                for tf in adjustments:
                    if tf == "4h":  # é•·æœŸã‚’ã‚ˆã‚Šé‡è¦–
                        adjustments[tf] *= 1.1
                    elif tf == "15m":  # çŸ­æœŸã‚’æŠ‘åˆ¶
                        adjustments[tf] *= 0.95

            return adjustments

        except Exception as e:
            logger.error(f"Market adaptation adjustment failed: {e}")
            return {tf: 1.0 for tf in self.timeframes}

    def _calculate_online_learning_adjustment(
        self,
        timeframe_predictions: Dict[str, Dict[str, Any]],
        market_context: Optional[Dict[str, Any]],
        recent_performance: Optional[Dict[str, float]],
    ) -> Optional[Dict[str, float]]:
        """ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒ™ãƒ¼ã‚¹èª¿æ•´è¨ˆç®—"""
        try:
            if not self.enable_online_learning or not self.online_models:
                return None

            # ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒè“„ç©ã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
            if len(self.prediction_outcomes) < self.min_samples_for_adjustment:
                return None

            # å®šæœŸçš„ãªå†å­¦ç¿’ãƒã‚§ãƒƒã‚¯
            if self.model_update_count % self.retraining_frequency != 0:
                self.model_update_count += 1
                return None

            adjustments = {}

            for timeframe in self.timeframes:
                if timeframe not in self.online_models:
                    adjustments[timeframe] = 1.0
                    continue

                # ç‰¹å¾´é‡æº–å‚™
                features = self._prepare_online_features(
                    timeframe, timeframe_predictions, market_context
                )

                if features is None or len(features) == 0:
                    adjustments[timeframe] = 1.0
                    continue

                # ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬
                model = self.online_models[timeframe]
                scaler = self.scalers[timeframe]

                try:
                    # ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                    if hasattr(scaler, "mean_"):  # æ—¢ã«å­¦ç¿’æ¸ˆã¿
                        features_scaled = scaler.transform([features])
                    else:
                        # åˆå›å­¦ç¿’
                        features_scaled = scaler.fit_transform([features])

                    # é‡ã¿èª¿æ•´äºˆæ¸¬
                    if hasattr(model, "predict"):
                        predicted_adjustment = model.predict(features_scaled)[0]
                        # èª¿æ•´ä¿‚æ•°ã‚’å®‰å…¨ãªç¯„å›²ã«åˆ¶é™
                        adjustment = max(0.8, min(1.2, predicted_adjustment))
                        adjustments[timeframe] = adjustment
                    else:
                        adjustments[timeframe] = 1.0

                except Exception as model_error:
                    logger.warning(
                        f"Online model prediction failed for {timeframe}: {model_error}"
                    )
                    adjustments[timeframe] = 1.0

            self.model_update_count += 1

            # æœ‰åŠ¹ãªèª¿æ•´ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿è¿”ã™
            if any(adj != 1.0 for adj in adjustments.values()):
                return adjustments
            else:
                return None

        except Exception as e:
            logger.error(f"Online learning adjustment failed: {e}")
            return None

    def _calculate_reinforcement_learning_adjustment(
        self,
        timeframe_predictions: Dict[str, Dict[str, Any]],
        market_context: Optional[Dict[str, Any]],
        recent_performance: Optional[Dict[str, float]],
    ) -> Optional[Dict[str, float]]:
        """å¼·åŒ–å­¦ç¿’ãƒ™ãƒ¼ã‚¹èª¿æ•´è¨ˆç®—ï¼ˆç°¡æ˜“Q-learningï¼‰"""
        try:
            if not self.enable_reinforcement_learning:
                return None

            # ååˆ†ãªå±¥æ­´ãŒå¿…è¦
            if len(self.weight_history) < 10:
                return None

            # ç¾åœ¨ã®çŠ¶æ…‹å®šç¾©
            current_state = self._encode_state(market_context, recent_performance)

            # Îµ-greedyè¡Œå‹•é¸æŠ
            if np.random.random() < self.epsilon:
                # æ¢ç´¢ï¼šãƒ©ãƒ³ãƒ€ãƒ è¡Œå‹•
                action_idx = np.random.randint(len(self.action_space))
            else:
                # æ´»ç”¨ï¼šæœ€è‰¯è¡Œå‹•
                q_values = [
                    self.q_table[current_state][i]
                    for i in range(len(self.action_space))
                ]
                action_idx = np.argmax(q_values)

            # é¸æŠã•ã‚ŒãŸè¡Œå‹•ï¼ˆé‡ã¿èª¿æ•´ï¼‰
            selected_action = self.action_space[action_idx]

            # Îµæ¸›è¡°
            self.epsilon = max(0.01, self.epsilon * self.epsilon_decay)

            # Qå€¤æ›´æ–°ï¼ˆå‰å›ã®è¡Œå‹•ã«å¯¾ã™ã‚‹å ±é…¬ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            if (
                recent_performance
                and len(self.weight_history) > 0
                and hasattr(self, "_last_rl_state")
                and hasattr(self, "_last_rl_action")
            ):

                reward = self._calculate_reward(recent_performance)
                last_state = self._last_rl_state
                last_action_idx = self._last_rl_action

                # Q(s,a) = Q(s,a) + Î±[r + Î³max(Q(s',a')) - Q(s,a)]
                current_q = self.q_table[last_state][last_action_idx]
                max_future_q = max(
                    [
                        self.q_table[current_state][i]
                        for i in range(len(self.action_space))
                    ]
                )

                new_q = current_q + self.learning_rate * (
                    reward + self.discount_factor * max_future_q - current_q
                )
                self.q_table[last_state][last_action_idx] = new_q

            # ç¾åœ¨ã®çŠ¶æ…‹ãƒ»è¡Œå‹•ã‚’è¨˜éŒ²
            self._last_rl_state = current_state
            self._last_rl_action = action_idx

            return selected_action

        except Exception as e:
            logger.error(f"Reinforcement learning adjustment failed: {e}")
            return None

    def _prepare_online_features(
        self,
        timeframe: str,
        timeframe_predictions: Dict[str, Dict[str, Any]],
        market_context: Optional[Dict[str, Any]],
    ) -> Optional[List[float]]:
        """ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ç”¨ç‰¹å¾´é‡æº–å‚™"""
        try:
            features = []

            # 1. ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥äºˆæ¸¬ç‰¹å¾´é‡
            if timeframe in timeframe_predictions:
                pred_data = timeframe_predictions[timeframe]
                features.extend(
                    [
                        pred_data.get("confidence", 0.5),
                        pred_data.get("unified_confidence", 0.5),
                        pred_data.get("model_agreement", 1.0),
                    ]
                )

                # äºˆæ¸¬ç¢ºç‡ç‰¹å¾´é‡
                probability = pred_data.get("probability", np.array([[0.5, 0.5]]))
                if isinstance(probability, np.ndarray) and probability.shape[1] >= 2:
                    features.extend(
                        [
                            probability[0, 0],  # ã‚¯ãƒ©ã‚¹0ç¢ºç‡
                            probability[0, 1],  # ã‚¯ãƒ©ã‚¹1ç¢ºç‡
                            abs(probability[0, 1] - 0.5),  # ä¸­ç«‹ã‹ã‚‰ã®è·é›¢
                        ]
                    )
                else:
                    features.extend([0.5, 0.5, 0.0])

            else:
                features.extend([0.5, 0.5, 1.0, 0.5, 0.5, 0.0])

            # 2. å¸‚å ´ç’°å¢ƒç‰¹å¾´é‡
            if market_context:
                features.extend(
                    [
                        market_context.get("vix_level", 20.0) / 100,  # æ­£è¦åŒ–
                        market_context.get("volatility", 0.02) * 50,  # æ­£è¦åŒ–
                        market_context.get("trend_strength", 0.5),
                        market_context.get("fear_greed", 50) / 100,  # æ­£è¦åŒ–
                    ]
                )
            else:
                features.extend([0.2, 1.0, 0.5, 0.5])

            # 3. å±¥æ­´ç‰¹å¾´é‡
            if len(self.performance_history[timeframe]) > 0:
                recent_performance = list(self.performance_history[timeframe])[-5:]
                features.extend(
                    [
                        np.mean(recent_performance),
                        (
                            np.std(recent_performance)
                            if len(recent_performance) > 1
                            else 0.0
                        ),
                        max(recent_performance) if recent_performance else 0.5,
                        min(recent_performance) if recent_performance else 0.5,
                    ]
                )
            else:
                features.extend([0.5, 0.0, 0.5, 0.5])

            # 4. é‡ã¿å±¥æ­´ç‰¹å¾´é‡
            if len(self.weight_history) > 0:
                recent_weights = [
                    w["final_weights"].get(timeframe, 0.33)
                    for w in list(self.weight_history)[-5:]
                ]
                features.extend(
                    [
                        np.mean(recent_weights),
                        np.std(recent_weights) if len(recent_weights) > 1 else 0.0,
                    ]
                )
            else:
                features.extend([0.33, 0.0])

            return features if len(features) > 0 else None

        except Exception as e:
            logger.error(f"Online feature preparation failed for {timeframe}: {e}")
            return None

    def _encode_state(
        self,
        market_context: Optional[Dict[str, Any]],
        recent_performance: Optional[Dict[str, float]],
    ) -> str:
        """å¼·åŒ–å­¦ç¿’ç”¨çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        try:
            state_components = []

            # å¸‚å ´ç’°å¢ƒçŠ¶æ…‹
            if market_context:
                regime = market_context.get("market_regime", "normal")
                vol_level = (
                    "high" if market_context.get("volatility", 0.02) > 0.04 else "low"
                )
                vix_level = (
                    "high" if market_context.get("vix_level", 20) > 25 else "low"
                )
                state_components.extend([regime, vol_level, vix_level])
            else:
                state_components.extend(["unknown", "unknown", "unknown"])

            # æ€§èƒ½çŠ¶æ…‹
            if recent_performance:
                avg_performance = np.mean(list(recent_performance.values()))
                perf_level = (
                    "high"
                    if avg_performance > 0.7
                    else ("low" if avg_performance < 0.4 else "medium")
                )
                state_components.append(perf_level)
            else:
                state_components.append("unknown")

            return "_".join(state_components)

        except Exception as e:
            logger.error(f"State encoding failed: {e}")
            return "default_state"

    def _calculate_reward(self, recent_performance: Dict[str, float]) -> float:
        """å¼·åŒ–å­¦ç¿’å ±é…¬è¨ˆç®—"""
        try:
            if not recent_performance:
                return 0.0

            # å¤šç›®çš„å ±é…¬è¨­è¨ˆ
            accuracy_reward = (
                recent_performance.get("accuracy", 0.5) - 0.5
            )  # -0.5 to 0.5
            profitability_reward = recent_performance.get("profitability", 0.5) - 0.5
            risk_reward = 0.5 - recent_performance.get(
                "risk", 0.5
            )  # ãƒªã‚¹ã‚¯ä½æ¸›ã§æ­£å ±é…¬

            # é‡ã¿ä»˜ãç·åˆå ±é…¬
            total_reward = (
                accuracy_reward * self.accuracy_weight
                + profitability_reward * self.profitability_weight
                + risk_reward * self.risk_weight
            )

            return total_reward

        except Exception as e:
            logger.error(f"Reward calculation failed: {e}")
            return 0.0

    def _apply_weight_adjustment(
        self,
        current_weights: Dict[str, float],
        adjustment_factors: Dict[str, float],
        influence_strength: float,
    ) -> Dict[str, float]:
        """é‡ã¿èª¿æ•´é©ç”¨"""
        try:
            adjusted_weights = {}

            for timeframe in self.timeframes:
                current = current_weights.get(timeframe, 0.33)
                factor = adjustment_factors.get(timeframe, 1.0)

                # å½±éŸ¿å¼·åº¦ã‚’è€ƒæ…®ã—ãŸèª¿æ•´
                adjustment = current * (1.0 + (factor - 1.0) * influence_strength)
                adjusted_weights[timeframe] = adjustment

            return adjusted_weights

        except Exception as e:
            logger.error(f"Weight adjustment application failed: {e}")
            return current_weights.copy()

    def _normalize_and_constrain_weights(
        self, weights: Dict[str, float]
    ) -> Dict[str, float]:
        """é‡ã¿æ­£è¦åŒ–ãƒ»åˆ¶ç´„é©ç”¨"""
        try:
            # åˆ¶ç´„é©ç”¨ï¼ˆå„é‡ã¿ã‚’0.1-0.8ã®ç¯„å›²ã«åˆ¶é™ï¼‰
            constrained_weights = {}
            for timeframe, weight in weights.items():
                constrained_weights[timeframe] = max(0.1, min(0.8, weight))

            # æ­£è¦åŒ–ï¼ˆåˆè¨ˆã‚’1ã«ã™ã‚‹ï¼‰
            total = sum(constrained_weights.values())
            if total > 0:
                normalized_weights = {
                    tf: weight / total for tf, weight in constrained_weights.items()
                }
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                normalized_weights = dict(zip(self.timeframes, self.base_weights))

            return normalized_weights

        except Exception as e:
            logger.error(f"Weight normalization failed: {e}")
            return dict(zip(self.timeframes, self.base_weights))

    def _calculate_weight_change_magnitude(
        self, old_weights: Dict[str, float], new_weights: Dict[str, float]
    ) -> float:
        """é‡ã¿å¤‰åŒ–ã®å¤§ãã•è¨ˆç®—"""
        try:
            changes = []
            for timeframe in self.timeframes:
                old = old_weights.get(timeframe, 0.33)
                new = new_weights.get(timeframe, 0.33)
                changes.append(abs(new - old))

            return np.mean(changes)

        except Exception as e:
            logger.error(f"Weight change magnitude calculation failed: {e}")
            return 0.0

    def _update_weights(
        self, new_weights: Dict[str, float], adjustment_info: Dict[str, Any]
    ):
        """é‡ã¿æ›´æ–°ãƒ»å±¥æ­´è¨˜éŒ²"""
        try:
            self.current_weights = new_weights.copy()

            # é‡ã¿å±¥æ­´è¨˜éŒ²
            weight_record = {
                "timestamp": datetime.now(),
                "final_weights": new_weights.copy(),
                "adjustment_info": adjustment_info,
            }
            self.weight_history.append(weight_record)

            # çµ±è¨ˆæ›´æ–°
            if adjustment_info.get("weight_change_magnitude", 0) > 0.02:  # 2%ä»¥ä¸Šã®å¤‰åŒ–
                self.adjustment_stats["successful_adjustments"] += 1

        except Exception as e:
            logger.error(f"Weight update failed: {e}")

    def record_prediction_outcome(
        self,
        timeframe_predictions: Dict[str, Dict[str, Any]],
        actual_outcome: Dict[str, Any],
        performance_metrics: Dict[str, float],
    ):
        """äºˆæ¸¬çµæœè¨˜éŒ²ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ›´æ–°"""
        try:
            # äºˆæ¸¬çµæœå±¥æ­´è¨˜éŒ²
            outcome_record = {
                "timestamp": datetime.now(),
                "predictions": timeframe_predictions.copy(),
                "actual_outcome": actual_outcome.copy(),
                "performance": performance_metrics.copy(),
            }
            self.prediction_outcomes.append(outcome_record)

            # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥æ€§èƒ½å±¥æ­´æ›´æ–°
            for timeframe in self.timeframes:
                if timeframe in performance_metrics:
                    self.performance_history[timeframe].append(
                        performance_metrics[timeframe]
                    )

            # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ›´æ–°
            if self.enable_online_learning:
                self._update_online_models(
                    timeframe_predictions, actual_outcome, performance_metrics
                )

        except Exception as e:
            logger.error(f"Prediction outcome recording failed: {e}")

    def _update_online_models(
        self,
        timeframe_predictions: Dict[str, Dict[str, Any]],
        actual_outcome: Dict[str, Any],
        performance_metrics: Dict[str, float],
    ):
        """ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ›´æ–°"""
        try:
            for timeframe in self.timeframes:
                if timeframe not in self.online_models:
                    continue

                # ç‰¹å¾´é‡ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæº–å‚™
                features = self._prepare_online_features(
                    timeframe, timeframe_predictions, actual_outcome
                )
                target = performance_metrics.get(timeframe, 0.5)

                if features is None or len(features) == 0:
                    continue

                # ãƒ¢ãƒ‡ãƒ«æ›´æ–°
                model = self.online_models[timeframe]
                scaler = self.scalers[timeframe]

                try:
                    features_scaled = scaler.partial_fit([features]).transform(
                        [features]
                    )

                    if hasattr(model, "partial_fit"):
                        model.partial_fit(features_scaled, [target])
                    else:
                        # ä¸€æ™‚çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
                        model.fit(features_scaled, [target])

                except Exception as model_error:
                    logger.warning(
                        f"Model update failed for {timeframe}: {model_error}"
                    )

        except Exception as e:
            logger.error(f"Online models update failed: {e}")

    def get_adjustment_statistics(self) -> Dict[str, Any]:
        """èª¿æ•´çµ±è¨ˆæƒ…å ±å–å¾—"""
        stats = self.adjustment_stats.copy()

        # æˆåŠŸç‡è¨ˆç®—
        if stats["total_adjustments"] > 0:
            stats["success_rate"] = (
                stats["successful_adjustments"] / stats["total_adjustments"]
            )
        else:
            stats["success_rate"] = 0.0

        # å­¦ç¿’çŠ¶æ³
        stats["online_models_available"] = len(self.online_models)
        stats["q_table_size"] = len(self.q_table)
        stats["current_epsilon"] = self.epsilon
        stats["prediction_outcomes_count"] = len(self.prediction_outcomes)
        stats["weight_history_count"] = len(self.weight_history)

        # ç¾åœ¨ã®é‡ã¿
        stats["current_weights"] = self.current_weights.copy()
        stats["weight_confidence"] = self.weight_confidence.copy()

        return stats

    def get_current_weights(self) -> Dict[str, float]:
        """ç¾åœ¨ã®å‹•çš„é‡ã¿å–å¾—"""
        return self.current_weights.copy()

    def reset_statistics(self):
        """çµ±è¨ˆãƒ»å±¥æ­´ãƒªã‚»ãƒƒãƒˆ"""
        for key in self.adjustment_stats:
            if isinstance(self.adjustment_stats[key], (int, float)):
                self.adjustment_stats[key] = (
                    0 if isinstance(self.adjustment_stats[key], int) else 0.0
                )

        for timeframe in self.timeframes:
            self.performance_history[timeframe].clear()

        self.prediction_outcomes.clear()
        self.weight_history.clear()
        self.market_context_history.clear()

        # Q-table ãƒªã‚»ãƒƒãƒˆ
        self.q_table.clear()
        self.epsilon = self.config.get("reinforcement_learning", {}).get("epsilon", 0.1)

        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒ¢ãƒ‡ãƒ«å†åˆæœŸåŒ–
        if self.enable_online_learning:
            self._initialize_online_models()

        logger.info("ğŸ“Š Dynamic weight adjuster statistics reset")


# ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼é–¢æ•°


def create_dynamic_weight_adjuster(config: Dict[str, Any]) -> DynamicWeightAdjuster:
    """
    å‹•çš„é‡ã¿èª¿æ•´ã‚·ã‚¹ãƒ†ãƒ ä½œæˆ

    Parameters:
    -----------
    config : Dict[str, Any]
        è¨­å®šè¾æ›¸

    Returns:
    --------
    DynamicWeightAdjuster
        åˆæœŸåŒ–æ¸ˆã¿å‹•çš„é‡ã¿èª¿æ•´ã‚·ã‚¹ãƒ†ãƒ 
    """
    return DynamicWeightAdjuster(config)
