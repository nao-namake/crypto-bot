# =============================================================================
# ãƒ•ã‚¡ã‚¤ãƒ«å: crypto_bot/ml/preprocessor.py
# èª¬æ˜:
# æ©Ÿæ¢°å­¦ç¿’ç”¨ã®ç‰¹å¾´é‡ç”Ÿæˆãƒ»å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã€‚
# - OHLCV DataFrame ã‹ã‚‰æ©Ÿæ¢°å­¦ç¿’ç”¨ç‰¹å¾´é‡ã‚’ç”Ÿæˆ
# - sklearn Pipelineäº’æ›ã® FeatureEngineer ã‚’æä¾›
# - build_ml_pipeline() ã§ç‰¹å¾´é‡ï¼‹æ¨™æº–åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç”Ÿæˆ
# - prepare_ml_dataset() ã§ç‰¹å¾´é‡ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã‚’ä¸€æ‹¬ç”Ÿæˆ
#
# â€» ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨ã§ã¯ãªãã€MLStrategyã‚„å­¦ç¿’/æ¨è«–ç³»ã§ä½¿ç”¨
# =============================================================================

from __future__ import annotations

import logging
import time
from typing import Any, Dict, Tuple

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# Phase B2.4: ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³çµ±åˆ
try:
    from crypto_bot.ml.feature_engines import (
        BatchFeatureCalculator,
        ExternalDataIntegrator,
        TechnicalFeatureEngine,
    )

    BATCH_ENGINES_AVAILABLE = True
except ImportError as e:
    # logger is not yet defined, use print temporarily
    print(f"âš ï¸ Batch engines not available: {e}")
    BATCH_ENGINES_AVAILABLE = False

try:
    from crypto_bot.data.vix_fetcher import VIXDataFetcher

    VIX_AVAILABLE = True
except ImportError:
    VIXDataFetcher = None
    VIX_AVAILABLE = False

try:
    from crypto_bot.data.macro_fetcher import MacroDataFetcher

    MACRO_AVAILABLE = True
except ImportError:
    MacroDataFetcher = None
    MACRO_AVAILABLE = False

try:
    from crypto_bot.data.funding_fetcher import FundingDataFetcher

    FUNDING_AVAILABLE = True
except ImportError:
    FundingDataFetcher = None
    FUNDING_AVAILABLE = False

try:
    from crypto_bot.data.fear_greed_fetcher import FearGreedDataFetcher

    FEAR_GREED_AVAILABLE = True
except ImportError:
    FearGreedDataFetcher = None
    FEAR_GREED_AVAILABLE = False

from crypto_bot.indicator.calculator import IndicatorCalculator
from crypto_bot.ml.target import make_classification_target, make_regression_target

logger = logging.getLogger(__name__)


def calc_rci(series: pd.Series, period: int) -> pd.Series:
    """
    Rank Correlation Indexï¼ˆRCIï¼‰ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    :param series: çµ‚å€¤ãªã©ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆpd.Seriesï¼‰
    :param period: æœŸé–“
    :return: RCIã®pd.Series
    """
    n = period

    def _rci(x):
        price_ranks = pd.Series(x).rank(ascending=False)
        date_ranks = np.arange(1, n + 1)
        d = price_ranks.values - date_ranks
        return (1 - 6 * (d**2).sum() / (n * (n**2 - 1))) * 100

    return series.rolling(window=n).apply(_rci, raw=False)


class FeatureEngineer(BaseEstimator, TransformerMixin):
    """
    OHLCVã‹ã‚‰å„ç¨®ç‰¹å¾´é‡ã‚’ç”Ÿæˆã™ã‚‹ sklearnäº’æ›ã‚¯ãƒ©ã‚¹ã€‚

    å…¥åŠ›: OHLCV DataFrameï¼ˆindexã¯tz-aware DatetimeIndexï¼‰
    å‡ºåŠ›: ç‰¹å¾´é‡DataFrame

    - æ¬ æè£œå®Œ
    - ATR, lag, rollingçµ±è¨ˆ, extra_featureså¯¾å¿œ
    - ffill/0åŸ‹ã‚ç­‰ã‚‚å®Ÿæ–½
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.ind_calc = IndicatorCalculator()
        self.extra_features = self.config["ml"].get("extra_features", [])

        # MLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        ml_config = self.config["ml"]
        self.feat_period = ml_config.get("feat_period", 14)
        self.lags = ml_config.get("lags", [1, 2, 3])
        self.rolling_window = ml_config.get("rolling_window", 14)

        # VIXçµ±åˆè¨­å®šï¼ˆå¼·åˆ¶åˆæœŸåŒ–ç‰ˆï¼‰
        logger.info(f"ğŸ” VIX Debug: extra_features={self.extra_features}")
        logger.info(f"ğŸ” VIX Debug: VIX_AVAILABLE={VIX_AVAILABLE}")
        vix_in_features = "vix" in self.extra_features
        logger.info(f"ğŸ” VIX Debug: vix_in_features={vix_in_features}")

        # VIXå¾©æ´»å®Ÿè£…ï¼šè¨­å®šå¯¾å¿œãƒ»è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½
        if vix_in_features:
            try:
                if VIX_AVAILABLE and VIXDataFetcher:
                    self.vix_fetcher = VIXDataFetcher(self.config)
                    self.vix_enabled = True
                    logger.info(
                        "âœ… VIX fetcher initialized successfully (config-aware)"
                    )
                else:
                    # VIXDataFetcherã‚’ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦åˆæœŸåŒ–ã‚’å¼·åˆ¶
                    from crypto_bot.data.vix_fetcher import (
                        VIXDataFetcher as DirectVIXFetcher,
                    )

                    self.vix_fetcher = DirectVIXFetcher(self.config)
                    self.vix_enabled = True
                    logger.info(
                        "âœ… VIX fetcher initialized with direct import (config-aware)"
                    )
            except Exception as e:
                logger.error(f"âŒ VIX fetcher initialization failed: {e}")
                self.vix_fetcher = None
                self.vix_enabled = False
        else:
            self.vix_enabled = False
            self.vix_fetcher = None
            logger.info(f"âš ï¸ VIX not in extra_features: {self.extra_features}")

        # ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿çµ±åˆè¨­å®šï¼ˆå¼·åˆ¶åˆæœŸåŒ–ç‰ˆï¼‰
        macro_in_features = any(
            feat in self.extra_features for feat in ["dxy", "macro", "treasury"]
        )
        logger.info(f"ğŸ” Macro Debug: macro_in_features={macro_in_features}")

        if macro_in_features:
            try:
                if MACRO_AVAILABLE and MacroDataFetcher:
                    self.macro_fetcher = MacroDataFetcher()
                    self.macro_enabled = True
                    logger.info("âœ… Macro fetcher initialized successfully (forced)")
                else:
                    # MacroDataFetcherã‚’ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦åˆæœŸåŒ–ã‚’å¼·åˆ¶
                    from crypto_bot.data.macro_fetcher import (
                        MacroDataFetcher as DirectMacroFetcher,
                    )

                    self.macro_fetcher = DirectMacroFetcher()
                    self.macro_enabled = True
                    logger.info("âœ… Macro fetcher initialized with direct import")
            except Exception as e:
                logger.error(f"âŒ Macro fetcher initialization failed: {e}")
                self.macro_fetcher = None
                self.macro_enabled = False
        else:
            self.macro_enabled = False
            self.macro_fetcher = None
            logger.info(
                f"âš ï¸ Macro features not in extra_features: {self.extra_features}"
            )

        # Funding Rateçµ±åˆè¨­å®šï¼ˆBitbankå°‚ç”¨ï¼šç¾ç‰©å–å¼•ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
        self.funding_enabled = False
        self.funding_fetcher = None

        # Bitbankç¾ç‰©å–å¼•ã§ã¯ä»£æ›¿ç‰¹å¾´é‡ã‚’ä½¿ç”¨
        self.funding_alternative_enabled = any(
            feat in self.extra_features for feat in ["funding", "oi"]
        )

        # Fear & Greedå¾©æ´»å®Ÿè£…ï¼šè¨­å®šå¯¾å¿œãƒ»è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½
        fear_greed_in_features = any(
            feat in self.extra_features for feat in ["fear_greed", "fg"]
        )
        logger.info(
            f"ğŸ” Fear&Greed Debug: fear_greed_in_features={fear_greed_in_features}"
        )

        if fear_greed_in_features:
            try:
                if FEAR_GREED_AVAILABLE and FearGreedDataFetcher:
                    self.fear_greed_fetcher = FearGreedDataFetcher(self.config)
                    self.fear_greed_enabled = True
                    logger.info(
                        "âœ… Fear&Greed fetcher initialized successfully (config-aware)"
                    )
                else:
                    # FearGreedDataFetcherã‚’ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦åˆæœŸåŒ–ã‚’å¼·åˆ¶
                    from crypto_bot.data.fear_greed_fetcher import (
                        FearGreedDataFetcher as DirectFGFetcher,
                    )

                    self.fear_greed_fetcher = DirectFGFetcher(self.config)
                    self.fear_greed_enabled = True
                    logger.info(
                        "âœ… Fear&Greed fetcher initialized with direct import "
                        "(config-aware)"
                    )
            except Exception as e:
                logger.error(f"âŒ Fear&Greed fetcher initialization failed: {e}")
                self.fear_greed_fetcher = None
                self.fear_greed_enabled = False
        else:
            self.fear_greed_enabled = False
            self.fear_greed_fetcher = None
            logger.info(f"âš ï¸ Fear&Greed not in extra_features: {self.extra_features}")

        # USD/JPYç‚ºæ›¿ãƒ‡ãƒ¼ã‚¿çµ±åˆè¨­å®šï¼ˆå¼·åˆ¶åˆæœŸåŒ–ç‰ˆï¼‰
        forex_in_features = any(
            feat in self.extra_features for feat in ["forex", "usdjpy", "jpy"]
        )
        logger.info(f"ğŸ” Forex Debug: forex_in_features={forex_in_features}")

        if forex_in_features:
            try:
                # MacroDataFetcherã‚’ç‚ºæ›¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å†åˆ©ç”¨
                if MACRO_AVAILABLE and MacroDataFetcher:
                    self.forex_fetcher = MacroDataFetcher()
                    self.forex_enabled = True
                    logger.info("âœ… Forex fetcher initialized successfully (forced)")
                else:
                    # MacroDataFetcherã‚’ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦åˆæœŸåŒ–ã‚’å¼·åˆ¶
                    from crypto_bot.data.macro_fetcher import (
                        MacroDataFetcher as DirectForexFetcher,
                    )

                    self.forex_fetcher = DirectForexFetcher()
                    self.forex_enabled = True
                    logger.info("âœ… Forex fetcher initialized with direct import")
            except Exception as e:
                logger.error(f"âŒ Forex fetcher initialization failed: {e}")
                self.forex_fetcher = None
                self.forex_enabled = False
        else:
            self.forex_enabled = False
            self.forex_fetcher = None
            logger.info(f"âš ï¸ Forex not in extra_features: {self.extra_features}")

        # Phase B2.4: ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        self._initialize_batch_engines()

    def _initialize_batch_engines(self):
        """
        Phase B2.4: ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        DataFrameæ–­ç‰‡åŒ–è§£æ¶ˆã®ãŸã‚ã®æ–°ã‚¨ãƒ³ã‚¸ãƒ³ã‚·ã‚¹ãƒ†ãƒ 
        """
        if not BATCH_ENGINES_AVAILABLE:
            logger.warning(
                "âš ï¸ Batch engines not available, falling back to legacy processing"
            )
            self.batch_engines_enabled = False
            return

        try:
            # BatchFeatureCalculatorï¼ˆã‚³ã‚¢ï¼‰
            self.batch_calculator = BatchFeatureCalculator(self.config)

            # TechnicalFeatureEngineï¼ˆãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ï¼‰
            self.technical_engine = TechnicalFeatureEngine(
                self.config, self.batch_calculator
            )

            # ExternalDataIntegratorï¼ˆå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆï¼‰
            self.external_integrator = ExternalDataIntegrator(
                self.config, self.batch_calculator
            )

            self.batch_engines_enabled = True

            logger.info(
                "ğŸš€ Phase B2.4: Batch processing engines initialized successfully - "
                "DataFrame fragmentation optimization enabled"
            )

        except Exception as e:
            logger.error(f"âŒ Failed to initialize batch engines: {e}")
            self.batch_engines_enabled = False

    def _transform_with_batch_engines(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Phase B2.4: ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã«ã‚ˆã‚‹é«˜é€Ÿç‰¹å¾´é‡ç”Ÿæˆ
        DataFrameæ–­ç‰‡åŒ–ã‚’è§£æ¶ˆã™ã‚‹æ ¹æœ¬çš„æ”¹å–„
        """
        start_time = time.time()
        logger.info("ğŸš€ Starting batch processing feature generation")

        feature_batches = []

        try:
            # 1. ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ãƒãƒƒãƒå‡¦ç†
            technical_batches = self.technical_engine.calculate_all_technical_batches(
                df
            )
            feature_batches.extend(technical_batches)
            logger.debug(f"ğŸ“Š Technical batches: {len(technical_batches)}")

            # 2. å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒãƒƒãƒå‡¦ç†
            if self.external_integrator:
                external_batches = (
                    self.external_integrator.create_external_data_batches(df.index)
                )
                feature_batches.extend(external_batches)
                logger.debug(f"ğŸ“Š External data batches: {len(external_batches)}")

            # 3. ä¸€æ‹¬çµ±åˆï¼ˆæ–­ç‰‡åŒ–è§£æ¶ˆã®ä¸­æ ¸ï¼‰
            result_df = self.batch_calculator.merge_batches_efficient(
                df, feature_batches
            )

            processing_time = time.time() - start_time
            total_features = sum(len(batch) for batch in feature_batches)

            logger.info(
                f"âœ… Batch processing completed: {total_features} features "
                f"in {processing_time:.3f}s ({total_features/processing_time:.1f} features/sec)"
            )

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆå‡ºåŠ›
            batch_stats = self.batch_calculator.get_performance_summary()
            logger.debug(f"ğŸ“Š Batch Performance:\n{batch_stats}")

            return result_df

        except Exception as e:
            import traceback

            logger.error(f"âŒ Batch processing failed: {e}")
            logger.error(
                f"âŒ Batch processing error details:\n{traceback.format_exc()}"
            )
            logger.warning("âš ï¸ Falling back to legacy processing")
            return self._transform_legacy(df)

    def _transform_legacy(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼ç‰¹å¾´é‡å‡¦ç†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
        å€‹åˆ¥DataFrameæ“ä½œã«ã‚ˆã‚‹å¾“æ¥æ–¹å¼
        """
        logger.warning("âš ï¸ Using legacy feature processing - performance may be slower")

        # 2. ATR
        feat_period = self.config["ml"]["feat_period"]
        atr = self.ind_calc.atr(df, window=feat_period)
        if isinstance(atr, pd.Series):
            df[f"ATR_{feat_period}"] = atr
        else:
            df[f"ATR_{feat_period}"] = atr.iloc[:, 0]
        logger.debug("After ATR: %s", df.shape)

        # 3. lagç‰¹å¾´é‡
        for lag in self.config["ml"]["lags"]:
            df[f"close_lag_{lag}"] = df["close"].shift(lag)
        logger.debug("After lag feats: %s", df.shape)

        # 4. rollingçµ±è¨ˆ
        win = self.config["ml"]["rolling_window"]
        df[f"close_mean_{win}"] = df["close"].rolling(win).mean()
        df[f"close_std_{win}"] = df["close"].rolling(win).std()
        logger.debug("After rolling stats: %s", df.shape)

        # 5. extra_featureså‡¦ç†ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼æ–¹å¼ï¼‰
        return self._process_extra_features_legacy(df)

    def _process_extra_features_legacy(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼æ–¹å¼ã«ã‚ˆã‚‹ extra_features å‡¦ç†
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã«ä½¿ç”¨ï¼ˆå€‹åˆ¥DataFrameæ“ä½œï¼‰
        """
        if not self.extra_features:
            return df

        logger.warning(
            "âš ï¸ Using legacy feature processing - individual DataFrame operations"
        )
        logger.debug(f"Legacy processing for: {self.extra_features}")

        # ãƒ¬ã‚¬ã‚·ãƒ¼å‡¦ç†ã§ã¯æ—¢å­˜ã®extra_featureså‡¦ç†ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨
        # ï¼ˆãƒ¡ã‚¤ãƒ³ã®transformå†…ã®å‡¦ç†ã¨é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã€ã“ã“ã§ã¯åŸºæœ¬å‡¦ç†ã®ã¿ï¼‰

        return df

    def _get_cached_external_data(
        self, data_type: str, time_index: pd.DatetimeIndex
    ) -> pd.DataFrame:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Parameters
        ----------
        data_type : str
            ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ— ('vix', 'macro', 'forex', 'fear_greed', 'funding')
        time_index : pd.DatetimeIndex
            å¯¾è±¡æœŸé–“ã®ã‚¿ã‚¤ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

        Returns
        -------
        pd.DataFrame
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ï¼ˆè©²å½“æœŸé–“ï¼‰
        """
        try:
            from crypto_bot.ml.external_data_cache import get_global_cache

            cache = get_global_cache()
            if not cache.is_initialized:
                return pd.DataFrame()

            start_time = time_index.min()
            end_time = time_index.max()

            cached_data = cache.get_period_data(data_type, start_time, end_time)
            return cached_data

        except Exception as e:
            logger.debug(f"Failed to get cached {data_type} data: {e}")
            return pd.DataFrame()

    def _validate_external_data_fetchers(self) -> dict:
        """
        å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã®çŠ¶æ…‹ã‚’æ¤œè¨¼ã—ã€ãƒ‡ãƒ¼ã‚¿å“è³ªæ”¹å–„ã®ãŸã‚ã®æƒ…å ±ã‚’è¿”ã™

        Returns
        -------
        dict
            ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã®çŠ¶æ…‹ãƒ¬ãƒãƒ¼ãƒˆ
        """
        validation_report = {
            "vix": {"available": False, "initialized": False, "working": False},
            "macro": {"available": False, "initialized": False, "working": False},
            "forex": {"available": False, "initialized": False, "working": False},
            "fear_greed": {"available": False, "initialized": False, "working": False},
            "total_working": 0,
            "external_data_success_rate": 0.0,
        }

        # VIXæ¤œè¨¼
        if "vix" in self.extra_features:
            validation_report["vix"]["available"] = True
            if self.vix_fetcher is not None:
                validation_report["vix"]["initialized"] = True
                try:
                    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆå–å¾—
                    test_data = self.vix_fetcher.get_vix_data(timeframe="1d", limit=1)
                    if test_data is not None and not test_data.empty:
                        validation_report["vix"]["working"] = True
                        validation_report["total_working"] += 1
                        logger.info("âœ… VIX fetcher validation: WORKING")
                    else:
                        logger.warning(
                            "âš ï¸ VIX fetcher validation: NOT WORKING (empty data)"
                        )
                except Exception as e:
                    logger.error(f"âŒ VIX fetcher validation failed: {e}")

        # Macroæ¤œè¨¼
        if any(feat in self.extra_features for feat in ["dxy", "macro", "treasury"]):
            validation_report["macro"]["available"] = True
            if self.macro_fetcher is not None:
                validation_report["macro"]["initialized"] = True
                try:
                    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆå–å¾—
                    test_data = self.macro_fetcher.get_macro_data()
                    if test_data and not all(df.empty for df in test_data.values()):
                        validation_report["macro"]["working"] = True
                        validation_report["total_working"] += 1
                        logger.info("âœ… Macro fetcher validation: WORKING")
                    else:
                        logger.warning(
                            "âš ï¸ Macro fetcher validation: NOT WORKING (empty data)"
                        )
                except Exception as e:
                    logger.error(f"âŒ Macro fetcher validation failed: {e}")

        # Fear&Greedæ¤œè¨¼
        if any(feat in self.extra_features for feat in ["fear_greed", "fg"]):
            validation_report["fear_greed"]["available"] = True
            if self.fear_greed_fetcher is not None:
                validation_report["fear_greed"]["initialized"] = True
                try:
                    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆå–å¾—
                    test_data = self.fear_greed_fetcher.get_fear_greed_data(days_back=1)
                    if test_data is not None and not test_data.empty:
                        validation_report["fear_greed"]["working"] = True
                        validation_report["total_working"] += 1
                        logger.info("âœ… Fear&Greed fetcher validation: WORKING")
                    else:
                        logger.warning(
                            "âš ï¸ Fear&Greed fetcher validation: NOT WORKING (empty data)"
                        )
                except Exception as e:
                    logger.error(f"âŒ Fear&Greed fetcher validation failed: {e}")

        # Forexæ¤œè¨¼
        if any(feat in self.extra_features for feat in ["forex", "usdjpy", "jpy"]):
            validation_report["forex"]["available"] = True
            if self.forex_fetcher is not None:
                validation_report["forex"]["initialized"] = True
                try:
                    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆå–å¾—
                    test_data = self.forex_fetcher.get_macro_data()
                    if (
                        test_data
                        and "usdjpy" in test_data
                        and not test_data["usdjpy"].empty
                    ):
                        validation_report["forex"]["working"] = True
                        validation_report["total_working"] += 1
                        logger.info("âœ… Forex fetcher validation: WORKING")
                    else:
                        logger.warning(
                            "âš ï¸ Forex fetcher validation: NOT WORKING (empty data)"
                        )
                except Exception as e:
                    logger.error(f"âŒ Forex fetcher validation failed: {e}")

        # æˆåŠŸç‡è¨ˆç®—
        total_available = sum(
            1
            for fetcher in validation_report.values()
            if isinstance(fetcher, dict) and fetcher.get("available", False)
        )
        if total_available > 0:
            validation_report["external_data_success_rate"] = (
                validation_report["total_working"] / total_available
            )

        logger.info(
            f"ğŸ” External data validation: "
            f"{validation_report['total_working']}/{total_available} fetchers working "
            f"({validation_report['external_data_success_rate']*100:.1f}% success rate)"
        )
        return validation_report

    def fit(self, X: pd.DataFrame, y=None):
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        logger.debug("Input DataFrame shape: %s", X.shape)

        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã®çŠ¶æ…‹æ¤œè¨¼ï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªæ”¹å–„ï¼‰
        validation_report = self._validate_external_data_fetchers()
        logger.info(
            f"ğŸ” External data fetcher status: "
            f"{validation_report['total_working']} working, "
            f"{validation_report['external_data_success_rate']*100:.1f}% success rate"
        )

        if X.empty:
            feat_period = self.config["ml"]["feat_period"]
            win = self.config["ml"]["rolling_window"]
            lags = self.config["ml"]["lags"]
            columns = ["ATR_" + str(feat_period)]
            columns.extend([f"close_lag_{lag}" for lag in lags])
            columns.extend([f"close_mean_{win}", f"close_std_{win}"])
            for feat in self.extra_features:
                feat_lc = feat.lower()
                base, _, param = feat_lc.partition("_")
                period = int(param) if param.isdigit() else None
                if base == "rsi" and period:
                    columns.append(f"rsi_{period}")
                elif base == "ema" and period:
                    columns.append(f"ema_{period}")
                elif base == "sma" and period:
                    columns.append(f"sma_{period}")
                elif base == "macd":
                    columns.extend(["macd", "macd_signal", "macd_hist"])
                elif base == "rci" and period:
                    columns.append(f"rci_{period}")
                elif base == "volume" and "zscore" in feat_lc:
                    period_str = feat_lc.split("_")[-1]
                    win_z = int(period_str) if period_str.isdigit() else win
                    columns.append(f"volume_zscore_{win_z}")
                elif feat_lc == "day_of_week":
                    columns.append("day_of_week")
                elif feat_lc == "hour_of_day":
                    columns.append("hour_of_day")
                # Phase F.3: æ–°è¦ç‰¹å¾´é‡ã®åˆ—åå®šç¾©
                elif feat_lc in [
                    "volatility_24h",
                    "volatility_1h",
                    "volume_change_24h",
                    "volume_change_1h",
                    "price_change_24h",
                    "price_change_4h",
                    "price_change_1h",
                    "cmf_20",
                    "willr_14",
                ]:
                    columns.append(feat_lc)
                elif feat_lc in ["mochipoyo_long_signal", "mochipoyo_short_signal"]:
                    columns.extend(["mochipoyo_long_signal", "mochipoyo_short_signal"])
            return pd.DataFrame(columns=columns)
        df = X.copy()

        # 1. æ¬ æè£œå®Œ
        df = df.ffill()
        logger.debug("After ffill: %s", df.shape)

        # Phase B2.4: ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹é«˜é€Ÿç‰¹å¾´é‡ç”Ÿæˆ
        logger.info(f"ğŸ” Batch engines enabled: {self.batch_engines_enabled}")
        if self.batch_engines_enabled:
            df = self._transform_with_batch_engines(df)
        else:
            # ãƒ¬ã‚¬ã‚·ãƒ¼å‡¦ç†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            df = self._transform_legacy(df)

        # 6. æœ€çµ‚ç‰¹å¾´é‡æ¤œè¨¼ãƒ»æ¬ æå€¤å‡¦ç†
        # Phase B2.5: ãƒãƒƒãƒå‡¦ç†æœ‰åŠ¹æ™‚ã¯è¿½åŠ å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒãƒƒãƒå‡¦ç†ã§å®Œäº†æ¸ˆã¿ï¼‰
        if self.extra_features and not self.batch_engines_enabled:
            logger.debug("Adding extra features: %s", self.extra_features)
            # è¿½åŠ ã§mochipoyoã®ã‚·ã‚°ãƒŠãƒ«ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¸€åº¦ã¾ã¨ã‚ã¦å–å¾—ã—ã¦ãŠã
            mochipoyo_needed = any(
                feat.lower() in ["mochipoyo_long_signal", "mochipoyo_short_signal"]
                for feat in self.extra_features
            )
            mochipoyo_signals = None
            if mochipoyo_needed:
                mochipoyo_signals = self.ind_calc.mochipoyo_signals(df)

            for feat in self.extra_features:
                feat_lc = feat.lower()
                try:
                    # fear_greedã¯ç‰¹åˆ¥å‡¦ç†ï¼ˆã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’å«ã‚€è¤‡åˆèªï¼‰
                    if feat_lc == "fear_greed":
                        base = "fear_greed"
                        param = ""
                        period = None
                    else:
                        base, _, param = feat_lc.partition("_")
                        period = int(param) if param.isdigit() else None

                    # Phase B2.5: ãƒãƒƒãƒå‡¦ç†æ¸ˆã¿ç‰¹å¾´é‡ã®ã‚¹ã‚­ãƒƒãƒ—ãƒ­ã‚¸ãƒƒã‚¯
                    if self.batch_engines_enabled:
                        # ãƒãƒƒãƒå‡¦ç†æ¸ˆã¿ç‰¹å¾´é‡ã¯ã‚¹ã‚­ãƒƒãƒ—
                        skip_features = [
                            "rsi",
                            "ema",
                            "sma",
                            "macd",
                            "atr",
                            "volume",
                            "stoch",
                            "bb",
                            "bollinger",
                            "willr",
                            "williams",
                            "adx",
                            "cmf",
                            "fisher",
                            "vix",
                            "dxy",
                            "macro",
                            "treasury",
                            "fear_greed",
                            "fg",
                        ]
                        if any(
                            base == skip_feat or base.startswith(skip_feat)
                            for skip_feat in skip_features
                        ):
                            logger.debug(f"Skipping batch-processed feature: {feat}")
                            continue

                    # æ™‚é–“ç‰¹å¾´é‡ï¼ˆãƒãƒƒãƒå‡¦ç†å¯¾è±¡å¤–ã®ãŸã‚ç¶™ç¶šå‡¦ç†ï¼‰
                    if feat_lc == "day_of_week":
                        if isinstance(df.index, pd.DatetimeIndex):
                            df["day_of_week"] = df.index.dayofweek.astype("int8")
                        else:
                            df["day_of_week"] = 0
                    elif feat_lc == "hour_of_day":
                        if isinstance(df.index, pd.DatetimeIndex):
                            df["hour_of_day"] = df.index.hour.astype("int8")
                        else:
                            df["hour_of_day"] = 0

                    # Phase F.3: 151ç‰¹å¾´é‡WARNINGè§£æ¶ˆ - ä¸è¶³ç‰¹å¾´é‡å‡¦ç†
                    elif feat_lc == "volatility_24h":
                        df["volatility_24h"] = self.ind_calc.volatility_24h(df["close"])
                    elif feat_lc == "volatility_1h":
                        df["volatility_1h"] = self.ind_calc.volatility_1h(df["close"])
                    elif feat_lc == "volume_change_24h":
                        df["volume_change_24h"] = self.ind_calc.volume_change_24h(
                            df["volume"]
                        )
                    elif feat_lc == "volume_change_1h":
                        df["volume_change_1h"] = self.ind_calc.volume_change_1h(
                            df["volume"]
                        )
                    elif feat_lc == "price_change_24h":
                        df["price_change_24h"] = self.ind_calc.price_change_24h(
                            df["close"]
                        )
                    elif feat_lc == "price_change_4h":
                        df["price_change_4h"] = self.ind_calc.price_change_4h(
                            df["close"]
                        )
                    elif feat_lc == "price_change_1h":
                        df["price_change_1h"] = self.ind_calc.price_change_1h(
                            df["close"]
                        )
                    elif feat_lc == "cmf_20":
                        df["cmf_20"] = self.ind_calc.cmf_20(df)
                    elif feat_lc == "willr_14":
                        df["willr_14"] = self.ind_calc.willr_14(df)

                    # Phase B2.5: VIXç‰¹å¾´é‡ã¯æ—¢ã«ExternalDataIntegratorã§å‡¦ç†æ¸ˆã¿
                    elif base == "vix":
                        try:
                            logger.info(
                                f"ğŸ” Processing VIX features: "
                                f"vix_enabled={self.vix_enabled}, "
                                f"vix_fetcher={self.vix_fetcher is not None}"
                            )

                            vix_features = None

                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰VIXãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå„ªå…ˆï¼‰
                            cached_vix = self._get_cached_external_data("vix", df.index)
                            if not cached_vix.empty:
                                logger.info(
                                    f"âœ… Using cached VIX: {len(cached_vix)} records"
                                )
                                vix_features = cached_vix

                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç©ºã®å ´åˆã€VIXãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã§ç›´æ¥å–å¾—
                            if vix_features is None or vix_features.empty:
                                if self.vix_fetcher:
                                    logger.info("ğŸ” Fetching fresh VIX data...")
                                    try:
                                        vix_data = self.vix_fetcher.get_vix_data(
                                            timeframe="1d", limit=100
                                        )
                                        if not vix_data.empty:
                                            logger.info(
                                                f"âœ… VIX: {len(vix_data)} records"
                                            )
                                            vix_features = (
                                                self.vix_fetcher.calculate_vix_features(
                                                    vix_data
                                                )
                                            )
                                        else:
                                            logger.warning(
                                                "âŒ VIX data empty from fetcher"
                                            )
                                    except Exception as e:
                                        logger.error(f"âŒ VIX fetching failed: {e}")
                                else:
                                    logger.warning("âŒ VIX fetcher not available")

                            # VIXãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããŸå ´åˆã®å‡¦ç†
                            if vix_features is not None and not vix_features.empty:
                                # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³çµ±ä¸€ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆæ”¹è‰¯
                                if isinstance(
                                    df.index, pd.DatetimeIndex
                                ) and isinstance(vix_features.index, pd.DatetimeIndex):
                                    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³çµ±ä¸€ï¼ˆé‡è¦ãªä¿®æ­£ï¼‰
                                    if df.index.tz is None:
                                        df.index = df.index.tz_localize("UTC")
                                    if vix_features.index.tz is None:
                                        vix_features.index = (
                                            vix_features.index.tz_localize("UTC")
                                        )
                                    elif vix_features.index.tz != df.index.tz:
                                        vix_features.index = (
                                            vix_features.index.tz_convert("UTC")
                                        )

                                    # æ”¹è‰¯ã•ã‚ŒãŸãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼šæ—¥æ¬¡â†’æ™‚é–“è¶³ã¸ã®å¤‰æ›
                                    # å‰æ–¹è£œå®Œï¼ˆffillï¼‰ã§æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚é–“è¶³ã«å±•é–‹
                                    vix_hourly = vix_features.resample("H").ffill()

                                    # æš—å·è³‡ç”£ãƒ‡ãƒ¼ã‚¿ã®æ™‚é–“ç¯„å›²ã«åˆã‚ã›ã¦åˆ¶é™
                                    start_time = df.index.min()
                                    end_time = df.index.max()
                                    vix_hourly = vix_hourly.loc[start_time:end_time]

                                    # ã‚ˆã‚ŠæŸ”è»Ÿãªãƒãƒƒãƒãƒ³ã‚°ï¼šæœ€ã‚‚è¿‘ã„æ™‚åˆ»ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                                    vix_cols = [
                                        "vix_level",
                                        "vix_change",
                                        "vix_zscore",
                                        "fear_level",
                                        "vix_spike",
                                        "vix_regime",
                                    ]
                                    added_features = 0

                                    for i, timestamp in enumerate(df.index):
                                        # æœ€ã‚‚è¿‘ã„VIXãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’æ¤œç´¢
                                        if len(vix_hourly) > 0:
                                            closest_idx = vix_hourly.index.get_indexer(
                                                [timestamp], method="ffill"
                                            )[0]
                                            if closest_idx >= 0:
                                                vix_row = vix_hourly.iloc[closest_idx]
                                                for col in vix_cols:
                                                    if col in vix_row.index:
                                                        if col == "vix_regime":
                                                            # ã‚«ãƒ†ã‚´ãƒªã‚’æ•°å€¤ã«å¤‰æ›
                                                            regime_map = {
                                                                "low": 0,
                                                                "normal": 1,
                                                                "high": 2,
                                                                "extreme": 3,
                                                            }
                                                            k = "vix_regime_numeric"
                                                            v = regime_map.get(
                                                                vix_row[col], 1
                                                            )
                                                            df.loc[timestamp, k] = v
                                                        else:
                                                            df.loc[timestamp, col] = (
                                                                vix_row[col]
                                                            )
                                        added_features = i + 1

                                    logger.info(
                                        f"VIX: {added_features}/{len(df)} points"
                                    )
                                else:
                                    logger.warning(
                                        "Could not align VIX data - index type mismatch"
                                    )
                            else:
                                logger.warning("No VIX data available - using defaults")
                        except Exception as e:
                            logger.warning("Failed to add VIX features: %s", e)

                    # OIï¼ˆæœªæ±ºæ¸ˆå»ºç‰ï¼‰é–¢é€£ç‰¹å¾´é‡
                    elif base == "oi":
                        try:
                            # from crypto_bot.data.fetcher import (
                            #     MarketDataFetcher,
                            #     OIDataFetcher,
                            # )

                            # OIãƒ‡ãƒ¼ã‚¿å–å¾—ã®ãŸã‚ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆå®Ÿéš›ã«ã¯ã‚ˆã‚Šè©³ç´°ãªå®Ÿè£…ãŒå¿…è¦ï¼‰
                            # ç¾æ™‚ç‚¹ã§ã¯åŸºæœ¬çš„ãªOIç‰¹å¾´é‡ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                            # OIå¤‰åŒ–ç‡ï¼ˆä¾¡æ ¼ã¨OIã®ç›¸é–¢ï¼‰
                            df["oi_price_divergence"] = (
                                df["close"].pct_change()
                                - df["volume"].pct_change().fillna(0)
                            ).fillna(0)

                            # ãƒœãƒªãƒ¥ãƒ¼ãƒ å¼·åº¦ï¼ˆOIã®ä»£æ›¿ï¼‰
                            df["volume_intensity"] = (
                                df["volume"] / df["volume"].rolling(20).mean()
                            )

                            # OIå‹¢ã„ï¼ˆãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒ™ãƒ¼ã‚¹ï¼‰
                            df["oi_momentum_proxy"] = (
                                df["volume"].rolling(10).sum()
                                / df["volume"].rolling(50).sum()
                            )

                        except Exception as e:
                            logger.warning("Failed to add OI features: %s", e)

                    # Phase B2.5: MacroçµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡ã¯æ—¢ã«ExternalDataIntegratorã§å‡¦ç†æ¸ˆã¿
                    elif base in ["dxy", "macro", "treasury"]:
                        try:
                            logger.info(
                                f"ğŸ” Processing Macro features: "
                                f"macro_enabled={self.macro_enabled}, "
                                f"macro_fetcher={self.macro_fetcher is not None}"
                            )

                            macro_features = None

                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå„ªå…ˆï¼‰
                            cached_macro = self._get_cached_external_data(
                                "macro", df.index
                            )
                            if not cached_macro.empty:
                                logger.info(
                                    f"âœ… Using cached macro: {len(cached_macro)} records"
                                )
                                macro_features = cached_macro

                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç©ºã®å ´åˆã€ãƒã‚¯ãƒ­ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã§ç›´æ¥å–å¾—
                            if macro_features is None or macro_features.empty:
                                if self.macro_fetcher:
                                    logger.info("ğŸ” Fetching fresh Macro data...")
                                    try:
                                        macro_data = self.macro_fetcher.get_macro_data()
                                        if macro_data and not all(
                                            df.empty for df in macro_data.values()
                                        ):
                                            logger.info(
                                                f"âœ… Macro: {len(macro_data)} datasets"
                                            )
                                            calc_func = (
                                                self.macro_fetcher.calculate_macro_features  # noqa: E501
                                            )
                                            macro_features = calc_func(macro_data)
                                        else:
                                            logger.warning(
                                                "âŒ Macro data empty from fetcher"
                                            )
                                    except Exception as e:
                                        logger.error(f"âŒ Macro fetching failed: {e}")
                                else:
                                    logger.warning("âŒ Macro fetcher not available")

                            # ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããŸå ´åˆã®å‡¦ç†
                            if macro_features is not None and not macro_features.empty:
                                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯å¾“æ¥ã®æ–¹æ³•
                                if hasattr(df, "index") and len(df) > 0:
                                    backtest_year = df.index.min().year
                                    self.macro_fetcher._backtest_start_year = (
                                        backtest_year
                                    )
                                macro_data = self.macro_fetcher.get_macro_data(limit=50)
                                if macro_data and not all(
                                    df.empty for df in macro_data.values()
                                ):
                                    macro_features = (
                                        self.macro_fetcher.calculate_macro_features(
                                            macro_data
                                        )
                                    )

                                    # æš—å·è³‡ç”£ãƒ‡ãƒ¼ã‚¿ã¨ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ã®æ™‚é–“è»¸ã‚’åˆã‚ã›ã‚‹
                                    if isinstance(
                                        df.index, pd.DatetimeIndex
                                    ) and isinstance(
                                        macro_features.index, pd.DatetimeIndex
                                    ):
                                        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’çµ±ä¸€
                                        if df.index.tz is None:
                                            df.index = df.index.tz_localize("UTC")
                                        if macro_features.index.tz is None:
                                            macro_features.index = (
                                                macro_features.index.tz_localize("UTC")
                                            )
                                        elif macro_features.index.tz != df.index.tz:
                                            macro_features.index = (
                                                macro_features.index.tz_convert("UTC")
                                            )

                                        # æ”¹è‰¯ã•ã‚ŒãŸãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼šæ—¥æ¬¡â†’æ™‚é–“è¶³ã¸ã®å¤‰æ›
                                        macro_hourly = macro_features.resample(
                                            "H"
                                        ).ffill()

                                        # æš—å·è³‡ç”£ãƒ‡ãƒ¼ã‚¿ã®æ™‚é–“ç¯„å›²ã«åˆã‚ã›ã¦åˆ¶é™
                                        start_time = df.index.min()
                                        end_time = df.index.max()
                                        macro_hourly = macro_hourly.loc[
                                            start_time:end_time
                                        ]

                                        # ã‚ˆã‚ŠæŸ”è»Ÿãªãƒãƒƒãƒãƒ³ã‚°ï¼šæœ€ã‚‚è¿‘ã„æ™‚åˆ»ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                                        macro_cols = [
                                            "dxy_level",
                                            "dxy_change",
                                            "dxy_zscore",
                                            "dxy_strength",
                                            "treasury_10y_level",
                                            "treasury_10y_change",
                                            "treasury_10y_zscore",
                                            "treasury_regime",
                                            "yield_curve_spread",
                                            "risk_sentiment",
                                            # USD/JPYç‚ºæ›¿ç‰¹å¾´é‡è¿½åŠ 
                                            "usdjpy_level",
                                            "usdjpy_change",
                                            "usdjpy_volatility",
                                            "usdjpy_zscore",
                                            "usdjpy_trend",
                                            "usdjpy_strength",
                                        ]
                                        added_features = 0

                                        for i, timestamp in enumerate(df.index):
                                            # æœ€ã‚‚è¿‘ã„ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’æ¤œç´¢
                                            if len(macro_hourly) > 0:
                                                closest_idx = (
                                                    macro_hourly.index.get_indexer(
                                                        [timestamp], method="ffill"
                                                    )[0]
                                                )
                                                if closest_idx >= 0:
                                                    macro_row = macro_hourly.iloc[
                                                        closest_idx
                                                    ]
                                                    for col in macro_cols:
                                                        if col in macro_row.index:
                                                            df.loc[timestamp, col] = (
                                                                macro_row[col]
                                                            )
                                                    added_features = i + 1

                                        logger.info(
                                            f"Added DXY/macro features to "
                                            f"{added_features}/{len(df)} data points"
                                        )
                                    else:
                                        logger.warning(
                                            "Index type mismatch for "
                                            "macro data alignment"
                                        )
                                else:
                                    logger.warning("No macro data available")
                            else:
                                logger.warning("Macro fetcher not initialized")
                        except Exception as e:
                            logger.warning("Failed to add macro features: %s", e)

                    # Funding Rate & Open Interest ç‰¹å¾´é‡ï¼ˆBitbankä¿¡ç”¨å–å¼•å°‚ç”¨ä»£æ›¿å®Ÿè£…ï¼‰
                    elif base in ["funding", "oi"]:
                        try:
                            # Bitbankä¿¡ç”¨å–å¼•ï¼ˆ1å€ãƒ¬ãƒãƒ¬ãƒƒã‚¸ï¼‰ã«é©ã—ãŸä»£æ›¿ç‰¹å¾´é‡ï¼ˆ17ç‰¹å¾´é‡ï¼‰
                            logger.info(
                                "Adding Bitbank margin trading alternative features"
                            )

                            # 1. é‡‘åˆ©ã‚³ã‚¹ãƒˆæ¨å®šç‰¹å¾´é‡ï¼ˆFunding Rateä»£æ›¿ï¼‰
                            # ä¾¡æ ¼å¤‰å‹•ç‡ã‹ã‚‰é‡‘åˆ©ã‚³ã‚¹ãƒˆã‚’æ¨å®šï¼ˆä¿¡ç”¨å–å¼•ã®å€Ÿå…¥ã‚³ã‚¹ãƒˆï¼‰
                            returns = df["close"].pct_change()
                            df["fr_rate"] = (
                                returns.rolling(20).std() * 100
                            )  # å¤‰å‹•ç‡ã‚’ã‚³ã‚¹ãƒˆä»£æ›¿
                            df["fr_change_1d"] = (
                                df["fr_rate"].pct_change(24) * 100
                            )  # 1æ—¥å¤‰åŒ–ç‡
                            df["fr_change_3d"] = (
                                df["fr_rate"].pct_change(72) * 100
                            )  # 3æ—¥å¤‰åŒ–ç‡

                            # 2. ä¿¡ç”¨å–å¼•ãƒªã‚¹ã‚¯æŒ‡æ¨™ï¼ˆFunding Z-Scoreä»£æ›¿ï¼‰
                            # ä¾¡æ ¼å¤‰å‹•ç‡ã®Z-Scoreï¼ˆä¿¡ç”¨å–å¼•ãƒªã‚¹ã‚¯ï¼‰
                            vol_ma_7 = returns.rolling(7 * 24).std()
                            vol_std_7 = (
                                returns.rolling(7 * 24).std().rolling(7 * 24).std()
                            )
                            df["fr_zscore_7d"] = (
                                returns.rolling(24).std() - vol_ma_7
                            ) / vol_std_7.replace(0, 1)

                            vol_ma_30 = returns.rolling(30 * 24).std()
                            vol_std_30 = (
                                returns.rolling(30 * 24).std().rolling(30 * 24).std()
                            )
                            df["fr_zscore_30d"] = (
                                returns.rolling(24).std() - vol_ma_30
                            ) / vol_std_30.replace(0, 1)

                            # 3. å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šï¼ˆä¿¡ç”¨å–å¼•ãƒªã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹ï¼‰
                            current_vol = returns.rolling(24).std()
                            df["fr_regime"] = 0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šä¸­ç«‹
                            df.loc[
                                current_vol > vol_ma_30 + vol_std_30, "fr_regime"
                            ] = 1  # é«˜ãƒªã‚¹ã‚¯
                            df.loc[
                                current_vol < vol_ma_30 - vol_std_30, "fr_regime"
                            ] = -1  # ä½ãƒªã‚¹ã‚¯

                            # 4. æ¥µç«¯å€¤æ¤œçŸ¥ï¼ˆä¿¡ç”¨å–å¼•ãƒªã‚¹ã‚¯è»¢æ›ã‚·ã‚°ãƒŠãƒ«ï¼‰
                            vol_q95 = current_vol.rolling(60 * 24).quantile(0.95)
                            vol_q05 = current_vol.rolling(60 * 24).quantile(0.05)
                            df["fr_extreme_long"] = (current_vol > vol_q95).astype(int)
                            df["fr_extreme_short"] = (current_vol < vol_q05).astype(int)

                            # 5. ä¿¡ç”¨å–å¼•ã‚³ã‚¹ãƒˆæ³¢å‹•ç‡ï¼ˆãƒªã‚¹ã‚¯æŒ‡æ¨™ï¼‰
                            df["fr_volatility"] = (
                                current_vol.rolling(7 * 24).std() * 100
                            )

                            # 6. ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼ˆä¿¡ç”¨å–å¼•æ–¹å‘æ€§ï¼‰
                            price_sma_3 = df["close"].rolling(3 * 24).mean()
                            price_sma_10 = df["close"].rolling(10 * 24).mean()
                            df["fr_trend_strength"] = (
                                (price_sma_3 - price_sma_10)
                                / price_sma_10.replace(0, 1)
                                * 100
                            )

                            # 7. ãƒã‚¸ã‚·ãƒ§ãƒ³è¦æ¨¡æ¨å®šï¼ˆOpen Interestä»£æ›¿ï¼‰
                            # å‡ºæ¥é«˜Ã—ä¾¡æ ¼ã§ä¿¡ç”¨å–å¼•è¦æ¨¡ã‚’æ¨å®š
                            position_size = df["volume"] * df["close"]
                            oi_ma_30 = position_size.rolling(30 * 24).mean()
                            df["oi_normalized"] = (
                                position_size / oi_ma_30.replace(0, 1)
                            ) - 1

                            # 8. ãƒã‚¸ã‚·ãƒ§ãƒ³å¤‰åŒ–ç‡ï¼ˆOIå¤‰åŒ–ç‡ä»£æ›¿ï¼‰
                            df["oi_change_1d"] = position_size.pct_change(24) * 100
                            df["oi_momentum_3d"] = position_size.pct_change(72) * 100

                            # 9. ãƒã‚¸ã‚·ãƒ§ãƒ³è¦æ¨¡Z-Scoreï¼ˆOI Z-Scoreä»£æ›¿ï¼‰
                            pos_ma_7 = position_size.rolling(7 * 24).mean()
                            pos_std_7 = position_size.rolling(7 * 24).std()
                            df["oi_zscore_7d"] = (
                                position_size - pos_ma_7
                            ) / pos_std_7.replace(0, 1)

                            # 10. ãƒã‚¸ã‚·ãƒ§ãƒ³æ–°é«˜å€¤ãƒ»æ–°å®‰å€¤ï¼ˆOIæ–°é«˜å€¤ãƒ»æ–°å®‰å€¤ä»£æ›¿ï¼‰
                            pos_max_30 = position_size.rolling(30 * 24).max()
                            pos_min_30 = position_size.rolling(30 * 24).min()
                            df["oi_new_high"] = (
                                position_size >= pos_max_30 * 0.98
                            ).astype(int)
                            df["oi_new_low"] = (
                                position_size <= pos_min_30 * 1.02
                            ).astype(int)

                            # 11. ä¿¡ç”¨å–å¼•åå‘æŒ‡æ¨™ï¼ˆãƒã‚¸ã‚·ãƒ§ãƒ³åå‘æŒ‡æ¨™ä»£æ›¿ï¼‰
                            # é‡‘åˆ©ã‚³ã‚¹ãƒˆã¨ãƒã‚¸ã‚·ãƒ§ãƒ³è¦æ¨¡ã®è¤‡åˆæŒ‡æ¨™
                            fr_abs = df["fr_rate"].abs()
                            oi_abs = df["oi_normalized"].abs()
                            df["position_bias"] = fr_abs * oi_abs

                            # æ¬ æå€¤å‡¦ç†
                            funding_cols = [
                                "fr_rate",
                                "fr_change_1d",
                                "fr_change_3d",
                                "fr_zscore_7d",
                                "fr_zscore_30d",
                                "fr_regime",
                                "fr_extreme_long",
                                "fr_extreme_short",
                                "fr_volatility",
                                "fr_trend_strength",
                                "oi_normalized",
                                "oi_change_1d",
                                "oi_momentum_3d",
                                "oi_zscore_7d",
                                "oi_new_high",
                                "oi_new_low",
                                "position_bias",
                            ]

                            for col in funding_cols:
                                if col in df.columns:
                                    df[col] = df[col].bfill().ffill().fillna(0)
                                    df[col] = df[col].replace(
                                        [float("inf"), float("-inf")], 0
                                    )

                                    # ç•°å¸¸å€¤ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                                    q975 = df[col].quantile(0.975)
                                    q025 = df[col].quantile(0.025)
                                    if (
                                        pd.notna(q975)
                                        and pd.notna(q025)
                                        and q975 != q025
                                    ):
                                        df[col] = df[col].clip(lower=q025, upper=q975)

                            logger.info(
                                f"Added {len(funding_cols)} Bitbank margin features"
                            )

                        except Exception as e:
                            logger.error(
                                "Failed to add Bitbank margin alternative features: %s",
                                e,
                            )
                            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§17ç‰¹å¾´é‡ã‚’è¿½åŠ 
                            default_cols = [
                                "fr_rate",
                                "fr_change_1d",
                                "fr_change_3d",
                                "fr_zscore_7d",
                                "fr_zscore_30d",
                                "fr_regime",
                                "fr_extreme_long",
                                "fr_extreme_short",
                                "fr_volatility",
                                "fr_trend_strength",
                                "oi_normalized",
                                "oi_change_1d",
                                "oi_momentum_3d",
                                "oi_zscore_7d",
                                "oi_new_high",
                                "oi_new_low",
                                "position_bias",
                            ]
                            for col in default_cols:
                                if col not in df.columns:
                                    df[col] = 0
                            logger.warning(
                                "Used default values for Bitbank margin features"
                            )

                    # Phase B2.5: Fear&Greedç‰¹å¾´é‡ã¯æ—¢ã«ExternalDataIntegratorã§å‡¦ç†æ¸ˆã¿
                    elif base in ["fear_greed", "fg"]:
                        try:
                            logger.info(
                                f"ğŸ” Processing Fear&Greed features: "
                                f"fear_greed_enabled={self.fear_greed_enabled}, "
                                f"fetcher={self.fear_greed_fetcher is not None}"
                            )

                            fg_features = None

                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰Fear&Greedãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå„ªå…ˆï¼‰
                            cached_fg = self._get_cached_external_data(
                                "fear_greed", df.index
                            )
                            if not cached_fg.empty:
                                logger.info(
                                    f"âœ… Cached Fear&Greed: {len(cached_fg)} records"
                                )
                                fg_features = cached_fg

                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç©ºã®å ´åˆã€Fear&Greedãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã§ç›´æ¥å–å¾—
                            if fg_features is None or fg_features.empty:
                                if self.fear_greed_fetcher:
                                    logger.info("ğŸ” Fetching fresh Fear&Greed data...")
                                    try:
                                        fg_data = (
                                            self.fear_greed_fetcher.get_fear_greed_data(
                                                days_back=30
                                            )
                                        )
                                        if not fg_data.empty:
                                            logger.info(
                                                f"âœ… Fear&Greed: {len(fg_data)} records"
                                            )
                                            calc_func = (
                                                self.fear_greed_fetcher.calculate_fear_greed_features  # noqa: E501
                                            )
                                            fg_features = calc_func(fg_data)
                                        else:
                                            logger.warning(
                                                "âŒ Fear&Greed data empty from fetcher"
                                            )
                                    except Exception as e:
                                        logger.error(
                                            f"âŒ Fear&Greed fetching failed: {e}"
                                        )
                                else:
                                    logger.warning(
                                        "âŒ Fear&Greed fetcher not available"
                                    )

                            # Fear&Greedãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããŸå ´åˆã®å‡¦ç†
                            if fg_features is not None and not fg_features.empty:
                                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯å¾“æ¥ã®æ–¹æ³•
                                fg_data = self.fear_greed_fetcher.get_fear_greed_data(
                                    days_back=30
                                )
                                if not fg_data.empty:
                                    fg_features = self.fear_greed_fetcher.calculate_fear_greed_features(  # noqa: E501
                                        fg_data
                                    )

                                    # VIXã¨ã®ç›¸é–¢ç‰¹å¾´é‡ã‚‚è¿½åŠ 
                                    if self.vix_fetcher:
                                        try:
                                            vix_data = self.vix_fetcher.get_vix_data(
                                                timeframe="1d"
                                            )
                                            if not vix_data.empty:
                                                vix_fg_correlation = self.fear_greed_fetcher.get_vix_correlation_features(  # noqa: E501
                                                    fg_data, vix_data
                                                )
                                                if not vix_fg_correlation.empty:
                                                    fg_features = pd.concat(
                                                        [
                                                            fg_features,
                                                            vix_fg_correlation,
                                                        ],
                                                        axis=1,
                                                    )
                                                    logger.debug(
                                                        "Added VIX-FG correlation features"  # noqa: E501
                                                    )
                                        except Exception as e:
                                            logger.warning(
                                                f"Failed to add VIX-FG correlation: {e}"
                                            )

                                    # æš—å·è³‡ç”£ãƒ‡ãƒ¼ã‚¿ã¨Fear & Greedãƒ‡ãƒ¼ã‚¿ã®æ™‚é–“è»¸ã‚’åˆã‚ã›ã‚‹
                                    if isinstance(
                                        df.index, pd.DatetimeIndex
                                    ) and isinstance(
                                        fg_features.index, pd.DatetimeIndex
                                    ):
                                        # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ãªã®ã§ã€æš—å·è³‡ç”£ã®æ™‚é–“è»¸ã«åˆã‚ã›ã¦ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                                        fg_resampled = fg_features.resample(
                                            "1h"
                                        ).ffill()

                                        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆã‚ã›ã¦çµåˆ
                                        common_index = df.index.intersection(
                                            fg_resampled.index
                                        )
                                        logger.debug(
                                            f"Fear & Greed alignment: crypto {len(df)}, fg {len(fg_resampled)}, common {len(common_index)}"  # noqa: E501
                                        )

                                        # å°ã•ãªãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ã®å ´åˆã¯æœ€æ–°ã®Fear & Greedãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                                        if len(common_index) == 0 and len(df) > 0:
                                            # Fear & Greedãƒ‡ãƒ¼ã‚¿ãŒæœŸé–“å¤–ã®å ´åˆã€æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã§å…¨è¡Œã‚’åŸ‹ã‚ã‚‹
                                            logger.warning(
                                                "Fear & Greed data period mismatch - using latest available data"  # noqa: E501
                                            )

                                            # æœ€æ–°ã®Fear & Greedãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                                            if not fg_resampled.empty:
                                                latest_fg_data = fg_resampled.iloc[
                                                    -1
                                                ]  # æœ€æ–°è¡Œ
                                                for col in fg_features.columns:
                                                    if col in latest_fg_data.index:
                                                        df[col] = latest_fg_data[col]
                                                logger.debug(
                                                    f"Filled all {len(df)} rows with latest Fear & Greed data"  # noqa: E501
                                                )
                                            else:
                                                # Fear & Greedãƒ‡ãƒ¼ã‚¿ãŒå…¨ããªã„å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åŸ‹ã‚ã‚‹
                                                logger.warning(
                                                    "No Fear & Greed data available - using default values"  # noqa: E501
                                                )
                                                for col in fg_features.columns:
                                                    if col == "fg_level":
                                                        df[col] = 50  # ä¸­ç«‹å€¤
                                                    elif col == "fg_regime":
                                                        df[col] = 3  # ä¸­ç«‹ãƒ¬ã‚¸ãƒ¼ãƒ 
                                                    else:
                                                        df[col] = 0  # ãã®ä»–ã¯0
                                        elif len(common_index) > 0:
                                            # Fear & Greedç‰¹å¾´é‡ã‚’è¿½åŠ 
                                            for col in fg_features.columns:
                                                if col in fg_resampled.columns:
                                                    df.loc[common_index, col] = (
                                                        fg_resampled.loc[
                                                            common_index, col
                                                        ]
                                                    )

                                            logger.debug(
                                                f"Added Fear & Greed features: {len(common_index)} data points aligned"  # noqa: E501
                                            )
                                        else:
                                            logger.warning(
                                                "Could not align Fear & Greed data with crypto data"  # noqa: E501
                                            )
                                    else:
                                        logger.warning(
                                            "Index type mismatch for Fear & Greed data alignment"  # noqa: E501
                                        )
                                else:
                                    logger.warning(
                                        "No Fear & Greed data available - adding default Fear & Greed features"  # noqa: E501
                                    )
                                    # Fear & Greedãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‰¹å¾´é‡ã‚’è¿½åŠ 
                                    from crypto_bot.data.fear_greed_fetcher import (
                                        get_available_fear_greed_features,
                                    )

                                    fg_feature_names = (
                                        get_available_fear_greed_features()
                                    )

                                    for col in fg_feature_names:
                                        if col not in df.columns:
                                            if col == "fg_level":
                                                df[col] = 50  # ä¸­ç«‹å€¤
                                            elif col == "fg_regime":
                                                df[col] = 3  # ä¸­ç«‹ãƒ¬ã‚¸ãƒ¼ãƒ 
                                            else:
                                                df[col] = 0  # ãã®ä»–ã¯0
                                    logger.debug(
                                        f"Added {len(fg_feature_names)} default Fear & Greed features"  # noqa: E501
                                    )
                            else:
                                logger.warning(
                                    "Fear & Greed fetcher not initialized - adding default Fear & Greed features"  # noqa: E501
                                )
                                # FetcherãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã‚‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‰¹å¾´é‡ã‚’è¿½åŠ 
                                from crypto_bot.data.fear_greed_fetcher import (
                                    get_available_fear_greed_features,
                                )

                                fg_feature_names = get_available_fear_greed_features()

                                for col in fg_feature_names:
                                    if col not in df.columns:
                                        if col == "fg_level":
                                            df[col] = 50  # ä¸­ç«‹å€¤
                                        elif col == "fg_regime":
                                            df[col] = 3  # ä¸­ç«‹ãƒ¬ã‚¸ãƒ¼ãƒ 
                                        else:
                                            df[col] = 0  # ãã®ä»–ã¯0
                                logger.debug(
                                    f"Added {len(fg_feature_names)} default Fear & Greed features"  # noqa: E501
                                )
                        except Exception as e:
                            logger.warning("Failed to add Fear & Greed features: %s", e)
                            logger.warning(
                                "Adding default Fear & Greed features due to error"
                            )
                            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‰¹å¾´é‡ã‚’è¿½åŠ ã—ã¦ç‰¹å¾´é‡æ•°ã‚’ä¸€è‡´ã•ã›ã‚‹
                            try:
                                from crypto_bot.data.fear_greed_fetcher import (
                                    get_available_fear_greed_features,
                                )

                                fg_feature_names = get_available_fear_greed_features()

                                for col in fg_feature_names:
                                    if col not in df.columns:
                                        if col == "fg_level":
                                            df[col] = 50  # ä¸­ç«‹å€¤
                                        elif col == "fg_regime":
                                            df[col] = 3  # ä¸­ç«‹ãƒ¬ã‚¸ãƒ¼ãƒ 
                                        else:
                                            df[col] = 0  # ãã®ä»–ã¯0
                                logger.debug(
                                    f"Added {len(fg_feature_names)} default Fear & Greed features after error"  # noqa: E501
                                )
                            except Exception as inner_e:
                                logger.error(
                                    f"Failed to add default Fear & Greed features: {inner_e}"  # noqa: E501
                                )

                    # mochipoyo_long_signal or mochipoyo_short_signal
                    elif (
                        feat_lc == "mochipoyo_long_signal"
                        or feat_lc == "mochipoyo_short_signal"
                    ):
                        if mochipoyo_signals is not None:
                            for signal_col in mochipoyo_signals.columns:
                                if signal_col not in df:
                                    df[signal_col] = mochipoyo_signals[signal_col]

                    # momentum_14 ç‰¹å¾´é‡
                    elif feat_lc == "momentum_14":
                        df["momentum_14"] = df["close"].pct_change(14).fillna(0)

                    # trend_strength ç‰¹å¾´é‡
                    elif feat_lc == "trend_strength":
                        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ã‚’è¨ˆç®—ï¼ˆADXãƒ™ãƒ¼ã‚¹ï¼‰
                        try:
                            import pandas_ta as ta

                            adx_result = ta.adx(
                                high=df["high"],
                                low=df["low"],
                                close=df["close"],
                                length=14,
                            )

                            if adx_result is not None and not adx_result.empty:
                                # ADXã®çµæœã¯DataFrameãªã®ã§ã€ADXåˆ—ã‚’å–å¾—
                                if (
                                    isinstance(adx_result, pd.DataFrame)
                                    and "ADX_14" in adx_result.columns
                                ):
                                    df["trend_strength"] = adx_result["ADX_14"].fillna(
                                        25
                                    )
                                elif isinstance(adx_result, pd.Series):
                                    df["trend_strength"] = adx_result.fillna(25)
                                else:
                                    df["trend_strength"] = 25
                            else:
                                df["trend_strength"] = 25  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                        except Exception as e:
                            logger.warning(f"Failed to calculate trend_strength: {e}")
                            df["trend_strength"] = 25  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

                    # volatility_regime ç‰¹å¾´é‡ (5ç‰¹å¾´é‡)
                    elif feat_lc == "volatility_regime":
                        try:
                            # è¤‡æ•°æœŸé–“ã§ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†æ
                            volatility_windows = [10, 20, 50]

                            for window in volatility_windows:
                                # rollingæ¨™æº–åå·®ã‚’è¨ˆç®—
                                vol = df["close"].rolling(window=window).std()

                                # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ ã‚’3æ®µéšã«åˆ†é¡ (0:ä½, 1:ä¸­, 2:é«˜)
                                vol_regime = pd.cut(vol, bins=3, labels=[0, 1, 2])
                                df[f"vol_regime_{window}"] = pd.to_numeric(
                                    vol_regime, errors="coerce"
                                ).fillna(1)

                                # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç™¾åˆ†ä½æ•°
                                df[f"vol_percentile_{window}"] = (
                                    vol.rolling(window=100, min_periods=window)
                                    .rank(pct=True)
                                    .fillna(0.5)
                                )

                            # çŸ­æœŸvsé•·æœŸãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¯”ç‡
                            short_vol = df["close"].rolling(window=10).std()
                            long_vol = df["close"].rolling(window=50).std()
                            df["vol_ratio_short_long"] = (short_vol / long_vol).fillna(
                                1.0
                            )

                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate volatility_regime: {e}"
                            )
                            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
                            for window in [10, 20, 50]:
                                df[f"vol_regime_{window}"] = 1
                                df[f"vol_percentile_{window}"] = 0.5
                            df["vol_ratio_short_long"] = 1.0

                    # momentum_signals ç‰¹å¾´é‡ (7ç‰¹å¾´é‡)
                    elif feat_lc == "momentum_signals":
                        try:
                            # è¤‡æ•°æœŸé–“ã®ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ä¿¡å·
                            momentum_periods = [1, 3, 7, 14, 21, 30]

                            for period in momentum_periods:
                                df[f"momentum_{period}"] = (
                                    df["close"].pct_change(period).fillna(0)
                                )

                            # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ åæŸãƒ»ç™ºæ•£æŒ‡æ¨™
                            mom_short = df["close"].pct_change(3)
                            mom_long = df["close"].pct_change(21)
                            df["momentum_convergence"] = (mom_short - mom_long).fillna(
                                0
                            )

                        except Exception as e:
                            logger.warning(f"Failed to calculate momentum_signals: {e}")
                            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
                            for period in [1, 3, 7, 14, 21, 30]:
                                df[f"momentum_{period}"] = 0
                            df["momentum_convergence"] = 0

                    # liquidity_indicators ç‰¹å¾´é‡ (8ç‰¹å¾´é‡)
                    elif feat_lc == "liquidity_indicators":
                        try:
                            # Amihudæµå‹•æ€§æŒ‡æ¨™ï¼ˆéæµå‹•æ€§åº¦ï¼‰
                            price_change = abs(df["close"].pct_change())
                            volume_scaled = (
                                df["volume"] / df["volume"].rolling(window=20).mean()
                            )
                            df["amihud_illiquidity"] = (
                                price_change / (volume_scaled + 1e-8)
                            ).fillna(0)

                            # ä¾¡æ ¼ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆæŒ‡æ¨™
                            df["price_impact"] = (
                                (df["high"] - df["low"]) / (df["volume"] + 1e-8)
                            ).fillna(0)

                            # ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒ»ãƒ—ãƒ©ã‚¤ã‚¹ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆVPTï¼‰
                            df["vpt"] = (
                                (df["volume"] * df["close"].pct_change())
                                .cumsum()
                                .fillna(0)
                            )

                            # å‡ºæ¥é«˜ç›¸å¯¾å¼·åº¦
                            df["volume_strength"] = (
                                df["volume"] / df["volume"].rolling(window=20).mean()
                            ).fillna(1.0)

                            # æµå‹•æ€§æ¯æ¸‡æŒ‡æ¨™
                            volume_ma = df["volume"].rolling(window=10).mean()
                            volume_std = df["volume"].rolling(window=10).std()
                            df["liquidity_drought"] = (
                                (volume_ma - df["volume"]) / (volume_std + 1e-8)
                            ).fillna(0)

                            # ãƒ“ãƒƒãƒ‰ãƒ»ã‚¢ã‚¹ã‚¯ä»£ç†æŒ‡æ¨™ï¼ˆé«˜å€¤-å®‰å€¤ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ï¼‰
                            typical_price = (df["high"] + df["low"] + df["close"]) / 3
                            df["spread_proxy"] = (
                                (df["high"] - df["low"]) / typical_price
                            ).fillna(0)

                            # å‡ºæ¥é«˜åŠ é‡å¹³å‡ä¾¡æ ¼ã‹ã‚‰ã®ä¹–é›¢
                            vwap = (typical_price * df["volume"]).rolling(
                                window=20
                            ).sum() / df["volume"].rolling(window=20).sum()
                            df["vwap_deviation"] = ((df["close"] - vwap) / vwap).fillna(
                                0
                            )

                            # æ³¨æ–‡ä¸å‡è¡¡ä»£ç†æŒ‡æ¨™
                            df["order_imbalance_proxy"] = (
                                (df["close"] - df["open"])
                                / (df["high"] - df["low"] + 1e-8)
                            ).fillna(0)

                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate liquidity_indicators: {e}"
                            )
                            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
                            liquidity_features = [
                                "amihud_illiquidity",
                                "price_impact",
                                "vpt",
                                "volume_strength",
                                "liquidity_drought",
                                "spread_proxy",
                                "vwap_deviation",
                                "order_imbalance_proxy",
                            ]
                            for feat in liquidity_features:
                                df[feat] = 0

                    # Phase 3.2A: è¿½åŠ ATRæœŸé–“ï¼ˆatr_7, atr_21ï¼‰
                    elif base == "atr" and period and period != self.feat_period:
                        atr_calc = self.ind_calc.atr(df, window=period)
                        if isinstance(atr_calc, pd.Series):
                            df[f"atr_{period}"] = atr_calc
                        else:
                            df[f"atr_{period}"] = atr_calc.iloc[:, 0]

                    # Phase 3.2B: ä¾¡æ ¼ãƒã‚¸ã‚·ãƒ§ãƒ³æŒ‡æ¨™ï¼ˆ5ç‰¹å¾´é‡ï¼‰
                    elif feat_lc == "price_position":
                        try:
                            # ä¾¡æ ¼ãƒ¬ãƒ³ã‚¸å†…ã®ä½ç½®ï¼ˆ%Kã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
                            high_20 = df["high"].rolling(20).max()
                            low_20 = df["low"].rolling(20).min()
                            df["price_position_20"] = (
                                (df["close"] - low_20) / (high_20 - low_20 + 1e-8)
                            ).fillna(0.5)

                            # ç•°ãªã‚‹æœŸé–“ã§ã®ä¾¡æ ¼ä½ç½®
                            high_50 = df["high"].rolling(50).max()
                            low_50 = df["low"].rolling(50).min()
                            df["price_position_50"] = (
                                (df["close"] - low_50) / (high_50 - low_50 + 1e-8)
                            ).fillna(0.5)

                            # ç§»å‹•å¹³å‡ã‹ã‚‰ã®ä½ç½®
                            sma_20 = df["close"].rolling(20).mean()
                            df["price_vs_sma20"] = (
                                (df["close"] - sma_20) / sma_20
                            ).fillna(0)

                            # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰å†…ä½ç½®ï¼ˆ%Bï¼‰
                            bb_middle = df["close"].rolling(20).mean()
                            bb_std = df["close"].rolling(20).std()
                            bb_upper = bb_middle + (bb_std * 2)
                            bb_lower = bb_middle - (bb_std * 2)
                            df["bb_position"] = (
                                (df["close"] - bb_lower) / (bb_upper - bb_lower + 1e-8)
                            ).fillna(0.5)

                            # æ—¥ä¸­ãƒ¬ãƒ³ã‚¸å†…ä½ç½®
                            df["intraday_position"] = (
                                (df["close"] - df["low"])
                                / (df["high"] - df["low"] + 1e-8)
                            ).fillna(0.5)

                        except Exception as e:
                            logger.warning(f"Failed to calculate price_position: {e}")
                            for i in range(5):
                                df[f"price_pos_{i}"] = 0.5

                    # Phase 3.2B: ãƒ­ãƒ¼ã‚½ã‚¯è¶³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ4ç‰¹å¾´é‡ï¼‰
                    elif feat_lc == "candle_patterns":
                        try:
                            # ãƒ‰ã‚¸ï¼ˆåå­—ç·šï¼‰
                            body_size = abs(df["close"] - df["open"])
                            candle_range = df["high"] - df["low"]
                            df["doji"] = (
                                body_size / (candle_range + 1e-8) < 0.1
                            ).astype(int)

                            # ãƒãƒ³ãƒãƒ¼/ãƒãƒ³ã‚®ãƒ³ã‚°ãƒãƒ³
                            upper_shadow = df["high"] - np.maximum(
                                df["open"], df["close"]
                            )
                            lower_shadow = (
                                np.minimum(df["open"], df["close"]) - df["low"]
                            )
                            df["hammer"] = (
                                (lower_shadow > body_size * 2)
                                & (upper_shadow < body_size * 0.5)
                            ).astype(int)

                            # ã‚¨ãƒ³ã‚´ãƒ«ãƒ•ã‚£ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³
                            prev_body = abs(df["close"].shift(1) - df["open"].shift(1))
                            curr_body = abs(df["close"] - df["open"])
                            df["engulfing"] = (
                                (curr_body > prev_body * 1.5)
                                & (df["close"] > df["open"])  # ç¾åœ¨ã®ãƒ­ãƒ¼ã‚½ã‚¯ãŒé™½ç·š
                                & (
                                    df["close"].shift(1) < df["open"].shift(1)
                                )  # å‰ã®ãƒ­ãƒ¼ã‚½ã‚¯ãŒé™°ç·š
                            ).astype(int)

                            # ãƒ”ãƒ³ãƒãƒ¼ï¼ˆä¸Šãƒ’ã‚²ãƒ»ä¸‹ãƒ’ã‚²ï¼‰
                            df["pinbar"] = (
                                (upper_shadow > body_size * 3)
                                | (lower_shadow > body_size * 3)
                            ).astype(int)

                        except Exception as e:
                            logger.warning(f"Failed to calculate candle_patterns: {e}")
                            for pattern in ["doji", "hammer", "engulfing", "pinbar"]:
                                df[pattern] = 0

                    # Phase 3.2B: ã‚µãƒãƒ¼ãƒˆãƒ»ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆ3ç‰¹å¾´é‡ï¼‰
                    elif feat_lc == "support_resistance":
                        try:
                            # ã‚µãƒãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«ã‹ã‚‰ã®è·é›¢
                            support_level = df["low"].rolling(50).min()
                            df["support_distance"] = (
                                (df["close"] - support_level) / support_level
                            ).fillna(0)

                            # ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«ã‹ã‚‰ã®è·é›¢
                            resistance_level = df["high"].rolling(50).max()
                            df["resistance_distance"] = (
                                (resistance_level - df["close"]) / resistance_level
                            ).fillna(0)

                            # ã‚µãƒãƒ¼ãƒˆãƒ»ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«ã®å¼·åº¦
                            support_tests = (
                                df["low"]
                                .rolling(10)
                                .apply(lambda x: (x <= x.min() * 1.02).sum())
                            )
                            df["support_strength"] = support_tests.fillna(0)

                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate support_resistance: {e}"
                            )
                            for sr in [
                                "support_distance",
                                "resistance_distance",
                                "support_strength",
                            ]:
                                df[sr] = 0

                    # Phase 3.2B: ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã‚·ã‚°ãƒŠãƒ«ï¼ˆ3ç‰¹å¾´é‡ï¼‰
                    elif feat_lc == "breakout_signals":
                        try:
                            # ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ
                            volume_ma = df["volume"].rolling(20).mean()
                            price_change = abs(df["close"].pct_change())
                            df["volume_breakout"] = (
                                (df["volume"] > volume_ma * 2)
                                & (
                                    price_change
                                    > price_change.rolling(20).quantile(0.8)
                                )
                            ).astype(int)

                            # ä¾¡æ ¼ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆï¼ˆãƒ¬ãƒ³ã‚¸ä¸Šæ”¾ã‚Œï¼‰
                            high_20 = df["high"].rolling(20).max()
                            df["price_breakout_up"] = (
                                df["close"] > high_20.shift(1)
                            ).astype(int)

                            # ä¾¡æ ¼ãƒ–ãƒ¬ã‚¤ã‚¯ãƒ€ã‚¦ãƒ³ï¼ˆãƒ¬ãƒ³ã‚¸ä¸‹æ”¾ã‚Œï¼‰
                            low_20 = df["low"].rolling(20).min()
                            df["price_breakout_down"] = (
                                df["close"] < low_20.shift(1)
                            ).astype(int)

                        except Exception as e:
                            logger.warning(f"Failed to calculate breakout_signals: {e}")
                            for bo in [
                                "volume_breakout",
                                "price_breakout_up",
                                "price_breakout_down",
                            ]:
                                df[bo] = 0

                    # Phase 3.2C: è‡ªå·±ç›¸é–¢ï¼ˆ5ç‰¹å¾´é‡ï¼‰
                    elif feat_lc == "autocorrelation":
                        try:
                            returns = df["close"].pct_change()
                            for lag in [1, 5, 10, 20, 50]:
                                df[f"autocorr_lag_{lag}"] = (
                                    returns.rolling(100).apply(
                                        lambda x: (
                                            x.autocorr(lag=lag) if len(x) > lag else 0
                                        )
                                    )
                                ).fillna(0)
                        except Exception as e:
                            logger.warning(f"Failed to calculate autocorrelation: {e}")
                            for lag in [1, 5, 10, 20, 50]:
                                df[f"autocorr_lag_{lag}"] = 0

                    # Phase 3.2C: å­£ç¯€æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ4ç‰¹å¾´é‡ï¼‰
                    elif feat_lc == "seasonal_patterns":
                        try:
                            if isinstance(df.index, pd.DatetimeIndex):
                                # æ›œæ—¥åŠ¹æœ
                                df["weekday_effect"] = df.index.dayofweek.astype(float)
                                # æ™‚é–“å¸¯åŠ¹æœ
                                df["hour_effect"] = df.index.hour.astype(float)
                                # æœˆåˆæœˆæœ«åŠ¹æœ
                                df["month_day_effect"] = df.index.day.astype(float)
                                # ã‚¢ã‚¸ã‚¢æ™‚é–“ãƒ»æ¬§ç±³æ™‚é–“åŒºåˆ†
                                asia_hours = (
                                    (df.index.hour >= 0) & (df.index.hour < 8)
                                ).astype(int)
                                df["asia_session"] = asia_hours
                            else:
                                for feat in [
                                    "weekday_effect",
                                    "hour_effect",
                                    "month_day_effect",
                                    "asia_session",
                                ]:
                                    df[feat] = 0
                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate seasonal_patterns: {e}"
                            )
                            for feat in [
                                "weekday_effect",
                                "hour_effect",
                                "month_day_effect",
                                "asia_session",
                            ]:
                                df[feat] = 0

                    # Phase 3.2C: ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡ºï¼ˆ3ç‰¹å¾´é‡ï¼‰
                    elif feat_lc == "regime_detection":
                        try:
                            returns = df["close"].pct_change()
                            vol = returns.rolling(20).std()

                            # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ 
                            vol_threshold = vol.rolling(100).quantile(0.8)
                            df["high_vol_regime"] = (vol > vol_threshold).astype(int)

                            # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¬ã‚¸ãƒ¼ãƒ ï¼ˆä¸Šæ˜‡ãƒ»ä¸‹é™ãƒ»æ¨ªã°ã„ï¼‰
                            price_ma_short = df["close"].rolling(10).mean()
                            price_ma_long = df["close"].rolling(50).mean()
                            trend_strength = (
                                price_ma_short - price_ma_long
                            ) / price_ma_long
                            df["trend_regime"] = np.where(
                                trend_strength > 0.02,
                                1,  # ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰
                                np.where(
                                    trend_strength < -0.02, -1, 0
                                ),  # ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰ vs æ¨ªã°ã„
                            )

                            # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ¬ã‚¸ãƒ¼ãƒ 
                            momentum = returns.rolling(14).mean()
                            df["momentum_regime"] = np.where(
                                momentum > momentum.rolling(100).quantile(0.8),
                                1,
                                np.where(
                                    momentum < momentum.rolling(100).quantile(0.2),
                                    -1,
                                    0,
                                ),
                            )

                        except Exception as e:
                            logger.warning(f"Failed to calculate regime_detection: {e}")
                            for regime in [
                                "high_vol_regime",
                                "trend_regime",
                                "momentum_regime",
                            ]:
                                df[regime] = 0

                    # Phase 3.2C: ã‚µã‚¤ã‚¯ãƒ«åˆ†æï¼ˆ3ç‰¹å¾´é‡ï¼‰
                    elif feat_lc == "cycle_analysis":
                        try:
                            # ä¾¡æ ¼ã‚µã‚¤ã‚¯ãƒ«ï¼ˆãƒ‡ãƒˆãƒ¬ãƒ³ãƒ‰ä¾¡æ ¼ï¼‰
                            price_ma = df["close"].rolling(50).mean()
                            df["price_cycle"] = (
                                (df["close"] - price_ma) / price_ma
                            ).fillna(0)

                            # ãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚µã‚¤ã‚¯ãƒ«
                            volume_ma = df["volume"].rolling(50).mean()
                            df["volume_cycle"] = (
                                (df["volume"] - volume_ma) / volume_ma
                            ).fillna(0)

                            # RSIã‚µã‚¤ã‚¯ãƒ«ï¼ˆéè²·ã„éå£²ã‚Šã‚µã‚¤ã‚¯ãƒ«ï¼‰
                            rsi = self.ind_calc.rsi(df["close"], window=14)
                            df["rsi_cycle"] = ((rsi - 50) / 50).fillna(0)

                        except Exception as e:
                            logger.warning(f"Failed to calculate cycle_analysis: {e}")
                            for cycle in ["price_cycle", "volume_cycle", "rsi_cycle"]:
                                df[cycle] = 0

                    # Phase 3.2D: ã‚¯ãƒ­ã‚¹ç›¸é–¢ï¼ˆ5ç‰¹å¾´é‡ï¼‰
                    elif feat_lc == "cross_correlation":
                        try:
                            returns = df["close"].pct_change()
                            volume_returns = df["volume"].pct_change()

                            # ä¾¡æ ¼-ãƒœãƒªãƒ¥ãƒ¼ãƒ ç›¸é–¢
                            for window in [10, 20, 50]:
                                df[f"price_volume_corr_{window}"] = (
                                    returns.rolling(window).corr(volume_returns)
                                ).fillna(0)

                            # é«˜å€¤-ä½å€¤ç›¸é–¢ï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ§‹é€ ï¼‰
                            high_returns = df["high"].pct_change()
                            low_returns = df["low"].pct_change()
                            df["high_low_corr_20"] = (
                                high_returns.rolling(20).corr(low_returns)
                            ).fillna(0)

                            # ä¾¡æ ¼ãƒ©ã‚°ç›¸é–¢ï¼ˆè‡ªå·±ç›¸é–¢ç°¡æ˜“ç‰ˆï¼‰
                            df["price_lag_corr"] = (
                                returns.rolling(30).corr(returns.shift(1))
                            ).fillna(0)

                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate cross_correlation: {e}"
                            )
                            corr_features = [
                                "price_volume_corr_10",
                                "price_volume_corr_20",
                                "price_volume_corr_50",
                                "high_low_corr_20",
                                "price_lag_corr",
                            ]
                            for feat in corr_features:
                                df[feat] = 0

                    # Phase 3.2D: ç›¸å¯¾å¼·åº¦ï¼ˆ5ç‰¹å¾´é‡ï¼‰
                    elif feat_lc == "relative_strength":
                        try:
                            # çŸ­æœŸ vs é•·æœŸç›¸å¯¾å¼·åº¦
                            short_ma = df["close"].rolling(10).mean()
                            long_ma = df["close"].rolling(50).mean()
                            df["short_long_strength"] = (
                                (short_ma - long_ma) / long_ma
                            ).fillna(0)

                            # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç›¸å¯¾å¼·åº¦
                            volume_short = df["volume"].rolling(10).mean()
                            volume_long = df["volume"].rolling(50).mean()
                            df["volume_relative_strength"] = (
                                (volume_short - volume_long) / volume_long
                            ).fillna(0)

                            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç›¸å¯¾å¼·åº¦
                            vol_short = df["close"].pct_change().rolling(10).std()
                            vol_long = df["close"].pct_change().rolling(50).std()
                            df["volatility_relative_strength"] = (
                                (vol_short - vol_long) / vol_long
                            ).fillna(0)

                            # ä¾¡æ ¼å‹•å‹‰ç›¸å¯¾å¼·åº¦
                            momentum_short = df["close"].pct_change(5)
                            momentum_long = df["close"].pct_change(20)
                            df["momentum_relative_strength"] = (
                                momentum_short - momentum_long
                            ).fillna(0)

                            # RSIç›¸å¯¾å¼·åº¦
                            rsi_14 = self.ind_calc.rsi(df["close"], window=14)
                            rsi_7 = self.ind_calc.rsi(df["close"], window=7)
                            df["rsi_relative_strength"] = (rsi_7 - rsi_14).fillna(0)

                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate relative_strength: {e}"
                            )
                            rs_features = [
                                "short_long_strength",
                                "volume_relative_strength",
                                "volatility_relative_strength",
                                "momentum_relative_strength",
                                "rsi_relative_strength",
                            ]
                            for feat in rs_features:
                                df[feat] = 0

                    # Phase 3.2D: ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰åˆ†æï¼ˆ5ç‰¹å¾´é‡ï¼‰
                    elif feat_lc == "spread_analysis":
                        try:
                            # é«˜å€¤-ä½å€¤ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰
                            hl_spread = (df["high"] - df["low"]) / df["close"]
                            df["hl_spread"] = hl_spread.fillna(0)
                            df["hl_spread_ma"] = hl_spread.rolling(20).mean().fillna(0)

                            # å§‹å€¤-çµ‚å€¤ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰
                            oc_spread = abs(df["open"] - df["close"]) / df["close"]
                            df["oc_spread"] = oc_spread.fillna(0)

                            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ï¼ˆçŸ­æœŸ vs é•·æœŸï¼‰
                            vol_short = df["close"].pct_change().rolling(5).std()
                            vol_long = df["close"].pct_change().rolling(20).std()
                            df["volatility_spread"] = (vol_short - vol_long).fillna(0)

                            # ã‚¿ã‚¤ãƒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ï¼ˆé€£ç¶šæœŸé–“ã®ä¾¡æ ¼å·®ï¼‰
                            df["time_spread"] = (
                                df["close"] - df["close"].shift(24)
                            ).fillna(
                                0
                            )  # 24æ™‚é–“å‰ã¨ã®å·®

                        except Exception as e:
                            logger.warning(f"Failed to calculate spread_analysis: {e}")
                            spread_features = [
                                "hl_spread",
                                "hl_spread_ma",
                                "oc_spread",
                                "volatility_spread",
                                "time_spread",
                            ]
                            for feat in spread_features:
                                df[feat] = 0

                    else:
                        logger.warning(f"Unknown extra feature spec: {feat} - skipping")

                except Exception as e:
                    logger.error("Failed to add extra feature %s: %s", feat, e)
                    raise  # å¤±æ•—ã—ãŸã‚‰æ¡ã‚Šã¤ã¶ã•ãšåœæ­¢ã™ã‚‹
            logger.debug("After extra feats: %s", df.shape)

        # 6. æ¬ æå†è£œå®Œï¼‹0åŸ‹ã‚
        df = df.ffill().fillna(0)

        # 7. ç„¡é™å¤§å€¤ãƒ»ç•°å¸¸å€¤ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        # ç„¡é™å¤§å€¤ã‚’å‰ã®æœ‰åŠ¹å€¤ã§ç½®æ›ã€ãã‚Œã‚‚ç„¡ã„å ´åˆã¯0
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.ffill().fillna(0)

        # ç•°å¸¸ã«å¤§ããªå€¤ã‚’ã‚¯ãƒªãƒƒãƒ—ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢ï¼‰
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            # 99.9%ileä»¥ä¸Šã®å€¤ã‚’ã‚¯ãƒªãƒƒãƒ—
            upper_bound = df[col].quantile(0.999)
            lower_bound = df[col].quantile(0.001)

            if np.isfinite(upper_bound) and np.isfinite(lower_bound):
                df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

        logger.debug("Final features shape after cleaning: %s", df.shape)

        # ãƒ‡ãƒ¼ã‚¿å“è³ªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå‹ç‡æ”¹å–„ã®ãŸã‚ã®é‡è¦ãªã‚¹ãƒ†ãƒƒãƒ—ï¼‰
        try:
            from crypto_bot.ml.data_quality_manager import DataQualityManager

            quality_manager = DataQualityManager(self.config)

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±ï¼‰
            metadata = {
                "feature_sources": {},
                "external_data_enabled": {
                    "vix": self.vix_enabled,
                    "macro": self.macro_enabled,
                    "fear_greed": self.fear_greed_enabled,
                    "funding": self.funding_enabled,
                },
            }

            # å„ç‰¹å¾´é‡ã®ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¨˜éŒ²
            for column in df.columns:
                vix_prefixes = ["vix_", "dxy_", "treasury_"]
                if any(column.startswith(prefix) for prefix in vix_prefixes):
                    source_type = "api" if self.macro_enabled else "default"
                    metadata["feature_sources"][column] = {"source_type": source_type}
                elif any(column.startswith(prefix) for prefix in ["fear_greed", "fg_"]):
                    source_type = "api" if self.fear_greed_enabled else "default"
                    metadata["feature_sources"][column] = {"source_type": source_type}
                elif any(column.startswith(prefix) for prefix in ["fr_", "oi_"]):
                    # Bitbankä»£æ›¿ç‰¹å¾´é‡
                    metadata["feature_sources"][column] = {"source_type": "calculated"}
                else:
                    metadata["feature_sources"][column] = {"source_type": "calculated"}

            # ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼
            quality_passed, quality_report = quality_manager.validate_data_quality(
                df, metadata
            )

            if not quality_passed:
                logger.warning(f"Data quality check failed: {quality_report}")

                # ãƒ‡ãƒ¼ã‚¿å“è³ªæ”¹å–„ã‚’è©¦è¡Œ
                df_improved, improvement_report = quality_manager.improve_data_quality(
                    df, metadata
                )
                df = df_improved

                logger.info(f"Data quality improvement applied: {improvement_report}")
            else:
                score = quality_report["quality_score"]
                logger.info(f"Data quality check passed: score={score:.1f}")

        except Exception as e:
            logger.warning(
                f"Data quality management failed: {e} - continuing with original data"
            )

        # Phase B2.4: ãƒãƒƒãƒå‡¦ç†å®Œäº†å¾Œã®æœ€çµ‚å‡¦ç†
        if self.batch_engines_enabled:
            logger.info("ğŸ”„ Batch processing completed - performing final validation")

        # 151ç‰¹å¾´é‡ã®ç¢ºå®Ÿãªä¿è¨¼ï¼ˆæœ€çµ‚ãƒã‚§ãƒƒã‚¯ï¼‰
        from crypto_bot.ml.feature_defaults import ensure_feature_consistency

        df = ensure_feature_consistency(df, target_count=151)
        logger.info(f"Final guaranteed feature count: {len(df.columns)}")

        return df


def build_ml_pipeline(config: Dict[str, Any]) -> Pipeline:
    """
    sklearnãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ï¼ˆç‰¹å¾´é‡ç”Ÿæˆâ†’æ¨™æº–åŒ–ï¼‰ã€‚
    ç©ºã®DataFrameã®å ´åˆã¯ã€ç‰¹å¾´é‡ç”Ÿæˆã®ã¿ã‚’è¡Œã„ã€æ¨™æº–åŒ–ã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚
    """

    class EmptyDataFrameScaler(BaseEstimator, TransformerMixin):
        """ç©ºã®DataFrameã®å ´åˆã®ãƒ€ãƒŸãƒ¼ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼"""

        def fit(self, X, y=None):
            return self

        def transform(self, X):
            return X

    class EmptyDataFramePipeline(Pipeline):
        """ç©ºã®DataFrameã®å ´åˆã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

        def fit_transform(self, X, y=None, **fit_params):
            if X.empty:
                # ç©ºã®DataFrameã®å ´åˆã¯ã€ç‰¹å¾´é‡ç”Ÿæˆã®ã¿ã‚’è¡Œã„ã€æ¨™æº–åŒ–ã¯ã‚¹ã‚­ãƒƒãƒ—
                features = self.named_steps["features"].transform(X)
                # DataFrameã‚’numpy.ndarrayã«å¤‰æ›
                return features.values
            return super().fit_transform(X, y, **fit_params)

        def transform(self, X):
            if X.empty:
                # ç©ºã®DataFrameã®å ´åˆã¯ã€ç‰¹å¾´é‡ç”Ÿæˆã®ã¿ã‚’è¡Œã„ã€æ¨™æº–åŒ–ã¯ã‚¹ã‚­ãƒƒãƒ—
                features = self.named_steps["features"].transform(X)
                # DataFrameã‚’numpy.ndarrayã«å¤‰æ›
                return features.values
            return super().transform(X)

    return EmptyDataFramePipeline(
        [
            ("features", FeatureEngineer(config)),
            (
                "scaler",
                (
                    EmptyDataFrameScaler()
                    if config.get("skip_scaling", False)
                    else StandardScaler()
                ),
            ),
        ]
    )


def prepare_ml_dataset(
    df: pd.DataFrame, config: Dict[str, Any]
) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
    """
    æ©Ÿæ¢°å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã€‚
    æˆ»ã‚Šå€¤: Xï¼ˆç‰¹å¾´é‡ï¼‰, y_regï¼ˆå›å¸°ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰, y_clfï¼ˆåˆ†é¡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰

    - å¿…è¦ãªã¶ã‚“ã ã‘æœ€åˆã®è¡Œã‚’ãƒ‰ãƒ­ãƒƒãƒ—ï¼ˆrolling/lagsï¼‰
    - horizon, thresholdã¯config["ml"]ã‹ã‚‰å–å¾—
    """
    logger.info(f"prepare_ml_dataset input df shape: {df.shape}")
    pipeline = build_ml_pipeline(config)
    X_arr = pipeline.fit_transform(df)

    logger.info(f"Pipeline output type: {type(X_arr)}")
    if hasattr(X_arr, "shape"):
        shape_info = X_arr.shape
    elif hasattr(X_arr, "__len__"):
        shape_info = len(X_arr)
    else:
        shape_info = "no len"

    logger.info(f"Pipeline output shape/len: {shape_info}")

    # X_arrãŒlistã®å ´åˆã¯numpy arrayã«å¤‰æ›
    if isinstance(X_arr, list):
        logger.warning(
            f"Pipeline returned list with {len(X_arr)} elements, converting to numpy array"  # noqa: E501
        )
        import numpy as np

        try:
            X_arr = np.array(X_arr)
        except Exception as e:
            logger.error(f"Failed to convert list to numpy array: {e}")
            # If conversion fails, return the list directly for debugging
            return X_arr

    # ----- ç›®çš„å¤‰æ•°ç”Ÿæˆ -----
    horizon = config["ml"]["horizon"]
    thresh = config["ml"].get("threshold", 0.0)
    y_reg = make_regression_target(df, horizon).rename(f"return_{horizon}")
    y_clf = make_classification_target(df, horizon, thresh).rename(f"up_{horizon}")

    # ----- è¡Œæ•°ã‚’æƒãˆã‚‹ -----
    win = config["ml"]["rolling_window"]
    lags = config["ml"]["lags"]
    drop_n = win + max(lags) if lags else win

    idx = df.index[drop_n:]
    X = pd.DataFrame(X_arr[drop_n:], index=idx)

    return X, y_reg.loc[idx], y_clf.loc[idx]
