# =============================================================================
# ãƒ•ã‚¡ã‚¤ãƒ«å: crypto_bot/ml/preprocessor.py
# èª¬æ˜Ž:
# æ©Ÿæ¢°å­¦ç¿’ç”¨ã®ç‰¹å¾´é‡ç”Ÿæˆãƒ»å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã€‚
# - OHLCV DataFrame ã‹ã‚‰æ©Ÿæ¢°å­¦ç¿’ç”¨ç‰¹å¾´é‡ã‚’ç”Ÿæˆ
# - sklearn Pipelineäº’æ›ã® FeatureEngineer ã‚’æä¾›
# - build_ml_pipeline() ã§ç‰¹å¾´é‡ï¼‹æ¨™æº–åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç”Ÿæˆ
# - prepare_ml_dataset() ã§ç‰¹å¾´é‡ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã‚’ä¸€æ‹¬ç”Ÿæˆ
#
# â€» ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨ã§ã¯ãªãã€MLStrategyã‚„å­¦ç¿’/æŽ¨è«–ç³»ã§ä½¿ç”¨
# =============================================================================

from __future__ import annotations

import logging
from typing import Any, Dict, Tuple

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

try:
    from crypto_bot.data.vix_fetcher import VIXDataFetcher

    VIX_AVAILABLE = True
except ImportError:
    VIXDataFetcher = None
    VIX_AVAILABLE = False

try:
    from crypto_bot.data.macro_fetcher import MacroDataFetcher

    MACRO_AVAILABLE = True
except ImportError:
    MacroDataFetcher = None
    MACRO_AVAILABLE = False

try:
    from crypto_bot.data.funding_fetcher import FundingDataFetcher

    FUNDING_AVAILABLE = True
except ImportError:
    FundingDataFetcher = None
    FUNDING_AVAILABLE = False

try:
    from crypto_bot.data.fear_greed_fetcher import FearGreedDataFetcher

    FEAR_GREED_AVAILABLE = True
except ImportError:
    FearGreedDataFetcher = None
    FEAR_GREED_AVAILABLE = False

from crypto_bot.indicator.calculator import IndicatorCalculator
from crypto_bot.ml.target import make_classification_target, make_regression_target

logger = logging.getLogger(__name__)


def calc_rci(series: pd.Series, period: int) -> pd.Series:
    """
    Rank Correlation Indexï¼ˆRCIï¼‰ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    :param series: çµ‚å€¤ãªã©ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆpd.Seriesï¼‰
    :param period: æœŸé–“
    :return: RCIã®pd.Series
    """
    n = period

    def _rci(x):
        price_ranks = pd.Series(x).rank(ascending=False)
        date_ranks = np.arange(1, n + 1)
        d = price_ranks.values - date_ranks
        return (1 - 6 * (d**2).sum() / (n * (n**2 - 1))) * 100

    return series.rolling(window=n).apply(_rci, raw=False)


class FeatureEngineer(BaseEstimator, TransformerMixin):
    """
    OHLCVã‹ã‚‰å„ç¨®ç‰¹å¾´é‡ã‚’ç”Ÿæˆã™ã‚‹ sklearnäº’æ›ã‚¯ãƒ©ã‚¹ã€‚

    å…¥åŠ›: OHLCV DataFrameï¼ˆindexã¯tz-aware DatetimeIndexï¼‰
    å‡ºåŠ›: ç‰¹å¾´é‡DataFrame

    - æ¬ æè£œå®Œ
    - ATR, lag, rollingçµ±è¨ˆ, extra_featureså¯¾å¿œ
    - ffill/0åŸ‹ã‚ç­‰ã‚‚å®Ÿæ–½
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.ind_calc = IndicatorCalculator()
        self.extra_features = self.config["ml"].get("extra_features", [])

        # MLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        ml_config = self.config["ml"]
        self.feat_period = ml_config.get("feat_period", 14)
        self.lags = ml_config.get("lags", [1, 2, 3])
        self.rolling_window = ml_config.get("rolling_window", 14)

        # VIXçµ±åˆè¨­å®šï¼ˆå¼·åˆ¶åˆæœŸåŒ–ç‰ˆï¼‰
        logger.info(f"ðŸ” VIX Debug: extra_features={self.extra_features}")
        logger.info(f"ðŸ” VIX Debug: VIX_AVAILABLE={VIX_AVAILABLE}")
        vix_in_features = "vix" in self.extra_features
        logger.info(f"ðŸ” VIX Debug: vix_in_features={vix_in_features}")

        # VIXå¾©æ´»å®Ÿè£…ï¼šè¨­å®šå¯¾å¿œãƒ»è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½
        if vix_in_features:
            try:
                if VIX_AVAILABLE and VIXDataFetcher:
                    self.vix_fetcher = VIXDataFetcher(self.config)
                    self.vix_enabled = True
                    logger.info(
                        "âœ… VIX fetcher initialized successfully (config-aware)"
                    )
                else:
                    # VIXDataFetcherã‚’ç›´æŽ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦åˆæœŸåŒ–ã‚’å¼·åˆ¶
                    from crypto_bot.data.vix_fetcher import (
                        VIXDataFetcher as DirectVIXFetcher,
                    )

                    self.vix_fetcher = DirectVIXFetcher(self.config)
                    self.vix_enabled = True
                    logger.info(
                        "âœ… VIX fetcher initialized with direct import (config-aware)"
                    )
            except Exception as e:
                logger.error(f"âŒ VIX fetcher initialization failed: {e}")
                self.vix_fetcher = None
                self.vix_enabled = False
        else:
            self.vix_enabled = False
            self.vix_fetcher = None
            logger.info(f"âš ï¸ VIX not in extra_features: {self.extra_features}")

        # ãƒžã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿çµ±åˆè¨­å®šï¼ˆå¼·åˆ¶åˆæœŸåŒ–ç‰ˆï¼‰
        macro_in_features = any(
            feat in self.extra_features for feat in ["dxy", "macro", "treasury"]
        )
        logger.info(f"ðŸ” Macro Debug: macro_in_features={macro_in_features}")

        if macro_in_features:
            try:
                if MACRO_AVAILABLE and MacroDataFetcher:
                    self.macro_fetcher = MacroDataFetcher()
                    self.macro_enabled = True
                    logger.info("âœ… Macro fetcher initialized successfully (forced)")
                else:
                    # MacroDataFetcherã‚’ç›´æŽ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦åˆæœŸåŒ–ã‚’å¼·åˆ¶
                    from crypto_bot.data.macro_fetcher import (
                        MacroDataFetcher as DirectMacroFetcher,
                    )

                    self.macro_fetcher = DirectMacroFetcher()
                    self.macro_enabled = True
                    logger.info("âœ… Macro fetcher initialized with direct import")
            except Exception as e:
                logger.error(f"âŒ Macro fetcher initialization failed: {e}")
                self.macro_fetcher = None
                self.macro_enabled = False
        else:
            self.macro_enabled = False
            self.macro_fetcher = None
            logger.info(
                f"âš ï¸ Macro features not in extra_features: {self.extra_features}"
            )

        # Funding Rateçµ±åˆè¨­å®šï¼ˆBitbankå°‚ç”¨ï¼šç¾ç‰©å–å¼•ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
        self.funding_enabled = False
        self.funding_fetcher = None

        # Bitbankç¾ç‰©å–å¼•ã§ã¯ä»£æ›¿ç‰¹å¾´é‡ã‚’ä½¿ç”¨
        self.funding_alternative_enabled = any(
            feat in self.extra_features for feat in ["funding", "oi"]
        )

        # Fear & Greedå¾©æ´»å®Ÿè£…ï¼šè¨­å®šå¯¾å¿œãƒ»è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½
        fear_greed_in_features = any(
            feat in self.extra_features for feat in ["fear_greed", "fg"]
        )
        logger.info(
            f"ðŸ” Fear&Greed Debug: fear_greed_in_features={fear_greed_in_features}"
        )

        if fear_greed_in_features:
            try:
                if FEAR_GREED_AVAILABLE and FearGreedDataFetcher:
                    self.fear_greed_fetcher = FearGreedDataFetcher(self.config)
                    self.fear_greed_enabled = True
                    logger.info(
                        "âœ… Fear&Greed fetcher initialized successfully (config-aware)"
                    )
                else:
                    # FearGreedDataFetcherã‚’ç›´æŽ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦åˆæœŸåŒ–ã‚’å¼·åˆ¶
                    from crypto_bot.data.fear_greed_fetcher import (
                        FearGreedDataFetcher as DirectFGFetcher,
                    )

                    self.fear_greed_fetcher = DirectFGFetcher(self.config)
                    self.fear_greed_enabled = True
                    logger.info(
                        "âœ… Fear&Greed fetcher initialized with direct import "
                        "(config-aware)"
                    )
            except Exception as e:
                logger.error(f"âŒ Fear&Greed fetcher initialization failed: {e}")
                self.fear_greed_fetcher = None
                self.fear_greed_enabled = False
        else:
            self.fear_greed_enabled = False
            self.fear_greed_fetcher = None
            logger.info(f"âš ï¸ Fear&Greed not in extra_features: {self.extra_features}")

        # USD/JPYç‚ºæ›¿ãƒ‡ãƒ¼ã‚¿çµ±åˆè¨­å®šï¼ˆå¼·åˆ¶åˆæœŸåŒ–ç‰ˆï¼‰
        forex_in_features = any(
            feat in self.extra_features for feat in ["forex", "usdjpy", "jpy"]
        )
        logger.info(f"ðŸ” Forex Debug: forex_in_features={forex_in_features}")

        if forex_in_features:
            try:
                # MacroDataFetcherã‚’ç‚ºæ›¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å†åˆ©ç”¨
                if MACRO_AVAILABLE and MacroDataFetcher:
                    self.forex_fetcher = MacroDataFetcher()
                    self.forex_enabled = True
                    logger.info("âœ… Forex fetcher initialized successfully (forced)")
                else:
                    # MacroDataFetcherã‚’ç›´æŽ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦åˆæœŸåŒ–ã‚’å¼·åˆ¶
                    from crypto_bot.data.macro_fetcher import (
                        MacroDataFetcher as DirectForexFetcher,
                    )

                    self.forex_fetcher = DirectForexFetcher()
                    self.forex_enabled = True
                    logger.info("âœ… Forex fetcher initialized with direct import")
            except Exception as e:
                logger.error(f"âŒ Forex fetcher initialization failed: {e}")
                self.forex_fetcher = None
                self.forex_enabled = False
        else:
            self.forex_enabled = False
            self.forex_fetcher = None
            logger.info(f"âš ï¸ Forex not in extra_features: {self.extra_features}")

    def _get_cached_external_data(
        self, data_type: str, time_index: pd.DatetimeIndex
    ) -> pd.DataFrame:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Parameters
        ----------
        data_type : str
            ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ— ('vix', 'macro', 'forex', 'fear_greed', 'funding')
        time_index : pd.DatetimeIndex
            å¯¾è±¡æœŸé–“ã®ã‚¿ã‚¤ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

        Returns
        -------
        pd.DataFrame
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ï¼ˆè©²å½“æœŸé–“ï¼‰
        """
        try:
            from crypto_bot.ml.external_data_cache import get_global_cache

            cache = get_global_cache()
            if not cache.is_initialized:
                return pd.DataFrame()

            start_time = time_index.min()
            end_time = time_index.max()

            cached_data = cache.get_period_data(data_type, start_time, end_time)
            return cached_data

        except Exception as e:
            logger.debug(f"Failed to get cached {data_type} data: {e}")
            return pd.DataFrame()

    def _validate_external_data_fetchers(self) -> dict:
        """
        å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã®çŠ¶æ…‹ã‚’æ¤œè¨¼ã—ã€ãƒ‡ãƒ¼ã‚¿å“è³ªæ”¹å–„ã®ãŸã‚ã®æƒ…å ±ã‚’è¿”ã™

        Returns
        -------
        dict
            ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã®çŠ¶æ…‹ãƒ¬ãƒãƒ¼ãƒˆ
        """
        validation_report = {
            "vix": {"available": False, "initialized": False, "working": False},
            "macro": {"available": False, "initialized": False, "working": False},
            "forex": {"available": False, "initialized": False, "working": False},
            "fear_greed": {"available": False, "initialized": False, "working": False},
            "total_working": 0,
            "external_data_success_rate": 0.0,
        }

        # VIXæ¤œè¨¼
        if "vix" in self.extra_features:
            validation_report["vix"]["available"] = True
            if self.vix_fetcher is not None:
                validation_report["vix"]["initialized"] = True
                try:
                    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆå–å¾—
                    test_data = self.vix_fetcher.get_vix_data(timeframe="1d", limit=1)
                    if test_data is not None and not test_data.empty:
                        validation_report["vix"]["working"] = True
                        validation_report["total_working"] += 1
                        logger.info("âœ… VIX fetcher validation: WORKING")
                    else:
                        logger.warning(
                            "âš ï¸ VIX fetcher validation: NOT WORKING (empty data)"
                        )
                except Exception as e:
                    logger.error(f"âŒ VIX fetcher validation failed: {e}")

        # Macroæ¤œè¨¼
        if any(feat in self.extra_features for feat in ["dxy", "macro", "treasury"]):
            validation_report["macro"]["available"] = True
            if self.macro_fetcher is not None:
                validation_report["macro"]["initialized"] = True
                try:
                    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆå–å¾—
                    test_data = self.macro_fetcher.get_macro_data()
                    if test_data and not all(df.empty for df in test_data.values()):
                        validation_report["macro"]["working"] = True
                        validation_report["total_working"] += 1
                        logger.info("âœ… Macro fetcher validation: WORKING")
                    else:
                        logger.warning(
                            "âš ï¸ Macro fetcher validation: NOT WORKING (empty data)"
                        )
                except Exception as e:
                    logger.error(f"âŒ Macro fetcher validation failed: {e}")

        # Fear&Greedæ¤œè¨¼
        if any(feat in self.extra_features for feat in ["fear_greed", "fg"]):
            validation_report["fear_greed"]["available"] = True
            if self.fear_greed_fetcher is not None:
                validation_report["fear_greed"]["initialized"] = True
                try:
                    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆå–å¾—
                    test_data = self.fear_greed_fetcher.get_fear_greed_data(days_back=1)
                    if test_data is not None and not test_data.empty:
                        validation_report["fear_greed"]["working"] = True
                        validation_report["total_working"] += 1
                        logger.info("âœ… Fear&Greed fetcher validation: WORKING")
                    else:
                        logger.warning(
                            "âš ï¸ Fear&Greed fetcher validation: NOT WORKING (empty data)"
                        )
                except Exception as e:
                    logger.error(f"âŒ Fear&Greed fetcher validation failed: {e}")

        # Forexæ¤œè¨¼
        if any(feat in self.extra_features for feat in ["forex", "usdjpy", "jpy"]):
            validation_report["forex"]["available"] = True
            if self.forex_fetcher is not None:
                validation_report["forex"]["initialized"] = True
                try:
                    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆå–å¾—
                    test_data = self.forex_fetcher.get_macro_data()
                    if (
                        test_data
                        and "usdjpy" in test_data
                        and not test_data["usdjpy"].empty
                    ):
                        validation_report["forex"]["working"] = True
                        validation_report["total_working"] += 1
                        logger.info("âœ… Forex fetcher validation: WORKING")
                    else:
                        logger.warning(
                            "âš ï¸ Forex fetcher validation: NOT WORKING (empty data)"
                        )
                except Exception as e:
                    logger.error(f"âŒ Forex fetcher validation failed: {e}")

        # æˆåŠŸçŽ‡è¨ˆç®—
        total_available = sum(
            1
            for fetcher in validation_report.values()
            if isinstance(fetcher, dict) and fetcher.get("available", False)
        )
        if total_available > 0:
            validation_report["external_data_success_rate"] = (
                validation_report["total_working"] / total_available
            )

        logger.info(
            f"ðŸ” External data validation: "
            f"{validation_report['total_working']}/{total_available} fetchers working "
            f"({validation_report['external_data_success_rate']*100:.1f}% success rate)"
        )
        return validation_report

    def fit(self, X: pd.DataFrame, y=None):
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        logger.debug("Input DataFrame shape: %s", X.shape)

        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã®çŠ¶æ…‹æ¤œè¨¼ï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªæ”¹å–„ï¼‰
        validation_report = self._validate_external_data_fetchers()
        logger.info(
            f"ðŸ” External data fetcher status: "
            f"{validation_report['total_working']} working, "
            f"{validation_report['external_data_success_rate']*100:.1f}% success rate"
        )

        if X.empty:
            feat_period = self.config["ml"]["feat_period"]
            win = self.config["ml"]["rolling_window"]
            lags = self.config["ml"]["lags"]
            columns = ["ATR_" + str(feat_period)]
            columns.extend([f"close_lag_{lag}" for lag in lags])
            columns.extend([f"close_mean_{win}", f"close_std_{win}"])
            for feat in self.extra_features:
                feat_lc = feat.lower()
                base, _, param = feat_lc.partition("_")
                period = int(param) if param.isdigit() else None
                if base == "rsi" and period:
                    columns.append(f"rsi_{period}")
                elif base == "ema" and period:
                    columns.append(f"ema_{period}")
                elif base == "sma" and period:
                    columns.append(f"sma_{period}")
                elif base == "macd":
                    columns.extend(["macd", "macd_signal", "macd_hist"])
                elif base == "rci" and period:
                    columns.append(f"rci_{period}")
                elif base == "volume" and "zscore" in feat_lc:
                    period_str = feat_lc.split("_")[-1]
                    win_z = int(period_str) if period_str.isdigit() else win
                    columns.append(f"volume_zscore_{win_z}")
                elif feat_lc == "day_of_week":
                    columns.append("day_of_week")
                elif feat_lc == "hour_of_day":
                    columns.append("hour_of_day")
                elif feat_lc in ["mochipoyo_long_signal", "mochipoyo_short_signal"]:
                    columns.extend(["mochipoyo_long_signal", "mochipoyo_short_signal"])
            return pd.DataFrame(columns=columns)
        df = X.copy()

        # 1. æ¬ æè£œå®Œ
        df = df.ffill()
        logger.debug("After ffill: %s", df.shape)

        # 2. ATR
        feat_period = self.config["ml"]["feat_period"]
        atr = self.ind_calc.atr(df, window=feat_period)
        if isinstance(atr, pd.Series):
            df[f"ATR_{feat_period}"] = atr
        else:
            df[f"ATR_{feat_period}"] = atr.iloc[:, 0]
        logger.debug("After ATR: %s", df.shape)

        # 3. lagç‰¹å¾´é‡
        for lag in self.config["ml"]["lags"]:
            df[f"close_lag_{lag}"] = df["close"].shift(lag)
        logger.debug("After lag feats: %s", df.shape)

        # 4. rollingçµ±è¨ˆ
        win = self.config["ml"]["rolling_window"]
        df[f"close_mean_{win}"] = df["close"].rolling(win).mean()
        df[f"close_std_{win}"] = df["close"].rolling(win).std()
        logger.debug("After rolling stats: %s", df.shape)

        # 5. extra_features
        if self.extra_features:
            logger.debug("Adding extra features: %s", self.extra_features)
            # è¿½åŠ ã§mochipoyoã®ã‚·ã‚°ãƒŠãƒ«ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¸€åº¦ã¾ã¨ã‚ã¦å–å¾—ã—ã¦ãŠã
            mochipoyo_needed = any(
                feat.lower() in ["mochipoyo_long_signal", "mochipoyo_short_signal"]
                for feat in self.extra_features
            )
            mochipoyo_signals = None
            if mochipoyo_needed:
                mochipoyo_signals = self.ind_calc.mochipoyo_signals(df)

            for feat in self.extra_features:
                feat_lc = feat.lower()
                try:
                    # fear_greedã¯ç‰¹åˆ¥å‡¦ç†ï¼ˆã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’å«ã‚€è¤‡åˆèªžï¼‰
                    if feat_lc == "fear_greed":
                        base = "fear_greed"
                        param = ""
                        period = None
                    else:
                        base, _, param = feat_lc.partition("_")
                        period = int(param) if param.isdigit() else None

                    # RSI
                    if base == "rsi" and period:
                        df[f"rsi_{period}"] = self.ind_calc.rsi(
                            df["close"], window=period
                        )
                    # EMA
                    elif base == "ema" and period:
                        df[f"ema_{period}"] = self.ind_calc.ema(
                            df["close"], window=period
                        )
                    # SMA
                    elif base == "sma" and period:
                        df[f"sma_{period}"] = self.ind_calc.sma(
                            df["close"], window=period
                        )
                    # MACD
                    elif base == "macd":
                        try:
                            macd_df = self.ind_calc.macd(df["close"])
                            # åˆ—åã‚’ãƒ†ã‚¹ãƒˆã®æœŸå¾…å€¤ã«åˆã‚ã›ã‚‹
                            df["macd"] = macd_df["MACD_12_26_9"]
                            df["macd_signal"] = macd_df["MACDs_12_26_9"]
                            df["macd_hist"] = macd_df["MACDh_12_26_9"]
                        except Exception as e:
                            logger.error("Failed to add extra feature macd: %s", e)
                            raise
                    # RCI
                    elif base == "rci" and period:
                        try:
                            # ã¾ãšã¯IndicatorCalculatorã§æä¾›ã•ã‚Œã¦ã„ã‚Œã°ãã¡ã‚‰å„ªå…ˆ
                            if hasattr(self.ind_calc, "rci"):
                                df[f"rci_{period}"] = self.ind_calc.rci(
                                    df["close"], window=period
                                )
                            else:
                                df[f"rci_{period}"] = calc_rci(df["close"], period)
                        except Exception as e:
                            logger.error(
                                "Failed to add extra feature rci_%s: %s", period, e
                            )
                            raise
                    # volume_zscore
                    elif base == "volume" and "zscore" in feat_lc:
                        # volume_zscore_20 ã®ã‚ˆã†ãªå½¢å¼ã‹ã‚‰æœŸé–“ã‚’æŠ½å‡º
                        period_str = feat_lc.split("_")[-1]
                        win_z = (
                            int(period_str)
                            if period_str.isdigit()
                            else self.config["ml"]["rolling_window"]
                        )
                        vol = df["volume"]
                        df[f"volume_zscore_{win_z}"] = (
                            vol - vol.rolling(win_z).mean()
                        ) / vol.rolling(win_z).std()
                    # æ›œæ—¥ãƒ»æ™‚é–“
                    elif feat_lc == "day_of_week":
                        if isinstance(df.index, pd.DatetimeIndex):
                            df["day_of_week"] = df.index.dayofweek.astype("int8")
                        else:
                            # ç©ºã®DataFrameã‚„DatetimeIndexã§ãªã„å ´åˆã¯0ã§åŸ‹ã‚ã‚‹
                            df["day_of_week"] = 0
                    elif feat_lc == "hour_of_day":
                        if isinstance(df.index, pd.DatetimeIndex):
                            df["hour_of_day"] = df.index.hour.astype("int8")
                        else:
                            # ç©ºã®DataFrameã‚„DatetimeIndexã§ãªã„å ´åˆã¯0ã§åŸ‹ã‚ã‚‹
                            df["hour_of_day"] = 0
                    # ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹
                    elif base == "stoch":
                        try:
                            stoch_df = self.ind_calc.stochastic(df)
                            if not stoch_df.empty:
                                df["stoch_k"] = stoch_df.iloc[:, 0]
                                df["stoch_d"] = stoch_df.iloc[:, 1]
                        except Exception as e:
                            logger.warning("Failed to add stochastic: %s", e)

                    # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
                    elif base == "bb" or base == "bollinger":
                        try:
                            bb_df = self.ind_calc.bollinger_bands(df["close"])
                            if not bb_df.empty:
                                df["bb_upper"] = bb_df.iloc[:, 2]  # BBU
                                df["bb_middle"] = bb_df.iloc[:, 1]  # BBM
                                df["bb_lower"] = bb_df.iloc[:, 0]  # BBL
                                df["bb_percent"] = bb_df.iloc[:, 3]  # %B
                                df["bb_width"] = bb_df.iloc[:, 4]  # Band Width
                        except Exception as e:
                            logger.warning("Failed to add Bollinger Bands: %s", e)

                    # Williams %R
                    elif base == "willr" or base == "williams":
                        try:
                            period_willr = period if period else 14
                            willr = self.ind_calc.williams_r(df, window=period_willr)
                            df[f"willr_{period_willr}"] = willr
                        except Exception as e:
                            logger.warning("Failed to add Williams %%R: %s", e)

                    # ADX
                    elif base == "adx":
                        try:
                            adx_df = self.ind_calc.adx(df)
                            if not adx_df.empty:
                                df["adx"] = adx_df.iloc[:, 0]
                                df["di_plus"] = adx_df.iloc[:, 1]
                                df["di_minus"] = adx_df.iloc[:, 2]
                        except Exception as e:
                            logger.warning("Failed to add ADX: %s", e)

                    # ãƒãƒ£ã‚¤ã‚­ãƒ³ãƒžãƒãƒ¼ãƒ•ãƒ­ãƒ¼
                    elif base == "cmf":
                        try:
                            period_cmf = period if period else 20
                            cmf = self.ind_calc.chaikin_money_flow(
                                df, window=period_cmf
                            )
                            df[f"cmf_{period_cmf}"] = cmf
                        except Exception as e:
                            logger.warning("Failed to add CMF: %s", e)

                    # ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ 
                    elif base == "fisher":
                        try:
                            fisher_df = self.ind_calc.fisher_transform(df)
                            if not fisher_df.empty:
                                df["fisher"] = fisher_df.iloc[:, 0]
                                df["fisher_signal"] = fisher_df.iloc[:, 1]
                        except Exception as e:
                            logger.warning("Failed to add Fisher Transform: %s", e)

                    # é«˜åº¦ãªè¤‡åˆã‚·ã‚°ãƒŠãƒ«
                    elif base == "advanced" and "signals" in feat_lc:
                        try:
                            advanced_df = self.ind_calc.advanced_signals(df)
                            for col in advanced_df.columns:
                                df[col] = advanced_df[col]
                        except Exception as e:
                            logger.warning("Failed to add advanced signals: %s", e)

                    # VIXææ€–æŒ‡æ•°é–¢é€£ç‰¹å¾´é‡ï¼ˆå¼·åˆ¶å–å¾—ç‰ˆï¼‰
                    elif base == "vix":
                        try:
                            logger.info(
                                f"ðŸ” Processing VIX features: "
                                f"vix_enabled={self.vix_enabled}, "
                                f"vix_fetcher={self.vix_fetcher is not None}"
                            )

                            vix_features = None

                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰VIXãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå„ªå…ˆï¼‰
                            cached_vix = self._get_cached_external_data("vix", df.index)
                            if not cached_vix.empty:
                                logger.info(
                                    f"âœ… Using cached VIX: {len(cached_vix)} records"
                                )
                                vix_features = cached_vix

                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç©ºã®å ´åˆã€VIXãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã§ç›´æŽ¥å–å¾—
                            if vix_features is None or vix_features.empty:
                                if self.vix_fetcher:
                                    logger.info("ðŸ” Fetching fresh VIX data...")
                                    try:
                                        vix_data = self.vix_fetcher.get_vix_data(
                                            timeframe="1d", limit=100
                                        )
                                        if not vix_data.empty:
                                            logger.info(
                                                f"âœ… VIX: {len(vix_data)} records"
                                            )
                                            vix_features = (
                                                self.vix_fetcher.calculate_vix_features(
                                                    vix_data
                                                )
                                            )
                                        else:
                                            logger.warning(
                                                "âŒ VIX data empty from fetcher"
                                            )
                                    except Exception as e:
                                        logger.error(f"âŒ VIX fetching failed: {e}")
                                else:
                                    logger.warning("âŒ VIX fetcher not available")

                            # VIXãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããŸå ´åˆã®å‡¦ç†
                            if vix_features is not None and not vix_features.empty:
                                # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³çµ±ä¸€ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆæ”¹è‰¯
                                if isinstance(
                                    df.index, pd.DatetimeIndex
                                ) and isinstance(vix_features.index, pd.DatetimeIndex):
                                    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³çµ±ä¸€ï¼ˆé‡è¦ãªä¿®æ­£ï¼‰
                                    if df.index.tz is None:
                                        df.index = df.index.tz_localize("UTC")
                                    if vix_features.index.tz is None:
                                        vix_features.index = (
                                            vix_features.index.tz_localize("UTC")
                                        )
                                    elif vix_features.index.tz != df.index.tz:
                                        vix_features.index = (
                                            vix_features.index.tz_convert("UTC")
                                        )

                                    # æ”¹è‰¯ã•ã‚ŒãŸãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼šæ—¥æ¬¡â†’æ™‚é–“è¶³ã¸ã®å¤‰æ›
                                    # å‰æ–¹è£œå®Œï¼ˆffillï¼‰ã§æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚é–“è¶³ã«å±•é–‹
                                    vix_hourly = vix_features.resample("H").ffill()

                                    # æš—å·è³‡ç”£ãƒ‡ãƒ¼ã‚¿ã®æ™‚é–“ç¯„å›²ã«åˆã‚ã›ã¦åˆ¶é™
                                    start_time = df.index.min()
                                    end_time = df.index.max()
                                    vix_hourly = vix_hourly.loc[start_time:end_time]

                                    # ã‚ˆã‚ŠæŸ”è»Ÿãªãƒžãƒƒãƒãƒ³ã‚°ï¼šæœ€ã‚‚è¿‘ã„æ™‚åˆ»ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                                    vix_cols = [
                                        "vix_level",
                                        "vix_change",
                                        "vix_zscore",
                                        "fear_level",
                                        "vix_spike",
                                        "vix_regime",
                                    ]
                                    added_features = 0

                                    for i, timestamp in enumerate(df.index):
                                        # æœ€ã‚‚è¿‘ã„VIXãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’æ¤œç´¢
                                        if len(vix_hourly) > 0:
                                            closest_idx = vix_hourly.index.get_indexer(
                                                [timestamp], method="ffill"
                                            )[0]
                                            if closest_idx >= 0:
                                                vix_row = vix_hourly.iloc[closest_idx]
                                                for col in vix_cols:
                                                    if col in vix_row.index:
                                                        if col == "vix_regime":
                                                            # ã‚«ãƒ†ã‚´ãƒªã‚’æ•°å€¤ã«å¤‰æ›
                                                            regime_map = {
                                                                "low": 0,
                                                                "normal": 1,
                                                                "high": 2,
                                                                "extreme": 3,
                                                            }
                                                            k = "vix_regime_numeric"
                                                            v = regime_map.get(
                                                                vix_row[col], 1
                                                            )
                                                            df.loc[timestamp, k] = v
                                                        else:
                                                            df.loc[timestamp, col] = (
                                                                vix_row[col]
                                                            )
                                        added_features = i + 1

                                    logger.info(
                                        f"VIX: {added_features}/{len(df)} points"
                                    )
                                else:
                                    logger.warning(
                                        "Could not align VIX data - index type mismatch"
                                    )
                            else:
                                logger.warning("No VIX data available - using defaults")
                        except Exception as e:
                            logger.warning("Failed to add VIX features: %s", e)

                    # OIï¼ˆæœªæ±ºæ¸ˆå»ºçŽ‰ï¼‰é–¢é€£ç‰¹å¾´é‡
                    elif base == "oi":
                        try:
                            # from crypto_bot.data.fetcher import (
                            #     MarketDataFetcher,
                            #     OIDataFetcher,
                            # )

                            # OIãƒ‡ãƒ¼ã‚¿å–å¾—ã®ãŸã‚ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆå®Ÿéš›ã«ã¯ã‚ˆã‚Šè©³ç´°ãªå®Ÿè£…ãŒå¿…è¦ï¼‰
                            # ç¾æ™‚ç‚¹ã§ã¯åŸºæœ¬çš„ãªOIç‰¹å¾´é‡ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                            # OIå¤‰åŒ–çŽ‡ï¼ˆä¾¡æ ¼ã¨OIã®ç›¸é–¢ï¼‰
                            df["oi_price_divergence"] = (
                                df["close"].pct_change()
                                - df["volume"].pct_change().fillna(0)
                            ).fillna(0)

                            # ãƒœãƒªãƒ¥ãƒ¼ãƒ å¼·åº¦ï¼ˆOIã®ä»£æ›¿ï¼‰
                            df["volume_intensity"] = (
                                df["volume"] / df["volume"].rolling(20).mean()
                            )

                            # OIå‹¢ã„ï¼ˆãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒ™ãƒ¼ã‚¹ï¼‰
                            df["oi_momentum_proxy"] = (
                                df["volume"].rolling(10).sum()
                                / df["volume"].rolling(50).sum()
                            )

                        except Exception as e:
                            logger.warning("Failed to add OI features: %s", e)

                    # ãƒžã‚¯ãƒ­çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡ï¼ˆDXY, é‡‘åˆ©ï¼‰ï¼ˆå¼·åˆ¶å–å¾—ç‰ˆï¼‰
                    elif base in ["dxy", "macro", "treasury"]:
                        try:
                            logger.info(
                                f"ðŸ” Processing Macro features: "
                                f"macro_enabled={self.macro_enabled}, "
                                f"macro_fetcher={self.macro_fetcher is not None}"
                            )

                            macro_features = None

                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒžã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå„ªå…ˆï¼‰
                            cached_macro = self._get_cached_external_data(
                                "macro", df.index
                            )
                            if not cached_macro.empty:
                                logger.info(
                                    f"âœ… Using cached macro: {len(cached_macro)} records"
                                )
                                macro_features = cached_macro

                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç©ºã®å ´åˆã€ãƒžã‚¯ãƒ­ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã§ç›´æŽ¥å–å¾—
                            if macro_features is None or macro_features.empty:
                                if self.macro_fetcher:
                                    logger.info("ðŸ” Fetching fresh Macro data...")
                                    try:
                                        macro_data = self.macro_fetcher.get_macro_data()
                                        if macro_data and not all(
                                            df.empty for df in macro_data.values()
                                        ):
                                            logger.info(
                                                f"âœ… Macro: {len(macro_data)} datasets"
                                            )
                                            calc_func = (
                                                self.macro_fetcher.calculate_macro_features  # noqa: E501
                                            )
                                            macro_features = calc_func(macro_data)
                                        else:
                                            logger.warning(
                                                "âŒ Macro data empty from fetcher"
                                            )
                                    except Exception as e:
                                        logger.error(f"âŒ Macro fetching failed: {e}")
                                else:
                                    logger.warning("âŒ Macro fetcher not available")

                            # ãƒžã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããŸå ´åˆã®å‡¦ç†
                            if macro_features is not None and not macro_features.empty:
                                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯å¾“æ¥ã®æ–¹æ³•
                                if hasattr(df, "index") and len(df) > 0:
                                    backtest_year = df.index.min().year
                                    self.macro_fetcher._backtest_start_year = (
                                        backtest_year
                                    )
                                macro_data = self.macro_fetcher.get_macro_data(limit=50)
                                if macro_data and not all(
                                    df.empty for df in macro_data.values()
                                ):
                                    macro_features = (
                                        self.macro_fetcher.calculate_macro_features(
                                            macro_data
                                        )
                                    )

                                    # æš—å·è³‡ç”£ãƒ‡ãƒ¼ã‚¿ã¨ãƒžã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ã®æ™‚é–“è»¸ã‚’åˆã‚ã›ã‚‹
                                    if isinstance(
                                        df.index, pd.DatetimeIndex
                                    ) and isinstance(
                                        macro_features.index, pd.DatetimeIndex
                                    ):
                                        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’çµ±ä¸€
                                        if df.index.tz is None:
                                            df.index = df.index.tz_localize("UTC")
                                        if macro_features.index.tz is None:
                                            macro_features.index = (
                                                macro_features.index.tz_localize("UTC")
                                            )
                                        elif macro_features.index.tz != df.index.tz:
                                            macro_features.index = (
                                                macro_features.index.tz_convert("UTC")
                                            )

                                        # æ”¹è‰¯ã•ã‚ŒãŸãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼šæ—¥æ¬¡â†’æ™‚é–“è¶³ã¸ã®å¤‰æ›
                                        macro_hourly = macro_features.resample(
                                            "H"
                                        ).ffill()

                                        # æš—å·è³‡ç”£ãƒ‡ãƒ¼ã‚¿ã®æ™‚é–“ç¯„å›²ã«åˆã‚ã›ã¦åˆ¶é™
                                        start_time = df.index.min()
                                        end_time = df.index.max()
                                        macro_hourly = macro_hourly.loc[
                                            start_time:end_time
                                        ]

                                        # ã‚ˆã‚ŠæŸ”è»Ÿãªãƒžãƒƒãƒãƒ³ã‚°ï¼šæœ€ã‚‚è¿‘ã„æ™‚åˆ»ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                                        macro_cols = [
                                            "dxy_level",
                                            "dxy_change",
                                            "dxy_zscore",
                                            "dxy_strength",
                                            "treasury_10y_level",
                                            "treasury_10y_change",
                                            "treasury_10y_zscore",
                                            "treasury_regime",
                                            "yield_curve_spread",
                                            "risk_sentiment",
                                            # USD/JPYç‚ºæ›¿ç‰¹å¾´é‡è¿½åŠ 
                                            "usdjpy_level",
                                            "usdjpy_change",
                                            "usdjpy_volatility",
                                            "usdjpy_zscore",
                                            "usdjpy_trend",
                                            "usdjpy_strength",
                                        ]
                                        added_features = 0

                                        for i, timestamp in enumerate(df.index):
                                            # æœ€ã‚‚è¿‘ã„ãƒžã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’æ¤œç´¢
                                            if len(macro_hourly) > 0:
                                                closest_idx = (
                                                    macro_hourly.index.get_indexer(
                                                        [timestamp], method="ffill"
                                                    )[0]
                                                )
                                                if closest_idx >= 0:
                                                    macro_row = macro_hourly.iloc[
                                                        closest_idx
                                                    ]
                                                    for col in macro_cols:
                                                        if col in macro_row.index:
                                                            df.loc[timestamp, col] = (
                                                                macro_row[col]
                                                            )
                                                    added_features = i + 1

                                        logger.info(
                                            f"Added DXY/macro features to "
                                            f"{added_features}/{len(df)} data points"
                                        )
                                    else:
                                        logger.warning(
                                            "Index type mismatch for "
                                            "macro data alignment"
                                        )
                                else:
                                    logger.warning("No macro data available")
                            else:
                                logger.warning("Macro fetcher not initialized")
                        except Exception as e:
                            logger.warning("Failed to add macro features: %s", e)

                    # Funding Rate & Open Interest ç‰¹å¾´é‡ï¼ˆBitbankä¿¡ç”¨å–å¼•å°‚ç”¨ä»£æ›¿å®Ÿè£…ï¼‰
                    elif base in ["funding", "oi"]:
                        try:
                            # Bitbankä¿¡ç”¨å–å¼•ï¼ˆ1å€ãƒ¬ãƒãƒ¬ãƒƒã‚¸ï¼‰ã«é©ã—ãŸä»£æ›¿ç‰¹å¾´é‡ï¼ˆ17ç‰¹å¾´é‡ï¼‰
                            logger.info(
                                "Adding Bitbank margin trading alternative features"
                            )

                            # 1. é‡‘åˆ©ã‚³ã‚¹ãƒˆæŽ¨å®šç‰¹å¾´é‡ï¼ˆFunding Rateä»£æ›¿ï¼‰
                            # ä¾¡æ ¼å¤‰å‹•çŽ‡ã‹ã‚‰é‡‘åˆ©ã‚³ã‚¹ãƒˆã‚’æŽ¨å®šï¼ˆä¿¡ç”¨å–å¼•ã®å€Ÿå…¥ã‚³ã‚¹ãƒˆï¼‰
                            returns = df["close"].pct_change()
                            df["fr_rate"] = (
                                returns.rolling(20).std() * 100
                            )  # å¤‰å‹•çŽ‡ã‚’ã‚³ã‚¹ãƒˆä»£æ›¿
                            df["fr_change_1d"] = (
                                df["fr_rate"].pct_change(24) * 100
                            )  # 1æ—¥å¤‰åŒ–çŽ‡
                            df["fr_change_3d"] = (
                                df["fr_rate"].pct_change(72) * 100
                            )  # 3æ—¥å¤‰åŒ–çŽ‡

                            # 2. ä¿¡ç”¨å–å¼•ãƒªã‚¹ã‚¯æŒ‡æ¨™ï¼ˆFunding Z-Scoreä»£æ›¿ï¼‰
                            # ä¾¡æ ¼å¤‰å‹•çŽ‡ã®Z-Scoreï¼ˆä¿¡ç”¨å–å¼•ãƒªã‚¹ã‚¯ï¼‰
                            vol_ma_7 = returns.rolling(7 * 24).std()
                            vol_std_7 = (
                                returns.rolling(7 * 24).std().rolling(7 * 24).std()
                            )
                            df["fr_zscore_7d"] = (
                                returns.rolling(24).std() - vol_ma_7
                            ) / vol_std_7.replace(0, 1)

                            vol_ma_30 = returns.rolling(30 * 24).std()
                            vol_std_30 = (
                                returns.rolling(30 * 24).std().rolling(30 * 24).std()
                            )
                            df["fr_zscore_30d"] = (
                                returns.rolling(24).std() - vol_ma_30
                            ) / vol_std_30.replace(0, 1)

                            # 3. å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šï¼ˆä¿¡ç”¨å–å¼•ãƒªã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹ï¼‰
                            current_vol = returns.rolling(24).std()
                            df["fr_regime"] = 0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šä¸­ç«‹
                            df.loc[
                                current_vol > vol_ma_30 + vol_std_30, "fr_regime"
                            ] = 1  # é«˜ãƒªã‚¹ã‚¯
                            df.loc[
                                current_vol < vol_ma_30 - vol_std_30, "fr_regime"
                            ] = -1  # ä½Žãƒªã‚¹ã‚¯

                            # 4. æ¥µç«¯å€¤æ¤œçŸ¥ï¼ˆä¿¡ç”¨å–å¼•ãƒªã‚¹ã‚¯è»¢æ›ã‚·ã‚°ãƒŠãƒ«ï¼‰
                            vol_q95 = current_vol.rolling(60 * 24).quantile(0.95)
                            vol_q05 = current_vol.rolling(60 * 24).quantile(0.05)
                            df["fr_extreme_long"] = (current_vol > vol_q95).astype(int)
                            df["fr_extreme_short"] = (current_vol < vol_q05).astype(int)

                            # 5. ä¿¡ç”¨å–å¼•ã‚³ã‚¹ãƒˆæ³¢å‹•çŽ‡ï¼ˆãƒªã‚¹ã‚¯æŒ‡æ¨™ï¼‰
                            df["fr_volatility"] = (
                                current_vol.rolling(7 * 24).std() * 100
                            )

                            # 6. ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼ˆä¿¡ç”¨å–å¼•æ–¹å‘æ€§ï¼‰
                            price_sma_3 = df["close"].rolling(3 * 24).mean()
                            price_sma_10 = df["close"].rolling(10 * 24).mean()
                            df["fr_trend_strength"] = (
                                (price_sma_3 - price_sma_10)
                                / price_sma_10.replace(0, 1)
                                * 100
                            )

                            # 7. ãƒã‚¸ã‚·ãƒ§ãƒ³è¦æ¨¡æŽ¨å®šï¼ˆOpen Interestä»£æ›¿ï¼‰
                            # å‡ºæ¥é«˜Ã—ä¾¡æ ¼ã§ä¿¡ç”¨å–å¼•è¦æ¨¡ã‚’æŽ¨å®š
                            position_size = df["volume"] * df["close"]
                            oi_ma_30 = position_size.rolling(30 * 24).mean()
                            df["oi_normalized"] = (
                                position_size / oi_ma_30.replace(0, 1)
                            ) - 1

                            # 8. ãƒã‚¸ã‚·ãƒ§ãƒ³å¤‰åŒ–çŽ‡ï¼ˆOIå¤‰åŒ–çŽ‡ä»£æ›¿ï¼‰
                            df["oi_change_1d"] = position_size.pct_change(24) * 100
                            df["oi_momentum_3d"] = position_size.pct_change(72) * 100

                            # 9. ãƒã‚¸ã‚·ãƒ§ãƒ³è¦æ¨¡Z-Scoreï¼ˆOI Z-Scoreä»£æ›¿ï¼‰
                            pos_ma_7 = position_size.rolling(7 * 24).mean()
                            pos_std_7 = position_size.rolling(7 * 24).std()
                            df["oi_zscore_7d"] = (
                                position_size - pos_ma_7
                            ) / pos_std_7.replace(0, 1)

                            # 10. ãƒã‚¸ã‚·ãƒ§ãƒ³æ–°é«˜å€¤ãƒ»æ–°å®‰å€¤ï¼ˆOIæ–°é«˜å€¤ãƒ»æ–°å®‰å€¤ä»£æ›¿ï¼‰
                            pos_max_30 = position_size.rolling(30 * 24).max()
                            pos_min_30 = position_size.rolling(30 * 24).min()
                            df["oi_new_high"] = (
                                position_size >= pos_max_30 * 0.98
                            ).astype(int)
                            df["oi_new_low"] = (
                                position_size <= pos_min_30 * 1.02
                            ).astype(int)

                            # 11. ä¿¡ç”¨å–å¼•åå‘æŒ‡æ¨™ï¼ˆãƒã‚¸ã‚·ãƒ§ãƒ³åå‘æŒ‡æ¨™ä»£æ›¿ï¼‰
                            # é‡‘åˆ©ã‚³ã‚¹ãƒˆã¨ãƒã‚¸ã‚·ãƒ§ãƒ³è¦æ¨¡ã®è¤‡åˆæŒ‡æ¨™
                            fr_abs = df["fr_rate"].abs()
                            oi_abs = df["oi_normalized"].abs()
                            df["position_bias"] = fr_abs * oi_abs

                            # æ¬ æå€¤å‡¦ç†
                            funding_cols = [
                                "fr_rate",
                                "fr_change_1d",
                                "fr_change_3d",
                                "fr_zscore_7d",
                                "fr_zscore_30d",
                                "fr_regime",
                                "fr_extreme_long",
                                "fr_extreme_short",
                                "fr_volatility",
                                "fr_trend_strength",
                                "oi_normalized",
                                "oi_change_1d",
                                "oi_momentum_3d",
                                "oi_zscore_7d",
                                "oi_new_high",
                                "oi_new_low",
                                "position_bias",
                            ]

                            for col in funding_cols:
                                if col in df.columns:
                                    df[col] = df[col].bfill().ffill().fillna(0)
                                    df[col] = df[col].replace(
                                        [float("inf"), float("-inf")], 0
                                    )

                                    # ç•°å¸¸å€¤ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                                    q975 = df[col].quantile(0.975)
                                    q025 = df[col].quantile(0.025)
                                    if (
                                        pd.notna(q975)
                                        and pd.notna(q025)
                                        and q975 != q025
                                    ):
                                        df[col] = df[col].clip(lower=q025, upper=q975)

                            logger.info(
                                f"Added {len(funding_cols)} Bitbank margin features"
                            )

                        except Exception as e:
                            logger.error(
                                "Failed to add Bitbank margin alternative features: %s",
                                e,
                            )
                            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§17ç‰¹å¾´é‡ã‚’è¿½åŠ 
                            default_cols = [
                                "fr_rate",
                                "fr_change_1d",
                                "fr_change_3d",
                                "fr_zscore_7d",
                                "fr_zscore_30d",
                                "fr_regime",
                                "fr_extreme_long",
                                "fr_extreme_short",
                                "fr_volatility",
                                "fr_trend_strength",
                                "oi_normalized",
                                "oi_change_1d",
                                "oi_momentum_3d",
                                "oi_zscore_7d",
                                "oi_new_high",
                                "oi_new_low",
                                "position_bias",
                            ]
                            for col in default_cols:
                                if col not in df.columns:
                                    df[col] = 0
                            logger.warning(
                                "Used default values for Bitbank margin features"
                            )

                    # Fear & Greed Indexç‰¹å¾´é‡ï¼ˆå¼·åˆ¶å–å¾—ç‰ˆï¼‰
                    elif base in ["fear_greed", "fg"]:
                        try:
                            logger.info(
                                f"ðŸ” Processing Fear&Greed features: "
                                f"fear_greed_enabled={self.fear_greed_enabled}, "
                                f"fetcher={self.fear_greed_fetcher is not None}"
                            )

                            fg_features = None

                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰Fear&Greedãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå„ªå…ˆï¼‰
                            cached_fg = self._get_cached_external_data(
                                "fear_greed", df.index
                            )
                            if not cached_fg.empty:
                                logger.info(
                                    f"âœ… Cached Fear&Greed: {len(cached_fg)} records"
                                )
                                fg_features = cached_fg

                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç©ºã®å ´åˆã€Fear&Greedãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã§ç›´æŽ¥å–å¾—
                            if fg_features is None or fg_features.empty:
                                if self.fear_greed_fetcher:
                                    logger.info("ðŸ” Fetching fresh Fear&Greed data...")
                                    try:
                                        fg_data = (
                                            self.fear_greed_fetcher.get_fear_greed_data(
                                                days_back=30
                                            )
                                        )
                                        if not fg_data.empty:
                                            logger.info(
                                                f"âœ… Fear&Greed: {len(fg_data)} records"
                                            )
                                            calc_func = (
                                                self.fear_greed_fetcher.calculate_fear_greed_features  # noqa: E501
                                            )
                                            fg_features = calc_func(fg_data)
                                        else:
                                            logger.warning(
                                                "âŒ Fear&Greed data empty from fetcher"
                                            )
                                    except Exception as e:
                                        logger.error(
                                            f"âŒ Fear&Greed fetching failed: {e}"
                                        )
                                else:
                                    logger.warning(
                                        "âŒ Fear&Greed fetcher not available"
                                    )

                            # Fear&Greedãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããŸå ´åˆã®å‡¦ç†
                            if fg_features is not None and not fg_features.empty:
                                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯å¾“æ¥ã®æ–¹æ³•
                                fg_data = self.fear_greed_fetcher.get_fear_greed_data(
                                    days_back=30
                                )
                                if not fg_data.empty:
                                    fg_features = self.fear_greed_fetcher.calculate_fear_greed_features(  # noqa: E501
                                        fg_data
                                    )

                                    # VIXã¨ã®ç›¸é–¢ç‰¹å¾´é‡ã‚‚è¿½åŠ 
                                    if self.vix_fetcher:
                                        try:
                                            vix_data = self.vix_fetcher.get_vix_data(
                                                timeframe="1d"
                                            )
                                            if not vix_data.empty:
                                                vix_fg_correlation = self.fear_greed_fetcher.get_vix_correlation_features(  # noqa: E501
                                                    fg_data, vix_data
                                                )
                                                if not vix_fg_correlation.empty:
                                                    fg_features = pd.concat(
                                                        [
                                                            fg_features,
                                                            vix_fg_correlation,
                                                        ],
                                                        axis=1,
                                                    )
                                                    logger.debug(
                                                        "Added VIX-FG correlation features"  # noqa: E501
                                                    )
                                        except Exception as e:
                                            logger.warning(
                                                f"Failed to add VIX-FG correlation: {e}"
                                            )

                                    # æš—å·è³‡ç”£ãƒ‡ãƒ¼ã‚¿ã¨Fear & Greedãƒ‡ãƒ¼ã‚¿ã®æ™‚é–“è»¸ã‚’åˆã‚ã›ã‚‹
                                    if isinstance(
                                        df.index, pd.DatetimeIndex
                                    ) and isinstance(
                                        fg_features.index, pd.DatetimeIndex
                                    ):
                                        # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ãªã®ã§ã€æš—å·è³‡ç”£ã®æ™‚é–“è»¸ã«åˆã‚ã›ã¦ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                                        fg_resampled = fg_features.resample(
                                            "1h"
                                        ).ffill()

                                        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆã‚ã›ã¦çµåˆ
                                        common_index = df.index.intersection(
                                            fg_resampled.index
                                        )
                                        logger.debug(
                                            f"Fear & Greed alignment: crypto {len(df)}, fg {len(fg_resampled)}, common {len(common_index)}"  # noqa: E501
                                        )

                                        # å°ã•ãªãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ã®å ´åˆã¯æœ€æ–°ã®Fear & Greedãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                                        if len(common_index) == 0 and len(df) > 0:
                                            # Fear & Greedãƒ‡ãƒ¼ã‚¿ãŒæœŸé–“å¤–ã®å ´åˆã€æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã§å…¨è¡Œã‚’åŸ‹ã‚ã‚‹
                                            logger.warning(
                                                "Fear & Greed data period mismatch - using latest available data"  # noqa: E501
                                            )

                                            # æœ€æ–°ã®Fear & Greedãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                                            if not fg_resampled.empty:
                                                latest_fg_data = fg_resampled.iloc[
                                                    -1
                                                ]  # æœ€æ–°è¡Œ
                                                for col in fg_features.columns:
                                                    if col in latest_fg_data.index:
                                                        df[col] = latest_fg_data[col]
                                                logger.debug(
                                                    f"Filled all {len(df)} rows with latest Fear & Greed data"  # noqa: E501
                                                )
                                            else:
                                                # Fear & Greedãƒ‡ãƒ¼ã‚¿ãŒå…¨ããªã„å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åŸ‹ã‚ã‚‹
                                                logger.warning(
                                                    "No Fear & Greed data available - using default values"  # noqa: E501
                                                )
                                                for col in fg_features.columns:
                                                    if col == "fg_level":
                                                        df[col] = 50  # ä¸­ç«‹å€¤
                                                    elif col == "fg_regime":
                                                        df[col] = 3  # ä¸­ç«‹ãƒ¬ã‚¸ãƒ¼ãƒ 
                                                    else:
                                                        df[col] = 0  # ãã®ä»–ã¯0
                                        elif len(common_index) > 0:
                                            # Fear & Greedç‰¹å¾´é‡ã‚’è¿½åŠ 
                                            for col in fg_features.columns:
                                                if col in fg_resampled.columns:
                                                    df.loc[common_index, col] = (
                                                        fg_resampled.loc[
                                                            common_index, col
                                                        ]
                                                    )

                                            logger.debug(
                                                f"Added Fear & Greed features: {len(common_index)} data points aligned"  # noqa: E501
                                            )
                                        else:
                                            logger.warning(
                                                "Could not align Fear & Greed data with crypto data"  # noqa: E501
                                            )
                                    else:
                                        logger.warning(
                                            "Index type mismatch for Fear & Greed data alignment"  # noqa: E501
                                        )
                                else:
                                    logger.warning(
                                        "No Fear & Greed data available - adding default Fear & Greed features"  # noqa: E501
                                    )
                                    # Fear & Greedãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‰¹å¾´é‡ã‚’è¿½åŠ 
                                    from crypto_bot.data.fear_greed_fetcher import (
                                        get_available_fear_greed_features,
                                    )

                                    fg_feature_names = (
                                        get_available_fear_greed_features()
                                    )

                                    for col in fg_feature_names:
                                        if col not in df.columns:
                                            if col == "fg_level":
                                                df[col] = 50  # ä¸­ç«‹å€¤
                                            elif col == "fg_regime":
                                                df[col] = 3  # ä¸­ç«‹ãƒ¬ã‚¸ãƒ¼ãƒ 
                                            else:
                                                df[col] = 0  # ãã®ä»–ã¯0
                                    logger.debug(
                                        f"Added {len(fg_feature_names)} default Fear & Greed features"  # noqa: E501
                                    )
                            else:
                                logger.warning(
                                    "Fear & Greed fetcher not initialized - adding default Fear & Greed features"  # noqa: E501
                                )
                                # FetcherãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã‚‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‰¹å¾´é‡ã‚’è¿½åŠ 
                                from crypto_bot.data.fear_greed_fetcher import (
                                    get_available_fear_greed_features,
                                )

                                fg_feature_names = get_available_fear_greed_features()

                                for col in fg_feature_names:
                                    if col not in df.columns:
                                        if col == "fg_level":
                                            df[col] = 50  # ä¸­ç«‹å€¤
                                        elif col == "fg_regime":
                                            df[col] = 3  # ä¸­ç«‹ãƒ¬ã‚¸ãƒ¼ãƒ 
                                        else:
                                            df[col] = 0  # ãã®ä»–ã¯0
                                logger.debug(
                                    f"Added {len(fg_feature_names)} default Fear & Greed features"  # noqa: E501
                                )
                        except Exception as e:
                            logger.warning("Failed to add Fear & Greed features: %s", e)
                            logger.warning(
                                "Adding default Fear & Greed features due to error"
                            )
                            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‰¹å¾´é‡ã‚’è¿½åŠ ã—ã¦ç‰¹å¾´é‡æ•°ã‚’ä¸€è‡´ã•ã›ã‚‹
                            try:
                                from crypto_bot.data.fear_greed_fetcher import (
                                    get_available_fear_greed_features,
                                )

                                fg_feature_names = get_available_fear_greed_features()

                                for col in fg_feature_names:
                                    if col not in df.columns:
                                        if col == "fg_level":
                                            df[col] = 50  # ä¸­ç«‹å€¤
                                        elif col == "fg_regime":
                                            df[col] = 3  # ä¸­ç«‹ãƒ¬ã‚¸ãƒ¼ãƒ 
                                        else:
                                            df[col] = 0  # ãã®ä»–ã¯0
                                logger.debug(
                                    f"Added {len(fg_feature_names)} default Fear & Greed features after error"  # noqa: E501
                                )
                            except Exception as inner_e:
                                logger.error(
                                    f"Failed to add default Fear & Greed features: {inner_e}"  # noqa: E501
                                )

                    # mochipoyo_long_signal or mochipoyo_short_signal
                    elif (
                        feat_lc == "mochipoyo_long_signal"
                        or feat_lc == "mochipoyo_short_signal"
                    ):
                        if mochipoyo_signals is not None:
                            for signal_col in mochipoyo_signals.columns:
                                if signal_col not in df:
                                    df[signal_col] = mochipoyo_signals[signal_col]

                    # momentum_14 ç‰¹å¾´é‡
                    elif feat_lc == "momentum_14":
                        df["momentum_14"] = df["close"].pct_change(14).fillna(0)

                    # trend_strength ç‰¹å¾´é‡
                    elif feat_lc == "trend_strength":
                        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ã‚’è¨ˆç®—ï¼ˆADXãƒ™ãƒ¼ã‚¹ï¼‰
                        try:
                            import pandas_ta as ta

                            adx_result = ta.adx(
                                high=df["high"],
                                low=df["low"],
                                close=df["close"],
                                length=14,
                            )

                            if adx_result is not None and not adx_result.empty:
                                # ADXã®çµæžœã¯DataFrameãªã®ã§ã€ADXåˆ—ã‚’å–å¾—
                                if (
                                    isinstance(adx_result, pd.DataFrame)
                                    and "ADX_14" in adx_result.columns
                                ):
                                    df["trend_strength"] = adx_result["ADX_14"].fillna(
                                        25
                                    )
                                elif isinstance(adx_result, pd.Series):
                                    df["trend_strength"] = adx_result.fillna(25)
                                else:
                                    df["trend_strength"] = 25
                            else:
                                df["trend_strength"] = 25  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                        except Exception as e:
                            logger.warning(f"Failed to calculate trend_strength: {e}")
                            df["trend_strength"] = 25  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

                    # volatility_regime ç‰¹å¾´é‡ (5ç‰¹å¾´é‡)
                    elif feat_lc == "volatility_regime":
                        try:
                            # è¤‡æ•°æœŸé–“ã§ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†æž
                            volatility_windows = [10, 20, 50]

                            for window in volatility_windows:
                                # rollingæ¨™æº–åå·®ã‚’è¨ˆç®—
                                vol = df["close"].rolling(window=window).std()

                                # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ ã‚’3æ®µéšŽã«åˆ†é¡ž (0:ä½Ž, 1:ä¸­, 2:é«˜)
                                vol_regime = pd.cut(vol, bins=3, labels=[0, 1, 2])
                                df[f"vol_regime_{window}"] = pd.to_numeric(
                                    vol_regime, errors="coerce"
                                ).fillna(1)

                                # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç™¾åˆ†ä½æ•°
                                df[f"vol_percentile_{window}"] = (
                                    vol.rolling(window=100, min_periods=window)
                                    .rank(pct=True)
                                    .fillna(0.5)
                                )

                            # çŸ­æœŸvsé•·æœŸãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¯”çŽ‡
                            short_vol = df["close"].rolling(window=10).std()
                            long_vol = df["close"].rolling(window=50).std()
                            df["vol_ratio_short_long"] = (short_vol / long_vol).fillna(
                                1.0
                            )

                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate volatility_regime: {e}"
                            )
                            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
                            for window in [10, 20, 50]:
                                df[f"vol_regime_{window}"] = 1
                                df[f"vol_percentile_{window}"] = 0.5
                            df["vol_ratio_short_long"] = 1.0

                    # momentum_signals ç‰¹å¾´é‡ (7ç‰¹å¾´é‡)
                    elif feat_lc == "momentum_signals":
                        try:
                            # è¤‡æ•°æœŸé–“ã®ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ä¿¡å·
                            momentum_periods = [1, 3, 7, 14, 21, 30]

                            for period in momentum_periods:
                                df[f"momentum_{period}"] = (
                                    df["close"].pct_change(period).fillna(0)
                                )

                            # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ åŽæŸãƒ»ç™ºæ•£æŒ‡æ¨™
                            mom_short = df["close"].pct_change(3)
                            mom_long = df["close"].pct_change(21)
                            df["momentum_convergence"] = (mom_short - mom_long).fillna(
                                0
                            )

                        except Exception as e:
                            logger.warning(f"Failed to calculate momentum_signals: {e}")
                            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
                            for period in [1, 3, 7, 14, 21, 30]:
                                df[f"momentum_{period}"] = 0
                            df["momentum_convergence"] = 0

                    # liquidity_indicators ç‰¹å¾´é‡ (8ç‰¹å¾´é‡)
                    elif feat_lc == "liquidity_indicators":
                        try:
                            # Amihudæµå‹•æ€§æŒ‡æ¨™ï¼ˆéžæµå‹•æ€§åº¦ï¼‰
                            price_change = abs(df["close"].pct_change())
                            volume_scaled = (
                                df["volume"] / df["volume"].rolling(window=20).mean()
                            )
                            df["amihud_illiquidity"] = (
                                price_change / (volume_scaled + 1e-8)
                            ).fillna(0)

                            # ä¾¡æ ¼ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆæŒ‡æ¨™
                            df["price_impact"] = (
                                (df["high"] - df["low"]) / (df["volume"] + 1e-8)
                            ).fillna(0)

                            # ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒ»ãƒ—ãƒ©ã‚¤ã‚¹ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆVPTï¼‰
                            df["vpt"] = (
                                (df["volume"] * df["close"].pct_change())
                                .cumsum()
                                .fillna(0)
                            )

                            # å‡ºæ¥é«˜ç›¸å¯¾å¼·åº¦
                            df["volume_strength"] = (
                                df["volume"] / df["volume"].rolling(window=20).mean()
                            ).fillna(1.0)

                            # æµå‹•æ€§æž¯æ¸‡æŒ‡æ¨™
                            volume_ma = df["volume"].rolling(window=10).mean()
                            volume_std = df["volume"].rolling(window=10).std()
                            df["liquidity_drought"] = (
                                (volume_ma - df["volume"]) / (volume_std + 1e-8)
                            ).fillna(0)

                            # ãƒ“ãƒƒãƒ‰ãƒ»ã‚¢ã‚¹ã‚¯ä»£ç†æŒ‡æ¨™ï¼ˆé«˜å€¤-å®‰å€¤ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ï¼‰
                            typical_price = (df["high"] + df["low"] + df["close"]) / 3
                            df["spread_proxy"] = (
                                (df["high"] - df["low"]) / typical_price
                            ).fillna(0)

                            # å‡ºæ¥é«˜åŠ é‡å¹³å‡ä¾¡æ ¼ã‹ã‚‰ã®ä¹–é›¢
                            vwap = (typical_price * df["volume"]).rolling(
                                window=20
                            ).sum() / df["volume"].rolling(window=20).sum()
                            df["vwap_deviation"] = ((df["close"] - vwap) / vwap).fillna(
                                0
                            )

                            # æ³¨æ–‡ä¸å‡è¡¡ä»£ç†æŒ‡æ¨™
                            df["order_imbalance_proxy"] = (
                                (df["close"] - df["open"])
                                / (df["high"] - df["low"] + 1e-8)
                            ).fillna(0)

                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate liquidity_indicators: {e}"
                            )
                            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
                            liquidity_features = [
                                "amihud_illiquidity",
                                "price_impact",
                                "vpt",
                                "volume_strength",
                                "liquidity_drought",
                                "spread_proxy",
                                "vwap_deviation",
                                "order_imbalance_proxy",
                            ]
                            for feat in liquidity_features:
                                df[feat] = 0

                    else:
                        logger.warning(f"Unknown extra feature spec: {feat} - skipping")

                except Exception as e:
                    logger.error("Failed to add extra feature %s: %s", feat, e)
                    raise  # å¤±æ•—ã—ãŸã‚‰æ¡ã‚Šã¤ã¶ã•ãšåœæ­¢ã™ã‚‹
            logger.debug("After extra feats: %s", df.shape)

        # 6. æ¬ æå†è£œå®Œï¼‹0åŸ‹ã‚
        df = df.ffill().fillna(0)

        # 7. ç„¡é™å¤§å€¤ãƒ»ç•°å¸¸å€¤ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        # ç„¡é™å¤§å€¤ã‚’å‰ã®æœ‰åŠ¹å€¤ã§ç½®æ›ã€ãã‚Œã‚‚ç„¡ã„å ´åˆã¯0
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.ffill().fillna(0)

        # ç•°å¸¸ã«å¤§ããªå€¤ã‚’ã‚¯ãƒªãƒƒãƒ—ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢ï¼‰
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            # 99.9%ileä»¥ä¸Šã®å€¤ã‚’ã‚¯ãƒªãƒƒãƒ—
            upper_bound = df[col].quantile(0.999)
            lower_bound = df[col].quantile(0.001)

            if np.isfinite(upper_bound) and np.isfinite(lower_bound):
                df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

        logger.debug("Final features shape after cleaning: %s", df.shape)

        # ãƒ‡ãƒ¼ã‚¿å“è³ªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå‹çŽ‡æ”¹å–„ã®ãŸã‚ã®é‡è¦ãªã‚¹ãƒ†ãƒƒãƒ—ï¼‰
        try:
            from crypto_bot.ml.data_quality_manager import DataQualityManager

            quality_manager = DataQualityManager(self.config)

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±ï¼‰
            metadata = {
                "feature_sources": {},
                "external_data_enabled": {
                    "vix": self.vix_enabled,
                    "macro": self.macro_enabled,
                    "fear_greed": self.fear_greed_enabled,
                    "funding": self.funding_enabled,
                },
            }

            # å„ç‰¹å¾´é‡ã®ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¨˜éŒ²
            for column in df.columns:
                vix_prefixes = ["vix_", "dxy_", "treasury_"]
                if any(column.startswith(prefix) for prefix in vix_prefixes):
                    source_type = "api" if self.macro_enabled else "default"
                    metadata["feature_sources"][column] = {"source_type": source_type}
                elif any(column.startswith(prefix) for prefix in ["fear_greed", "fg_"]):
                    source_type = "api" if self.fear_greed_enabled else "default"
                    metadata["feature_sources"][column] = {"source_type": source_type}
                elif any(column.startswith(prefix) for prefix in ["fr_", "oi_"]):
                    # Bitbankä»£æ›¿ç‰¹å¾´é‡
                    metadata["feature_sources"][column] = {"source_type": "calculated"}
                else:
                    metadata["feature_sources"][column] = {"source_type": "calculated"}

            # ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼
            quality_passed, quality_report = quality_manager.validate_data_quality(
                df, metadata
            )

            if not quality_passed:
                logger.warning(f"Data quality check failed: {quality_report}")

                # ãƒ‡ãƒ¼ã‚¿å“è³ªæ”¹å–„ã‚’è©¦è¡Œ
                df_improved, improvement_report = quality_manager.improve_data_quality(
                    df, metadata
                )
                df = df_improved

                logger.info(f"Data quality improvement applied: {improvement_report}")
            else:
                score = quality_report["quality_score"]
                logger.info(f"Data quality check passed: score={score:.1f}")

        except Exception as e:
            logger.warning(
                f"Data quality management failed: {e} - continuing with original data"
            )

        # 101ç‰¹å¾´é‡ã®ç¢ºå®Ÿãªä¿è¨¼ï¼ˆæœ€çµ‚ãƒã‚§ãƒƒã‚¯ï¼‰
        from crypto_bot.ml.feature_defaults import ensure_feature_consistency

        df = ensure_feature_consistency(df, target_count=101)
        logger.info(f"Final guaranteed feature count: {len(df.columns)}")

        return df


def build_ml_pipeline(config: Dict[str, Any]) -> Pipeline:
    """
    sklearnãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ï¼ˆç‰¹å¾´é‡ç”Ÿæˆâ†’æ¨™æº–åŒ–ï¼‰ã€‚
    ç©ºã®DataFrameã®å ´åˆã¯ã€ç‰¹å¾´é‡ç”Ÿæˆã®ã¿ã‚’è¡Œã„ã€æ¨™æº–åŒ–ã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚
    """

    class EmptyDataFrameScaler(BaseEstimator, TransformerMixin):
        """ç©ºã®DataFrameã®å ´åˆã®ãƒ€ãƒŸãƒ¼ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼"""

        def fit(self, X, y=None):
            return self

        def transform(self, X):
            return X

    class EmptyDataFramePipeline(Pipeline):
        """ç©ºã®DataFrameã®å ´åˆã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

        def fit_transform(self, X, y=None, **fit_params):
            if X.empty:
                # ç©ºã®DataFrameã®å ´åˆã¯ã€ç‰¹å¾´é‡ç”Ÿæˆã®ã¿ã‚’è¡Œã„ã€æ¨™æº–åŒ–ã¯ã‚¹ã‚­ãƒƒãƒ—
                features = self.named_steps["features"].transform(X)
                # DataFrameã‚’numpy.ndarrayã«å¤‰æ›
                return features.values
            return super().fit_transform(X, y, **fit_params)

        def transform(self, X):
            if X.empty:
                # ç©ºã®DataFrameã®å ´åˆã¯ã€ç‰¹å¾´é‡ç”Ÿæˆã®ã¿ã‚’è¡Œã„ã€æ¨™æº–åŒ–ã¯ã‚¹ã‚­ãƒƒãƒ—
                features = self.named_steps["features"].transform(X)
                # DataFrameã‚’numpy.ndarrayã«å¤‰æ›
                return features.values
            return super().transform(X)

    return EmptyDataFramePipeline(
        [
            ("features", FeatureEngineer(config)),
            (
                "scaler",
                (
                    EmptyDataFrameScaler()
                    if config.get("skip_scaling", False)
                    else StandardScaler()
                ),
            ),
        ]
    )


def prepare_ml_dataset(
    df: pd.DataFrame, config: Dict[str, Any]
) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
    """
    æ©Ÿæ¢°å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã€‚
    æˆ»ã‚Šå€¤: Xï¼ˆç‰¹å¾´é‡ï¼‰, y_regï¼ˆå›žå¸°ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰, y_clfï¼ˆåˆ†é¡žã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰

    - å¿…è¦ãªã¶ã‚“ã ã‘æœ€åˆã®è¡Œã‚’ãƒ‰ãƒ­ãƒƒãƒ—ï¼ˆrolling/lagsï¼‰
    - horizon, thresholdã¯config["ml"]ã‹ã‚‰å–å¾—
    """
    logger.info(f"prepare_ml_dataset input df shape: {df.shape}")
    pipeline = build_ml_pipeline(config)
    X_arr = pipeline.fit_transform(df)

    logger.info(f"Pipeline output type: {type(X_arr)}")
    if hasattr(X_arr, "shape"):
        shape_info = X_arr.shape
    elif hasattr(X_arr, "__len__"):
        shape_info = len(X_arr)
    else:
        shape_info = "no len"

    logger.info(f"Pipeline output shape/len: {shape_info}")

    # X_arrãŒlistã®å ´åˆã¯numpy arrayã«å¤‰æ›
    if isinstance(X_arr, list):
        logger.warning(
            f"Pipeline returned list with {len(X_arr)} elements, converting to numpy array"  # noqa: E501
        )
        import numpy as np

        try:
            X_arr = np.array(X_arr)
        except Exception as e:
            logger.error(f"Failed to convert list to numpy array: {e}")
            # If conversion fails, return the list directly for debugging
            return X_arr

    # ----- ç›®çš„å¤‰æ•°ç”Ÿæˆ -----
    horizon = config["ml"]["horizon"]
    thresh = config["ml"].get("threshold", 0.0)
    y_reg = make_regression_target(df, horizon).rename(f"return_{horizon}")
    y_clf = make_classification_target(df, horizon, thresh).rename(f"up_{horizon}")

    # ----- è¡Œæ•°ã‚’æƒãˆã‚‹ -----
    win = config["ml"]["rolling_window"]
    lags = config["ml"]["lags"]
    drop_n = win + max(lags) if lags else win

    idx = df.index[drop_n:]
    X = pd.DataFrame(X_arr[drop_n:], index=idx)

    return X, y_reg.loc[idx], y_clf.loc[idx]
