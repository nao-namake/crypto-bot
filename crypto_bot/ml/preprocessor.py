# =============================================================================
# „Éï„Ç°„Ç§„É´Âêç: crypto_bot/ml/preprocessor.py
# Ë™¨Êòé:
# Ê©üÊ¢∞Â≠¶ÁøíÁî®„ÅÆÁâπÂæ¥ÈáèÁîüÊàê„ÉªÂâçÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥„ÇíÊèê‰æõ„ÄÇ
# - OHLCV DataFrame „Åã„ÇâÊ©üÊ¢∞Â≠¶ÁøíÁî®ÁâπÂæ¥Èáè„ÇíÁîüÊàê
# - sklearn Pipeline‰∫íÊèõ„ÅÆ FeatureEngineer „ÇíÊèê‰æõ
# - build_ml_pipeline() „ÅßÁâπÂæ¥ÈáèÔºãÊ®ôÊ∫ñÂåñ„Éë„Ç§„Éó„É©„Ç§„É≥ÁîüÊàê
# - prepare_ml_dataset() „ÅßÁâπÂæ¥Èáè„Éª„Çø„Éº„Ç≤„ÉÉ„ÉàÂàó„Çí‰∏ÄÊã¨ÁîüÊàê
#
# ‚Äª „Éê„ÉÉ„ÇØ„ÉÜ„Çπ„ÉàÁî®„Åß„ÅØ„Å™„Åè„ÄÅMLStrategy„ÇÑÂ≠¶Áøí/Êé®Ë´ñÁ≥ª„Åß‰ΩøÁî®
# =============================================================================

from __future__ import annotations

import logging
import time
from typing import Any, Dict, Tuple

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# Phase B2.4: „Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Ç®„É≥„Ç∏„É≥Áµ±Âêà
try:
    from crypto_bot.ml.feature_engines import (
        BatchFeatureCalculator,
        ExternalDataIntegrator,
        TechnicalFeatureEngine,
    )

    BATCH_ENGINES_AVAILABLE = True
except ImportError as e:
    # logger is not yet defined, use print temporarily
    print(f"‚ö†Ô∏è Batch engines not available: {e}")
    BATCH_ENGINES_AVAILABLE = False

# Phase H.11: ÁâπÂæ¥Èáè„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞Âº∑Âåñ„Ç∑„Çπ„ÉÜ„É†Áµ±Âêà
try:
    from crypto_bot.ml.feature_engineering_enhanced import (
        FeatureEngineeringEnhanced,
        enhance_feature_engineering,
    )

    ENHANCED_FEATURES_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è Enhanced feature engineering not available: {e}")
    ENHANCED_FEATURES_AVAILABLE = False

try:
    from crypto_bot.data.vix_fetcher import VIXDataFetcher

    VIX_AVAILABLE = True
except ImportError:
    VIXDataFetcher = None
    VIX_AVAILABLE = False

try:
    from crypto_bot.data.macro_fetcher import MacroDataFetcher

    MACRO_AVAILABLE = True
except ImportError:
    MacroDataFetcher = None
    MACRO_AVAILABLE = False

try:
    from crypto_bot.data.funding_fetcher import FundingDataFetcher

    FUNDING_AVAILABLE = True
except ImportError:
    FundingDataFetcher = None
    FUNDING_AVAILABLE = False

try:
    from crypto_bot.data.fear_greed_fetcher import FearGreedDataFetcher

    FEAR_GREED_AVAILABLE = True
except ImportError:
    FearGreedDataFetcher = None
    FEAR_GREED_AVAILABLE = False

from crypto_bot.indicator.calculator import IndicatorCalculator
from crypto_bot.ml.target import make_classification_target, make_regression_target

logger = logging.getLogger(__name__)


def calc_rci(series: pd.Series, period: int) -> pd.Series:
    """
    Rank Correlation IndexÔºàRCIÔºâ„ÇíË®àÁÆó„Åô„Çã„ÄÇ
    :param series: ÁµÇÂÄ§„Å™„Å©„ÅÆ‰æ°Ê†º„Éá„Éº„ÇøÔºàpd.SeriesÔºâ
    :param period: ÊúüÈñì
    :return: RCI„ÅÆpd.Series
    """
    n = period

    def _rci(x):
        price_ranks = pd.Series(x).rank(ascending=False)
        date_ranks = np.arange(1, n + 1)
        d = price_ranks.values - date_ranks
        return (1 - 6 * (d**2).sum() / (n * (n**2 - 1))) * 100

    return series.rolling(window=n).apply(_rci, raw=False)


class FeatureEngineer(BaseEstimator, TransformerMixin):
    """
    OHLCV„Åã„ÇâÂêÑÁ®ÆÁâπÂæ¥Èáè„ÇíÁîüÊàê„Åô„Çã sklearn‰∫íÊèõ„ÇØ„É©„Çπ„ÄÇ

    ÂÖ•Âäõ: OHLCV DataFrameÔºàindex„ÅØtz-aware DatetimeIndexÔºâ
    Âá∫Âäõ: ÁâπÂæ¥ÈáèDataFrame

    - Ê¨†ÊêçË£úÂÆå
    - ATR, lag, rollingÁµ±Ë®à, extra_featuresÂØæÂøú
    - ffill/0Âüã„ÇÅÁ≠â„ÇÇÂÆüÊñΩ
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.ind_calc = IndicatorCalculator()
        self.extra_features = self.config["ml"].get("extra_features", [])

        # ML„Éë„É©„É°„Éº„ÇøË®≠ÂÆö
        ml_config = self.config["ml"]
        self.feat_period = ml_config.get("feat_period", 14)
        self.lags = ml_config.get("lags", [1, 2, 3])
        self.rolling_window = ml_config.get("rolling_window", 14)

        # VIXÁµ±ÂêàË®≠ÂÆöÔºàÂº∑Âà∂ÂàùÊúüÂåñÁâàÔºâ
        logger.info(f"üîç VIX Debug: extra_features={self.extra_features}")
        logger.info(f"üîç VIX Debug: VIX_AVAILABLE={VIX_AVAILABLE}")
        vix_in_features = "vix" in self.extra_features
        logger.info(f"üîç VIX Debug: vix_in_features={vix_in_features}")

        # VIXÂæ©Ê¥ªÂÆüË£ÖÔºöË®≠ÂÆöÂØæÂøú„ÉªË§áÊï∞„Éá„Éº„Çø„ÇΩ„Éº„Çπ„Éª„Ç≠„É£„ÉÉ„Ç∑„É•Ê©üËÉΩ
        if vix_in_features:
            try:
                if VIX_AVAILABLE and VIXDataFetcher:
                    self.vix_fetcher = VIXDataFetcher(self.config)
                    self.vix_enabled = True
                    logger.info(
                        "‚úÖ VIX fetcher initialized successfully (config-aware)"
                    )
                else:
                    # VIXDataFetcher„ÇíÁõ¥Êé•„Ç§„É≥„Éù„Éº„Éà„Åó„Å¶ÂàùÊúüÂåñ„ÇíÂº∑Âà∂
                    from crypto_bot.data.vix_fetcher import (
                        VIXDataFetcher as DirectVIXFetcher,
                    )

                    self.vix_fetcher = DirectVIXFetcher(self.config)
                    self.vix_enabled = True
                    logger.info(
                        "‚úÖ VIX fetcher initialized with direct import (config-aware)"
                    )
            except Exception as e:
                logger.error(f"‚ùå VIX fetcher initialization failed: {e}")
                self.vix_fetcher = None
                self.vix_enabled = False
        else:
            self.vix_enabled = False
            self.vix_fetcher = None
            logger.info(f"‚ö†Ô∏è VIX not in extra_features: {self.extra_features}")

        # „Éû„ÇØ„É≠„Éá„Éº„ÇøÁµ±ÂêàË®≠ÂÆöÔºàÂº∑Âà∂ÂàùÊúüÂåñÁâàÔºâ
        macro_in_features = any(
            feat in self.extra_features for feat in ["dxy", "macro", "treasury"]
        )
        logger.info(f"üîç Macro Debug: macro_in_features={macro_in_features}")

        if macro_in_features:
            try:
                if MACRO_AVAILABLE and MacroDataFetcher:
                    self.macro_fetcher = MacroDataFetcher()
                    self.macro_enabled = True
                    logger.info("‚úÖ Macro fetcher initialized successfully (forced)")
                else:
                    # MacroDataFetcher„ÇíÁõ¥Êé•„Ç§„É≥„Éù„Éº„Éà„Åó„Å¶ÂàùÊúüÂåñ„ÇíÂº∑Âà∂
                    from crypto_bot.data.macro_fetcher import (
                        MacroDataFetcher as DirectMacroFetcher,
                    )

                    self.macro_fetcher = DirectMacroFetcher()
                    self.macro_enabled = True
                    logger.info("‚úÖ Macro fetcher initialized with direct import")
            except Exception as e:
                logger.error(f"‚ùå Macro fetcher initialization failed: {e}")
                self.macro_fetcher = None
                self.macro_enabled = False
        else:
            self.macro_enabled = False
            self.macro_fetcher = None
            logger.info(
                f"‚ö†Ô∏è Macro features not in extra_features: {self.extra_features}"
            )

        # Funding RateÁµ±ÂêàË®≠ÂÆöÔºàBitbankÂ∞ÇÁî®ÔºöÁèæÁâ©ÂèñÂºï„ÅÆ„Åü„ÇÅÁÑ°ÂäπÂåñÔºâ
        self.funding_enabled = False
        self.funding_fetcher = None

        # BitbankÁèæÁâ©ÂèñÂºï„Åß„ÅØ‰ª£ÊõøÁâπÂæ¥Èáè„Çí‰ΩøÁî®
        self.funding_alternative_enabled = any(
            feat in self.extra_features for feat in ["funding", "oi"]
        )

        # Fear & GreedÂæ©Ê¥ªÂÆüË£ÖÔºöË®≠ÂÆöÂØæÂøú„ÉªË§áÊï∞„Éá„Éº„Çø„ÇΩ„Éº„Çπ„Éª„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÊ©üËÉΩ
        fear_greed_in_features = any(
            feat in self.extra_features for feat in ["fear_greed", "fg"]
        )
        logger.info(
            f"üîç Fear&Greed Debug: fear_greed_in_features={fear_greed_in_features}"
        )

        if fear_greed_in_features:
            try:
                if FEAR_GREED_AVAILABLE and FearGreedDataFetcher:
                    self.fear_greed_fetcher = FearGreedDataFetcher(self.config)
                    self.fear_greed_enabled = True
                    logger.info(
                        "‚úÖ Fear&Greed fetcher initialized successfully (config-aware)"
                    )
                else:
                    # FearGreedDataFetcher„ÇíÁõ¥Êé•„Ç§„É≥„Éù„Éº„Éà„Åó„Å¶ÂàùÊúüÂåñ„ÇíÂº∑Âà∂
                    from crypto_bot.data.fear_greed_fetcher import (
                        FearGreedDataFetcher as DirectFGFetcher,
                    )

                    self.fear_greed_fetcher = DirectFGFetcher(self.config)
                    self.fear_greed_enabled = True
                    logger.info(
                        "‚úÖ Fear&Greed fetcher initialized with direct import "
                        "(config-aware)"
                    )
            except Exception as e:
                logger.error(f"‚ùå Fear&Greed fetcher initialization failed: {e}")
                self.fear_greed_fetcher = None
                self.fear_greed_enabled = False
        else:
            self.fear_greed_enabled = False
            self.fear_greed_fetcher = None
            logger.info(f"‚ö†Ô∏è Fear&Greed not in extra_features: {self.extra_features}")

        # USD/JPYÁÇ∫Êõø„Éá„Éº„ÇøÁµ±ÂêàË®≠ÂÆöÔºàÂº∑Âà∂ÂàùÊúüÂåñÁâàÔºâ
        forex_in_features = any(
            feat in self.extra_features for feat in ["forex", "usdjpy", "jpy"]
        )
        logger.info(f"üîç Forex Debug: forex_in_features={forex_in_features}")

        if forex_in_features:
            try:
                # MacroDataFetcher„ÇíÁÇ∫Êõø„Éá„Éº„ÇøÂèñÂæó„Å´ÂÜçÂà©Áî®
                if MACRO_AVAILABLE and MacroDataFetcher:
                    self.forex_fetcher = MacroDataFetcher()
                    self.forex_enabled = True
                    logger.info("‚úÖ Forex fetcher initialized successfully (forced)")
                else:
                    # MacroDataFetcher„ÇíÁõ¥Êé•„Ç§„É≥„Éù„Éº„Éà„Åó„Å¶ÂàùÊúüÂåñ„ÇíÂº∑Âà∂
                    from crypto_bot.data.macro_fetcher import (
                        MacroDataFetcher as DirectForexFetcher,
                    )

                    self.forex_fetcher = DirectForexFetcher()
                    self.forex_enabled = True
                    logger.info("‚úÖ Forex fetcher initialized with direct import")
            except Exception as e:
                logger.error(f"‚ùå Forex fetcher initialization failed: {e}")
                self.forex_fetcher = None
                self.forex_enabled = False
        else:
            self.forex_enabled = False
            self.forex_fetcher = None
            logger.info(f"‚ö†Ô∏è Forex not in extra_features: {self.extra_features}")

        # Phase B2.4: „Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Ç®„É≥„Ç∏„É≥ÂàùÊúüÂåñ
        self._initialize_batch_engines()

    def _initialize_batch_engines(self):
        """
        Phase B2.4: „Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Ç®„É≥„Ç∏„É≥ÂàùÊúüÂåñ
        DataFrameÊñ≠ÁâáÂåñËß£Ê∂à„ÅÆ„Åü„ÇÅ„ÅÆÊñ∞„Ç®„É≥„Ç∏„É≥„Ç∑„Çπ„ÉÜ„É†
        """
        if not BATCH_ENGINES_AVAILABLE:
            logger.warning(
                "‚ö†Ô∏è Batch engines not available, falling back to legacy processing"
            )
            self.batch_engines_enabled = False
            return

        try:
            # BatchFeatureCalculatorÔºà„Ç≥„Ç¢Ôºâ
            self.batch_calculator = BatchFeatureCalculator(self.config)

            # TechnicalFeatureEngineÔºà„ÉÜ„ÇØ„Éã„Ç´„É´ÊåáÊ®ôÔºâ
            self.technical_engine = TechnicalFeatureEngine(
                self.config, self.batch_calculator
            )

            # Phase H.25: Â§ñÈÉ®„Éá„Éº„ÇøÁÑ°ÂäπÂåñÊôÇ„ÅØExternalDataIntegrator„Çí„Çπ„Ç≠„ÉÉ„Éó
            external_data_enabled = (
                self.config.get("ml", {}).get("external_data", {}).get("enabled", True)
            )
            if external_data_enabled:
                # ExternalDataIntegratorÔºàÂ§ñÈÉ®„Éá„Éº„ÇøÁµ±ÂêàÔºâ
                self.external_integrator = ExternalDataIntegrator(
                    self.config, self.batch_calculator
                )
            else:
                self.external_integrator = None
                logger.info("‚ö†Ô∏è ExternalDataIntegrator skipped - external data disabled")

            self.batch_engines_enabled = True

            logger.info(
                "üöÄ Phase B2.4: Batch processing engines initialized successfully - "
                "DataFrame fragmentation optimization enabled"
            )

        except Exception as e:
            logger.error(f"‚ùå Failed to initialize batch engines: {e}")
            self.batch_engines_enabled = False

    def _transform_with_batch_engines(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Phase B2.4: „Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Ç®„É≥„Ç∏„É≥„Å´„Çà„ÇãÈ´òÈÄüÁâπÂæ¥ÈáèÁîüÊàê
        DataFrameÊñ≠ÁâáÂåñ„ÇíËß£Ê∂à„Åô„ÇãÊ†πÊú¨ÁöÑÊîπÂñÑ
        """
        start_time = time.time()
        logger.info("üöÄ Starting batch processing feature generation")

        feature_batches = []

        try:
            # 1. „ÉÜ„ÇØ„Éã„Ç´„É´ÊåáÊ®ô„Éê„ÉÉ„ÉÅÂá¶ÁêÜ
            technical_batches = self.technical_engine.calculate_all_technical_batches(
                df
            )
            feature_batches.extend(technical_batches)
            logger.debug(f"üìä Technical batches: {len(technical_batches)}")

            # 2. Â§ñÈÉ®„Éá„Éº„ÇøÁµ±Âêà„Éê„ÉÉ„ÉÅÂá¶ÁêÜ
            if self.external_integrator:
                external_batches = (
                    self.external_integrator.create_external_data_batches(df.index)
                )
                feature_batches.extend(external_batches)
                logger.debug(f"üìä External data batches: {len(external_batches)}")

            # 3. ‰∏ÄÊã¨Áµ±ÂêàÔºàÊñ≠ÁâáÂåñËß£Ê∂à„ÅÆ‰∏≠Ê†∏Ôºâ
            result_df = self.batch_calculator.merge_batches_efficient(
                df, feature_batches
            )

            processing_time = time.time() - start_time
            total_features = sum(len(batch) for batch in feature_batches)

            logger.info(
                f"‚úÖ Batch processing completed: {total_features} features "
                f"in {processing_time:.3f}s ({total_features/processing_time:.1f} features/sec)"
            )

            # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÁµ±Ë®àÂá∫Âäõ
            batch_stats = self.batch_calculator.get_performance_summary()
            logger.debug(f"üìä Batch Performance:\n{batch_stats}")

            return result_df

        except Exception as e:
            import traceback

            logger.error(f"‚ùå Batch processing failed: {e}")
            logger.error(
                f"‚ùå Batch processing error details:\n{traceback.format_exc()}"
            )
            logger.warning("‚ö†Ô∏è Falling back to legacy processing")
            return self._transform_legacy(df)

    def _transform_legacy(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        „É¨„Ç¨„Ç∑„ÉºÁâπÂæ¥ÈáèÂá¶ÁêÜÔºà„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÁî®Ôºâ
        ÂÄãÂà•DataFrameÊìç‰Ωú„Å´„Çà„ÇãÂæìÊù•ÊñπÂºè
        """
        logger.warning("‚ö†Ô∏è Using legacy feature processing - performance may be slower")

        # 2. ATR
        feat_period = self.config["ml"]["feat_period"]
        atr = self.ind_calc.atr(df, window=feat_period)
        if isinstance(atr, pd.Series):
            df[f"ATR_{feat_period}"] = atr
        else:
            df[f"ATR_{feat_period}"] = atr.iloc[:, 0]
        logger.debug("After ATR: %s", df.shape)

        # 3. lagÁâπÂæ¥Èáè
        for lag in self.config["ml"]["lags"]:
            df[f"close_lag_{lag}"] = df["close"].shift(lag)
        logger.debug("After lag feats: %s", df.shape)

        # 4. rollingÁµ±Ë®à
        win = self.config["ml"]["rolling_window"]
        df[f"close_mean_{win}"] = df["close"].rolling(win).mean()
        df[f"close_std_{win}"] = df["close"].rolling(win).std()
        logger.debug("After rolling stats: %s", df.shape)

        # 5. extra_featuresÂá¶ÁêÜÔºà„É¨„Ç¨„Ç∑„ÉºÊñπÂºèÔºâ
        return self._process_extra_features_legacy(df)

    def _process_extra_features_legacy(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        „É¨„Ç¨„Ç∑„ÉºÊñπÂºè„Å´„Çà„Çã extra_features Âá¶ÁêÜ
        „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÊôÇ„Å´‰ΩøÁî®ÔºàÂÄãÂà•DataFrameÊìç‰ΩúÔºâ
        """
        if not self.extra_features:
            return df

        logger.warning(
            "‚ö†Ô∏è Using legacy feature processing - individual DataFrame operations"
        )
        logger.debug(f"Legacy processing for: {self.extra_features}")

        # „É¨„Ç¨„Ç∑„ÉºÂá¶ÁêÜ„Åß„ÅØÊó¢Â≠ò„ÅÆextra_featuresÂá¶ÁêÜ„É´„Éº„Éó„Çí‰ΩøÁî®
        # Ôºà„É°„Ç§„É≥„ÅÆtransformÂÜÖ„ÅÆÂá¶ÁêÜ„Å®ÈáçË§á„ÇíÈÅø„Åë„Çã„Åü„ÇÅ„ÄÅ„Åì„Åì„Åß„ÅØÂü∫Êú¨Âá¶ÁêÜ„ÅÆ„ÅøÔºâ

        return df

    def _get_cached_external_data(
        self, data_type: str, time_index: pd.DatetimeIndex
    ) -> pd.DataFrame:
        """
        „Ç≠„É£„ÉÉ„Ç∑„É•„Åã„ÇâÂ§ñÈÉ®„Éá„Éº„Çø„ÇíÂèñÂæó

        Parameters
        ----------
        data_type : str
            „Éá„Éº„Çø„Çø„Ç§„Éó ('vix', 'macro', 'forex', 'fear_greed', 'funding')
        time_index : pd.DatetimeIndex
            ÂØæË±°ÊúüÈñì„ÅÆ„Çø„Ç§„É†„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ

        Returns
        -------
        pd.DataFrame
            „Ç≠„É£„ÉÉ„Ç∑„É•„Åï„Çå„ÅüÂ§ñÈÉ®„Éá„Éº„ÇøÔºàË©≤ÂΩìÊúüÈñìÔºâ
        """
        try:
            from crypto_bot.ml.external_data_cache import get_global_cache

            cache = get_global_cache()
            if not cache.is_initialized:
                return pd.DataFrame()

            start_time = time_index.min()
            end_time = time_index.max()

            cached_data = cache.get_period_data(data_type, start_time, end_time)
            return cached_data

        except Exception as e:
            logger.debug(f"Failed to get cached {data_type} data: {e}")
            return pd.DataFrame()

    def _validate_external_data_fetchers(self) -> dict:
        """
        Â§ñÈÉ®„Éá„Éº„Çø„Éï„Çß„ÉÉ„ÉÅ„É£„Éº„ÅÆÁä∂ÊÖã„ÇíÊ§úË®º„Åó„ÄÅ„Éá„Éº„ÇøÂìÅË≥™ÊîπÂñÑ„ÅÆ„Åü„ÇÅ„ÅÆÊÉÖÂ†±„ÇíËøî„Åô

        Returns
        -------
        dict
            „Éï„Çß„ÉÉ„ÉÅ„É£„Éº„ÅÆÁä∂ÊÖã„É¨„Éù„Éº„Éà
        """
        validation_report = {
            "vix": {"available": False, "initialized": False, "working": False},
            "macro": {"available": False, "initialized": False, "working": False},
            "forex": {"available": False, "initialized": False, "working": False},
            "fear_greed": {"available": False, "initialized": False, "working": False},
            "total_working": 0,
            "external_data_success_rate": 0.0,
        }

        # VIXÊ§úË®º
        if "vix" in self.extra_features:
            validation_report["vix"]["available"] = True
            if self.vix_fetcher is not None:
                validation_report["vix"]["initialized"] = True
                try:
                    # Á∞°Âçò„Å™„ÉÜ„Çπ„ÉàÂèñÂæó
                    test_data = self.vix_fetcher.get_vix_data(timeframe="1d", limit=1)
                    if test_data is not None and not test_data.empty:
                        validation_report["vix"]["working"] = True
                        validation_report["total_working"] += 1
                        logger.info("‚úÖ VIX fetcher validation: WORKING")
                    else:
                        logger.warning(
                            "‚ö†Ô∏è VIX fetcher validation: NOT WORKING (empty data)"
                        )
                except Exception as e:
                    logger.error(f"‚ùå VIX fetcher validation failed: {e}")

        # MacroÊ§úË®º
        if any(feat in self.extra_features for feat in ["dxy", "macro", "treasury"]):
            validation_report["macro"]["available"] = True
            if self.macro_fetcher is not None:
                validation_report["macro"]["initialized"] = True
                try:
                    # Á∞°Âçò„Å™„ÉÜ„Çπ„ÉàÂèñÂæó
                    test_data = self.macro_fetcher.get_macro_data()
                    if test_data and not all(df.empty for df in test_data.values()):
                        validation_report["macro"]["working"] = True
                        validation_report["total_working"] += 1
                        logger.info("‚úÖ Macro fetcher validation: WORKING")
                    else:
                        logger.warning(
                            "‚ö†Ô∏è Macro fetcher validation: NOT WORKING (empty data)"
                        )
                except Exception as e:
                    logger.error(f"‚ùå Macro fetcher validation failed: {e}")

        # Fear&GreedÊ§úË®º
        if any(feat in self.extra_features for feat in ["fear_greed", "fg"]):
            validation_report["fear_greed"]["available"] = True
            if self.fear_greed_fetcher is not None:
                validation_report["fear_greed"]["initialized"] = True
                try:
                    # Á∞°Âçò„Å™„ÉÜ„Çπ„ÉàÂèñÂæó
                    test_data = self.fear_greed_fetcher.get_fear_greed_data(days_back=1)
                    if test_data is not None and not test_data.empty:
                        validation_report["fear_greed"]["working"] = True
                        validation_report["total_working"] += 1
                        logger.info("‚úÖ Fear&Greed fetcher validation: WORKING")
                    else:
                        logger.warning(
                            "‚ö†Ô∏è Fear&Greed fetcher validation: NOT WORKING (empty data)"
                        )
                except Exception as e:
                    logger.error(f"‚ùå Fear&Greed fetcher validation failed: {e}")

        # ForexÊ§úË®º
        if any(feat in self.extra_features for feat in ["forex", "usdjpy", "jpy"]):
            validation_report["forex"]["available"] = True
            if self.forex_fetcher is not None:
                validation_report["forex"]["initialized"] = True
                try:
                    # Á∞°Âçò„Å™„ÉÜ„Çπ„ÉàÂèñÂæó
                    test_data = self.forex_fetcher.get_macro_data()
                    if (
                        test_data
                        and "usdjpy" in test_data
                        and not test_data["usdjpy"].empty
                    ):
                        validation_report["forex"]["working"] = True
                        validation_report["total_working"] += 1
                        logger.info("‚úÖ Forex fetcher validation: WORKING")
                    else:
                        logger.warning(
                            "‚ö†Ô∏è Forex fetcher validation: NOT WORKING (empty data)"
                        )
                except Exception as e:
                    logger.error(f"‚ùå Forex fetcher validation failed: {e}")

        # ÊàêÂäüÁéáË®àÁÆó
        total_available = sum(
            1
            for fetcher in validation_report.values()
            if isinstance(fetcher, dict) and fetcher.get("available", False)
        )
        if total_available > 0:
            validation_report["external_data_success_rate"] = (
                validation_report["total_working"] / total_available
            )

        logger.info(
            f"üîç External data validation: "
            f"{validation_report['total_working']}/{total_available} fetchers working "
            f"({validation_report['external_data_success_rate']*100:.1f}% success rate)"
        )
        return validation_report

    def fit(self, X: pd.DataFrame, y=None):
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        logger.debug("Input DataFrame shape: %s", X.shape)

        # Â§ñÈÉ®„Éá„Éº„Çø„Éï„Çß„ÉÉ„ÉÅ„É£„Éº„ÅÆÁä∂ÊÖãÊ§úË®ºÔºà„Éá„Éº„ÇøÂìÅË≥™ÊîπÂñÑÔºâ
        validation_report = self._validate_external_data_fetchers()
        logger.info(
            f"üîç External data fetcher status: "
            f"{validation_report['total_working']} working, "
            f"{validation_report['external_data_success_rate']*100:.1f}% success rate"
        )

        if X.empty:
            feat_period = self.config["ml"]["feat_period"]
            win = self.config["ml"]["rolling_window"]
            lags = self.config["ml"]["lags"]
            columns = ["ATR_" + str(feat_period)]
            columns.extend([f"close_lag_{lag}" for lag in lags])
            columns.extend([f"close_mean_{win}", f"close_std_{win}"])
            for feat in self.extra_features:
                feat_lc = feat.lower()
                base, _, param = feat_lc.partition("_")
                period = int(param) if param.isdigit() else None
                if base == "rsi" and period:
                    columns.append(f"rsi_{period}")
                elif base == "ema" and period:
                    columns.append(f"ema_{period}")
                elif base == "sma" and period:
                    columns.append(f"sma_{period}")
                elif base == "macd":
                    columns.extend(["macd", "macd_signal", "macd_hist"])
                elif base == "rci" and period:
                    columns.append(f"rci_{period}")
                elif base == "volume" and "zscore" in feat_lc:
                    period_str = feat_lc.split("_")[-1]
                    win_z = int(period_str) if period_str.isdigit() else win
                    columns.append(f"volume_zscore_{win_z}")
                elif feat_lc == "day_of_week":
                    columns.append("day_of_week")
                elif feat_lc == "hour_of_day":
                    columns.append("hour_of_day")
                # Phase F.3: Êñ∞Ë¶èÁâπÂæ¥Èáè„ÅÆÂàóÂêçÂÆöÁæ©
                elif feat_lc in [
                    "volatility_24h",
                    "volatility_1h",
                    "volume_change_24h",
                    "volume_change_1h",
                    "price_change_24h",
                    "price_change_4h",
                    "price_change_1h",
                    "cmf_20",
                    "willr_14",
                ]:
                    columns.append(feat_lc)
                elif feat_lc in ["mochipoyo_long_signal", "mochipoyo_short_signal"]:
                    columns.extend(["mochipoyo_long_signal", "mochipoyo_short_signal"])
            return pd.DataFrame(columns=columns)
        df = X.copy()

        # 1. Ê¨†ÊêçË£úÂÆå
        df = df.ffill()
        logger.debug("After ffill: %s", df.shape)

        # Phase B2.4: „Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Å´„Çà„ÇãÈ´òÈÄüÁâπÂæ¥ÈáèÁîüÊàê
        logger.info(f"üîç Batch engines enabled: {self.batch_engines_enabled}")
        if self.batch_engines_enabled:
            df = self._transform_with_batch_engines(df)
        else:
            # „É¨„Ç¨„Ç∑„ÉºÂá¶ÁêÜÔºà„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ
            df = self._transform_legacy(df)

        # 6. ÊúÄÁµÇÁâπÂæ¥ÈáèÊ§úË®º„ÉªÊ¨†ÊêçÂÄ§Âá¶ÁêÜ
        # Phase B2.5: „Éê„ÉÉ„ÉÅÂá¶ÁêÜÊúâÂäπÊôÇ„ÅØËøΩÂä†Âá¶ÁêÜ„Çí„Çπ„Ç≠„ÉÉ„ÉóÔºà„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„ÅßÂÆå‰∫ÜÊ∏à„ÅøÔºâ
        if self.extra_features and not self.batch_engines_enabled:
            logger.debug("Adding extra features: %s", self.extra_features)
            # ËøΩÂä†„Åßmochipoyo„ÅÆ„Ç∑„Ç∞„Éä„É´„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØ‰∏ÄÂ∫¶„Åæ„Å®„ÇÅ„Å¶ÂèñÂæó„Åó„Å¶„Åä„Åè
            mochipoyo_needed = any(
                feat.lower() in ["mochipoyo_long_signal", "mochipoyo_short_signal"]
                for feat in self.extra_features
            )
            mochipoyo_signals = None
            if mochipoyo_needed:
                mochipoyo_signals = self.ind_calc.mochipoyo_signals(df)

            for feat in self.extra_features:
                feat_lc = feat.lower()
                try:
                    # fear_greed„ÅØÁâπÂà•Âá¶ÁêÜÔºà„Ç¢„É≥„ÉÄ„Éº„Çπ„Ç≥„Ç¢„ÇíÂê´„ÇÄË§áÂêàË™ûÔºâ
                    if feat_lc == "fear_greed":
                        base = "fear_greed"
                        param = ""
                        period = None
                    else:
                        base, _, param = feat_lc.partition("_")
                        period = int(param) if param.isdigit() else None

                    # Phase B2.5: „Éê„ÉÉ„ÉÅÂá¶ÁêÜÊ∏à„ÅøÁâπÂæ¥Èáè„ÅÆ„Çπ„Ç≠„ÉÉ„Éó„É≠„Ç∏„ÉÉ„ÇØ
                    if self.batch_engines_enabled:
                        # „Éê„ÉÉ„ÉÅÂá¶ÁêÜÊ∏à„ÅøÁâπÂæ¥Èáè„ÅØ„Çπ„Ç≠„ÉÉ„Éó
                        skip_features = [
                            "rsi",
                            "ema",
                            "sma",
                            "macd",
                            "atr",
                            "volume",
                            "stoch",
                            "bb",
                            "bollinger",
                            "willr",
                            "williams",
                            "adx",
                            "cmf",
                            "fisher",
                            "vix",
                            "dxy",
                            "macro",
                            "treasury",
                            "fear_greed",
                            "fg",
                        ]
                        if any(
                            base == skip_feat or base.startswith(skip_feat)
                            for skip_feat in skip_features
                        ):
                            logger.debug(f"Skipping batch-processed feature: {feat}")
                            continue

                    # ÊôÇÈñìÁâπÂæ¥ÈáèÔºà„Éê„ÉÉ„ÉÅÂá¶ÁêÜÂØæË±°Â§ñ„ÅÆ„Åü„ÇÅÁ∂ôÁ∂öÂá¶ÁêÜÔºâ
                    if feat_lc == "day_of_week":
                        if isinstance(df.index, pd.DatetimeIndex):
                            df["day_of_week"] = df.index.dayofweek.astype("int8")
                        else:
                            df["day_of_week"] = 0
                    elif feat_lc == "hour_of_day":
                        if isinstance(df.index, pd.DatetimeIndex):
                            df["hour_of_day"] = df.index.hour.astype("int8")
                        else:
                            df["hour_of_day"] = 0

                    # Phase F.3: 151ÁâπÂæ¥ÈáèWARNINGËß£Ê∂à - ‰∏çË∂≥ÁâπÂæ¥ÈáèÂá¶ÁêÜ
                    elif feat_lc == "volatility_24h":
                        df["volatility_24h"] = self.ind_calc.volatility_24h(df["close"])
                    elif feat_lc == "volatility_1h":
                        df["volatility_1h"] = self.ind_calc.volatility_1h(df["close"])
                    elif feat_lc == "volume_change_24h":
                        df["volume_change_24h"] = self.ind_calc.volume_change_24h(
                            df["volume"]
                        )
                    elif feat_lc == "volume_change_1h":
                        df["volume_change_1h"] = self.ind_calc.volume_change_1h(
                            df["volume"]
                        )
                    elif feat_lc == "price_change_24h":
                        df["price_change_24h"] = self.ind_calc.price_change_24h(
                            df["close"]
                        )
                    elif feat_lc == "price_change_4h":
                        df["price_change_4h"] = self.ind_calc.price_change_4h(
                            df["close"]
                        )
                    elif feat_lc == "price_change_1h":
                        df["price_change_1h"] = self.ind_calc.price_change_1h(
                            df["close"]
                        )
                    elif feat_lc == "cmf_20":
                        df["cmf_20"] = self.ind_calc.cmf_20(df)
                    elif feat_lc == "willr_14":
                        df["willr_14"] = self.ind_calc.willr_14(df)

                    # Phase B2.5: VIXÁâπÂæ¥Èáè„ÅØÊó¢„Å´ExternalDataIntegrator„ÅßÂá¶ÁêÜÊ∏à„Åø
                    elif base == "vix":
                        try:
                            logger.info(
                                f"üîç Processing VIX features: "
                                f"vix_enabled={self.vix_enabled}, "
                                f"vix_fetcher={self.vix_fetcher is not None}"
                            )

                            vix_features = None

                            # „Ç≠„É£„ÉÉ„Ç∑„É•„Åã„ÇâVIX„Éá„Éº„Çø„ÇíÂèñÂæóÔºàÂÑ™ÂÖàÔºâ
                            cached_vix = self._get_cached_external_data("vix", df.index)
                            if not cached_vix.empty:
                                logger.info(
                                    f"‚úÖ Using cached VIX: {len(cached_vix)} records"
                                )
                                vix_features = cached_vix

                            # „Ç≠„É£„ÉÉ„Ç∑„É•„ÅåÁ©∫„ÅÆÂ†¥Âêà„ÄÅVIX„Éï„Çß„ÉÉ„ÉÅ„É£„Éº„ÅßÁõ¥Êé•ÂèñÂæó
                            if vix_features is None or vix_features.empty:
                                if self.vix_fetcher:
                                    logger.info("üîç Fetching fresh VIX data...")
                                    try:
                                        vix_data = self.vix_fetcher.get_vix_data(
                                            timeframe="1d", limit=100
                                        )
                                        if not vix_data.empty:
                                            logger.info(
                                                f"‚úÖ VIX: {len(vix_data)} records"
                                            )
                                            vix_features = (
                                                self.vix_fetcher.calculate_vix_features(
                                                    vix_data
                                                )
                                            )
                                        else:
                                            logger.warning(
                                                "‚ùå VIX data empty from fetcher"
                                            )
                                    except Exception as e:
                                        logger.error(f"‚ùå VIX fetching failed: {e}")
                                else:
                                    logger.warning("‚ùå VIX fetcher not available")

                            # VIX„Éá„Éº„Çø„ÅåÂèñÂæó„Åß„Åç„ÅüÂ†¥Âêà„ÅÆÂá¶ÁêÜ
                            if vix_features is not None and not vix_features.empty:
                                # „Çø„Ç§„É†„Çæ„Éº„É≥Áµ±‰∏Ä„Éª„Éá„Éº„Çø„Ç¢„É©„Ç§„É°„É≥„ÉàÊîπËâØ
                                if isinstance(
                                    df.index, pd.DatetimeIndex
                                ) and isinstance(vix_features.index, pd.DatetimeIndex):
                                    # „Çø„Ç§„É†„Çæ„Éº„É≥Áµ±‰∏ÄÔºàÈáçË¶Å„Å™‰øÆÊ≠£Ôºâ
                                    if df.index.tz is None:
                                        df.index = df.index.tz_localize("UTC")
                                    if vix_features.index.tz is None:
                                        vix_features.index = (
                                            vix_features.index.tz_localize("UTC")
                                        )
                                    elif vix_features.index.tz != df.index.tz:
                                        vix_features.index = (
                                            vix_features.index.tz_convert("UTC")
                                        )

                                    # ÊîπËâØ„Åï„Çå„Åü„É™„Çµ„É≥„Éó„É™„É≥„Ç∞ÔºöÊó•Ê¨°‚ÜíÊôÇÈñìË∂≥„Å∏„ÅÆÂ§âÊèõ
                                    # ÂâçÊñπË£úÂÆåÔºàffillÔºâ„ÅßÊó•Ê¨°„Éá„Éº„Çø„ÇíÊôÇÈñìË∂≥„Å´Â±ïÈñã
                                    vix_hourly = vix_features.resample("H").ffill()

                                    # ÊöóÂè∑Ë≥áÁî£„Éá„Éº„Çø„ÅÆÊôÇÈñìÁØÑÂõ≤„Å´Âêà„Çè„Åõ„Å¶Âà∂Èôê
                                    start_time = df.index.min()
                                    end_time = df.index.max()
                                    vix_hourly = vix_hourly.loc[start_time:end_time]

                                    # „Çà„ÇäÊüîËªü„Å™„Éû„ÉÉ„ÉÅ„É≥„Ç∞ÔºöÊúÄ„ÇÇËøë„ÅÑÊôÇÂàª„ÅÆ„Éá„Éº„Çø„Çí‰ΩøÁî®
                                    vix_cols = [
                                        "vix_level",
                                        "vix_change",
                                        "vix_zscore",
                                        "fear_level",
                                        "vix_spike",
                                        "vix_regime",
                                    ]
                                    added_features = 0

                                    for i, timestamp in enumerate(df.index):
                                        # ÊúÄ„ÇÇËøë„ÅÑVIX„Éá„Éº„Çø„Éù„Ç§„É≥„Éà„ÇíÊ§úÁ¥¢
                                        if len(vix_hourly) > 0:
                                            closest_idx = vix_hourly.index.get_indexer(
                                                [timestamp], method="ffill"
                                            )[0]
                                            if closest_idx >= 0:
                                                vix_row = vix_hourly.iloc[closest_idx]
                                                for col in vix_cols:
                                                    if col in vix_row.index:
                                                        if col == "vix_regime":
                                                            # „Ç´„ÉÜ„Ç¥„É™„ÇíÊï∞ÂÄ§„Å´Â§âÊèõ
                                                            regime_map = {
                                                                "low": 0,
                                                                "normal": 1,
                                                                "high": 2,
                                                                "extreme": 3,
                                                            }
                                                            k = "vix_regime_numeric"
                                                            v = regime_map.get(
                                                                vix_row[col], 1
                                                            )
                                                            df.loc[timestamp, k] = v
                                                        else:
                                                            df.loc[timestamp, col] = (
                                                                vix_row[col]
                                                            )
                                        added_features = i + 1

                                    logger.info(
                                        f"VIX: {added_features}/{len(df)} points"
                                    )
                                else:
                                    logger.warning(
                                        "Could not align VIX data - index type mismatch"
                                    )
                            else:
                                logger.warning("No VIX data available - using defaults")
                        except Exception as e:
                            logger.warning("Failed to add VIX features: %s", e)

                    # OIÔºàÊú™Ê±∫Ê∏àÂª∫ÁéâÔºâÈñ¢ÈÄ£ÁâπÂæ¥Èáè
                    elif base == "oi":
                        try:
                            # from crypto_bot.data.fetcher import (
                            #     MarketDataFetcher,
                            #     OIDataFetcher,
                            # )

                            # OI„Éá„Éº„ÇøÂèñÂæó„ÅÆ„Åü„ÇÅ„ÅÆ„Éó„É¨„Éº„Çπ„Éõ„É´„ÉÄ„ÉºÔºàÂÆüÈöõ„Å´„ÅØ„Çà„ÇäË©≥Á¥∞„Å™ÂÆüË£Ö„ÅåÂøÖË¶ÅÔºâ
                            # ÁèæÊôÇÁÇπ„Åß„ÅØÂü∫Êú¨ÁöÑ„Å™OIÁâπÂæ¥Èáè„Çí„Ç∑„Éü„É•„É¨„Éº„Éà
                            # OIÂ§âÂåñÁéáÔºà‰æ°Ê†º„Å®OI„ÅÆÁõ∏Èñ¢Ôºâ
                            df["oi_price_divergence"] = (
                                df["close"].pct_change()
                                - df["volume"].pct_change().fillna(0)
                            ).fillna(0)

                            # „Éú„É™„É•„Éº„É†Âº∑Â∫¶ÔºàOI„ÅÆ‰ª£ÊõøÔºâ
                            df["volume_intensity"] = (
                                df["volume"] / df["volume"].rolling(20).mean()
                            )

                            # OIÂã¢„ÅÑÔºà„Éú„É™„É•„Éº„É†„Éô„Éº„ÇπÔºâ
                            df["oi_momentum_proxy"] = (
                                df["volume"].rolling(10).sum()
                                / df["volume"].rolling(50).sum()
                            )

                        except Exception as e:
                            logger.warning("Failed to add OI features: %s", e)

                    # Phase B2.5: MacroÁµåÊ∏à„Éá„Éº„ÇøÁâπÂæ¥Èáè„ÅØÊó¢„Å´ExternalDataIntegrator„ÅßÂá¶ÁêÜÊ∏à„Åø
                    elif base in ["dxy", "macro", "treasury"]:
                        try:
                            logger.info(
                                f"üîç Processing Macro features: "
                                f"macro_enabled={self.macro_enabled}, "
                                f"macro_fetcher={self.macro_fetcher is not None}"
                            )

                            macro_features = None

                            # „Ç≠„É£„ÉÉ„Ç∑„É•„Åã„Çâ„Éû„ÇØ„É≠„Éá„Éº„Çø„ÇíÂèñÂæóÔºàÂÑ™ÂÖàÔºâ
                            cached_macro = self._get_cached_external_data(
                                "macro", df.index
                            )
                            if not cached_macro.empty:
                                logger.info(
                                    f"‚úÖ Using cached macro: {len(cached_macro)} records"
                                )
                                macro_features = cached_macro

                            # „Ç≠„É£„ÉÉ„Ç∑„É•„ÅåÁ©∫„ÅÆÂ†¥Âêà„ÄÅ„Éû„ÇØ„É≠„Éï„Çß„ÉÉ„ÉÅ„É£„Éº„ÅßÁõ¥Êé•ÂèñÂæó
                            if macro_features is None or macro_features.empty:
                                if self.macro_fetcher:
                                    logger.info("üîç Fetching fresh Macro data...")
                                    try:
                                        macro_data = self.macro_fetcher.get_macro_data()
                                        if macro_data and not all(
                                            df.empty for df in macro_data.values()
                                        ):
                                            logger.info(
                                                f"‚úÖ Macro: {len(macro_data)} datasets"
                                            )
                                            calc_func = (
                                                self.macro_fetcher.calculate_macro_features  # noqa: E501
                                            )
                                            macro_features = calc_func(macro_data)
                                        else:
                                            logger.warning(
                                                "‚ùå Macro data empty from fetcher"
                                            )
                                    except Exception as e:
                                        logger.error(f"‚ùå Macro fetching failed: {e}")
                                else:
                                    logger.warning("‚ùå Macro fetcher not available")

                            # „Éû„ÇØ„É≠„Éá„Éº„Çø„ÅåÂèñÂæó„Åß„Åç„ÅüÂ†¥Âêà„ÅÆÂá¶ÁêÜ
                            if macro_features is not None and not macro_features.empty:
                                # „Ç≠„É£„ÉÉ„Ç∑„É•„Åå„Å™„ÅÑÂ†¥Âêà„ÅØÂæìÊù•„ÅÆÊñπÊ≥ï
                                if hasattr(df, "index") and len(df) > 0:
                                    backtest_year = df.index.min().year
                                    self.macro_fetcher._backtest_start_year = (
                                        backtest_year
                                    )
                                macro_data = self.macro_fetcher.get_macro_data(limit=50)
                                if macro_data and not all(
                                    df.empty for df in macro_data.values()
                                ):
                                    macro_features = (
                                        self.macro_fetcher.calculate_macro_features(
                                            macro_data
                                        )
                                    )

                                    # ÊöóÂè∑Ë≥áÁî£„Éá„Éº„Çø„Å®„Éû„ÇØ„É≠„Éá„Éº„Çø„ÅÆÊôÇÈñìËª∏„ÇíÂêà„Çè„Åõ„Çã
                                    if isinstance(
                                        df.index, pd.DatetimeIndex
                                    ) and isinstance(
                                        macro_features.index, pd.DatetimeIndex
                                    ):
                                        # „Çø„Ç§„É†„Çæ„Éº„É≥„ÇíÁµ±‰∏Ä
                                        if df.index.tz is None:
                                            df.index = df.index.tz_localize("UTC")
                                        if macro_features.index.tz is None:
                                            macro_features.index = (
                                                macro_features.index.tz_localize("UTC")
                                            )
                                        elif macro_features.index.tz != df.index.tz:
                                            macro_features.index = (
                                                macro_features.index.tz_convert("UTC")
                                            )

                                        # ÊîπËâØ„Åï„Çå„Åü„É™„Çµ„É≥„Éó„É™„É≥„Ç∞ÔºöÊó•Ê¨°‚ÜíÊôÇÈñìË∂≥„Å∏„ÅÆÂ§âÊèõ
                                        macro_hourly = macro_features.resample(
                                            "H"
                                        ).ffill()

                                        # ÊöóÂè∑Ë≥áÁî£„Éá„Éº„Çø„ÅÆÊôÇÈñìÁØÑÂõ≤„Å´Âêà„Çè„Åõ„Å¶Âà∂Èôê
                                        start_time = df.index.min()
                                        end_time = df.index.max()
                                        macro_hourly = macro_hourly.loc[
                                            start_time:end_time
                                        ]

                                        # „Çà„ÇäÊüîËªü„Å™„Éû„ÉÉ„ÉÅ„É≥„Ç∞ÔºöÊúÄ„ÇÇËøë„ÅÑÊôÇÂàª„ÅÆ„Éá„Éº„Çø„Çí‰ΩøÁî®
                                        macro_cols = [
                                            "dxy_level",
                                            "dxy_change",
                                            "dxy_zscore",
                                            "dxy_strength",
                                            "treasury_10y_level",
                                            "treasury_10y_change",
                                            "treasury_10y_zscore",
                                            "treasury_regime",
                                            "yield_curve_spread",
                                            "risk_sentiment",
                                            # USD/JPYÁÇ∫ÊõøÁâπÂæ¥ÈáèËøΩÂä†
                                            "usdjpy_level",
                                            "usdjpy_change",
                                            "usdjpy_volatility",
                                            "usdjpy_zscore",
                                            "usdjpy_trend",
                                            "usdjpy_strength",
                                        ]
                                        added_features = 0

                                        for i, timestamp in enumerate(df.index):
                                            # ÊúÄ„ÇÇËøë„ÅÑ„Éû„ÇØ„É≠„Éá„Éº„Çø„Éù„Ç§„É≥„Éà„ÇíÊ§úÁ¥¢
                                            if len(macro_hourly) > 0:
                                                closest_idx = (
                                                    macro_hourly.index.get_indexer(
                                                        [timestamp], method="ffill"
                                                    )[0]
                                                )
                                                if closest_idx >= 0:
                                                    macro_row = macro_hourly.iloc[
                                                        closest_idx
                                                    ]
                                                    for col in macro_cols:
                                                        if col in macro_row.index:
                                                            df.loc[timestamp, col] = (
                                                                macro_row[col]
                                                            )
                                                    added_features = i + 1

                                        logger.info(
                                            f"Added DXY/macro features to "
                                            f"{added_features}/{len(df)} data points"
                                        )
                                    else:
                                        logger.warning(
                                            "Index type mismatch for "
                                            "macro data alignment"
                                        )
                                else:
                                    logger.warning("No macro data available")
                            else:
                                logger.warning("Macro fetcher not initialized")
                        except Exception as e:
                            logger.warning("Failed to add macro features: %s", e)

                    # Funding Rate & Open Interest ÁâπÂæ¥ÈáèÔºàBitbank‰ø°Áî®ÂèñÂºïÂ∞ÇÁî®‰ª£ÊõøÂÆüË£ÖÔºâ
                    elif base in ["funding", "oi"]:
                        try:
                            # Bitbank‰ø°Áî®ÂèñÂºïÔºà1ÂÄç„É¨„Éê„É¨„ÉÉ„Ç∏Ôºâ„Å´ÈÅ©„Åó„Åü‰ª£ÊõøÁâπÂæ¥ÈáèÔºà17ÁâπÂæ¥ÈáèÔºâ
                            logger.info(
                                "Adding Bitbank margin trading alternative features"
                            )

                            # 1. ÈáëÂà©„Ç≥„Çπ„ÉàÊé®ÂÆöÁâπÂæ¥ÈáèÔºàFunding Rate‰ª£ÊõøÔºâ
                            # ‰æ°Ê†ºÂ§âÂãïÁéá„Åã„ÇâÈáëÂà©„Ç≥„Çπ„Éà„ÇíÊé®ÂÆöÔºà‰ø°Áî®ÂèñÂºï„ÅÆÂÄüÂÖ•„Ç≥„Çπ„ÉàÔºâ
                            returns = df["close"].pct_change()
                            df["fr_rate"] = (
                                returns.rolling(20).std() * 100
                            )  # Â§âÂãïÁéá„Çí„Ç≥„Çπ„Éà‰ª£Êõø
                            df["fr_change_1d"] = (
                                df["fr_rate"].pct_change(24) * 100
                            )  # 1Êó•Â§âÂåñÁéá
                            df["fr_change_3d"] = (
                                df["fr_rate"].pct_change(72) * 100
                            )  # 3Êó•Â§âÂåñÁéá

                            # 2. ‰ø°Áî®ÂèñÂºï„É™„Çπ„ÇØÊåáÊ®ôÔºàFunding Z-Score‰ª£ÊõøÔºâ
                            # ‰æ°Ê†ºÂ§âÂãïÁéá„ÅÆZ-ScoreÔºà‰ø°Áî®ÂèñÂºï„É™„Çπ„ÇØÔºâ
                            vol_ma_7 = returns.rolling(7 * 24).std()
                            vol_std_7 = (
                                returns.rolling(7 * 24).std().rolling(7 * 24).std()
                            )
                            df["fr_zscore_7d"] = (
                                returns.rolling(24).std() - vol_ma_7
                            ) / vol_std_7.replace(0, 1)

                            vol_ma_30 = returns.rolling(30 * 24).std()
                            vol_std_30 = (
                                returns.rolling(30 * 24).std().rolling(30 * 24).std()
                            )
                            df["fr_zscore_30d"] = (
                                returns.rolling(24).std() - vol_ma_30
                            ) / vol_std_30.replace(0, 1)

                            # 3. Â∏ÇÂ†¥„É¨„Ç∏„Éº„É†Âà§ÂÆöÔºà‰ø°Áî®ÂèñÂºï„É™„Çπ„ÇØ„Éô„Éº„ÇπÔºâ
                            current_vol = returns.rolling(24).std()
                            df["fr_regime"] = 0  # „Éá„Éï„Ç©„É´„ÉàÔºö‰∏≠Á´ã
                            df.loc[
                                current_vol > vol_ma_30 + vol_std_30, "fr_regime"
                            ] = 1  # È´ò„É™„Çπ„ÇØ
                            df.loc[
                                current_vol < vol_ma_30 - vol_std_30, "fr_regime"
                            ] = -1  # ‰Ωé„É™„Çπ„ÇØ

                            # 4. Ê•µÁ´ØÂÄ§Ê§úÁü•Ôºà‰ø°Áî®ÂèñÂºï„É™„Çπ„ÇØËª¢Êèõ„Ç∑„Ç∞„Éä„É´Ôºâ
                            vol_q95 = current_vol.rolling(60 * 24).quantile(0.95)
                            vol_q05 = current_vol.rolling(60 * 24).quantile(0.05)
                            df["fr_extreme_long"] = (current_vol > vol_q95).astype(int)
                            df["fr_extreme_short"] = (current_vol < vol_q05).astype(int)

                            # 5. ‰ø°Áî®ÂèñÂºï„Ç≥„Çπ„ÉàÊ≥¢ÂãïÁéáÔºà„É™„Çπ„ÇØÊåáÊ®ôÔºâ
                            df["fr_volatility"] = (
                                current_vol.rolling(7 * 24).std() * 100
                            )

                            # 6. „Éà„É¨„É≥„ÉâÂº∑Â∫¶Ôºà‰ø°Áî®ÂèñÂºïÊñπÂêëÊÄßÔºâ
                            price_sma_3 = df["close"].rolling(3 * 24).mean()
                            price_sma_10 = df["close"].rolling(10 * 24).mean()
                            df["fr_trend_strength"] = (
                                (price_sma_3 - price_sma_10)
                                / price_sma_10.replace(0, 1)
                                * 100
                            )

                            # 7. „Éù„Ç∏„Ç∑„Éß„É≥Ë¶èÊ®°Êé®ÂÆöÔºàOpen Interest‰ª£ÊõøÔºâ
                            # Âá∫Êù•È´ò√ó‰æ°Ê†º„Åß‰ø°Áî®ÂèñÂºïË¶èÊ®°„ÇíÊé®ÂÆö
                            position_size = df["volume"] * df["close"]
                            oi_ma_30 = position_size.rolling(30 * 24).mean()
                            df["oi_normalized"] = (
                                position_size / oi_ma_30.replace(0, 1)
                            ) - 1

                            # 8. „Éù„Ç∏„Ç∑„Éß„É≥Â§âÂåñÁéáÔºàOIÂ§âÂåñÁéá‰ª£ÊõøÔºâ
                            df["oi_change_1d"] = position_size.pct_change(24) * 100
                            df["oi_momentum_3d"] = position_size.pct_change(72) * 100

                            # 9. „Éù„Ç∏„Ç∑„Éß„É≥Ë¶èÊ®°Z-ScoreÔºàOI Z-Score‰ª£ÊõøÔºâ
                            pos_ma_7 = position_size.rolling(7 * 24).mean()
                            pos_std_7 = position_size.rolling(7 * 24).std()
                            df["oi_zscore_7d"] = (
                                position_size - pos_ma_7
                            ) / pos_std_7.replace(0, 1)

                            # 10. „Éù„Ç∏„Ç∑„Éß„É≥Êñ∞È´òÂÄ§„ÉªÊñ∞ÂÆâÂÄ§ÔºàOIÊñ∞È´òÂÄ§„ÉªÊñ∞ÂÆâÂÄ§‰ª£ÊõøÔºâ
                            pos_max_30 = position_size.rolling(30 * 24).max()
                            pos_min_30 = position_size.rolling(30 * 24).min()
                            df["oi_new_high"] = (
                                position_size >= pos_max_30 * 0.98
                            ).astype(int)
                            df["oi_new_low"] = (
                                position_size <= pos_min_30 * 1.02
                            ).astype(int)

                            # 11. ‰ø°Áî®ÂèñÂºïÂÅèÂêëÊåáÊ®ôÔºà„Éù„Ç∏„Ç∑„Éß„É≥ÂÅèÂêëÊåáÊ®ô‰ª£ÊõøÔºâ
                            # ÈáëÂà©„Ç≥„Çπ„Éà„Å®„Éù„Ç∏„Ç∑„Éß„É≥Ë¶èÊ®°„ÅÆË§áÂêàÊåáÊ®ô
                            fr_abs = df["fr_rate"].abs()
                            oi_abs = df["oi_normalized"].abs()
                            df["position_bias"] = fr_abs * oi_abs

                            # Ê¨†ÊêçÂÄ§Âá¶ÁêÜ
                            funding_cols = [
                                "fr_rate",
                                "fr_change_1d",
                                "fr_change_3d",
                                "fr_zscore_7d",
                                "fr_zscore_30d",
                                "fr_regime",
                                "fr_extreme_long",
                                "fr_extreme_short",
                                "fr_volatility",
                                "fr_trend_strength",
                                "oi_normalized",
                                "oi_change_1d",
                                "oi_momentum_3d",
                                "oi_zscore_7d",
                                "oi_new_high",
                                "oi_new_low",
                                "position_bias",
                            ]

                            for col in funding_cols:
                                if col in df.columns:
                                    df[col] = df[col].bfill().ffill().fillna(0)
                                    df[col] = df[col].replace(
                                        [float("inf"), float("-inf")], 0
                                    )

                                    # Áï∞Â∏∏ÂÄ§„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞
                                    q975 = df[col].quantile(0.975)
                                    q025 = df[col].quantile(0.025)
                                    if (
                                        pd.notna(q975)
                                        and pd.notna(q025)
                                        and q975 != q025
                                    ):
                                        df[col] = df[col].clip(lower=q025, upper=q975)

                            logger.info(
                                f"Added {len(funding_cols)} Bitbank margin features"
                            )

                        except Exception as e:
                            logger.error(
                                "Failed to add Bitbank margin alternative features: %s",
                                e,
                            )
                            # „Ç®„É©„ÉºÊôÇ„ÅØ„Éá„Éï„Ç©„É´„ÉàÂÄ§„Åß17ÁâπÂæ¥Èáè„ÇíËøΩÂä†
                            default_cols = [
                                "fr_rate",
                                "fr_change_1d",
                                "fr_change_3d",
                                "fr_zscore_7d",
                                "fr_zscore_30d",
                                "fr_regime",
                                "fr_extreme_long",
                                "fr_extreme_short",
                                "fr_volatility",
                                "fr_trend_strength",
                                "oi_normalized",
                                "oi_change_1d",
                                "oi_momentum_3d",
                                "oi_zscore_7d",
                                "oi_new_high",
                                "oi_new_low",
                                "position_bias",
                            ]
                            for col in default_cols:
                                if col not in df.columns:
                                    df[col] = 0
                            logger.warning(
                                "Used default values for Bitbank margin features"
                            )

                    # Phase B2.5: Fear&GreedÁâπÂæ¥Èáè„ÅØÊó¢„Å´ExternalDataIntegrator„ÅßÂá¶ÁêÜÊ∏à„Åø
                    elif base in ["fear_greed", "fg"]:
                        try:
                            logger.info(
                                f"üîç Processing Fear&Greed features: "
                                f"fear_greed_enabled={self.fear_greed_enabled}, "
                                f"fetcher={self.fear_greed_fetcher is not None}"
                            )

                            fg_features = None

                            # „Ç≠„É£„ÉÉ„Ç∑„É•„Åã„ÇâFear&Greed„Éá„Éº„Çø„ÇíÂèñÂæóÔºàÂÑ™ÂÖàÔºâ
                            cached_fg = self._get_cached_external_data(
                                "fear_greed", df.index
                            )
                            if not cached_fg.empty:
                                logger.info(
                                    f"‚úÖ Cached Fear&Greed: {len(cached_fg)} records"
                                )
                                fg_features = cached_fg

                            # „Ç≠„É£„ÉÉ„Ç∑„É•„ÅåÁ©∫„ÅÆÂ†¥Âêà„ÄÅFear&Greed„Éï„Çß„ÉÉ„ÉÅ„É£„Éº„ÅßÁõ¥Êé•ÂèñÂæó
                            if fg_features is None or fg_features.empty:
                                if self.fear_greed_fetcher:
                                    logger.info("üîç Fetching fresh Fear&Greed data...")
                                    try:
                                        fg_data = (
                                            self.fear_greed_fetcher.get_fear_greed_data(
                                                days_back=30
                                            )
                                        )
                                        if not fg_data.empty:
                                            logger.info(
                                                f"‚úÖ Fear&Greed: {len(fg_data)} records"
                                            )
                                            calc_func = (
                                                self.fear_greed_fetcher.calculate_fear_greed_features  # noqa: E501
                                            )
                                            fg_features = calc_func(fg_data)
                                        else:
                                            logger.warning(
                                                "‚ùå Fear&Greed data empty from fetcher"
                                            )
                                    except Exception as e:
                                        logger.error(
                                            f"‚ùå Fear&Greed fetching failed: {e}"
                                        )
                                else:
                                    logger.warning(
                                        "‚ùå Fear&Greed fetcher not available"
                                    )

                            # Fear&Greed„Éá„Éº„Çø„ÅåÂèñÂæó„Åß„Åç„ÅüÂ†¥Âêà„ÅÆÂá¶ÁêÜ
                            if fg_features is not None and not fg_features.empty:
                                # „Ç≠„É£„ÉÉ„Ç∑„É•„Åå„Å™„ÅÑÂ†¥Âêà„ÅØÂæìÊù•„ÅÆÊñπÊ≥ï
                                fg_data = self.fear_greed_fetcher.get_fear_greed_data(
                                    days_back=30
                                )
                                if not fg_data.empty:
                                    fg_features = self.fear_greed_fetcher.calculate_fear_greed_features(  # noqa: E501
                                        fg_data
                                    )

                                    # VIX„Å®„ÅÆÁõ∏Èñ¢ÁâπÂæ¥Èáè„ÇÇËøΩÂä†
                                    if self.vix_fetcher:
                                        try:
                                            vix_data = self.vix_fetcher.get_vix_data(
                                                timeframe="1d"
                                            )
                                            if not vix_data.empty:
                                                vix_fg_correlation = self.fear_greed_fetcher.get_vix_correlation_features(  # noqa: E501
                                                    fg_data, vix_data
                                                )
                                                if not vix_fg_correlation.empty:
                                                    fg_features = pd.concat(
                                                        [
                                                            fg_features,
                                                            vix_fg_correlation,
                                                        ],
                                                        axis=1,
                                                    )
                                                    logger.debug(
                                                        "Added VIX-FG correlation features"  # noqa: E501
                                                    )
                                        except Exception as e:
                                            logger.warning(
                                                f"Failed to add VIX-FG correlation: {e}"
                                            )

                                    # ÊöóÂè∑Ë≥áÁî£„Éá„Éº„Çø„Å®Fear & Greed„Éá„Éº„Çø„ÅÆÊôÇÈñìËª∏„ÇíÂêà„Çè„Åõ„Çã
                                    if isinstance(
                                        df.index, pd.DatetimeIndex
                                    ) and isinstance(
                                        fg_features.index, pd.DatetimeIndex
                                    ):
                                        # Êó•Ê¨°„Éá„Éº„Çø„Å™„ÅÆ„Åß„ÄÅÊöóÂè∑Ë≥áÁî£„ÅÆÊôÇÈñìËª∏„Å´Âêà„Çè„Åõ„Å¶„É™„Çµ„É≥„Éó„É™„É≥„Ç∞
                                        fg_resampled = fg_features.resample(
                                            "1h"
                                        ).ffill()

                                        # „Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÇíÂêà„Çè„Åõ„Å¶ÁµêÂêà
                                        common_index = df.index.intersection(
                                            fg_resampled.index
                                        )
                                        logger.debug(
                                            f"Fear & Greed alignment: crypto {len(df)}, fg {len(fg_resampled)}, common {len(common_index)}"  # noqa: E501
                                        )

                                        # Â∞è„Åï„Å™„Éá„Éº„Çø„ÉÅ„É£„É≥„ÇØ„ÅÆÂ†¥Âêà„ÅØÊúÄÊñ∞„ÅÆFear & Greed„Éá„Éº„Çø„Çí‰ΩøÁî®
                                        if len(common_index) == 0 and len(df) > 0:
                                            # Fear & Greed„Éá„Éº„Çø„ÅåÊúüÈñìÂ§ñ„ÅÆÂ†¥Âêà„ÄÅÊúÄÊñ∞„Éá„Éº„Çø„ÅßÂÖ®Ë°å„ÇíÂüã„ÇÅ„Çã
                                            logger.warning(
                                                "Fear & Greed data period mismatch - using latest available data"  # noqa: E501
                                            )

                                            # ÊúÄÊñ∞„ÅÆFear & Greed„Éá„Éº„Çø„ÇíÂèñÂæó
                                            if not fg_resampled.empty:
                                                latest_fg_data = fg_resampled.iloc[
                                                    -1
                                                ]  # ÊúÄÊñ∞Ë°å
                                                for col in fg_features.columns:
                                                    if col in latest_fg_data.index:
                                                        df[col] = latest_fg_data[col]
                                                logger.debug(
                                                    f"Filled all {len(df)} rows with latest Fear & Greed data"  # noqa: E501
                                                )
                                            else:
                                                # Fear & Greed„Éá„Éº„Çø„ÅåÂÖ®„Åè„Å™„ÅÑÂ†¥Âêà„ÄÅ„Éá„Éï„Ç©„É´„ÉàÂÄ§„ÅßÂüã„ÇÅ„Çã
                                                logger.warning(
                                                    "No Fear & Greed data available - using default values"  # noqa: E501
                                                )
                                                for col in fg_features.columns:
                                                    if col == "fg_level":
                                                        df[col] = 50  # ‰∏≠Á´ãÂÄ§
                                                    elif col == "fg_regime":
                                                        df[col] = 3  # ‰∏≠Á´ã„É¨„Ç∏„Éº„É†
                                                    else:
                                                        df[col] = 0  # „Åù„ÅÆ‰ªñ„ÅØ0
                                        elif len(common_index) > 0:
                                            # Fear & GreedÁâπÂæ¥Èáè„ÇíËøΩÂä†
                                            for col in fg_features.columns:
                                                if col in fg_resampled.columns:
                                                    df.loc[common_index, col] = (
                                                        fg_resampled.loc[
                                                            common_index, col
                                                        ]
                                                    )

                                            logger.debug(
                                                f"Added Fear & Greed features: {len(common_index)} data points aligned"  # noqa: E501
                                            )
                                        else:
                                            logger.warning(
                                                "Could not align Fear & Greed data with crypto data"  # noqa: E501
                                            )
                                    else:
                                        logger.warning(
                                            "Index type mismatch for Fear & Greed data alignment"  # noqa: E501
                                        )
                                else:
                                    logger.warning(
                                        "No Fear & Greed data available - adding default Fear & Greed features"  # noqa: E501
                                    )
                                    # Fear & Greed„Éá„Éº„Çø„ÅåÂèñÂæó„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÄÅ„Éá„Éï„Ç©„É´„ÉàÁâπÂæ¥Èáè„ÇíËøΩÂä†
                                    from crypto_bot.data.fear_greed_fetcher import (
                                        get_available_fear_greed_features,
                                    )

                                    fg_feature_names = (
                                        get_available_fear_greed_features()
                                    )

                                    for col in fg_feature_names:
                                        if col not in df.columns:
                                            if col == "fg_level":
                                                df[col] = 50  # ‰∏≠Á´ãÂÄ§
                                            elif col == "fg_regime":
                                                df[col] = 3  # ‰∏≠Á´ã„É¨„Ç∏„Éº„É†
                                            else:
                                                df[col] = 0  # „Åù„ÅÆ‰ªñ„ÅØ0
                                    logger.debug(
                                        f"Added {len(fg_feature_names)} default Fear & Greed features"  # noqa: E501
                                    )
                            else:
                                logger.warning(
                                    "Fear & Greed fetcher not initialized - adding default Fear & Greed features"  # noqa: E501
                                )
                                # Fetcher„ÅåÂàùÊúüÂåñ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÇÇ„Éá„Éï„Ç©„É´„ÉàÁâπÂæ¥Èáè„ÇíËøΩÂä†
                                from crypto_bot.data.fear_greed_fetcher import (
                                    get_available_fear_greed_features,
                                )

                                fg_feature_names = get_available_fear_greed_features()

                                for col in fg_feature_names:
                                    if col not in df.columns:
                                        if col == "fg_level":
                                            df[col] = 50  # ‰∏≠Á´ãÂÄ§
                                        elif col == "fg_regime":
                                            df[col] = 3  # ‰∏≠Á´ã„É¨„Ç∏„Éº„É†
                                        else:
                                            df[col] = 0  # „Åù„ÅÆ‰ªñ„ÅØ0
                                logger.debug(
                                    f"Added {len(fg_feature_names)} default Fear & Greed features"  # noqa: E501
                                )
                        except Exception as e:
                            logger.warning("Failed to add Fear & Greed features: %s", e)
                            logger.warning(
                                "Adding default Fear & Greed features due to error"
                            )
                            # „Ç®„É©„ÉºÊôÇ„ÇÇ„Éá„Éï„Ç©„É´„ÉàÁâπÂæ¥Èáè„ÇíËøΩÂä†„Åó„Å¶ÁâπÂæ¥ÈáèÊï∞„Çí‰∏ÄËá¥„Åï„Åõ„Çã
                            try:
                                from crypto_bot.data.fear_greed_fetcher import (
                                    get_available_fear_greed_features,
                                )

                                fg_feature_names = get_available_fear_greed_features()

                                for col in fg_feature_names:
                                    if col not in df.columns:
                                        if col == "fg_level":
                                            df[col] = 50  # ‰∏≠Á´ãÂÄ§
                                        elif col == "fg_regime":
                                            df[col] = 3  # ‰∏≠Á´ã„É¨„Ç∏„Éº„É†
                                        else:
                                            df[col] = 0  # „Åù„ÅÆ‰ªñ„ÅØ0
                                logger.debug(
                                    f"Added {len(fg_feature_names)} default Fear & Greed features after error"  # noqa: E501
                                )
                            except Exception as inner_e:
                                logger.error(
                                    f"Failed to add default Fear & Greed features: {inner_e}"  # noqa: E501
                                )

                    # mochipoyo_long_signal or mochipoyo_short_signal
                    elif (
                        feat_lc == "mochipoyo_long_signal"
                        or feat_lc == "mochipoyo_short_signal"
                    ):
                        if mochipoyo_signals is not None:
                            for signal_col in mochipoyo_signals.columns:
                                if signal_col not in df:
                                    df[signal_col] = mochipoyo_signals[signal_col]

                    # momentum_14 ÁâπÂæ¥Èáè
                    elif feat_lc == "momentum_14":
                        df["momentum_14"] = df["close"].pct_change(14).fillna(0)

                    # trend_strength ÁâπÂæ¥Èáè
                    elif feat_lc == "trend_strength":
                        # „Éà„É¨„É≥„ÉâÂº∑Â∫¶„ÇíË®àÁÆóÔºàADX„Éô„Éº„ÇπÔºâ
                        try:
                            import pandas_ta as ta

                            adx_result = ta.adx(
                                high=df["high"],
                                low=df["low"],
                                close=df["close"],
                                length=14,
                            )

                            if adx_result is not None and not adx_result.empty:
                                # ADX„ÅÆÁµêÊûú„ÅØDataFrame„Å™„ÅÆ„Åß„ÄÅADXÂàó„ÇíÂèñÂæó
                                if (
                                    isinstance(adx_result, pd.DataFrame)
                                    and "ADX_14" in adx_result.columns
                                ):
                                    df["trend_strength"] = adx_result["ADX_14"].fillna(
                                        25
                                    )
                                elif isinstance(adx_result, pd.Series):
                                    df["trend_strength"] = adx_result.fillna(25)
                                else:
                                    df["trend_strength"] = 25
                            else:
                                df["trend_strength"] = 25  # „Éá„Éï„Ç©„É´„ÉàÂÄ§
                        except Exception as e:
                            logger.warning(f"Failed to calculate trend_strength: {e}")
                            df["trend_strength"] = 25  # „Éá„Éï„Ç©„É´„ÉàÂÄ§

                    # volatility_regime ÁâπÂæ¥Èáè (5ÁâπÂæ¥Èáè)
                    elif feat_lc == "volatility_regime":
                        try:
                            # Ë§áÊï∞ÊúüÈñì„Åß„ÅÆ„Éú„É©„ÉÜ„Ç£„É™„ÉÜ„Ç£„É¨„Ç∏„Éº„É†ÂàÜÊûê
                            volatility_windows = [10, 20, 50]

                            for window in volatility_windows:
                                # rollingÊ®ôÊ∫ñÂÅèÂ∑Æ„ÇíË®àÁÆó
                                vol = df["close"].rolling(window=window).std()

                                # „Éú„É©„ÉÜ„Ç£„É™„ÉÜ„Ç£„É¨„Ç∏„Éº„É†„Çí3ÊÆµÈöé„Å´ÂàÜÈ°û (0:‰Ωé, 1:‰∏≠, 2:È´ò)
                                vol_regime = pd.cut(vol, bins=3, labels=[0, 1, 2])
                                df[f"vol_regime_{window}"] = pd.to_numeric(
                                    vol_regime, errors="coerce"
                                ).fillna(1)

                                # „Éú„É©„ÉÜ„Ç£„É™„ÉÜ„Ç£ÁôæÂàÜ‰ΩçÊï∞
                                df[f"vol_percentile_{window}"] = (
                                    vol.rolling(window=100, min_periods=window)
                                    .rank(pct=True)
                                    .fillna(0.5)
                                )

                            # Áü≠ÊúüvsÈï∑Êúü„Éú„É©„ÉÜ„Ç£„É™„ÉÜ„Ç£ÊØîÁéá
                            short_vol = df["close"].rolling(window=10).std()
                            long_vol = df["close"].rolling(window=50).std()
                            df["vol_ratio_short_long"] = (short_vol / long_vol).fillna(
                                1.0
                            )

                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate volatility_regime: {e}"
                            )
                            # „Éá„Éï„Ç©„É´„ÉàÂÄ§Ë®≠ÂÆö
                            for window in [10, 20, 50]:
                                df[f"vol_regime_{window}"] = 1
                                df[f"vol_percentile_{window}"] = 0.5
                            df["vol_ratio_short_long"] = 1.0

                    # momentum_signals ÁâπÂæ¥Èáè (7ÁâπÂæ¥Èáè)
                    elif feat_lc == "momentum_signals":
                        try:
                            # Ë§áÊï∞ÊúüÈñì„ÅÆ„É¢„É°„É≥„Çø„É†‰ø°Âè∑
                            momentum_periods = [1, 3, 7, 14, 21, 30]

                            for period in momentum_periods:
                                df[f"momentum_{period}"] = (
                                    df["close"].pct_change(period).fillna(0)
                                )

                            # „É¢„É°„É≥„Çø„É†ÂèéÊùü„ÉªÁô∫Êï£ÊåáÊ®ô
                            mom_short = df["close"].pct_change(3)
                            mom_long = df["close"].pct_change(21)
                            df["momentum_convergence"] = (mom_short - mom_long).fillna(
                                0
                            )

                        except Exception as e:
                            logger.warning(f"Failed to calculate momentum_signals: {e}")
                            # „Éá„Éï„Ç©„É´„ÉàÂÄ§Ë®≠ÂÆö
                            for period in [1, 3, 7, 14, 21, 30]:
                                df[f"momentum_{period}"] = 0
                            df["momentum_convergence"] = 0

                    # liquidity_indicators ÁâπÂæ¥Èáè (8ÁâπÂæ¥Èáè)
                    elif feat_lc == "liquidity_indicators":
                        try:
                            # AmihudÊµÅÂãïÊÄßÊåáÊ®ôÔºàÈùûÊµÅÂãïÊÄßÂ∫¶Ôºâ
                            price_change = abs(df["close"].pct_change())
                            volume_scaled = (
                                df["volume"] / df["volume"].rolling(window=20).mean()
                            )
                            df["amihud_illiquidity"] = (
                                price_change / (volume_scaled + 1e-8)
                            ).fillna(0)

                            # ‰æ°Ê†º„Ç§„É≥„Éë„ÇØ„ÉàÊåáÊ®ô
                            df["price_impact"] = (
                                (df["high"] - df["low"]) / (df["volume"] + 1e-8)
                            ).fillna(0)

                            # „Éú„É™„É•„Éº„É†„Éª„Éó„É©„Ç§„Çπ„Éª„Éà„É¨„É≥„ÉâÔºàVPTÔºâ
                            df["vpt"] = (
                                (df["volume"] * df["close"].pct_change())
                                .cumsum()
                                .fillna(0)
                            )

                            # Âá∫Êù•È´òÁõ∏ÂØæÂº∑Â∫¶
                            df["volume_strength"] = (
                                df["volume"] / df["volume"].rolling(window=20).mean()
                            ).fillna(1.0)

                            # ÊµÅÂãïÊÄßÊûØÊ∏áÊåáÊ®ô
                            volume_ma = df["volume"].rolling(window=10).mean()
                            volume_std = df["volume"].rolling(window=10).std()
                            df["liquidity_drought"] = (
                                (volume_ma - df["volume"]) / (volume_std + 1e-8)
                            ).fillna(0)

                            # „Éì„ÉÉ„Éâ„Éª„Ç¢„Çπ„ÇØ‰ª£ÁêÜÊåáÊ®ôÔºàÈ´òÂÄ§-ÂÆâÂÄ§„Çπ„Éó„É¨„ÉÉ„ÉâÔºâ
                            typical_price = (df["high"] + df["low"] + df["close"]) / 3
                            df["spread_proxy"] = (
                                (df["high"] - df["low"]) / typical_price
                            ).fillna(0)

                            # Âá∫Êù•È´òÂä†ÈáçÂπ≥Âùá‰æ°Ê†º„Åã„Çâ„ÅÆ‰πñÈõ¢
                            vwap = (typical_price * df["volume"]).rolling(
                                window=20
                            ).sum() / df["volume"].rolling(window=20).sum()
                            df["vwap_deviation"] = ((df["close"] - vwap) / vwap).fillna(
                                0
                            )

                            # Ê≥®Êñá‰∏çÂùáË°°‰ª£ÁêÜÊåáÊ®ô
                            df["order_imbalance_proxy"] = (
                                (df["close"] - df["open"])
                                / (df["high"] - df["low"] + 1e-8)
                            ).fillna(0)

                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate liquidity_indicators: {e}"
                            )
                            # „Éá„Éï„Ç©„É´„ÉàÂÄ§Ë®≠ÂÆö
                            liquidity_features = [
                                "amihud_illiquidity",
                                "price_impact",
                                "vpt",
                                "volume_strength",
                                "liquidity_drought",
                                "spread_proxy",
                                "vwap_deviation",
                                "order_imbalance_proxy",
                            ]
                            for feat in liquidity_features:
                                df[feat] = 0

                    # Phase 3.2A: ËøΩÂä†ATRÊúüÈñìÔºàatr_7, atr_21Ôºâ
                    elif base == "atr" and period and period != self.feat_period:
                        atr_calc = self.ind_calc.atr(df, window=period)
                        if isinstance(atr_calc, pd.Series):
                            df[f"atr_{period}"] = atr_calc
                        else:
                            df[f"atr_{period}"] = atr_calc.iloc[:, 0]

                    # Phase 3.2B: ‰æ°Ê†º„Éù„Ç∏„Ç∑„Éß„É≥ÊåáÊ®ôÔºà5ÁâπÂæ¥ÈáèÔºâ
                    elif feat_lc == "price_position":
                        try:
                            # ‰æ°Ê†º„É¨„É≥„Ç∏ÂÜÖ„ÅÆ‰ΩçÁΩÆÔºà%K„Çπ„Çø„Ç§„É´Ôºâ
                            high_20 = df["high"].rolling(20).max()
                            low_20 = df["low"].rolling(20).min()
                            df["price_position_20"] = (
                                (df["close"] - low_20) / (high_20 - low_20 + 1e-8)
                            ).fillna(0.5)

                            # Áï∞„Å™„ÇãÊúüÈñì„Åß„ÅÆ‰æ°Ê†º‰ΩçÁΩÆ
                            high_50 = df["high"].rolling(50).max()
                            low_50 = df["low"].rolling(50).min()
                            df["price_position_50"] = (
                                (df["close"] - low_50) / (high_50 - low_50 + 1e-8)
                            ).fillna(0.5)

                            # ÁßªÂãïÂπ≥Âùá„Åã„Çâ„ÅÆ‰ΩçÁΩÆ
                            sma_20 = df["close"].rolling(20).mean()
                            df["price_vs_sma20"] = (
                                (df["close"] - sma_20) / sma_20
                            ).fillna(0)

                            # „Éú„É™„É≥„Ç∏„É£„Éº„Éê„É≥„ÉâÂÜÖ‰ΩçÁΩÆÔºà%BÔºâ
                            bb_middle = df["close"].rolling(20).mean()
                            bb_std = df["close"].rolling(20).std()
                            bb_upper = bb_middle + (bb_std * 2)
                            bb_lower = bb_middle - (bb_std * 2)
                            df["bb_position"] = (
                                (df["close"] - bb_lower) / (bb_upper - bb_lower + 1e-8)
                            ).fillna(0.5)

                            # Êó•‰∏≠„É¨„É≥„Ç∏ÂÜÖ‰ΩçÁΩÆ
                            df["intraday_position"] = (
                                (df["close"] - df["low"])
                                / (df["high"] - df["low"] + 1e-8)
                            ).fillna(0.5)

                        except Exception as e:
                            logger.warning(f"Failed to calculate price_position: {e}")
                            for i in range(5):
                                df[f"price_pos_{i}"] = 0.5

                    # Phase 3.2B: „É≠„Éº„ÇΩ„ÇØË∂≥„Éë„Çø„Éº„É≥Ôºà4ÁâπÂæ¥ÈáèÔºâ
                    elif feat_lc == "candle_patterns":
                        try:
                            # „Éâ„Ç∏ÔºàÂçÅÂ≠óÁ∑öÔºâ
                            body_size = abs(df["close"] - df["open"])
                            candle_range = df["high"] - df["low"]
                            df["doji"] = (
                                body_size / (candle_range + 1e-8) < 0.1
                            ).astype(int)

                            # „Éè„É≥„Éû„Éº/„Éè„É≥„ÇÆ„É≥„Ç∞„Éû„É≥
                            upper_shadow = df["high"] - np.maximum(
                                df["open"], df["close"]
                            )
                            lower_shadow = (
                                np.minimum(df["open"], df["close"]) - df["low"]
                            )
                            df["hammer"] = (
                                (lower_shadow > body_size * 2)
                                & (upper_shadow < body_size * 0.5)
                            ).astype(int)

                            # „Ç®„É≥„Ç¥„É´„Éï„Ç£„É≥„Ç∞„Éë„Çø„Éº„É≥
                            prev_body = abs(df["close"].shift(1) - df["open"].shift(1))
                            curr_body = abs(df["close"] - df["open"])
                            df["engulfing"] = (
                                (curr_body > prev_body * 1.5)
                                & (df["close"] > df["open"])  # ÁèæÂú®„ÅÆ„É≠„Éº„ÇΩ„ÇØ„ÅåÈôΩÁ∑ö
                                & (
                                    df["close"].shift(1) < df["open"].shift(1)
                                )  # Ââç„ÅÆ„É≠„Éº„ÇΩ„ÇØ„ÅåÈô∞Á∑ö
                            ).astype(int)

                            # „Éî„É≥„Éê„ÉºÔºà‰∏ä„Éí„Ç≤„Éª‰∏ã„Éí„Ç≤Ôºâ
                            df["pinbar"] = (
                                (upper_shadow > body_size * 3)
                                | (lower_shadow > body_size * 3)
                            ).astype(int)

                        except Exception as e:
                            logger.warning(f"Failed to calculate candle_patterns: {e}")
                            for pattern in ["doji", "hammer", "engulfing", "pinbar"]:
                                df[pattern] = 0

                    # Phase 3.2B: „Çµ„Éù„Éº„Éà„Éª„É¨„Ç∏„Çπ„Çø„É≥„ÇπÔºà3ÁâπÂæ¥ÈáèÔºâ
                    elif feat_lc == "support_resistance":
                        try:
                            # „Çµ„Éù„Éº„Éà„É¨„Éô„É´„Åã„Çâ„ÅÆË∑ùÈõ¢
                            support_level = df["low"].rolling(50).min()
                            df["support_distance"] = (
                                (df["close"] - support_level) / support_level
                            ).fillna(0)

                            # „É¨„Ç∏„Çπ„Çø„É≥„Çπ„É¨„Éô„É´„Åã„Çâ„ÅÆË∑ùÈõ¢
                            resistance_level = df["high"].rolling(50).max()
                            df["resistance_distance"] = (
                                (resistance_level - df["close"]) / resistance_level
                            ).fillna(0)

                            # „Çµ„Éù„Éº„Éà„Éª„É¨„Ç∏„Çπ„Çø„É≥„Çπ„É¨„Éô„É´„ÅÆÂº∑Â∫¶
                            support_tests = (
                                df["low"]
                                .rolling(10)
                                .apply(lambda x: (x <= x.min() * 1.02).sum())
                            )
                            df["support_strength"] = support_tests.fillna(0)

                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate support_resistance: {e}"
                            )
                            for sr in [
                                "support_distance",
                                "resistance_distance",
                                "support_strength",
                            ]:
                                df[sr] = 0

                    # Phase 3.2B: „Éñ„É¨„Ç§„ÇØ„Ç¢„Ç¶„Éà„Ç∑„Ç∞„Éä„É´Ôºà3ÁâπÂæ¥ÈáèÔºâ
                    elif feat_lc == "breakout_signals":
                        try:
                            # „Éú„É™„É•„Éº„É†„Éñ„É¨„Ç§„ÇØ„Ç¢„Ç¶„Éà
                            volume_ma = df["volume"].rolling(20).mean()
                            price_change = abs(df["close"].pct_change())
                            df["volume_breakout"] = (
                                (df["volume"] > volume_ma * 2)
                                & (
                                    price_change
                                    > price_change.rolling(20).quantile(0.8)
                                )
                            ).astype(int)

                            # ‰æ°Ê†º„Éñ„É¨„Ç§„ÇØ„Ç¢„Ç¶„ÉàÔºà„É¨„É≥„Ç∏‰∏äÊîæ„ÇåÔºâ
                            high_20 = df["high"].rolling(20).max()
                            df["price_breakout_up"] = (
                                df["close"] > high_20.shift(1)
                            ).astype(int)

                            # ‰æ°Ê†º„Éñ„É¨„Ç§„ÇØ„ÉÄ„Ç¶„É≥Ôºà„É¨„É≥„Ç∏‰∏ãÊîæ„ÇåÔºâ
                            low_20 = df["low"].rolling(20).min()
                            df["price_breakout_down"] = (
                                df["close"] < low_20.shift(1)
                            ).astype(int)

                        except Exception as e:
                            logger.warning(f"Failed to calculate breakout_signals: {e}")
                            for bo in [
                                "volume_breakout",
                                "price_breakout_up",
                                "price_breakout_down",
                            ]:
                                df[bo] = 0

                    # Phase 3.2C: Ëá™Â∑±Áõ∏Èñ¢Ôºà5ÁâπÂæ¥ÈáèÔºâ
                    elif feat_lc == "autocorrelation":
                        try:
                            returns = df["close"].pct_change()
                            for lag in [1, 5, 10, 20, 50]:
                                df[f"autocorr_lag_{lag}"] = (
                                    returns.rolling(100).apply(
                                        lambda x: (
                                            x.autocorr(lag=lag) if len(x) > lag else 0
                                        )
                                    )
                                ).fillna(0)
                        except Exception as e:
                            logger.warning(f"Failed to calculate autocorrelation: {e}")
                            for lag in [1, 5, 10, 20, 50]:
                                df[f"autocorr_lag_{lag}"] = 0

                    # Phase 3.2C: Â≠£ÁØÄÊÄß„Éë„Çø„Éº„É≥Ôºà4ÁâπÂæ¥ÈáèÔºâ
                    elif feat_lc == "seasonal_patterns":
                        try:
                            if isinstance(df.index, pd.DatetimeIndex):
                                # ÊõúÊó•ÂäπÊûú
                                df["weekday_effect"] = df.index.dayofweek.astype(float)
                                # ÊôÇÈñìÂ∏ØÂäπÊûú
                                df["hour_effect"] = df.index.hour.astype(float)
                                # ÊúàÂàùÊúàÊú´ÂäπÊûú
                                df["month_day_effect"] = df.index.day.astype(float)
                                # „Ç¢„Ç∏„Ç¢ÊôÇÈñì„ÉªÊ¨ßÁ±≥ÊôÇÈñìÂå∫ÂàÜ
                                asia_hours = (
                                    (df.index.hour >= 0) & (df.index.hour < 8)
                                ).astype(int)
                                df["asia_session"] = asia_hours
                            else:
                                for feat in [
                                    "weekday_effect",
                                    "hour_effect",
                                    "month_day_effect",
                                    "asia_session",
                                ]:
                                    df[feat] = 0
                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate seasonal_patterns: {e}"
                            )
                            for feat in [
                                "weekday_effect",
                                "hour_effect",
                                "month_day_effect",
                                "asia_session",
                            ]:
                                df[feat] = 0

                    # Phase 3.2C: „É¨„Ç∏„Éº„É†Ê§úÂá∫Ôºà3ÁâπÂæ¥ÈáèÔºâ
                    elif feat_lc == "regime_detection":
                        try:
                            returns = df["close"].pct_change()
                            vol = returns.rolling(20).std()

                            # È´ò„Éú„É©„ÉÜ„Ç£„É™„ÉÜ„Ç£„É¨„Ç∏„Éº„É†
                            vol_threshold = vol.rolling(100).quantile(0.8)
                            df["high_vol_regime"] = (vol > vol_threshold).astype(int)

                            # „Éà„É¨„É≥„Éâ„É¨„Ç∏„Éº„É†Ôºà‰∏äÊòá„Éª‰∏ãÈôç„ÉªÊ®™„Å∞„ÅÑÔºâ
                            price_ma_short = df["close"].rolling(10).mean()
                            price_ma_long = df["close"].rolling(50).mean()
                            trend_strength = (
                                price_ma_short - price_ma_long
                            ) / price_ma_long
                            df["trend_regime"] = np.where(
                                trend_strength > 0.02,
                                1,  # ‰∏äÊòá„Éà„É¨„É≥„Éâ
                                np.where(
                                    trend_strength < -0.02, -1, 0
                                ),  # ‰∏ãÈôç„Éà„É¨„É≥„Éâ vs Ê®™„Å∞„ÅÑ
                            )

                            # „É¢„É°„É≥„Çø„É†„É¨„Ç∏„Éº„É†
                            momentum = returns.rolling(14).mean()
                            df["momentum_regime"] = np.where(
                                momentum > momentum.rolling(100).quantile(0.8),
                                1,
                                np.where(
                                    momentum < momentum.rolling(100).quantile(0.2),
                                    -1,
                                    0,
                                ),
                            )

                        except Exception as e:
                            logger.warning(f"Failed to calculate regime_detection: {e}")
                            for regime in [
                                "high_vol_regime",
                                "trend_regime",
                                "momentum_regime",
                            ]:
                                df[regime] = 0

                    # Phase 3.2C: „Çµ„Ç§„ÇØ„É´ÂàÜÊûêÔºà3ÁâπÂæ¥ÈáèÔºâ
                    elif feat_lc == "cycle_analysis":
                        try:
                            # ‰æ°Ê†º„Çµ„Ç§„ÇØ„É´Ôºà„Éá„Éà„É¨„É≥„Éâ‰æ°Ê†ºÔºâ
                            price_ma = df["close"].rolling(50).mean()
                            df["price_cycle"] = (
                                (df["close"] - price_ma) / price_ma
                            ).fillna(0)

                            # „Éú„É™„É•„Éº„É†„Çµ„Ç§„ÇØ„É´
                            volume_ma = df["volume"].rolling(50).mean()
                            df["volume_cycle"] = (
                                (df["volume"] - volume_ma) / volume_ma
                            ).fillna(0)

                            # RSI„Çµ„Ç§„ÇØ„É´ÔºàÈÅéË≤∑„ÅÑÈÅéÂ£≤„Çä„Çµ„Ç§„ÇØ„É´Ôºâ
                            rsi = self.ind_calc.rsi(df["close"], window=14)
                            df["rsi_cycle"] = ((rsi - 50) / 50).fillna(0)

                        except Exception as e:
                            logger.warning(f"Failed to calculate cycle_analysis: {e}")
                            for cycle in ["price_cycle", "volume_cycle", "rsi_cycle"]:
                                df[cycle] = 0

                    # Phase 3.2D: „ÇØ„É≠„ÇπÁõ∏Èñ¢Ôºà5ÁâπÂæ¥ÈáèÔºâ
                    elif feat_lc == "cross_correlation":
                        try:
                            returns = df["close"].pct_change()
                            volume_returns = df["volume"].pct_change()

                            # ‰æ°Ê†º-„Éú„É™„É•„Éº„É†Áõ∏Èñ¢
                            for window in [10, 20, 50]:
                                df[f"price_volume_corr_{window}"] = (
                                    returns.rolling(window).corr(volume_returns)
                                ).fillna(0)

                            # È´òÂÄ§-‰ΩéÂÄ§Áõ∏Èñ¢Ôºà„Éú„É©„ÉÜ„Ç£„É™„ÉÜ„Ç£ÊßãÈÄ†Ôºâ
                            high_returns = df["high"].pct_change()
                            low_returns = df["low"].pct_change()
                            df["high_low_corr_20"] = (
                                high_returns.rolling(20).corr(low_returns)
                            ).fillna(0)

                            # ‰æ°Ê†º„É©„Ç∞Áõ∏Èñ¢ÔºàËá™Â∑±Áõ∏Èñ¢Á∞°ÊòìÁâàÔºâ
                            df["price_lag_corr"] = (
                                returns.rolling(30).corr(returns.shift(1))
                            ).fillna(0)

                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate cross_correlation: {e}"
                            )
                            corr_features = [
                                "price_volume_corr_10",
                                "price_volume_corr_20",
                                "price_volume_corr_50",
                                "high_low_corr_20",
                                "price_lag_corr",
                            ]
                            for feat in corr_features:
                                df[feat] = 0

                    # Phase 3.2D: Áõ∏ÂØæÂº∑Â∫¶Ôºà5ÁâπÂæ¥ÈáèÔºâ
                    elif feat_lc == "relative_strength":
                        try:
                            # Áü≠Êúü vs Èï∑ÊúüÁõ∏ÂØæÂº∑Â∫¶
                            short_ma = df["close"].rolling(10).mean()
                            long_ma = df["close"].rolling(50).mean()
                            df["short_long_strength"] = (
                                (short_ma - long_ma) / long_ma
                            ).fillna(0)

                            # „Éú„É™„É•„Éº„É†Áõ∏ÂØæÂº∑Â∫¶
                            volume_short = df["volume"].rolling(10).mean()
                            volume_long = df["volume"].rolling(50).mean()
                            df["volume_relative_strength"] = (
                                (volume_short - volume_long) / volume_long
                            ).fillna(0)

                            # „Éú„É©„ÉÜ„Ç£„É™„ÉÜ„Ç£Áõ∏ÂØæÂº∑Â∫¶
                            vol_short = df["close"].pct_change().rolling(10).std()
                            vol_long = df["close"].pct_change().rolling(50).std()
                            df["volatility_relative_strength"] = (
                                (vol_short - vol_long) / vol_long
                            ).fillna(0)

                            # ‰æ°Ê†ºÂãïÂãâÁõ∏ÂØæÂº∑Â∫¶
                            momentum_short = df["close"].pct_change(5)
                            momentum_long = df["close"].pct_change(20)
                            df["momentum_relative_strength"] = (
                                momentum_short - momentum_long
                            ).fillna(0)

                            # RSIÁõ∏ÂØæÂº∑Â∫¶
                            rsi_14 = self.ind_calc.rsi(df["close"], window=14)
                            rsi_7 = self.ind_calc.rsi(df["close"], window=7)
                            df["rsi_relative_strength"] = (rsi_7 - rsi_14).fillna(0)

                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate relative_strength: {e}"
                            )
                            rs_features = [
                                "short_long_strength",
                                "volume_relative_strength",
                                "volatility_relative_strength",
                                "momentum_relative_strength",
                                "rsi_relative_strength",
                            ]
                            for feat in rs_features:
                                df[feat] = 0

                    # Phase 3.2D: „Çπ„Éó„É¨„ÉÉ„ÉâÂàÜÊûêÔºà5ÁâπÂæ¥ÈáèÔºâ
                    elif feat_lc == "spread_analysis":
                        try:
                            # È´òÂÄ§-‰ΩéÂÄ§„Çπ„Éó„É¨„ÉÉ„Éâ
                            hl_spread = (df["high"] - df["low"]) / df["close"]
                            df["hl_spread"] = hl_spread.fillna(0)
                            df["hl_spread_ma"] = hl_spread.rolling(20).mean().fillna(0)

                            # ÂßãÂÄ§-ÁµÇÂÄ§„Çπ„Éó„É¨„ÉÉ„Éâ
                            oc_spread = abs(df["open"] - df["close"]) / df["close"]
                            df["oc_spread"] = oc_spread.fillna(0)

                            # „Éú„É©„ÉÜ„Ç£„É™„ÉÜ„Ç£„Çπ„Éó„É¨„ÉÉ„ÉâÔºàÁü≠Êúü vs Èï∑ÊúüÔºâ
                            vol_short = df["close"].pct_change().rolling(5).std()
                            vol_long = df["close"].pct_change().rolling(20).std()
                            df["volatility_spread"] = (vol_short - vol_long).fillna(0)

                            # „Çø„Ç§„É†„Çπ„Éó„É¨„ÉÉ„ÉâÔºàÈÄ£Á∂öÊúüÈñì„ÅÆ‰æ°Ê†ºÂ∑ÆÔºâ
                            df["time_spread"] = (
                                df["close"] - df["close"].shift(24)
                            ).fillna(
                                0
                            )  # 24ÊôÇÈñìÂâç„Å®„ÅÆÂ∑Æ

                        except Exception as e:
                            logger.warning(f"Failed to calculate spread_analysis: {e}")
                            spread_features = [
                                "hl_spread",
                                "hl_spread_ma",
                                "oc_spread",
                                "volatility_spread",
                                "time_spread",
                            ]
                            for feat in spread_features:
                                df[feat] = 0

                    else:
                        logger.warning(f"Unknown extra feature spec: {feat} - skipping")

                except Exception as e:
                    logger.error("Failed to add extra feature %s: %s", feat, e)
                    raise  # Â§±Êïó„Åó„Åü„ÇâÊè°„Çä„Å§„Å∂„Åï„ÅöÂÅúÊ≠¢„Åô„Çã
            logger.debug("After extra feats: %s", df.shape)

        # 6. Ê¨†ÊêçÂÜçË£úÂÆåÔºã0Âüã„ÇÅ
        df = df.ffill().fillna(0)

        # 7. ÁÑ°ÈôêÂ§ßÂÄ§„ÉªÁï∞Â∏∏ÂÄ§„ÅÆ„ÇØ„É™„Éº„Éã„É≥„Ç∞
        # ÁÑ°ÈôêÂ§ßÂÄ§„ÇíÂâç„ÅÆÊúâÂäπÂÄ§„ÅßÁΩÆÊèõ„ÄÅ„Åù„Çå„ÇÇÁÑ°„ÅÑÂ†¥Âêà„ÅØ0
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.ffill().fillna(0)

        # Áï∞Â∏∏„Å´Â§ß„Åç„Å™ÂÄ§„Çí„ÇØ„É™„ÉÉ„ÉóÔºà„Ç™„Éº„Éê„Éº„Éï„É≠„ÉºÈò≤Ê≠¢Ôºâ
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            # 99.9%ile‰ª•‰∏ä„ÅÆÂÄ§„Çí„ÇØ„É™„ÉÉ„Éó
            upper_bound = df[col].quantile(0.999)
            lower_bound = df[col].quantile(0.001)

            if np.isfinite(upper_bound) and np.isfinite(lower_bound):
                df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

        logger.debug("Final features shape after cleaning: %s", df.shape)

        # „Éá„Éº„ÇøÂìÅË≥™ÁÆ°ÁêÜ„Ç∑„Çπ„ÉÜ„É†ÔºàÂãùÁéáÊîπÂñÑ„ÅÆ„Åü„ÇÅ„ÅÆÈáçË¶Å„Å™„Çπ„ÉÜ„ÉÉ„ÉóÔºâ
        try:
            from crypto_bot.ml.data_quality_manager import DataQualityManager

            quality_manager = DataQualityManager(self.config)

            # „É°„Çø„Éá„Éº„Çø‰ΩúÊàêÔºàÂ§ñÈÉ®„Éá„Éº„Çø„ÇΩ„Éº„ÇπÊÉÖÂ†±Ôºâ
            metadata = {
                "feature_sources": {},
                "external_data_enabled": {
                    "vix": self.vix_enabled,
                    "macro": self.macro_enabled,
                    "fear_greed": self.fear_greed_enabled,
                    "funding": self.funding_enabled,
                },
            }

            # ÂêÑÁâπÂæ¥Èáè„ÅÆ„ÇΩ„Éº„ÇπÊÉÖÂ†±„ÇíË®òÈå≤
            for column in df.columns:
                vix_prefixes = ["vix_", "dxy_", "treasury_"]
                if any(column.startswith(prefix) for prefix in vix_prefixes):
                    source_type = "api" if self.macro_enabled else "default"
                    metadata["feature_sources"][column] = {"source_type": source_type}
                elif any(column.startswith(prefix) for prefix in ["fear_greed", "fg_"]):
                    source_type = "api" if self.fear_greed_enabled else "default"
                    metadata["feature_sources"][column] = {"source_type": source_type}
                elif any(column.startswith(prefix) for prefix in ["fr_", "oi_"]):
                    # Bitbank‰ª£ÊõøÁâπÂæ¥Èáè
                    metadata["feature_sources"][column] = {"source_type": "calculated"}
                else:
                    metadata["feature_sources"][column] = {"source_type": "calculated"}

            # „Éá„Éº„ÇøÂìÅË≥™Ê§úË®º
            quality_passed, quality_report = quality_manager.validate_data_quality(
                df, metadata
            )

            if not quality_passed:
                logger.warning(f"Data quality check failed: {quality_report}")

                # „Éá„Éº„ÇøÂìÅË≥™ÊîπÂñÑ„ÇíË©¶Ë°å
                df_improved, improvement_report = quality_manager.improve_data_quality(
                    df, metadata
                )
                df = df_improved

                logger.info(f"Data quality improvement applied: {improvement_report}")
            else:
                score = quality_report["quality_score"]
                logger.info(f"Data quality check passed: score={score:.1f}")

        except Exception as e:
            logger.warning(
                f"Data quality management failed: {e} - continuing with original data"
            )

        # Phase B2.4: „Éê„ÉÉ„ÉÅÂá¶ÁêÜÂÆå‰∫ÜÂæå„ÅÆÊúÄÁµÇÂá¶ÁêÜ
        if self.batch_engines_enabled:
            logger.info("üîÑ Batch processing completed - performing final validation")

        # 125ÁâπÂæ¥Èáè„ÅÆÁ¢∫ÂÆü„Å™‰øùË®ºÔºàÊúÄÁµÇ„ÉÅ„Çß„ÉÉ„ÇØÔºâPhase H.25
        from crypto_bot.ml.feature_defaults import ensure_feature_consistency

        df = ensure_feature_consistency(
            df, target_count=125
        )  # Phase H.25: 125ÁâπÂæ¥Èáè„Å´Áµ±‰∏ÄÔºàÂ§ñÈÉ®APIÈô§Â§ñÔºâ
        logger.info(f"Final guaranteed feature count: {len(df.columns)}")

        return df


def build_ml_pipeline(config: Dict[str, Any]) -> Pipeline:
    """
    sklearn„Éë„Ç§„Éó„É©„Ç§„É≥ÂåñÔºàÁâπÂæ¥ÈáèÁîüÊàê‚ÜíÊ®ôÊ∫ñÂåñÔºâ„ÄÇ
    Á©∫„ÅÆDataFrame„ÅÆÂ†¥Âêà„ÅØ„ÄÅÁâπÂæ¥ÈáèÁîüÊàê„ÅÆ„Åø„ÇíË°å„ÅÑ„ÄÅÊ®ôÊ∫ñÂåñ„ÅØ„Çπ„Ç≠„ÉÉ„Éó„Åô„Çã„ÄÇ
    """

    class EmptyDataFrameScaler(BaseEstimator, TransformerMixin):
        """Á©∫„ÅÆDataFrame„ÅÆÂ†¥Âêà„ÅÆ„ÉÄ„Éü„Éº„Çπ„Ç±„Éº„É©„Éº"""

        def fit(self, X, y=None):
            return self

        def transform(self, X):
            return X

    class EmptyDataFramePipeline(Pipeline):
        """Á©∫„ÅÆDataFrame„ÅÆÂ†¥Âêà„ÅÆ„Éë„Ç§„Éó„É©„Ç§„É≥"""

        def fit_transform(self, X, y=None, **fit_params):
            if X.empty:
                # Á©∫„ÅÆDataFrame„ÅÆÂ†¥Âêà„ÅØ„ÄÅÁâπÂæ¥ÈáèÁîüÊàê„ÅÆ„Åø„ÇíË°å„ÅÑ„ÄÅÊ®ôÊ∫ñÂåñ„ÅØ„Çπ„Ç≠„ÉÉ„Éó
                features = self.named_steps["features"].transform(X)
                # DataFrame„Çínumpy.ndarray„Å´Â§âÊèõ
                return features.values
            return super().fit_transform(X, y, **fit_params)

        def transform(self, X):
            if X.empty:
                # Á©∫„ÅÆDataFrame„ÅÆÂ†¥Âêà„ÅØ„ÄÅÁâπÂæ¥ÈáèÁîüÊàê„ÅÆ„Åø„ÇíË°å„ÅÑ„ÄÅÊ®ôÊ∫ñÂåñ„ÅØ„Çπ„Ç≠„ÉÉ„Éó
                features = self.named_steps["features"].transform(X)
                # DataFrame„Çínumpy.ndarray„Å´Â§âÊèõ
                return features.values
            return super().transform(X)

    return EmptyDataFramePipeline(
        [
            ("features", FeatureEngineer(config)),
            (
                "scaler",
                (
                    EmptyDataFrameScaler()
                    if config.get("skip_scaling", False)
                    else StandardScaler()
                ),
            ),
        ]
    )


def prepare_ml_dataset(
    df: pd.DataFrame, config: Dict[str, Any]
) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
    """
    Ê©üÊ¢∞Â≠¶ÁøíÁî®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí‰ΩúÊàê„ÄÇ
    Êàª„ÇäÂÄ§: XÔºàÁâπÂæ¥ÈáèÔºâ, y_regÔºàÂõûÂ∏∞„Çø„Éº„Ç≤„ÉÉ„ÉàÔºâ, y_clfÔºàÂàÜÈ°û„Çø„Éº„Ç≤„ÉÉ„ÉàÔºâ

    - ÂøÖË¶Å„Å™„Å∂„Çì„Å†„ÅëÊúÄÂàù„ÅÆË°å„Çí„Éâ„É≠„ÉÉ„ÉóÔºàrolling/lagsÔºâ
    - horizon, threshold„ÅØconfig["ml"]„Åã„ÇâÂèñÂæó
    """
    logger.info(f"prepare_ml_dataset input df shape: {tuple(df.shape)}")
    pipeline = build_ml_pipeline(config)
    X_arr = pipeline.fit_transform(df)

    logger.info(f"Pipeline output type: {type(X_arr)}")
    if hasattr(X_arr, "shape"):
        shape_info = X_arr.shape
    elif hasattr(X_arr, "__len__"):
        shape_info = len(X_arr)
    else:
        shape_info = "no len"

    logger.info(f"Pipeline output shape/len: {shape_info}")

    # X_arr„Åålist„ÅÆÂ†¥Âêà„ÅØnumpy array„Å´Â§âÊèõ
    if isinstance(X_arr, list):
        logger.warning(
            f"Pipeline returned list with {len(X_arr)} elements, converting to numpy array"  # noqa: E501
        )
        import numpy as np

        try:
            X_arr = np.array(X_arr)
        except Exception as e:
            logger.error(f"Failed to convert list to numpy array: {e}")
            # If conversion fails, return the list directly for debugging
            return X_arr

    # ----- ÁõÆÁöÑÂ§âÊï∞ÁîüÊàê -----
    horizon = config["ml"]["horizon"]
    thresh = config["ml"].get("threshold", 0.0)
    y_reg = make_regression_target(df, horizon).rename(f"return_{horizon}")
    y_clf = make_classification_target(df, horizon, thresh).rename(f"up_{horizon}")

    # ----- Ë°åÊï∞„ÇíÊèÉ„Åà„Çã -----
    win = config["ml"]["rolling_window"]
    lags = config["ml"]["lags"]
    drop_n = win + max(lags) if lags else win

    idx = df.index[drop_n:]
    X = pd.DataFrame(X_arr[drop_n:], index=idx)

    return X, y_reg.loc[idx], y_clf.loc[idx]


# Phase H.11: ÁâπÂæ¥ÈáèÂÆåÂÖ®ÊÄß‰øùË®º„Ç∑„Çπ„ÉÜ„É†Áµ±Âêà
def prepare_ml_dataset_enhanced(
    df: pd.DataFrame, config: Dict[str, Any]
) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
    """
    ÁâπÂæ¥ÈáèÂÆåÂÖ®ÊÄß‰øùË®º‰ªò„ÅçMLÁî®„Éá„Éº„Çø„Çª„ÉÉ„Éà‰ΩúÊàê

    Args:
        df: OHLCV„Éá„Éº„Çø
        config: Ë®≠ÂÆöËæûÊõ∏

    Returns:
        (ÁâπÂæ¥ÈáèDataFrame, ÂõûÂ∏∞„Çø„Éº„Ç≤„ÉÉ„Éà, ÂàÜÈ°û„Çø„Éº„Ç≤„ÉÉ„Éà)
    """
    logger.info("üöÄ [ENHANCED-ML] Starting enhanced ML dataset preparation...")
    logger.info(f"üìä [ENHANCED-ML] Input shape: {tuple(df.shape)}")

    # Phase H.11: ÁâπÂæ¥ÈáèÂÆåÂÖ®ÊÄß‰øùË®ºÂÆüË°å
    if ENHANCED_FEATURES_AVAILABLE:
        logger.info("‚úÖ [ENHANCED-ML] Using enhanced feature engineering system")
        enhanced_df, feature_report = enhance_feature_engineering(df, config)

        # ÁâπÂæ¥Èáè„É¨„Éù„Éº„Éà„ÅÆÂá∫Âäõ
        logger.info("üìã [ENHANCED-ML] Feature completeness report:")
        logger.info(
            f"   - Implementation rate: {feature_report['audit_result']['implementation_rate']:.1%}"
        )
        logger.info(
            f"   - Generated features: {len(feature_report['generated_features'])}"
        )
        logger.info(f"   - Final features: {feature_report['final_feature_count']}")
        logger.info(
            f"   - Completeness rate: {feature_report['completeness_rate']:.1%}"
        )

        # ÂìÅË≥™„ÅÆ‰Ωé„ÅÑÁâπÂæ¥Èáè„ÅÆË≠¶Âëä
        low_quality_features = [
            f for f, score in feature_report["quality_scores"].items() if score < 0.5
        ]
        if low_quality_features:
            logger.warning(
                f"‚ö†Ô∏è [ENHANCED-ML] Low quality features ({len(low_quality_features)}): {low_quality_features[:5]}..."
            )

        # Âº∑Âåñ„Åï„Çå„ÅüDataFrame„Çí‰ΩøÁî®„Åó„Å¶MLÂá¶ÁêÜÁ∂ôÁ∂ö
        logger.info(f"üìä [ENHANCED-ML] Enhanced shape: {tuple(enhanced_df.shape)}")
        result_df = enhanced_df
    else:
        logger.warning(
            "‚ö†Ô∏è [ENHANCED-ML] Enhanced features not available, falling back to standard processing"
        )
        result_df = df

    # Ê®ôÊ∫ñ„ÅÆMLÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥ÂÆüË°å
    pipeline = build_ml_pipeline(config)
    X_arr = pipeline.fit_transform(result_df)

    logger.info(
        f"üîß [ENHANCED-ML] Pipeline output shape: {tuple(X_arr.shape) if hasattr(X_arr, 'shape') else len(X_arr)}"
    )

    # X_arr„Åålist„ÅÆÂ†¥Âêà„ÅØnumpy array„Å´Â§âÊèõ
    if isinstance(X_arr, list):
        logger.warning("üîÑ [ENHANCED-ML] Converting list to numpy array")
        import numpy as np

        try:
            X_arr = np.array(X_arr)
        except Exception as e:
            logger.error(f"‚ùå [ENHANCED-ML] Array conversion failed: {e}")
            return X_arr, None, None

    # ÁõÆÁöÑÂ§âÊï∞ÁîüÊàê
    horizon = config["ml"]["horizon"]
    thresh = config["ml"].get("threshold", 0.0)
    y_reg = make_regression_target(result_df, horizon).rename(f"return_{horizon}")
    y_clf = make_classification_target(result_df, horizon, thresh).rename(
        f"up_{horizon}"
    )

    # Ë°åÊï∞Ë™øÊï¥
    win = config["ml"]["rolling_window"]
    lags = config["ml"]["lags"]
    drop_n = win + max(lags) if lags else win

    idx = result_df.index[drop_n:]
    X = pd.DataFrame(X_arr[drop_n:], index=idx)

    logger.info(
        f"‚úÖ [ENHANCED-ML] Enhanced ML dataset ready: X{tuple(X.shape)}, y_reg{tuple(y_reg.loc[idx].shape)}, y_clf{tuple(y_clf.loc[idx].shape)}"
    )

    return X, y_reg.loc[idx], y_clf.loc[idx]


def ensure_feature_coverage(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Ë®≠ÂÆö„Éï„Ç°„Ç§„É´„ÅÆÁâπÂæ¥Èáè„Ç´„Éê„É¨„ÉÉ„Ç∏Á¢∫‰øù

    Args:
        config: Ë®≠ÂÆöËæûÊõ∏

    Returns:
        ÁâπÂæ¥Èáè„Ç´„Éê„É¨„ÉÉ„Ç∏‰øùË®ºÊ∏à„ÅøË®≠ÂÆöËæûÊõ∏
    """
    if not ENHANCED_FEATURES_AVAILABLE:
        logger.warning("‚ö†Ô∏è Enhanced feature engineering not available")
        return config

    logger.info("üîç [COVERAGE] Ensuring feature coverage in configuration...")

    enhanced_config = config.copy()

    # MLË®≠ÂÆö„Åã„ÇâË¶ÅÊ±ÇÁâπÂæ¥Èáè„ÇíÂèñÂæó
    ml_features = config.get("ml", {}).get("extra_features", [])
    strategy_features = (
        config.get("strategy", {})
        .get("params", {})
        .get("ml", {})
        .get("extra_features", [])
    )

    all_features = list(set(ml_features + strategy_features))

    if not all_features:
        logger.warning("‚ö†Ô∏è [COVERAGE] No features specified in configuration")
        return enhanced_config

    # ÁâπÂæ¥ÈáèÂÆüË£ÖÁõ£Êüª
    enhancer = FeatureEngineeringEnhanced()
    audit_result = enhancer.audit_feature_implementation(all_features)

    # Êú™ÂÆüË£ÖÁâπÂæ¥Èáè„ÅÆË≠¶Âëä„Å®„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØË®≠ÂÆö
    if audit_result["missing"]:
        logger.warning(
            f"‚ö†Ô∏è [COVERAGE] Unimplemented features detected ({len(audit_result['missing'])})"
        )
        logger.info(
            f"   Missing: {audit_result['missing'][:10]}..."
        )  # ÊúÄÂàù„ÅÆ10ÂÄã„ÇíË°®Á§∫

        # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØË®≠ÂÆö„ÇíËøΩÂä†
        enhanced_config.setdefault("feature_fallback", {})
        enhanced_config["feature_fallback"]["auto_generate_missing"] = True
        enhanced_config["feature_fallback"]["missing_features"] = audit_result[
            "missing"
        ]

    # Â§ñÈÉ®„Éá„Éº„Çø‰æùÂ≠òÁâπÂæ¥Èáè„ÅÆË®≠ÂÆöÁ¢∫Ë™ç
    if audit_result["external_dependent"]:
        logger.info(
            f"üì° [COVERAGE] External data features ({len(audit_result['external_dependent'])})"
        )

        # Â§ñÈÉ®„Éá„Éº„ÇøË®≠ÂÆö„ÅÆÂ≠òÂú®Á¢∫Ë™ç
        external_config = enhanced_config.get("ml", {}).get("external_data", {})
        if not external_config.get("enabled", False):
            logger.warning(
                "‚ö†Ô∏è [COVERAGE] External data features requested but external_data not enabled"
            )
            enhanced_config.setdefault("ml", {}).setdefault("external_data", {})[
                "enabled"
            ] = True

    logger.info("‚úÖ [COVERAGE] Feature coverage ensured:")
    logger.info(f"   - Implementation rate: {audit_result['implementation_rate']:.1%}")
    logger.info(f"   - Total features: {audit_result['total_requested']}")
    logger.info(f"   - Implemented: {len(audit_result['implemented'])}")
    logger.info(f"   - Missing: {len(audit_result['missing'])}")

    return enhanced_config
