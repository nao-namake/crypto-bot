"""
ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 

è¤‡æ•°ã®MLãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ã¦äºˆæ¸¬ç²¾åº¦ãƒ»ä¿¡é ¼åº¦ã‚’å‘ä¸Šã•ã›ã‚‹é«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã€‚
ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ãƒ»ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ãƒ»äºˆæ¸¬ä¿¡é ¼åº¦å‘ä¸Šãƒ»å‹•çš„é–¾å€¤æœ€é©åŒ–ã«å¯¾å¿œã€‚
"""

from __future__ import annotations

import logging
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from lightgbm import LGBMClassifier
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.calibration import CalibratedClassifierCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import StratifiedKFold, cross_val_predict
from xgboost import XGBClassifier

logger = logging.getLogger(__name__)


class TradingEnsembleClassifier(BaseEstimator, ClassifierMixin):
    """
    å–å¼•ç‰¹åŒ–å‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚¯ãƒ©ã‚¹

    å‹ç‡ã¨åç›Šæ€§å‘ä¸Šã«ç„¦ç‚¹ã‚’å½“ã¦ãŸè¨­è¨ˆ:
    - å–å¼•ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°
    - å‹•çš„é–¾å€¤æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
    - ãƒªã‚¹ã‚¯èª¿æ•´å‹äºˆæ¸¬ä¿¡é ¼åº¦
    - å¸‚å ´ç’°å¢ƒé©å¿œå‹é‡ã¿èª¿æ•´
    - æ—¢å­˜MLStrategyå®Œå…¨çµ±åˆ
    """

    def __init__(
        self,
        base_models: Optional[List[BaseEstimator]] = None,
        meta_model: Optional[BaseEstimator] = None,
        ensemble_method: str = "trading_stacking",
        trading_metrics: Optional[Dict[str, float]] = None,
        risk_adjustment: bool = True,
        cv_folds: int = 5,
        confidence_threshold: float = 0.65,
    ):
        """
        å–å¼•ç‰¹åŒ–å‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’å™¨ã®åˆæœŸåŒ–

        Parameters:
        -----------
        base_models : List[BaseEstimator], optional
            ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
        meta_model : BaseEstimator, optional
            ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆå–å¼•æœ€é©åŒ–ç”¨ï¼‰
        ensemble_method : str
            ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³• ("trading_stacking", "risk_weighted", "performance_voting")
        trading_metrics : Dict[str, float], optional
            å–å¼•ãƒ¡ãƒˆãƒªã‚¯ã‚¹é‡ã¿ï¼ˆsharpe_ratio, win_rate, max_drawdownç­‰ï¼‰
        risk_adjustment : bool
            ãƒªã‚¹ã‚¯èª¿æ•´ã®ä½¿ç”¨æœ‰ç„¡
        cv_folds : int
            äº¤å·®æ¤œè¨¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰æ•°
        confidence_threshold : float
            å–å¼•ä¿¡é ¼åº¦é–¾å€¤ï¼ˆã‚ˆã‚Šä¿å®ˆçš„ï¼‰
        """
        self.ensemble_method = ensemble_method
        self.trading_metrics = trading_metrics or {
            "sharpe_ratio": 0.4,
            "win_rate": 0.3,
            "max_drawdown": -0.2,
            "profit_factor": 0.1,
        }
        self.risk_adjustment = risk_adjustment
        self.cv_folds = cv_folds
        self.confidence_threshold = confidence_threshold

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
        if base_models is None:
            self.base_models = self._create_default_base_models()
        else:
            self.base_models = base_models

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«
        if meta_model is None:
            self.meta_model = LogisticRegression(
                random_state=42, max_iter=1000, class_weight="balanced"
            )
        else:
            self.meta_model = meta_model

        # å­¦ç¿’æ¸ˆã¿çŠ¶æ…‹
        self.fitted_base_models = []
        self.fitted_meta_model = None
        self.feature_names_ = None
        self.classes_ = None
        self.model_performance_ = {}
        self.trading_weights_ = None
        self.risk_metrics_ = {}
        self.market_regime_weights_ = {}
        self.is_fitted = False  # Phase H.16.1: is_fittedå±æ€§è¿½åŠ 

        # å–å¼•ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡
        self.prediction_history_ = []
        self.performance_cache_ = {}

        # Calibration setting
        self.use_calibration = True

        # Additional attributes for compatibility
        self.ensemble_weights_ = None
        self.voting_weights = None
        self.last_X_ = None

    def _create_default_base_models(self) -> List[BaseEstimator]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"""
        return [
            # LightGBMï¼ˆé«˜é€Ÿãƒ»é«˜ç²¾åº¦ï¼‰
            LGBMClassifier(
                objective="binary",
                boosting_type="gbdt",
                num_leaves=31,
                learning_rate=0.05,
                feature_fraction=0.9,
                bagging_fraction=0.8,
                bagging_freq=5,
                verbose=-1,
                random_state=42,
            ),
            # XGBoostï¼ˆãƒ­ãƒã‚¹ãƒˆï¼‰
            XGBClassifier(
                objective="binary:logistic",
                learning_rate=0.05,
                max_depth=6,
                min_child_weight=1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                verbosity=0,
            ),
            # Random Forestï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰
            RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                max_features="sqrt",
                random_state=42,
                n_jobs=-1,
            ),
        ]

    def fit(self, X: pd.DataFrame, y: pd.Series) -> "TradingEnsembleClassifier":
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆPhase H.26: å …ç‰¢åŒ–ç‰ˆï¼‰

        Parameters:
        -----------
        X : pd.DataFrame
            ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        y : pd.Series
            ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿
        """
        try:
            # Phase H.26: ãƒ‡ãƒ¼ã‚¿å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if not self._validate_training_data(X, y):
                logger.error("Training data validation failed")
                return self._create_fallback_ensemble()

            # Phase H.26: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•´åˆæ€§ç¢ºä¿
            X_clean, y_clean = self._align_and_clean_data(X, y)

            # Phase H.26: æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ãƒã‚§ãƒƒã‚¯
            min_samples = max(20, len(X_clean.columns) // 5)  # å‹•çš„æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
            if len(y_clean) < min_samples:
                logger.warning(
                    f"Insufficient training samples: {len(y_clean)} < {min_samples}"
                )
                return self._create_fallback_ensemble()

            self.feature_names_ = X_clean.columns.tolist()
            self.classes_ = np.unique(y_clean)

            logger.info(f"Training ensemble with {len(self.base_models)} base models")
            logger.info(f"Ensemble method: {self.ensemble_method}")
            logger.info(
                f"Training samples: {len(y_clean)}, Features: {len(X_clean.columns)}"
            )

            # 1. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆå …ç‰¢åŒ–ï¼‰
            successful_models = self._fit_base_models_robust(X_clean, y_clean)
            if successful_models == 0:
                logger.error("All base models failed to train")
                return self._create_fallback_ensemble()

            # 2. å–å¼•ç‰¹åŒ–å‹ãƒ¡ã‚¿ç‰¹å¾´é‡ç”Ÿæˆï¼ˆå …ç‰¢åŒ–ï¼‰
            if self.ensemble_method == "trading_stacking" and successful_models >= 2:
                try:
                    self._fit_trading_meta_model(X_clean, y_clean)
                except Exception as e:
                    logger.warning(
                        f"Meta model training failed: {e}, using simple voting"
                    )
                    self.ensemble_method = "risk_weighted"
                    self._calculate_risk_weights(X_clean, y_clean)
            elif self.ensemble_method == "risk_weighted":
                self._calculate_risk_weights(X_clean, y_clean)

            # 3. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è©•ä¾¡ï¼ˆå …ç‰¢åŒ–ï¼‰
            try:
                self._evaluate_model_performance(X_clean, y_clean)
            except Exception as e:
                logger.warning(f"Performance evaluation failed: {e}")
                self.model_performance_ = {}

            # 4. å–å¼•ç‰¹åŒ–å‹é‡ã¿è¨ˆç®—ï¼ˆå …ç‰¢åŒ–ï¼‰
            try:
                self._calculate_trading_weights()
            except Exception as e:
                logger.warning(f"Trading weights calculation failed: {e}")
                self.trading_weights_ = [1.0 / len(self.fitted_base_models)] * len(
                    self.fitted_base_models
                )

            # Phase H.26: å­¦ç¿’å®Œäº†ãƒ•ãƒ©ã‚°è¨­å®š
            self.is_fitted = True

            logger.info(
                f"âœ… Ensemble training completed successfully ({successful_models}/{len(self.base_models)} models)"
            )
            return self

        except Exception as e:
            logger.error(f"âŒ Ensemble training failed completely: {e}")
            return self._create_fallback_ensemble()

    def _fit_base_models(self, X: pd.DataFrame, y: pd.Series):
        """ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
        self.fitted_base_models = []

        for i, model in enumerate(self.base_models):
            logger.info(
                f"Training base model {i+1}/{len(self.base_models)}: "
                f"{type(model).__name__}"
            )

            try:
                # ç¢ºç‡æ ¡æ­£ä»˜ããƒ¢ãƒ‡ãƒ«
                if self.use_calibration:
                    calibrated_model = CalibratedClassifierCV(
                        model, method="sigmoid", cv=3
                    )
                    calibrated_model.fit(X, y)
                    self.fitted_base_models.append(calibrated_model)
                else:
                    model.fit(X, y)
                    self.fitted_base_models.append(model)

                logger.info(f"Base model {i+1} training completed")

            except Exception as e:
                logger.error(f"Failed to train base model {i+1}: {e}")
                # å¤±æ•—ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ä»£æ›¿
                dummy_model = LogisticRegression(random_state=42)
                dummy_model.fit(X, y)
                self.fitted_base_models.append(dummy_model)

    def _fit_trading_meta_model(self, X: pd.DataFrame, y: pd.Series):
        """å–å¼•ç‰¹åŒ–å‹ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
        logger.info("Generating trading-optimized meta-features")

        # å–å¼•ç‰¹åŒ–å‹ãƒ¡ã‚¿ç‰¹å¾´é‡ç”Ÿæˆï¼ˆç¢ºç‡ãƒ»ä¿¡é ¼åº¦ãƒ»ãƒªã‚¹ã‚¯èª¿æ•´å€¤ï¼‰
        meta_features = np.zeros((len(X), len(self.fitted_base_models) * 3))

        cv = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=42)

        for i, model in enumerate(self.base_models):
            try:
                # äº¤å·®æ¤œè¨¼äºˆæ¸¬
                cv_predictions = cross_val_predict(
                    model, X, y, cv=cv, method="predict_proba"
                )

                # åŸºæœ¬ç¢ºç‡
                if cv_predictions.shape[1] > 1:
                    proba = cv_predictions[:, 1]
                else:
                    proba = cv_predictions[:, 0]
                meta_features[:, i * 3] = proba

                # äºˆæ¸¬ä¿¡é ¼åº¦ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰
                entropy = -np.sum(
                    cv_predictions * np.log(cv_predictions + 1e-8), axis=1
                )
                confidence = 1 - (entropy / np.log(2))  # æ­£è¦åŒ–
                meta_features[:, i * 3 + 1] = confidence

                # ãƒªã‚¹ã‚¯èª¿æ•´å€¤ï¼ˆæ¥µç«¯ãªäºˆæ¸¬ã¸ã®é‡ã¿èª¿æ•´ï¼‰
                risk_adjustment = 1.0 - np.abs(proba - 0.5) * 2  # 0.5ã‹ã‚‰é›¢ã‚Œã‚‹ã»ã©ä½ä¸‹
                if self.risk_adjustment:
                    meta_features[:, i * 3 + 2] = risk_adjustment
                else:
                    meta_features[:, i * 3 + 2] = 1.0

            except Exception as e:
                logger.error(
                    f"Failed to generate trading meta-features for model {i+1}: {e}"
                )
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«å€¤
                meta_features[:, i * 3 : i * 3 + 3] = [0.5, 0.5, 1.0]

        # å–å¼•æœ€é©åŒ–ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        self.fitted_meta_model = self.meta_model.fit(meta_features, y)
        logger.info("Trading meta-model training completed")

    def _calculate_risk_weights(self, X: pd.DataFrame, y: pd.Series):
        """ãƒªã‚¹ã‚¯åŠ é‡å‹é‡ã¿è¨ˆç®—"""
        logger.info("Calculating risk-adjusted weights")

        risk_weights = []
        cv = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=42)

        for model in self.base_models:
            try:
                # äºˆæ¸¬ç²¾åº¦è¨ˆç®—
                cv_predictions = cross_val_predict(model, X, y, cv=cv)
                accuracy = np.mean(cv_predictions == y)

                # äºˆæ¸¬å®‰å®šæ€§è¨ˆç®—ï¼ˆäºˆæ¸¬åˆ†æ•£ï¼‰
                cv_probas = cross_val_predict(
                    model, X, y, cv=cv, method="predict_proba"
                )
                stability = 1.0 - np.std(cv_probas[:, 1])  # ä½åˆ†æ•£ = é«˜å®‰å®šæ€§

                # ãƒªã‚¹ã‚¯èª¿æ•´é‡ã¿ï¼ˆç²¾åº¦Ã—å®‰å®šæ€§ï¼‰
                risk_weight = accuracy * stability
                risk_weights.append(risk_weight)

            except Exception as e:
                logger.error(f"Failed to calculate risk weight: {e}")
                risk_weights.append(0.5)

        # æ­£è¦åŒ–
        total_weight = sum(risk_weights)
        if total_weight > 0:
            self.trading_weights_ = [w / total_weight for w in risk_weights]
        else:
            self.trading_weights_ = [1.0 / len(risk_weights)] * len(risk_weights)

        logger.info(f"Calculated risk weights: {self.trading_weights_}")

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """äºˆæ¸¬å€¤ã®å‡ºåŠ›"""
        predictions_proba = self.predict_proba(X)
        return (predictions_proba[:, 1] > 0.5).astype(int)

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """å–å¼•ç‰¹åŒ–å‹ç¢ºç‡äºˆæ¸¬ã®å‡ºåŠ›"""
        if not self.fitted_base_models:
            raise ValueError("Model not fitted yet")

        # Store last X for model agreement calculation
        self.last_X_ = X

        if self.ensemble_method == "trading_stacking":
            return self._predict_proba_trading_stacking(X)
        elif self.ensemble_method == "risk_weighted":
            return self._predict_proba_risk_weighted(X)
        else:  # performance_voting
            return self._predict_proba_performance_voting(X)

    def _predict_proba_trading_stacking(self, X: pd.DataFrame) -> np.ndarray:
        """å–å¼•ç‰¹åŒ–å‹ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°äºˆæ¸¬"""
        # å–å¼•ç‰¹åŒ–å‹ãƒ¡ã‚¿ç‰¹å¾´é‡ç”Ÿæˆ
        meta_features = np.zeros((len(X), len(self.fitted_base_models) * 3))

        for i, model in enumerate(self.fitted_base_models):
            try:
                proba = model.predict_proba(X)
                base_proba = proba[:, 1] if proba.shape[1] > 1 else proba[:, 0]

                # åŸºæœ¬ç¢ºç‡
                meta_features[:, i * 3] = base_proba

                # äºˆæ¸¬ä¿¡é ¼åº¦
                entropy = -np.sum(proba * np.log(proba + 1e-8), axis=1)
                confidence = 1 - (entropy / np.log(2))
                meta_features[:, i * 3 + 1] = confidence

                # ãƒªã‚¹ã‚¯èª¿æ•´å€¤
                risk_adjustment = 1.0 - np.abs(base_proba - 0.5) * 2
                if self.risk_adjustment:
                    meta_features[:, i * 3 + 2] = risk_adjustment
                else:
                    meta_features[:, i * 3 + 2] = 1.0

            except Exception as e:
                logger.error(f"Trading prediction failed for base model {i+1}: {e}")
                meta_features[:, i * 3 : i * 3 + 3] = [0.5, 0.5, 1.0]

        # å–å¼•æœ€é©åŒ–ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬
        return self.fitted_meta_model.predict_proba(meta_features)

    def _predict_proba_risk_weighted(self, X: pd.DataFrame) -> np.ndarray:
        """ãƒªã‚¹ã‚¯åŠ é‡å‹äºˆæ¸¬"""
        weights = self.trading_weights_ or [1.0 / len(self.fitted_base_models)] * len(
            self.fitted_base_models
        )

        weighted_proba = np.zeros((len(X), 2))
        total_weight = 0

        for i, (model, weight) in enumerate(zip(self.fitted_base_models, weights)):
            try:
                proba = model.predict_proba(X)

                # ãƒªã‚¹ã‚¯èª¿æ•´ï¼ˆæ¥µç«¯ãªäºˆæ¸¬ã‚’æŠ‘åˆ¶ï¼‰
                if self.risk_adjustment:
                    # 0.5ã‹ã‚‰é›¢ã‚Œã™ãã‚‹äºˆæ¸¬ã«ã¯é‡ã¿ã‚’ä¸‹ã’ã‚‹
                    extreme_penalty = 1.0 - np.abs(proba[:, 1] - 0.5) * 0.5
                    adjusted_weight = weight * np.mean(extreme_penalty)
                else:
                    adjusted_weight = weight

                weighted_proba += proba * adjusted_weight
                total_weight += adjusted_weight

            except Exception as e:
                logger.error(f"Risk-weighted prediction failed for model {i+1}: {e}")

        return weighted_proba / max(total_weight, 1e-8)

    def _predict_proba_performance_voting(self, X: pd.DataFrame) -> np.ndarray:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ é‡æŠ•ç¥¨äºˆæ¸¬"""
        # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ™ãƒ¼ã‚¹ã®é‡ã¿ä½¿ç”¨
        performance_weights = []
        for _model_name, performance in self.model_performance_.items():
            # å–å¼•ãƒ¡ãƒˆãƒªã‚¯ã‚¹é‡ã¿ä»˜ãç·åˆã‚¹ã‚³ã‚¢
            win_rate_score = performance.get(
                "accuracy", 0.5
            ) * self.trading_metrics.get("win_rate", 0.3)
            sharpe_score = performance.get("f1", 0.5) * self.trading_metrics.get(
                "sharpe_ratio", 0.4
            )
            drawdown_penalty = (1.0 - performance.get("precision", 0.5)) * abs(
                self.trading_metrics.get("max_drawdown", -0.2)
            )
            score = win_rate_score + sharpe_score + drawdown_penalty
            performance_weights.append(score)

        # æ­£è¦åŒ–
        total_perf = sum(performance_weights)
        if total_perf > 0:
            performance_weights = [w / total_perf for w in performance_weights]
        else:
            performance_weights = [1.0 / len(self.fitted_base_models)] * len(
                self.fitted_base_models
            )

        weighted_proba = np.zeros((len(X), 2))

        for i, (model, weight) in enumerate(
            zip(self.fitted_base_models, performance_weights)
        ):
            try:
                proba = model.predict_proba(X)
                weighted_proba += proba * weight
            except Exception as e:
                logger.error(f"Performance voting failed for model {i+1}: {e}")

        return weighted_proba

    def _predict_proba_weighted_voting(self, X: pd.DataFrame) -> np.ndarray:
        """é‡ã¿ä»˜ãæŠ•ç¥¨äºˆæ¸¬"""
        weights = (
            self.ensemble_weights_
            or self.voting_weights
            or [1.0] * len(self.fitted_base_models)
        )

        weighted_proba = np.zeros((len(X), 2))
        total_weight = 0

        for i, (model, weight) in enumerate(zip(self.fitted_base_models, weights)):
            try:
                proba = model.predict_proba(X)
                weighted_proba += proba * weight
                total_weight += weight
            except Exception as e:
                logger.error(f"Weighted prediction failed for model {i+1}: {e}")

        return weighted_proba / max(total_weight, 1e-8)

    def _predict_proba_voting(self, X: pd.DataFrame) -> np.ndarray:
        """å˜ç´”æŠ•ç¥¨äºˆæ¸¬"""
        return self._predict_proba_weighted_voting(X)

    def predict_with_trading_confidence(
        self, X: pd.DataFrame, market_context: Dict = None
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Dict]:
        """
        å–å¼•ç‰¹åŒ–å‹ä¿¡é ¼åº¦ä»˜ãäºˆæ¸¬ï¼ˆPhase H.26: å …ç‰¢åŒ–ç‰ˆï¼‰

        Parameters:
        -----------
        X : pd.DataFrame
            ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        market_context : Dict, optional
            å¸‚å ´ç’°å¢ƒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆVIXã€ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç­‰ï¼‰

        Returns:
        --------
        predictions : np.ndarray
            äºˆæ¸¬ã‚¯ãƒ©ã‚¹
        probabilities : np.ndarray
            äºˆæ¸¬ç¢ºç‡
        confidence_scores : np.ndarray
            å–å¼•ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        trading_info : Dict
            å–å¼•åˆ¤æ–­ã«é–¢ã™ã‚‹è©³ç´°æƒ…å ±
        """
        # Phase H.26: äºˆæ¸¬å‰ã®å …ç‰¢æ€§ãƒã‚§ãƒƒã‚¯
        try:
            # å­¦ç¿’æ¸ˆã¿ãƒã‚§ãƒƒã‚¯
            if not hasattr(self, "is_fitted") or not self.is_fitted:
                logger.warning("Model not fitted, using fallback predictions")
                return self._create_fallback_predictions(X)

            # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            if not self.fitted_base_models or len(self.fitted_base_models) == 0:
                logger.warning("No fitted base models, using fallback predictions")
                return self._create_fallback_predictions(X)

            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            if X is None or X.empty:
                logger.error("Empty input data")
                return self._create_fallback_predictions(X)

            # ç‰¹å¾´é‡æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            if hasattr(self, "feature_names_") and self.feature_names_:
                if list(X.columns) != self.feature_names_:
                    logger.warning("Feature mismatch, attempting alignment")
                    X = self._align_prediction_features(X)

            # NaNå€¤ãƒã‚§ãƒƒã‚¯
            nan_ratio = X.isna().sum().sum() / (len(X) * len(X.columns))
            if nan_ratio > 0.8:  # 80%ä»¥ä¸ŠãŒNaN
                logger.error(f"Too many NaN values for prediction: {nan_ratio:.2%}")
                return self._create_fallback_predictions(X)

            # NaNå€¤è£œå®Œ
            X_clean = X.ffill().bfill().fillna(0)

            probabilities = self.predict_proba(X_clean)

        except Exception as e:
            logger.error(f"Prediction failed: {e}, using fallback")
            return self._create_fallback_predictions(X)

        # å‹•çš„é–¾å€¤è¨ˆç®—
        dynamic_threshold = self._calculate_dynamic_threshold(X, market_context)
        predictions = (probabilities[:, 1] > dynamic_threshold).astype(int)

        # å–å¼•ç‰¹åŒ–å‹ä¿¡é ¼åº¦è¨ˆç®—
        confidence_scores = self._calculate_trading_confidence(
            probabilities, X, market_context
        )

        # å–å¼•æƒ…å ±ã¾ã¨ã‚
        trading_info = {
            "dynamic_threshold": dynamic_threshold,
            "base_threshold": 0.5,
            "market_regime": self._assess_market_regime(market_context),
            "risk_level": self._assess_risk_level(confidence_scores),
            "recommended_position_size": self._calculate_position_sizing(
                confidence_scores, market_context
            ),
        }

        return predictions, probabilities, confidence_scores, trading_info

    def _calculate_dynamic_threshold(
        self, X: pd.DataFrame, market_context: Dict = None
    ) -> float:
        """å‹•çš„é–¾å€¤è¨ˆç®—"""
        base_threshold = self.confidence_threshold

        if market_context is None:
            return base_threshold

        # VIXãƒ™ãƒ¼ã‚¹èª¿æ•´
        vix_level = market_context.get("vix_level", 20.0)
        if vix_level > 35:  # é«˜VIX
            threshold_adj = 0.1  # ã‚ˆã‚Šä¿å®ˆçš„
        elif vix_level > 25:  # ä¸­VIX
            threshold_adj = 0.05
        elif vix_level < 15:  # ä½VIX
            threshold_adj = -0.05  # ã‚ˆã‚Šç©æ¥µçš„
        else:
            threshold_adj = 0.0

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ™ãƒ¼ã‚¹èª¿æ•´
        volatility = market_context.get("volatility", 0.02)
        if volatility > 0.05:  # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            threshold_adj += 0.05
        elif volatility < 0.01:  # ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            threshold_adj -= 0.02

        # æœ€çµ‚é–¾å€¤
        dynamic_threshold = base_threshold + threshold_adj
        return np.clip(dynamic_threshold, 0.3, 0.8)  # ç¯„å›²åˆ¶é™

    def _calculate_trading_confidence(
        self, probabilities: np.ndarray, X: pd.DataFrame, market_context: Dict = None
    ) -> np.ndarray:
        """å–å¼•ç‰¹åŒ–å‹ä¿¡é ¼åº¦è¨ˆç®—"""
        # åŸºæœ¬ä¿¡é ¼åº¦ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰
        epsilon = 1e-8
        entropy = -np.sum(probabilities * np.log(probabilities + epsilon), axis=1)
        max_entropy = np.log(2)
        entropy_confidence = 1 - (entropy / max_entropy)

        # ç¢ºç‡ã®æ¥µç«¯åº¦ï¼ˆ0.5ã‹ã‚‰é›¢ã‚Œã‚‹ã»ã©é«˜ä¿¡é ¼åº¦ï¼‰
        probability_confidence = np.abs(probabilities[:, 1] - 0.5) * 2

        # ãƒ¢ãƒ‡ãƒ«åˆæ„åº¦
        model_agreement = self._calculate_model_agreement_score(X)

        # å¸‚å ´ç’°å¢ƒèª¿æ•´
        market_adjustment = self._get_market_confidence_adjustment(market_context)

        # ç·åˆä¿¡é ¼åº¦
        trading_confidence = (
            0.3 * entropy_confidence
            + 0.3 * probability_confidence
            + 0.2 * model_agreement
            + 0.2 * market_adjustment
        )

        return np.clip(trading_confidence, 0.0, 1.0)

    def _calculate_model_agreement_score(self, X: pd.DataFrame) -> np.ndarray:
        """ãƒ¢ãƒ‡ãƒ«é–“åˆæ„åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if len(self.fitted_base_models) < 2:
            return np.ones(len(X))

        individual_predictions = []
        for model in self.fitted_base_models:
            try:
                proba = model.predict_proba(X)
                individual_predictions.append(proba[:, 1])
            except Exception:
                individual_predictions.append(np.full(len(X), 0.5))

        # äºˆæ¸¬ã®æ¨™æº–åå·®è¨ˆç®—ï¼ˆä½ã„ = é«˜åˆæ„åº¦ï¼‰
        pred_array = np.array(individual_predictions).T
        agreement = 1.0 - np.std(pred_array, axis=1) / 0.5  # æ­£è¦åŒ–

        return np.clip(agreement, 0.0, 1.0)

    def _get_market_confidence_adjustment(self, market_context: Dict = None) -> float:
        """å¸‚å ´ç’°å¢ƒã«ã‚ˆã‚‹ä¿¡é ¼åº¦èª¿æ•´"""
        if market_context is None:
            return 0.5

        # VIXãƒ™ãƒ¼ã‚¹èª¿æ•´
        vix_level = market_context.get("vix_level", 20.0)
        if vix_level > 35:
            vix_adj = 0.2  # ä¸å®‰å®šãªå¸‚å ´ã§ã¯ä¿¡é ¼åº¦ä½ä¸‹
        elif vix_level < 15:
            vix_adj = 0.8  # å®‰å®šå¸‚å ´ã§ã¯ä¿¡é ¼åº¦å‘ä¸Š
        else:
            vix_adj = 0.5

        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦èª¿æ•´
        trend_strength = market_context.get("trend_strength", 0.5)
        trend_adj = trend_strength  # å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ã¯é«˜ä¿¡é ¼åº¦

        return (vix_adj + trend_adj) / 2.0

    def _assess_market_regime(self, market_context: Dict = None) -> str:
        """å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ è©•ä¾¡"""
        if market_context is None:
            return "unknown"

        vix_level = market_context.get("vix_level", 20.0)
        volatility = market_context.get("volatility", 0.02)

        if vix_level > 35 or volatility > 0.06:
            return "crisis"
        elif vix_level > 25 or volatility > 0.04:
            return "volatile"
        elif vix_level < 15 and volatility < 0.02:
            return "calm"
        else:
            return "normal"

    def _assess_risk_level(self, confidence_scores: np.ndarray) -> str:
        """ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«è©•ä¾¡"""
        avg_confidence = np.mean(confidence_scores)

        if avg_confidence > 0.8:
            return "low"
        elif avg_confidence > 0.6:
            return "medium"
        elif avg_confidence > 0.4:
            return "high"
        else:
            return "very_high"

    def _calculate_position_sizing(
        self, confidence_scores: np.ndarray, market_context: Dict = None
    ) -> float:
        """ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚¸ãƒ³ã‚°æ¨å¥¨è¨ˆç®—"""
        avg_confidence = np.mean(confidence_scores)

        # åŸºæœ¬ã‚µã‚¤ã‚ºï¼ˆä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
        base_size = avg_confidence * 0.1  # æœ€å¤§10%

        # å¸‚å ´ç’°å¢ƒèª¿æ•´
        if market_context:
            vix_level = market_context.get("vix_level", 20.0)
            if vix_level > 35:
                base_size *= 0.5  # åŠåˆ†ã«å‰Šæ¸›
            elif vix_level < 15:
                base_size *= 1.2  # 20%å¢—åŠ 

        return min(base_size, 0.15)  # æœ€å¤§15%åˆ¶é™

    def _calculate_confidence_scores(self, probabilities: np.ndarray) -> np.ndarray:
        """
        äºˆæ¸¬ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—

        ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»åˆ†æ•£ãƒ»ä¸€è‡´åº¦ã‚’çµ„ã¿åˆã‚ã›ãŸä¿¡é ¼åº¦æŒ‡æ¨™
        """
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦ï¼ˆä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ = é«˜ä¿¡é ¼åº¦ï¼‰
        epsilon = 1e-8
        entropy = -np.sum(probabilities * np.log(probabilities + epsilon), axis=1)
        max_entropy = np.log(2)  # 2ã‚¯ãƒ©ã‚¹åˆ†é¡ã®æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        entropy_confidence = 1 - (entropy / max_entropy)

        # ç¢ºç‡ã®æœ€å¤§å€¤ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        max_prob_confidence = np.max(probabilities, axis=1)

        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«é–“ã®ä¸€è‡´åº¦ï¼ˆã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ä»¥å¤–ã®å ´åˆï¼‰
        if self.ensemble_method != "stacking" and len(self.fitted_base_models) > 1:
            model_agreement = self._calculate_model_agreement(probabilities)
        else:
            model_agreement = np.ones(len(probabilities))

        # ç·åˆä¿¡é ¼åº¦ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
        confidence_scores = (
            0.4 * entropy_confidence + 0.4 * max_prob_confidence + 0.2 * model_agreement
        )

        return confidence_scores

    def _calculate_model_agreement(
        self, ensemble_probabilities: np.ndarray
    ) -> np.ndarray:
        """ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«é–“ã®äºˆæ¸¬ä¸€è‡´åº¦è¨ˆç®—"""
        # å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡ã‚’å–å¾—
        individual_probabilities = []

        for model in self.fitted_base_models:
            try:
                proba = model.predict_proba(self.last_X_)  # æœ€å¾Œã«äºˆæ¸¬ã—ãŸX
                individual_probabilities.append(proba[:, 1])
            except Exception:
                individual_probabilities.append(
                    np.full(len(ensemble_probabilities), 0.5)
                )

        if len(individual_probabilities) < 2:
            return np.ones(len(ensemble_probabilities))

        # æ¨™æº–åå·®ãƒ™ãƒ¼ã‚¹ã®ä¸€è‡´åº¦ï¼ˆä½åˆ†æ•£ = é«˜ä¸€è‡´åº¦ï¼‰
        prob_array = np.array(individual_probabilities).T
        # æœ€å¤§æ¨™æº–åå·®ã§æ­£è¦åŒ–
        agreement = 1 - np.std(prob_array, axis=1) / 0.5

        return np.clip(agreement, 0, 1)

    def _evaluate_model_performance(self, X: pd.DataFrame, y: pd.Series):
        """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è©•ä¾¡"""
        self.model_performance_ = {}

        # äº¤å·®æ¤œè¨¼ã§ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è©•ä¾¡
        cv = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=42)

        for i, model in enumerate(self.base_models):
            try:
                cv_predictions = cross_val_predict(model, X, y, cv=cv)

                performance = {
                    "accuracy": accuracy_score(y, cv_predictions),
                    "precision": precision_score(
                        y, cv_predictions, average="weighted", zero_division=0
                    ),
                    "recall": recall_score(
                        y, cv_predictions, average="weighted", zero_division=0
                    ),
                    "f1": f1_score(
                        y, cv_predictions, average="weighted", zero_division=0
                    ),
                }

                model_key = f"model_{i+1}_{type(model).__name__}"
                self.model_performance_[model_key] = performance
                logger.info(f"Model {i+1} performance: {performance}")

            except Exception as e:
                logger.error(f"Performance evaluation failed for model {i+1}: {e}")
                model_key = f"model_{i+1}_{type(model).__name__}"
                self.model_performance_[model_key] = {
                    "accuracy": 0.5,
                    "precision": 0.5,
                    "recall": 0.5,
                    "f1": 0.5,
                }

    def _calculate_trading_weights(self):
        """å–å¼•ç‰¹åŒ–å‹å‹•çš„é‡ã¿è¨ˆç®—"""
        if not self.model_performance_:
            self.trading_weights_ = [1.0] * len(self.fitted_base_models)
            return

        # å–å¼•ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        trading_scores = []
        for _model_name, performance in self.model_performance_.items():
            # å‹ç‡é‡è¦–ã®ç·åˆã‚¹ã‚³ã‚¢
            win_rate_score = performance.get(
                "accuracy", 0.5
            ) * self.trading_metrics.get("win_rate", 0.3)
            sharpe_score = performance.get("precision", 0.5) * self.trading_metrics.get(
                "sharpe_ratio", 0.4
            )
            f1_score = performance.get("f1", 0.5) * 0.2
            drawdown_penalty = (
                (1.0 - performance.get("recall", 0.5))
                * abs(self.trading_metrics.get("max_drawdown", -0.2))
                * 0.1
            )
            score = win_rate_score + sharpe_score + f1_score + drawdown_penalty
            trading_scores.append(max(score, 0.1))  # æœ€å°é‡ã¿ä¿è¨¼

        # é‡ã¿ã®æ­£è¦åŒ–
        total_score = sum(trading_scores)
        if total_score > 0:
            self.trading_weights_ = [score / total_score for score in trading_scores]
        else:
            self.trading_weights_ = [1.0 / len(self.fitted_base_models)] * len(
                self.fitted_base_models
            )

        logger.info(f"Calculated trading weights: {self.trading_weights_}")

        # ãƒªã‚¹ã‚¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        self.risk_metrics_ = {
            "weight_diversity": np.std(self.trading_weights_),
            "max_weight": max(self.trading_weights_),
            "min_weight": min(self.trading_weights_),
            "weight_concentration": (
                max(self.trading_weights_) / np.mean(self.trading_weights_)
            ),
        }

    def get_trading_ensemble_info(self) -> Dict[str, Any]:
        """å–å¼•ç‰¹åŒ–å‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æƒ…å ±ã®å–å¾—"""
        return {
            "ensemble_method": self.ensemble_method,
            "num_base_models": len(self.fitted_base_models),
            "risk_adjustment": self.risk_adjustment,
            "cv_folds": self.cv_folds,
            "confidence_threshold": self.confidence_threshold,
            "trading_metrics": self.trading_metrics,
            "model_performance": self.model_performance_,
            "trading_weights": self.trading_weights_,
            "risk_metrics": self.risk_metrics_,
            "base_model_types": [type(model).__name__ for model in self.base_models],
            "prediction_history_size": len(self.prediction_history_),
            "market_regime_weights": self.market_regime_weights_,
        }

    def update_confidence_threshold(self, new_threshold: float):
        """ä¿¡é ¼åº¦é–¾å€¤ã®å‹•çš„æ›´æ–°"""
        self.confidence_threshold = new_threshold
        logger.info(f"Confidence threshold updated to: {new_threshold}")

    def get_feature_importance(self) -> pd.DataFrame:
        """çµ±åˆç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—"""
        if not self.feature_names_:
            return pd.DataFrame()

        importance_data = []

        for _i, model in enumerate(self.fitted_base_models):
            if hasattr(model, "feature_importances_"):
                importance_data.append(model.feature_importances_)
            elif hasattr(model, "coef_"):
                importance_data.append(np.abs(model.coef_[0]))
            else:
                # é‡è¦åº¦ãŒå–å¾—ã§ããªã„å ´åˆã¯å‡ç­‰é‡ã¿
                importance_data.append(np.ones(len(self.feature_names_)))

        if not importance_data:
            return pd.DataFrame()

        # é‡ã¿ä»˜ãå¹³å‡é‡è¦åº¦è¨ˆç®—
        weights = self.ensemble_weights_ or [1.0] * len(importance_data)
        weighted_importance = np.zeros(len(self.feature_names_))

        for importance, weight in zip(importance_data, weights):
            weighted_importance += importance * weight

        weighted_importance /= sum(weights)

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        importance_df = pd.DataFrame(
            {"feature": self.feature_names_, "importance": weighted_importance}
        ).sort_values("importance", ascending=False)

        return importance_df

    # Phase H.26: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’å …ç‰¢åŒ–ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    def _validate_training_data(self, X: pd.DataFrame, y: pd.Series) -> bool:
        """ãƒ‡ãƒ¼ã‚¿å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            # åŸºæœ¬çš„ãªå­˜åœ¨ç¢ºèª
            if X is None or y is None:
                logger.error("X or y is None")
                return False

            if X.empty or y.empty:
                logger.error("X or y is empty")
                return False

            # ã‚µã‚¤ã‚ºç¢ºèª
            if len(X) != len(y):
                logger.error(f"Size mismatch: X={len(X)}, y={len(y)}")
                return False

            # æœ€å°ã‚µã‚¤ã‚ºç¢ºèª
            if len(X) < 10:  # çµ¶å¯¾æœ€å°
                logger.error(f"Dataset too small: {len(X)} samples")
                return False

            # NaNå€¤ãƒã‚§ãƒƒã‚¯
            x_nan_ratio = X.isna().sum().sum() / (len(X) * len(X.columns))
            y_nan_ratio = y.isna().sum() / len(y)

            if x_nan_ratio > 0.5:  # 50%ä»¥ä¸ŠãŒNaN
                logger.error(f"Too many NaN values in X: {x_nan_ratio:.2%}")
                return False

            if y_nan_ratio > 0.3:  # 30%ä»¥ä¸ŠãŒNaN
                logger.error(f"Too many NaN values in y: {y_nan_ratio:.2%}")
                return False

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã®å¤šæ§˜æ€§ç¢ºèª
            unique_targets = len(y.unique())
            if unique_targets < 2:
                logger.error(f"Insufficient target diversity: {unique_targets} classes")
                return False

            logger.info(
                f"âœ… Data validation passed: {len(X)} samples, X_NaN={x_nan_ratio:.2%}, y_NaN={y_nan_ratio:.2%}"
            )
            return True

        except Exception as e:
            logger.error(f"Data validation failed: {e}")
            return False

    def _align_and_clean_data(
        self, X: pd.DataFrame, y: pd.Series
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•´åˆæ€§ç¢ºä¿ã¨NaNé™¤å»"""
        try:
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•´åˆ
            common_index = X.index.intersection(y.index)
            if len(common_index) == 0:
                logger.warning("No common index, using positional alignment")
                # ä½ç½®ãƒ™ãƒ¼ã‚¹æ•´åˆ
                min_len = min(len(X), len(y))
                X_aligned = X.iloc[:min_len].reset_index(drop=True)
                y_aligned = y.iloc[:min_len].reset_index(drop=True)
            else:
                X_aligned = X.loc[common_index]
                y_aligned = y.loc[common_index]

            # NaNé™¤å»
            # ã¾ãšyã®NaNã‚’é™¤å»
            y_valid_mask = ~y_aligned.isna()
            y_clean = y_aligned[y_valid_mask]
            X_temp = X_aligned[y_valid_mask]

            # æ¬¡ã«Xã®è¡Œã§å…¨ã¦NaNã®è¡Œã‚’é™¤å»
            x_valid_mask = ~X_temp.isna().all(axis=1)
            X_clean = X_temp[x_valid_mask]
            y_clean = y_clean[x_valid_mask]

            # å„åˆ—ã§æ®‹ã£ã¦ã„ã‚‹NaNã‚’å‰æ–¹è£œå®Œï¼ˆPhase H.26: pandaséæ¨å¥¨å¯¾å¿œï¼‰
            X_clean = X_clean.ffill().bfill().fillna(0)

            logger.info(
                f"Data alignment: {len(X)} â†’ {len(X_clean)} samples after cleaning"
            )
            return X_clean, y_clean

        except Exception as e:
            logger.error(f"Data alignment failed: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”ãªæ•´åˆ
            min_len = min(len(X), len(y))
            return X.iloc[:min_len].fillna(0), y.iloc[:min_len].fillna(0)

    def _fit_base_models_robust(self, X: pd.DataFrame, y: pd.Series) -> int:
        """å …ç‰¢ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å­¦ç¿’"""
        self.fitted_base_models = []
        successful_models = 0

        for i, model in enumerate(self.base_models):
            try:
                logger.info(
                    f"Training base model {i+1}/{len(self.base_models)}: {type(model).__name__}"
                )

                # ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ãƒã‚§ãƒƒã‚¯
                model_min_samples = self._get_model_min_samples(model)
                if len(y) < model_min_samples:
                    logger.warning(
                        f"Insufficient samples for {type(model).__name__}: {len(y)} < {model_min_samples}"
                    )
                    continue

                # ç¢ºç‡æ ¡æ­£ä»˜ããƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
                if hasattr(self, "use_calibration") and self.use_calibration:
                    calibrated_model = CalibratedClassifierCV(
                        model, method="sigmoid", cv=3
                    )
                    calibrated_model.fit(X, y)
                    self.fitted_base_models.append(calibrated_model)
                else:
                    model.fit(X, y)
                    self.fitted_base_models.append(model)

                successful_models += 1
                logger.info(f"âœ… Base model {i+1} training completed")

            except Exception as e:
                logger.error(f"âŒ Base model {i+1} failed: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªLogisticRegression
                try:
                    fallback_model = LogisticRegression(random_state=42, max_iter=100)
                    fallback_model.fit(X, y)
                    self.fitted_base_models.append(fallback_model)
                    successful_models += 1
                    logger.warning(f"âš ï¸ Using fallback model for base model {i+1}")
                except Exception as fallback_error:
                    logger.error(f"âŒ Fallback model also failed: {fallback_error}")
                    continue

        logger.info(
            f"Base models training completed: {successful_models}/{len(self.base_models)} successful"
        )
        return successful_models

    def _get_model_min_samples(self, model) -> int:
        """ãƒ¢ãƒ‡ãƒ«åˆ¥æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°å–å¾—"""
        model_name = type(model).__name__

        # Phase H.29: 4hã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å¯¾å¿œã®ãŸã‚æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ç·©å’Œ
        if model_name in ["XGBClassifier", "LGBMClassifier"]:
            return 30  # å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ç³»ï¼ˆ50â†’30ã«ç·©å’Œï¼‰
        elif model_name == "RandomForestClassifier":
            return 20  # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼ˆ30â†’20ã«ç·©å’Œï¼‰
        elif model_name in ["LogisticRegression", "SVC"]:
            return 15  # ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆ20â†’15ã«ç·©å’Œï¼‰
        else:
            return 20  # ãã®ä»–ï¼ˆ25â†’20ã«ç·©å’Œï¼‰

    def _create_fallback_ensemble(self) -> "TradingEnsembleClassifier":
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ç°¡æ˜“ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆ"""
        logger.warning("Creating fallback ensemble with single LogisticRegression")

        try:
            # æœ€å°é™ã®LogisticRegressionãƒ¢ãƒ‡ãƒ«
            fallback_model = LogisticRegression(
                random_state=42, max_iter=100, class_weight="balanced"
            )
            self.fitted_base_models = [fallback_model]
            self.ensemble_method = "simple_fallback"
            self.is_fitted = False  # ã¾ã å­¦ç¿’ã—ã¦ã„ãªã„
            self.feature_names_ = []
            self.classes_ = np.array([0, 1])
            self.model_performance_ = {}
            self.trading_weights_ = [1.0]

            return self

        except Exception as e:
            logger.error(f"Failed to create fallback ensemble: {e}")
            # å®Œå…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            self.fitted_base_models = []
            self.is_fitted = False
            return self

    def _create_fallback_predictions(
        self, X: pd.DataFrame
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Dict]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯äºˆæ¸¬ç”Ÿæˆ"""
        try:
            n_samples = len(X)

            # åŸºæœ¬çš„ãªç¢ºç‡ï¼ˆã‚ãšã‹ã«æ¥½è¦³çš„ï¼‰
            fallback_probabilities = np.column_stack(
                [
                    np.full(n_samples, 0.45),  # ã‚¯ãƒ©ã‚¹0ç¢ºç‡
                    np.full(n_samples, 0.55),  # ã‚¯ãƒ©ã‚¹1ç¢ºç‡ï¼ˆã‚ãšã‹ã«æ¥½è¦³çš„ï¼‰
                ]
            )

            # äºˆæ¸¬ã‚¯ãƒ©ã‚¹
            predictions = np.ones(n_samples, dtype=int)  # ã‚ãšã‹ã«æ¥½è¦³çš„

            # ä½ã‚ã®ä¿¡é ¼åº¦
            confidence_scores = np.full(n_samples, 0.35)

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æƒ…å ±
            trading_info = {
                "method": "fallback",
                "ensemble_enabled": False,
                "dynamic_threshold": 0.5,
                "market_regime": "unknown",
                "risk_level": "high",
                "model_agreement": 0.5,
                "prediction_quality": "poor",
            }

            logger.warning(f"Using fallback predictions for {n_samples} samples")
            return predictions, fallback_probabilities, confidence_scores, trading_info

        except Exception as e:
            logger.error(f"Fallback prediction failed: {e}")
            # æœ€å¾Œã®æ‰‹æ®µ
            n_samples = max(1, len(X) if X is not None and not X.empty else 1)
            return (
                np.zeros(n_samples, dtype=int),
                np.full((n_samples, 2), 0.5),
                np.full(n_samples, 0.1),
                {"method": "emergency_fallback", "ensemble_enabled": False},
            )

    def _align_prediction_features(self, X: pd.DataFrame) -> pd.DataFrame:
        """äºˆæ¸¬ç”¨ç‰¹å¾´é‡æ•´åˆ"""
        try:
            if not hasattr(self, "feature_names_") or not self.feature_names_:
                logger.warning("No stored feature names, using input as-is")
                return X

            # ä¸è¶³ã™ã‚‹ç‰¹å¾´é‡ã‚’0ã§è£œå®Œ
            missing_features = set(self.feature_names_) - set(X.columns)
            if missing_features:
                logger.warning(
                    f"Adding missing features with zeros: {len(missing_features)} features"
                )
                for feature in missing_features:
                    X[feature] = 0.0

            # ä½™åˆ†ãªç‰¹å¾´é‡ã‚’å‰Šé™¤
            extra_features = set(X.columns) - set(self.feature_names_)
            if extra_features:
                logger.warning(
                    f"Removing extra features: {len(extra_features)} features"
                )
                X = X.drop(columns=extra_features)

            # é †åºã‚’åˆã‚ã›ã‚‹
            X = X[self.feature_names_]

            logger.info(f"Feature alignment completed: {len(X.columns)} features")
            return X

        except Exception as e:
            logger.error(f"Feature alignment failed: {e}")
            return X  # å…ƒã®ã¾ã¾è¿”ã™


def create_trading_ensemble(config: Dict[str, Any]) -> TradingEnsembleClassifier:
    """
    å–å¼•ç‰¹åŒ–å‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®ä½œæˆ

    Parameters:
    -----------
    config : Dict[str, Any]
        å–å¼•ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®š

    Returns:
    --------
    TradingEnsembleClassifier
        å–å¼•æœ€é©åŒ–ã•ã‚ŒãŸã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åˆ†é¡å™¨
    """
    ensemble_config = config.get("ml", {}).get("ensemble", {})

    risk_adjustment = ensemble_config.get("risk_adjustment", True)

    # confidence_thresholdã‚’è¤‡æ•°ã®å ´æ‰€ã‹ã‚‰å–å¾—ï¼ˆå„ªå…ˆé †ä½é †ï¼‰
    confidence_from_ensemble = ensemble_config.get("confidence_threshold")
    confidence_from_ml = config.get("ml", {}).get("confidence_threshold")
    confidence_from_strategy = config.get("strategy", {}).get("confidence_threshold")

    # å„ªå…ˆé †ä½ã«å¾“ã£ã¦å€¤ã‚’é¸æŠï¼ˆproduction.ymlã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤0.35ã«åˆã‚ã›ã‚‹ï¼‰
    if confidence_from_ensemble is not None:
        confidence_threshold = confidence_from_ensemble
    elif confidence_from_ml is not None:
        confidence_threshold = confidence_from_ml
    elif confidence_from_strategy is not None:
        confidence_threshold = confidence_from_strategy
    else:
        confidence_threshold = 0.35  # production.ymlã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

    # å–å¼•ãƒ¡ãƒˆãƒªã‚¯ã‚¹é‡ã¿è¨­å®š
    trading_metrics = ensemble_config.get(
        "trading_metrics",
        {
            "sharpe_ratio": 0.4,  # ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªé‡è¦–
            "win_rate": 0.3,  # å‹ç‡é‡è¦–
            "max_drawdown": -0.2,  # ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³å›é¿
            "profit_factor": 0.1,  # åˆ©ç›Šç‡è€ƒæ…®
        },
    )

    # Phase 16.1-Bä¿®æ­£: äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«è‡ªå‹•èª­ã¿è¾¼ã¿æ©Ÿèƒ½è¿½åŠ 
    try:
        from pathlib import Path

        import joblib

        # äº‹å‰å­¦ç¿’æ¸ˆã¿ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿è©¦è¡Œ
        production_model_path = Path("models/production/model.pkl")
        if production_model_path.exists():
            logger.info(
                f"ğŸ”„ Loading pre-trained ensemble model from: {production_model_path}"
            )
            ensemble = joblib.load(production_model_path)

            # è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æ›´æ–°
            ensemble.confidence_threshold = confidence_threshold
            if hasattr(ensemble, "risk_adjustment"):
                ensemble.risk_adjustment = risk_adjustment

            logger.info(
                f"âœ… Loaded pre-trained ensemble with {len(ensemble.base_models)} models"
            )
            return ensemble

        else:
            logger.warning(f"âš ï¸  Pre-trained model not found at {production_model_path}")

    except Exception as e:
        logger.warning(f"âš ï¸  Failed to load pre-trained model: {e}")

    # ChatGPTææ¡ˆ: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«å¼·åˆ¶ä½¿ç”¨ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ”¹å–„
    logger.warning(
        "âš ï¸  No pre-trained ensemble model available! "
        "Creating simple_fallback mode for basic operation"
    )

    # ã‚·ãƒ³ãƒ—ãƒ«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åˆ†é¡å™¨ã‚’ä½œæˆ
    ensemble = TradingEnsembleClassifier(
        ensemble_method="simple_fallback",
        trading_metrics=trading_metrics,
        risk_adjustment=risk_adjustment,
        confidence_threshold=confidence_threshold,
    )

    logger.warning(
        "âš ï¸  Created fallback ensemble: simple_fallback mode "
        "(to prevent 'Strategy does not use ensemble models' error)"
    )
    return ensemble
