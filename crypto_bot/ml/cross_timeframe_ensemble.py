# =============================================================================
# ãƒ•ã‚¡ã‚¤ãƒ«å: crypto_bot/ml/cross_timeframe_ensemble.py
# èª¬æ˜:
# Phase C1: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ é–“çµ±åˆå°‚ç”¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# è¤‡æ•°ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ äºˆæ¸¬ã®é‡ã¿ä»˜ãçµ±åˆãƒ»å‹•çš„é‡ã¿èª¿æ•´ãƒ»å¸‚å ´ç’°å¢ƒé€£å‹•
# æ—¢å­˜multi_timeframe_ensemble.pyçµ±åˆãƒ­ã‚¸ãƒƒã‚¯åˆ†é›¢ãƒ»æœ€é©åŒ–ãƒ»Phase Bçµ±åˆ
# =============================================================================

import logging
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

import numpy as np

from crypto_bot.execution.engine import Signal
from crypto_bot.utils.ensemble_confidence import EnsembleConfidenceCalculator

logger = logging.getLogger(__name__)


class CrossTimeframeIntegrator:
    """
    ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµ±åˆã‚·ã‚¹ãƒ†ãƒ 

    æ©Ÿèƒ½:
    - è¤‡æ•°ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ äºˆæ¸¬ã®é‡ã¿ä»˜ãçµ±åˆ
    - å‹•çš„é‡ã¿èª¿æ•´ï¼ˆå“è³ªãƒ»ä¿¡é ¼åº¦ãƒ»åˆæ„åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
    - å¸‚å ´ç’°å¢ƒé€£å‹•é‡ã¿èª¿æ•´
    - Phase C1ä¿¡é ¼åº¦è¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ æ´»ç”¨
    - çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡ãƒ»æœ€é©åŒ–
    """

    def __init__(self, config: Dict[str, Any]):
        """
        ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ é–“çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–

        Parameters:
        -----------
        config : Dict[str, Any]
            è¨­å®šè¾æ›¸ï¼ˆãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ»ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šå«ã‚€ï¼‰
        """
        self.config = config

        # ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ è¨­å®šå–å¾—ï¼ˆPhase H.12: 4hå†…éƒ¨å‡¦ç†å¾©æ´»ãƒ»æœ¬æ¥è¨­è¨ˆå¾©æ—§ï¼‰
        multi_config = self.config.get("multi_timeframe", {})
        self.timeframes = multi_config.get(
            "timeframes", ["15m", "1h", "4h"]
        )  # æœ¬æ¥è¨­è¨ˆå¾©æ—§
        self.base_weights = multi_config.get("weights", [0.3, 0.5, 0.2])  # æœ¬æ¥é‡ã¿å¾©æ—§
        self.consensus_threshold = multi_config.get(
            "timeframe_consensus_threshold", 0.6
        )
        self.missing_data_tolerance = multi_config.get("missing_data_tolerance", 0.1)

        # çµ±åˆæ–¹æ³•è¨­å®š
        self.integration_method = multi_config.get(
            "integration_method", "quality_weighted_ensemble"
        )
        self.dynamic_weighting_enabled = multi_config.get("dynamic_weighting", True)
        self.market_adaptation_enabled = multi_config.get("market_adaptation", True)

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®š
        ensemble_config = self.config.get("ml", {}).get("ensemble", {})
        self.ensemble_confidence_threshold = ensemble_config.get(
            "confidence_threshold", 0.65
        )
        self.risk_adjustment_enabled = ensemble_config.get("risk_adjustment", True)

        # ä¿¡é ¼åº¦è¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆPhase C1çµ±åˆï¼‰
        self.confidence_calculator = EnsembleConfidenceCalculator(self.config)

        # é‡ã¿èª¿æ•´å±¥æ­´ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”¨ï¼‰
        self.weight_history: List[Dict[str, Any]] = []
        self.integration_history: List[Dict[str, Any]] = []
        self.max_history_size = 100

        # çµ±è¨ˆãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡
        self.integration_stats = {
            "integrations_performed": 0,
            "dynamic_weight_adjustments": 0,
            "market_adaptations": 0,
            "consensus_calculations": 0,
            "quality_assessments": 0,
            "high_consensus_signals": 0,
            "low_consensus_rejections": 0,
        }

        # ç¾åœ¨ã®å‹•çš„é‡ã¿ï¼ˆåˆæœŸåŒ–ï¼‰
        self.current_dynamic_weights = self.base_weights.copy()
        self.last_weight_update = datetime.now()

        logger.info("ğŸ”— CrossTimeframeIntegrator initialized")
        logger.info(f"   Timeframes: {self.timeframes}")
        logger.info(f"   Base weights: {self.base_weights}")
        logger.info(f"   Integration method: {self.integration_method}")

    def integrate_timeframe_predictions(
        self,
        timeframe_predictions: Dict[str, Dict[str, Any]],
        market_context: Optional[Dict[str, Any]] = None,
    ) -> Tuple[float, Dict[str, Any]]:
        """
        ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµ±åˆï¼ˆPhase C1ã‚³ã‚¢æ©Ÿèƒ½ï¼‰

        Parameters:
        -----------
        timeframe_predictions : Dict[str, Dict[str, Any]]
            å„ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®äºˆæ¸¬çµæœ
            å½¢å¼: {timeframe: {"prediction": array, "probability": array, "confidence": float, ...}}
        market_context : Dict[str, Any], optional
            å¸‚å ´ç’°å¢ƒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
        --------
        Tuple[float, Dict[str, Any]]
            (çµ±åˆã‚·ã‚°ãƒŠãƒ«å€¤, çµ±åˆè©³ç´°æƒ…å ±)
        """
        self.integration_stats["integrations_performed"] += 1

        try:
            # 1. äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ»æ¤œè¨¼
            processed_predictions = self._preprocess_predictions(timeframe_predictions)

            if not processed_predictions:
                logger.warning("No valid timeframe predictions for integration")
                return 0.5, self._create_fallback_integration_info()

            # 2. å‹•çš„é‡ã¿è¨ˆç®—ï¼ˆPhase C1å¼·åŒ–æ©Ÿèƒ½ï¼‰
            if self.dynamic_weighting_enabled:
                dynamic_weights = self._calculate_dynamic_weights(
                    processed_predictions, market_context
                )
                self.integration_stats["dynamic_weight_adjustments"] += 1
            else:
                dynamic_weights = self._get_base_weights_for_predictions(
                    processed_predictions
                )

            # 3. çµ±åˆã‚·ã‚°ãƒŠãƒ«è¨ˆç®—
            integrated_signal, signal_components = self._calculate_integrated_signal(
                processed_predictions, dynamic_weights
            )

            # 4. åˆæ„åº¦è¨ˆç®—ï¼ˆPhase C1çµ±åˆï¼‰
            consensus_score = self._calculate_cross_timeframe_consensus(
                processed_predictions, dynamic_weights
            )
            self.integration_stats["consensus_calculations"] += 1

            # 5. å“è³ªè©•ä¾¡
            integration_quality = self._assess_integration_quality(
                processed_predictions, consensus_score, market_context
            )
            self.integration_stats["quality_assessments"] += 1

            # 6. çµ±åˆæƒ…å ±æ§‹ç¯‰
            integration_info = {
                "integration_method": self.integration_method,
                "timeframe_count": len(processed_predictions),
                "dynamic_weights": dynamic_weights,
                "signal_components": signal_components,
                "consensus_score": consensus_score,
                "integration_quality": integration_quality,
                "market_adaptation": market_context is not None,
                "meets_consensus_threshold": consensus_score
                >= self.consensus_threshold,
            }

            # 7. å±¥æ­´è¨˜éŒ²ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡
            self._record_integration(
                integrated_signal,
                integration_info,
                processed_predictions,
                market_context,
            )

            # 8. çµ±è¨ˆæ›´æ–°
            if consensus_score >= self.consensus_threshold:
                self.integration_stats["high_consensus_signals"] += 1
            else:
                self.integration_stats["low_consensus_rejections"] += 1

            logger.debug(
                f"ğŸ”— Cross-timeframe integration: signal={integrated_signal:.3f}, "
                f"consensus={consensus_score:.3f}, quality={integration_quality}"
            )

            return integrated_signal, integration_info

        except Exception as e:
            logger.error(f"âŒ Cross-timeframe integration failed: {e}")
            return 0.5, self._create_fallback_integration_info()

    def _preprocess_predictions(
        self, timeframe_predictions: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Dict[str, Any]]:
        """äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ»æ¤œè¨¼"""
        processed = {}

        for timeframe, pred_data in timeframe_predictions.items():
            try:
                # å¿…é ˆãƒ‡ãƒ¼ã‚¿å­˜åœ¨ç¢ºèª
                if not self._validate_prediction_data(pred_data):
                    logger.warning(f"Invalid prediction data for {timeframe}")
                    continue

                # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–ãƒ»æ¨™æº–åŒ–
                processed_data = {
                    "timeframe": timeframe,
                    "prediction": self._extract_prediction_value(pred_data),
                    "probability": self._extract_probability_value(pred_data),
                    "confidence": pred_data.get(
                        "unified_confidence", pred_data.get("confidence", 0.5)
                    ),
                    "model_agreement": pred_data.get("model_agreement", 1.0),
                    "prediction_quality": pred_data.get(
                        "prediction_quality", "unknown"
                    ),
                    "market_regime": pred_data.get("market_regime", "normal"),
                    "original_data": pred_data,
                }

                processed[timeframe] = processed_data

            except Exception as e:
                logger.error(f"Failed to preprocess {timeframe} prediction: {e}")
                continue

        return processed

    def _validate_prediction_data(self, pred_data: Dict[str, Any]) -> bool:
        """äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿å¦¥å½“æ€§æ¤œè¨¼"""
        required_fields = ["prediction", "probability", "confidence"]
        return all(field in pred_data for field in required_fields)

    def _extract_prediction_value(self, pred_data: Dict[str, Any]) -> float:
        """äºˆæ¸¬å€¤æŠ½å‡ºãƒ»æ­£è¦åŒ–"""
        prediction = pred_data.get("prediction")
        if isinstance(prediction, np.ndarray):
            return float(prediction[0])
        elif isinstance(prediction, (int, float)):
            return float(prediction)
        else:
            return 0.0

    def _extract_probability_value(self, pred_data: Dict[str, Any]) -> float:
        """ç¢ºç‡å€¤æŠ½å‡ºãƒ»æ­£è¦åŒ–"""
        probability = pred_data.get("probability")
        if isinstance(probability, np.ndarray):
            if probability.ndim == 2 and probability.shape[1] > 1:
                return float(probability[0, 1])  # æ­£ä¾‹ã‚¯ãƒ©ã‚¹ç¢ºç‡
            else:
                return float(probability[0])
        elif isinstance(probability, (int, float)):
            return float(probability)
        else:
            return 0.5

    def _calculate_dynamic_weights(
        self,
        processed_predictions: Dict[str, Dict[str, Any]],
        market_context: Optional[Dict[str, Any]] = None,
    ) -> List[float]:
        """
        å‹•çš„é‡ã¿è¨ˆç®—ï¼ˆPhase C1ã‚³ã‚¢æ©Ÿèƒ½ï¼‰

        å“è³ªãƒ»ä¿¡é ¼åº¦ãƒ»å¸‚å ´ç’°å¢ƒã«åŸºã¥ãé‡ã¿èª¿æ•´
        """
        timeframes_order = list(processed_predictions.keys())
        base_weights = self._get_base_weights_for_predictions(processed_predictions)

        # 1. å“è³ªãƒ™ãƒ¼ã‚¹èª¿æ•´
        quality_adjustments = []
        for tf in timeframes_order:
            pred_data = processed_predictions[tf]
            confidence = pred_data["confidence"]
            model_agreement = pred_data["model_agreement"]

            # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            quality_score = (confidence * 0.7) + (model_agreement * 0.3)
            quality_adjustments.append(quality_score)

        # 2. å¸‚å ´ç’°å¢ƒãƒ™ãƒ¼ã‚¹èª¿æ•´
        market_adjustments = []
        if self.market_adaptation_enabled and market_context:
            market_regime = self.confidence_calculator.assess_market_regime(
                market_context
            )

            for tf in timeframes_order:
                if market_regime == "crisis":
                    # å±æ©Ÿæ™‚ï¼šé•·æœŸã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ é‡è¦–
                    if tf == "4h":
                        market_adj = 1.2
                    elif tf == "1h":
                        market_adj = 1.0
                    else:  # 15m
                        market_adj = 0.8
                elif market_regime == "calm":
                    # å®‰å®šæ™‚ï¼šçŸ­æœŸã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ é‡è¦–
                    if tf == "15m":
                        market_adj = 1.2
                    elif tf == "1h":
                        market_adj = 1.0
                    else:  # 4h
                        market_adj = 0.9
                else:
                    market_adj = 1.0

                market_adjustments.append(market_adj)

            self.integration_stats["market_adaptations"] += 1
        else:
            market_adjustments = [1.0] * len(timeframes_order)

        # 3. å±¥æ­´ãƒ™ãƒ¼ã‚¹èª¿æ•´ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼‰
        performance_adjustments = self._calculate_performance_adjustments(
            timeframes_order
        )

        # 4. çµ±åˆé‡ã¿è¨ˆç®—
        dynamic_weights = []
        for i, tf in enumerate(timeframes_order):
            base_weight = base_weights[i]
            quality_adj = quality_adjustments[i]
            market_adj = market_adjustments[i]
            performance_adj = performance_adjustments[i]

            # é‡ã¿çµ±åˆï¼ˆä¹—ç®—ãƒ»åŠ ç®—ã®çµ„ã¿åˆã‚ã›ï¼‰
            adjusted_weight = base_weight * quality_adj * market_adj * performance_adj
            dynamic_weights.append(adjusted_weight)

        # 5. æ­£è¦åŒ–
        total_weight = sum(dynamic_weights)
        if total_weight > 0:
            dynamic_weights = [w / total_weight for w in dynamic_weights]
        else:
            dynamic_weights = [1.0 / len(timeframes_order)] * len(timeframes_order)

        # 6. å±¥æ­´è¨˜éŒ²
        self._record_weight_update(
            timeframes_order,
            dynamic_weights,
            {
                "quality_adjustments": quality_adjustments,
                "market_adjustments": market_adjustments,
                "performance_adjustments": performance_adjustments,
                "market_context": market_context,
            },
        )

        return dynamic_weights

    def _get_base_weights_for_predictions(
        self, processed_predictions: Dict[str, Dict[str, Any]]
    ) -> List[float]:
        """äºˆæ¸¬ã«å¯¾å¿œã™ã‚‹åŸºæœ¬é‡ã¿å–å¾—"""
        weights = []
        for tf in processed_predictions.keys():
            try:
                index = self.timeframes.index(tf)
                weights.append(self.base_weights[index])
            except (ValueError, IndexError):
                weights.append(1.0 / len(processed_predictions))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé‡ã¿

        # æ­£è¦åŒ–
        total = sum(weights)
        if total > 0:
            return [w / total for w in weights]
        else:
            return [1.0 / len(weights)] * len(weights)

    def _calculate_performance_adjustments(
        self, timeframes_order: List[str]
    ) -> List[float]:
        """å±¥æ­´ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹èª¿æ•´è¨ˆç®—"""
        if not self.integration_history:
            return [1.0] * len(timeframes_order)

        # æœ€è¿‘ã®çµ±åˆçµæœã‹ã‚‰å„ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡
        recent_history = self.integration_history[-20:]  # æœ€æ–°20ä»¶

        performance_scores = {}
        for tf in timeframes_order:
            # å„ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®äºˆæ¸¬ç²¾åº¦ãƒ»è²¢çŒ®åº¦ã‚’è¨ˆç®—
            tf_contributions = []
            for record in recent_history:
                if tf in record.get("timeframe_contributions", {}):
                    contribution = record["timeframe_contributions"][tf]
                    tf_contributions.append(contribution)

            if tf_contributions:
                avg_performance = np.mean(tf_contributions)
                performance_scores[tf] = max(0.5, min(1.5, avg_performance))  # ç¯„å›²åˆ¶é™
            else:
                performance_scores[tf] = 1.0

        return [performance_scores.get(tf, 1.0) for tf in timeframes_order]

    def _calculate_integrated_signal(
        self, processed_predictions: Dict[str, Dict[str, Any]], weights: List[float]
    ) -> Tuple[float, Dict[str, float]]:
        """çµ±åˆã‚·ã‚°ãƒŠãƒ«è¨ˆç®—"""
        signal_values = []
        signal_components = {}

        timeframes_order = list(processed_predictions.keys())

        for i, tf in enumerate(timeframes_order):
            pred_data = processed_predictions[tf]

            # ã‚·ã‚°ãƒŠãƒ«å€¤å¤‰æ›
            probability = pred_data["probability"]
            if probability > 0.6:
                signal_value = 0.75  # å¼·æ°—
            elif probability < 0.4:
                signal_value = 0.25  # å¼±æ°—
            else:
                signal_value = 0.5  # ä¸­ç«‹

            signal_values.append(signal_value)
            signal_components[tf] = signal_value

        # é‡ã¿ä»˜ãçµ±åˆè¨ˆç®—
        if signal_values and len(weights) == len(signal_values):
            weighted_sum = sum(s * w for s, w in zip(signal_values, weights))
            integrated_signal = weighted_sum
        else:
            integrated_signal = 0.5  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

        return integrated_signal, signal_components

    def _calculate_cross_timeframe_consensus(
        self, processed_predictions: Dict[str, Dict[str, Any]], weights: List[float]
    ) -> float:
        """ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ é–“åˆæ„åº¦è¨ˆç®—"""
        probabilities = [
            pred_data["probability"] for pred_data in processed_predictions.values()
        ]

        # Phase C1çµ±åˆ: çµ±ä¸€ä¿¡é ¼åº¦è¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ ä½¿ç”¨
        consensus_score = self.confidence_calculator.calculate_consensus_score(
            probabilities, weights
        )

        return consensus_score

    def _assess_integration_quality(
        self,
        processed_predictions: Dict[str, Dict[str, Any]],
        consensus_score: float,
        market_context: Optional[Dict[str, Any]],
    ) -> str:
        """çµ±åˆå“è³ªè©•ä¾¡"""
        # åŸºæœ¬å“è³ªè¦å› 
        confidence_scores = [
            pred_data["confidence"] for pred_data in processed_predictions.values()
        ]
        avg_confidence = np.mean(confidence_scores) if confidence_scores else 0.5

        # çµ±åˆå“è³ªåˆ¤å®š
        if consensus_score > 0.8 and avg_confidence > 0.7:
            return "excellent"
        elif consensus_score > 0.6 and avg_confidence > 0.6:
            return "good"
        elif consensus_score > 0.4 and avg_confidence > 0.5:
            return "fair"
        else:
            return "poor"

    def _record_integration(
        self,
        integrated_signal: float,
        integration_info: Dict[str, Any],
        processed_predictions: Dict[str, Dict[str, Any]],
        market_context: Optional[Dict[str, Any]],
    ):
        """çµ±åˆå±¥æ­´è¨˜éŒ²"""
        record = {
            "timestamp": datetime.now(),
            "integrated_signal": integrated_signal,
            "consensus_score": integration_info["consensus_score"],
            "integration_quality": integration_info["integration_quality"],
            "timeframe_count": integration_info["timeframe_count"],
            "dynamic_weights": integration_info["dynamic_weights"],
            "timeframe_contributions": {},
            "market_regime": (
                market_context.get("market_regime", "unknown")
                if market_context
                else "unknown"
            ),
        }

        # å„ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®è²¢çŒ®åº¦è¨˜éŒ²
        for tf, pred_data in processed_predictions.items():
            record["timeframe_contributions"][tf] = pred_data["confidence"]

        self.integration_history.append(record)

        # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.integration_history) > self.max_history_size:
            self.integration_history.pop(0)

    def _record_weight_update(
        self,
        timeframes: List[str],
        weights: List[float],
        adjustment_details: Dict[str, Any],
    ):
        """é‡ã¿æ›´æ–°å±¥æ­´è¨˜éŒ²"""
        record = {
            "timestamp": datetime.now(),
            "timeframes": timeframes,
            "weights": weights,
            "adjustment_details": adjustment_details,
        }

        self.weight_history.append(record)

        # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.weight_history) > self.max_history_size:
            self.weight_history.pop(0)

        # ç¾åœ¨ã®å‹•çš„é‡ã¿æ›´æ–°
        self.current_dynamic_weights = weights.copy()
        self.last_weight_update = datetime.now()

    def _create_fallback_integration_info(self) -> Dict[str, Any]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµ±åˆæƒ…å ±ä½œæˆ"""
        return {
            "integration_method": "fallback",
            "timeframe_count": 0,
            "dynamic_weights": [],
            "signal_components": {},
            "consensus_score": 0.0,
            "integration_quality": "poor",
            "market_adaptation": False,
            "meets_consensus_threshold": False,
            "error": True,
        }

    def create_final_signal(
        self,
        integrated_signal: float,
        integration_info: Dict[str, Any],
        current_price: float,
        position_exists: bool = False,
    ) -> Signal:
        """
        æœ€çµ‚å–å¼•ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ

        Parameters:
        -----------
        integrated_signal : float
            çµ±åˆã‚·ã‚°ãƒŠãƒ«å€¤
        integration_info : Dict[str, Any]
            çµ±åˆè©³ç´°æƒ…å ±
        current_price : float
            ç¾åœ¨ä¾¡æ ¼
        position_exists : bool
            ãƒã‚¸ã‚·ãƒ§ãƒ³å­˜åœ¨ãƒ•ãƒ©ã‚°

        Returns:
        --------
        Signal
            æœ€çµ‚å–å¼•ã‚·ã‚°ãƒŠãƒ«
        """
        consensus_score = integration_info["consensus_score"]
        integration_quality = integration_info["integration_quality"]

        # åˆæ„åº¦é–¾å€¤ãƒã‚§ãƒƒã‚¯
        if consensus_score < self.consensus_threshold:
            logger.debug(
                f"ğŸš« Low consensus rejection: {consensus_score:.3f} < {self.consensus_threshold}"
            )
            return Signal()  # ã‚·ã‚°ãƒŠãƒ«ãªã—

        if position_exists:
            # ã‚¨ã‚°ã‚¸ãƒƒãƒˆåˆ¤å®š
            exit_threshold = 0.4 + (1.0 - consensus_score) * 0.1
            if integrated_signal < exit_threshold:
                logger.info(
                    f"ğŸšª Cross-timeframe EXIT: signal={integrated_signal:.3f} < {exit_threshold:.3f}"
                )
                return Signal(side="SELL", price=current_price)
            return Signal()  # ãƒ›ãƒ¼ãƒ«ãƒ‰

        else:
            # ã‚¨ãƒ³ãƒˆãƒªãƒ¼åˆ¤å®š
            entry_threshold = 0.5 + (self.ensemble_confidence_threshold - 0.5)

            if integrated_signal > entry_threshold and integration_quality in [
                "good",
                "excellent",
            ]:
                logger.info(
                    f"ğŸ“ˆ Cross-timeframe LONG: signal={integrated_signal:.3f}, "
                    f"consensus={consensus_score:.3f}, quality={integration_quality}"
                )
                return Signal(side="BUY", price=current_price)

            elif integrated_signal < (
                1.0 - entry_threshold
            ) and integration_quality in ["good", "excellent"]:
                logger.info(
                    f"ğŸ“‰ Cross-timeframe SHORT: signal={integrated_signal:.3f}, "
                    f"consensus={consensus_score:.3f}, quality={integration_quality}"
                )
                return Signal(side="SELL", price=current_price)

            return Signal()  # ãƒ›ãƒ¼ãƒ«ãƒ‰

    def get_integrator_info(self) -> Dict[str, Any]:
        """çµ±åˆã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—"""
        return {
            "timeframes": self.timeframes,
            "base_weights": self.base_weights,
            "current_dynamic_weights": self.current_dynamic_weights,
            "integration_method": self.integration_method,
            "consensus_threshold": self.consensus_threshold,
            "dynamic_weighting_enabled": self.dynamic_weighting_enabled,
            "market_adaptation_enabled": self.market_adaptation_enabled,
            "integration_stats": self.integration_stats.copy(),
            "integration_history_size": len(self.integration_history),
            "weight_history_size": len(self.weight_history),
            "last_weight_update": (
                self.last_weight_update.isoformat() if self.last_weight_update else None
            ),
        }

    def get_recent_performance(self) -> Dict[str, Any]:
        """æœ€è¿‘ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        if not self.integration_history:
            return {}

        recent_integrations = self.integration_history[-20:]

        try:
            consensus_scores = [r["consensus_score"] for r in recent_integrations]
            integrated_signals = [r["integrated_signal"] for r in recent_integrations]

            quality_counts = {}
            for r in recent_integrations:
                quality = r["integration_quality"]
                quality_counts[quality] = quality_counts.get(quality, 0) + 1

            return {
                "avg_consensus": np.mean(consensus_scores),
                "consensus_stability": np.std(consensus_scores),
                "avg_signal": np.mean(integrated_signals),
                "signal_volatility": np.std(integrated_signals),
                "integration_count": len(recent_integrations),
                "quality_distribution": quality_counts,
            }

        except Exception as e:
            logger.error(f"Performance analysis failed: {e}")
            return {}

    def reset_statistics(self):
        """çµ±è¨ˆãƒªã‚»ãƒƒãƒˆ"""
        for key in self.integration_stats:
            self.integration_stats[key] = 0
        self.integration_history.clear()
        self.weight_history.clear()
        self.current_dynamic_weights = self.base_weights.copy()
        logger.info("ğŸ“Š Cross-timeframe integrator statistics reset")


# ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼é–¢æ•°


def create_cross_timeframe_integrator(
    config: Dict[str, Any],
) -> CrossTimeframeIntegrator:
    """
    ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ é–“çµ±åˆã‚·ã‚¹ãƒ†ãƒ ä½œæˆ

    Parameters:
    -----------
    config : Dict[str, Any]
        è¨­å®šè¾æ›¸

    Returns:
    --------
    CrossTimeframeIntegrator
        åˆæœŸåŒ–æ¸ˆã¿çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
    """
    return CrossTimeframeIntegrator(config)
