"""
ç‰¹å¾´é‡é †åºç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
Phase H.17: å­¦ç¿’æ™‚ã¨äºˆæ¸¬æ™‚ã®ç‰¹å¾´é‡é †åºã‚’å®Œå…¨ä¸€è‡´ã•ã›ã‚‹

ç›®çš„:
- XGBoost/RandomForestã®feature_names mismatchã‚¨ãƒ©ãƒ¼è§£æ±º
- 155ç‰¹å¾´é‡ã®æ±ºå®šè«–çš„é †åºç®¡ç†
- å­¦ç¿’ãƒ»äºˆæ¸¬é–“ã®ä¸€è²«æ€§ä¿è¨¼
"""

import json
import logging
from pathlib import Path
from typing import List, Optional

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)


class FeatureOrderManager:
    """
    ç‰¹å¾´é‡é †åºã®æ±ºå®šè«–çš„ç®¡ç†ã‚¯ãƒ©ã‚¹

    æ©Ÿèƒ½:
    - 155ç‰¹å¾´é‡ã®å›ºå®šé †åºå®šç¾©
    - å­¦ç¿’æ™‚ã®ç‰¹å¾´é‡é †åºè¨˜éŒ²
    - äºˆæ¸¬æ™‚ã®ç‰¹å¾´é‡é †åºæ•´åˆ
    - ç‰¹å¾´é‡é †åºã®æ¤œè¨¼ãƒ»ãƒ­ã‚°å‡ºåŠ›
    """

    # Phase H.25: 125ç‰¹å¾´é‡ã®æ±ºå®šè«–çš„é †åºï¼ˆå¤–éƒ¨APIç‰¹å¾´é‡ã‚’é™¤å¤–ï¼‰
    FEATURE_ORDER_125 = [
        # åŸºæœ¬OHLCVç‰¹å¾´é‡
        "open",
        "high",
        "low",
        "close",
        "volume",
        # ãƒ©ã‚°ç‰¹å¾´é‡
        "close_lag_1",
        "close_lag_2",
        "close_lag_3",
        "close_lag_4",
        "close_lag_5",
        "volume_lag_1",
        "volume_lag_2",
        "volume_lag_3",
        "volume_lag_4",
        "volume_lag_5",
        # ãƒªã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡
        "returns_1",
        "returns_2",
        "returns_3",
        "returns_5",
        "returns_10",
        "log_returns_1",
        "log_returns_2",
        "log_returns_3",
        "log_returns_5",
        "log_returns_10",
        # ç§»å‹•å¹³å‡
        "sma_5",
        "sma_10",
        "sma_20",
        "sma_50",
        "sma_100",
        "sma_200",
        "ema_5",
        "ema_10",
        "ema_20",
        "ema_50",
        "ema_100",
        "ema_200",
        # ä¾¡æ ¼ä½ç½®
        "price_position_20",
        "price_position_50",
        "price_vs_sma20",
        "bb_position",
        "intraday_position",
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        "bb_upper",
        "bb_middle",
        "bb_lower",
        "bb_width",
        "bb_squeeze",
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™
        "rsi_14",
        "rsi_7",
        "rsi_21",
        "rsi_oversold",
        "rsi_overbought",
        "macd",
        "macd_signal",
        "macd_hist",
        "macd_cross_up",
        "macd_cross_down",
        "stoch_k",
        "stoch_d",
        "stoch_oversold",
        "stoch_overbought",
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        "atr_14",
        "atr_7",
        "atr_21",
        "volatility_20",
        "volatility_50",
        "high_low_ratio",
        "true_range",
        "volatility_ratio",
        # å‡ºæ¥é«˜æŒ‡æ¨™
        "volume_sma_20",
        "volume_ratio",
        "volume_trend",
        "vwap",
        "vwap_distance",
        "obv",
        "obv_sma",
        "cmf",
        "mfi",
        "ad_line",
        # ãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™
        "adx_14",
        "plus_di",
        "minus_di",
        "trend_strength",
        "trend_direction",
        "cci_20",
        "williams_r",
        "ultimate_oscillator",
        # ãƒãƒ¼ã‚±ãƒƒãƒˆæ§‹é€ 
        "support_distance",
        "resistance_distance",
        "support_strength",
        "volume_breakout",
        "price_breakout_up",
        "price_breakout_down",
        # ãƒ­ãƒ¼ã‚½ã‚¯è¶³ãƒ‘ã‚¿ãƒ¼ãƒ³
        "doji",
        "hammer",
        "engulfing",
        "pinbar",
        # çµ±è¨ˆçš„ç‰¹å¾´é‡
        "skewness_20",
        "kurtosis_20",
        "zscore",
        "mean_reversion_20",
        "mean_reversion_50",
        # æ™‚ç³»åˆ—ç‰¹å¾´é‡
        "hour",
        "day_of_week",
        "is_weekend",
        "is_asian_session",
        "is_european_session",
        "is_us_session",
        # Phase H.25: å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡ã‚’å‰Šé™¤ï¼ˆVIX, Fear&Greed, ãƒã‚¯ãƒ­, Funding, ç›¸é–¢ï¼‰
        # è¿½åŠ ã®æŠ€è¡“æŒ‡æ¨™
        "roc_10",
        "roc_20",
        "trix",
        "mass_index",
        "keltner_upper",
        "keltner_lower",
        "donchian_upper",
        "donchian_lower",
        "ichimoku_conv",
        "ichimoku_base",
        # ãã®ä»–ã®æ´¾ç”Ÿç‰¹å¾´é‡
        "price_efficiency",
        "trend_consistency",
        "volume_price_correlation",
        "volatility_regime",
        "momentum_quality",
        "market_phase",
        "momentum_14",  # Phase H.23.7: momentum_14è¿½åŠ ã§155ç‰¹å¾´é‡ã«çµ±ä¸€
    ]

    def __init__(self, feature_order_file: Optional[str] = None):
        """
        åˆæœŸåŒ–

        Args:
            feature_order_file: ç‰¹å¾´é‡é †åºã‚’ä¿å­˜/èª­è¾¼ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.feature_order_file = feature_order_file or "feature_order.json"
        self.stored_order: Optional[List[str]] = None

        # ä¿å­˜ã•ã‚ŒãŸé †åºãŒã‚ã‚Œã°èª­ã¿è¾¼ã‚€
        self._load_stored_order()

        logger.info("ğŸ”§ FeatureOrderManager initialized")
        logger.info(f"  - Default order: {len(self.FEATURE_ORDER_125)} features")
        logger.info(f"  - Storage file: {self.feature_order_file}")

    def _load_stored_order(self):
        """ä¿å­˜ã•ã‚ŒãŸç‰¹å¾´é‡é †åºã‚’èª­ã¿è¾¼ã‚€"""
        try:
            path = Path(self.feature_order_file)
            if path.exists():
                with open(path, "r") as f:
                    data = json.load(f)
                    self.stored_order = data.get("feature_order", [])
                    logger.info(
                        f"âœ… Loaded stored feature order: "
                        f"{len(self.stored_order)} features"
                    )
            else:
                logger.info("ğŸ“ No stored feature order found, using default")
        except Exception as e:
            logger.error(f"âŒ Failed to load feature order: {e}")
            self.stored_order = None

    def save_feature_order(self, features: List[str]):
        """
        ç‰¹å¾´é‡é †åºã‚’ä¿å­˜ï¼ˆå­¦ç¿’æ™‚ã«ä½¿ç”¨ï¼‰

        Args:
            features: å­¦ç¿’æ™‚ã®ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
        """
        try:
            data = {
                "feature_order": features,
                "num_features": len(features),
                "timestamp": pd.Timestamp.now().isoformat(),
            }

            with open(self.feature_order_file, "w") as f:
                json.dump(data, f, indent=2)

            self.stored_order = features
            logger.info(f"âœ… Saved feature order: {len(features)} features")

            # é †åºã®æœ€åˆã¨æœ€å¾Œã‚’è¡¨ç¤º
            if len(features) > 10:
                logger.info(f"  First 5: {features[:5]}")
                logger.info(f"  Last 5: {features[-5:]}")
            else:
                logger.info(f"  Features: {features}")

        except Exception as e:
            logger.error(f"âŒ Failed to save feature order: {e}")

    def get_consistent_order(self, current_features: List[str]) -> List[str]:
        """
        ä¸€è²«æ€§ã®ã‚ã‚‹ç‰¹å¾´é‡é †åºã‚’å–å¾—

        Args:
            current_features: ç¾åœ¨ã®ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ

        Returns:
            é †åºèª¿æ•´ã•ã‚ŒãŸç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
        """
        # ä¿å­˜ã•ã‚ŒãŸé †åºãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
        if self.stored_order:
            logger.info(
                f"ğŸ“‹ Using stored feature order ({len(self.stored_order)} features)"
            )
            return self._align_to_stored_order(current_features)

        # ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé †åºã‚’ä½¿ç”¨
        logger.info("ğŸ“‹ Using default feature order")
        return self._align_to_default_order(current_features)

    def _align_to_stored_order(self, current_features: List[str]) -> List[str]:
        """ä¿å­˜ã•ã‚ŒãŸé †åºã«åˆã‚ã›ã¦æ•´åˆ—"""
        current_set = set(current_features)
        stored_set = set(self.stored_order)

        # å…±é€šã®ç‰¹å¾´é‡ã‚’ä¿å­˜ã•ã‚ŒãŸé †åºã§ä¸¦ã¹ã‚‹
        aligned = [f for f in self.stored_order if f in current_set]

        # æ–°ã—ã„ç‰¹å¾´é‡ãŒã‚ã‚Œã°æœ€å¾Œã«è¿½åŠ 
        new_features = current_set - stored_set
        if new_features:
            logger.warning(f"âš ï¸ New features not in stored order: {new_features}")
            aligned.extend(sorted(new_features))

        # ä¸è¶³ã—ã¦ã„ã‚‹ç‰¹å¾´é‡ã‚’è­¦å‘Š
        missing_features = stored_set - current_set
        if missing_features:
            logger.warning(
                f"âš ï¸ Features in stored order but missing now: {missing_features}"
            )

        logger.info(
            f"âœ… Aligned features: {len(aligned)} (was {len(current_features)})"
        )
        return aligned

    def _align_to_default_order(self, current_features: List[str]) -> List[str]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé †åºã«åˆã‚ã›ã¦æ•´åˆ—"""
        current_set = set(current_features)

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé †åºã«å­˜åœ¨ã™ã‚‹ç‰¹å¾´é‡ã‚’æŠ½å‡º
        aligned = [f for f in self.FEATURE_ORDER_125 if f in current_set]

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ãªã„ç‰¹å¾´é‡ã‚’è¿½åŠ 
        extra_features = current_set - set(self.FEATURE_ORDER_125)
        if extra_features:
            logger.info(
                f"ğŸ“ Extra features not in default order: {len(extra_features)}"
            )
            aligned.extend(sorted(extra_features))

        logger.info(f"âœ… Aligned to default order: {len(aligned)} features")
        return aligned

    def ensure_column_order(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        DataFrameã®åˆ—é †åºã‚’ä¿è¨¼

        Args:
            df: é †åºèª¿æ•´ã™ã‚‹DataFrame

        Returns:
            åˆ—é †åºãŒèª¿æ•´ã•ã‚ŒãŸDataFrame
        """
        if df.empty:
            return df

        original_columns = list(df.columns)
        ordered_columns = self.get_consistent_order(original_columns)

        # é †åºãŒå¤‰ã‚ã£ãŸå ´åˆã®ã¿ä¸¦ã³æ›¿ãˆ
        if original_columns != ordered_columns:
            logger.info(
                f"ğŸ”„ Reordering columns: "
                f"{len(original_columns)} -> {len(ordered_columns)}"
            )
            df = df[ordered_columns]

            # å¤‰æ›´å†…å®¹ã‚’è¡¨ç¤º
            if len(ordered_columns) <= 20:
                logger.debug(f"  Original: {original_columns[:10]}...")
                logger.debug(f"  Ordered: {ordered_columns[:10]}...")
        else:
            logger.debug("âœ… Column order already consistent")

        return df

    def validate_features(
        self, train_features: List[str], predict_features: List[str]
    ) -> bool:
        """
        å­¦ç¿’æ™‚ã¨äºˆæ¸¬æ™‚ã®ç‰¹å¾´é‡ã‚’æ¤œè¨¼

        Args:
            train_features: å­¦ç¿’æ™‚ã®ç‰¹å¾´é‡
            predict_features: äºˆæ¸¬æ™‚ã®ç‰¹å¾´é‡

        Returns:
            ä¸€è‡´ã—ã¦ã„ã‚Œã°True
        """
        train_set = set(train_features)
        predict_set = set(predict_features)

        # å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        if train_set == predict_set and train_features == predict_features:
            logger.info("âœ… Feature validation passed: perfect match")
            return True

        # å·®åˆ†åˆ†æ
        missing_in_predict = train_set - predict_set
        extra_in_predict = predict_set - train_set

        if missing_in_predict:
            logger.error(f"âŒ Features missing in prediction: {missing_in_predict}")
        if extra_in_predict:
            logger.error(f"âŒ Extra features in prediction: {extra_in_predict}")

        # é †åºã®é•ã„ã‚’ãƒã‚§ãƒƒã‚¯
        common_features = train_set & predict_set
        if common_features:
            train_order = [f for f in train_features if f in common_features]
            predict_order = [f for f in predict_features if f in common_features]

            if train_order != predict_order:
                logger.error("âŒ Feature order mismatch detected")
                # æœ€åˆã®ä¸ä¸€è‡´ã‚’è¡¨ç¤º
                for i, (t, p) in enumerate(zip(train_order, predict_order)):
                    if t != p:
                        logger.error(f"  Position {i}: '{t}' vs '{p}'")
                        break

        return False

    # Phase H.26: 125ç‰¹å¾´é‡å®Œå…¨æ€§ä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ 
    def ensure_125_features_completeness(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        125ç‰¹å¾´é‡ã®å®Œå…¨æ€§ã‚’ä¿è¨¼ã™ã‚‹åŒ…æ‹¬çš„ã‚·ã‚¹ãƒ†ãƒ 

        Args:
            df: å…¥åŠ›DataFrame

        Returns:
            æ­£ç¢ºã«125å€‹ã®ç‰¹å¾´é‡ã‚’æŒã¤DataFrame
        """
        logger.info(
            f"ğŸ” Starting 125-feature completeness check: {len(df.columns)} input features"
        )

        try:
            # Step 1: é‡è¤‡ç‰¹å¾´é‡ã®æ¤œå‡ºãƒ»æ’é™¤
            df_dedup = self._remove_duplicate_features(df)

            # Step 2: 125ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã¨ã®ç…§åˆ
            df_aligned = self._align_to_target_features(df_dedup)

            # Step 3: ä¸è¶³ç‰¹å¾´é‡ã®è‡ªå‹•è£œå®Œ
            df_complete = self._fill_missing_features(df_aligned)

            # Step 4: ä½™åˆ†ç‰¹å¾´é‡ã®ç®¡ç†
            df_trimmed = self._trim_excess_features(df_complete)

            # Step 5: ç‰¹å¾´é‡å“è³ªãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£
            df_quality = self._ensure_feature_quality(df_trimmed)

            # Step 6: æœ€çµ‚æ¤œè¨¼
            df_final = self._final_125_validation(df_quality)

            logger.info(
                f"âœ… 125-feature completeness guaranteed: {len(df_final.columns)} features"
            )
            return df_final

        except Exception as e:
            logger.error(f"âŒ 125-feature completeness failed: {e}")
            return self._emergency_125_fallback(df)

    def _remove_duplicate_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """é‡è¤‡ç‰¹å¾´é‡ã®æ¤œå‡ºãƒ»æ’é™¤"""
        original_count = len(df.columns)

        # å®Œå…¨ã«åŒã˜åå‰ã®é‡è¤‡ã‚’é™¤å»
        df_dedup = df.loc[:, ~df.columns.duplicated()]

        # å€¤ãŒåŒã˜ç‰¹å¾´é‡ã®æ¤œå‡ºï¼ˆç›¸é–¢1.0ã®ç‰¹å¾´é‡ï¼‰
        duplicate_pairs = []
        columns = df_dedup.columns.tolist()

        for i in range(len(columns)):
            for j in range(i + 1, len(columns)):
                col1, col2 = columns[i], columns[j]
                try:
                    # NaNã‚’é™¤å¤–ã—ã¦ç›¸é–¢è¨ˆç®—
                    clean_data1 = df_dedup[col1].dropna()
                    clean_data2 = df_dedup[col2].dropna()

                    if len(clean_data1) > 0 and len(clean_data2) > 0:
                        common_index = clean_data1.index.intersection(clean_data2.index)
                        if len(common_index) > 5:  # æœ€å°5ãƒã‚¤ãƒ³ãƒˆã§åˆ¤å®š
                            corr = clean_data1.loc[common_index].corr(
                                clean_data2.loc[common_index]
                            )
                            if abs(corr) > 0.999:  # ã»ã¼å®Œå…¨ç›¸é–¢
                                duplicate_pairs.append((col1, col2, corr))
                except Exception:
                    continue

        # é‡è¤‡ç‰¹å¾´é‡ã®å‰Šé™¤ï¼ˆã‚ˆã‚Šé‡è¦ã§ãªã„æ–¹ã‚’å‰Šé™¤ï¼‰
        features_to_remove = set()
        for col1, col2, corr in duplicate_pairs:
            # 125ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹æ–¹ã‚’å„ªå…ˆ
            if col1 in self.FEATURE_ORDER_125 and col2 not in self.FEATURE_ORDER_125:
                features_to_remove.add(col2)
            elif col2 in self.FEATURE_ORDER_125 and col1 not in self.FEATURE_ORDER_125:
                features_to_remove.add(col1)
            else:
                # ã©ã¡ã‚‰ã‚‚125ãƒªã‚¹ãƒˆã«ãªã„å ´åˆã¯å¾Œã®æ–¹ã‚’å‰Šé™¤
                features_to_remove.add(col2)

        if features_to_remove:
            df_dedup = df_dedup.drop(columns=list(features_to_remove))
            logger.info(f"Removed {len(features_to_remove)} duplicate features")

        logger.debug(
            f"Deduplication: {original_count} â†’ {len(df_dedup.columns)} features"
        )
        return df_dedup

    def _align_to_target_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """125ç‰¹å¾´é‡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒªã‚¹ãƒˆã¨ã®ç…§åˆ"""
        current_features = set(df.columns)
        target_features = set(self.FEATURE_ORDER_125)

        # ç¾åœ¨ã®ç‰¹å¾´é‡ã®åˆ†æ
        present_target = current_features.intersection(target_features)
        missing_target = target_features - current_features
        extra_features = current_features - target_features

        logger.info(
            f"Feature alignment: {len(present_target)}/125 target features present"
        )
        logger.info(f"Missing target features: {len(missing_target)}")
        logger.info(f"Extra features: {len(extra_features)}")

        if missing_target:
            logger.debug(f"Missing features sample: {list(missing_target)[:10]}")

        return df

    def _fill_missing_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¸è¶³ç‰¹å¾´é‡ã®è‡ªå‹•è£œå®Œ"""
        current_features = set(df.columns)
        target_features = set(self.FEATURE_ORDER_125)
        missing_features = target_features - current_features

        if not missing_features:
            logger.debug("No missing features to fill")
            return df

        logger.info(f"Filling {len(missing_features)} missing features")
        df_filled = df.copy()

        # çµ±è¨ˆæƒ…å ±ã®äº‹å‰è¨ˆç®—ï¼ˆåŠ¹ç‡åŒ–ï¼‰
        mean_price = df_filled.get("close", pd.Series([100.0])).mean()
        if pd.isna(mean_price) or mean_price <= 0:
            mean_price = 100.0

        # Phase H.26: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è­¦å‘Šå¯¾å¿œ - ä¸€æ‹¬ã§DataFrameä½œæˆ
        missing_data = {}
        for feature in missing_features:
            try:
                # ç‰¹å¾´é‡ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸé©åˆ‡ãªè£œå®Œå€¤ã‚’è¨ˆç®—
                fill_value = self._calculate_smart_fill_value(
                    feature, df_filled, mean_price
                )
                missing_data[feature] = fill_value
            except Exception as e:
                logger.warning(f"Failed to fill feature {feature}: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: 0ã§è£œå®Œ
                missing_data[feature] = 0.0

        # ä¸€æ‹¬ã§DataFrameã«è¿½åŠ ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼‰
        if missing_data:
            missing_df = pd.DataFrame(missing_data, index=df_filled.index)
            df_filled = pd.concat([df_filled, missing_df], axis=1)

        logger.info(f"âœ… Filled {len(missing_features)} missing features")
        return df_filled

    def _calculate_smart_fill_value(
        self, feature: str, df: pd.DataFrame, mean_price: float
    ) -> float:
        """ç‰¹å¾´é‡ã«å¿œã˜ãŸé©åˆ‡ãªè£œå®Œå€¤ã‚’è¨ˆç®—"""
        # ä¾¡æ ¼ç³»ç‰¹å¾´é‡
        if any(
            x in feature.lower()
            for x in ["close", "open", "high", "low", "price", "sma", "ema"]
        ):
            return mean_price

        # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç³»ç‰¹å¾´é‡
        elif "volume" in feature.lower():
            if "volume" in df.columns:
                return df["volume"].mean() if not df["volume"].isna().all() else 1000.0
            return 1000.0

        # RSIç³»ï¼ˆ0-100ã®ç¯„å›²ï¼‰
        elif "rsi" in feature.lower():
            return 50.0

        # ATRç³»ï¼ˆä¾¡æ ¼ã®æ•°%ï¼‰
        elif "atr" in feature.lower():
            return mean_price * 0.02

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç³»
        elif "volatility" in feature.lower() or "vol" in feature.lower():
            return 0.02

        # æ¯”ç‡ãƒ»ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆç³»
        elif any(x in feature.lower() for x in ["ratio", "pct", "change", "return"]):
            return 0.0

        # ãƒã‚¤ãƒŠãƒªæŒ‡æ¨™ï¼ˆ0/1ï¼‰
        elif any(
            x in feature.lower()
            for x in ["is_", "oversold", "overbought", "cross", "breakout"]
        ):
            return 0.0

        # æ­£è¦åŒ–ã•ã‚ŒãŸæŒ‡æ¨™
        elif any(x in feature.lower() for x in ["zscore", "normalized"]):
            return 0.0

        # ãã®ä»–ã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
        elif any(
            x in feature.lower() for x in ["macd", "stoch", "adx", "cci", "williams"]
        ):
            return 0.0

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        else:
            return 0.0

    def _trim_excess_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä½™åˆ†ç‰¹å¾´é‡ã®ç®¡ç†ï¼ˆ125ç‰¹å¾´é‡ã«èª¿æ•´ï¼‰"""
        if len(df.columns) <= 125:
            return df

        logger.info(f"Trimming excess features: {len(df.columns)} â†’ 125")

        # 125ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã®é †åºã«å¾“ã£ã¦é¸æŠ
        ordered_features = []
        available_features = set(df.columns)

        # ã¾ãš125ãƒªã‚¹ãƒˆã®ç‰¹å¾´é‡ã‚’é †åºé€šã‚Šã«è¿½åŠ 
        for feature in self.FEATURE_ORDER_125:
            if feature in available_features:
                ordered_features.append(feature)

        # ã¾ã 125ã«é”ã—ã¦ã„ãªã„å ´åˆã¯ã€æ®‹ã‚Šã®é‡è¦ãã†ãªç‰¹å¾´é‡ã‚’è¿½åŠ 
        if len(ordered_features) < 125:
            remaining_features = available_features - set(ordered_features)
            # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ã§å®‰å®šåŒ–ï¼‰
            remaining_sorted = sorted(remaining_features)
            needed = 125 - len(ordered_features)
            ordered_features.extend(remaining_sorted[:needed])

        # æ­£ç¢ºã«125ç‰¹å¾´é‡ã‚’é¸æŠ
        selected_features = ordered_features[:125]

        logger.info(f"Selected {len(selected_features)} features for final dataset")
        return df[selected_features]

    def _ensure_feature_quality(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾´é‡å“è³ªãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£"""
        df_quality = df.copy()
        issues_fixed = 0

        for column in df_quality.columns:
            try:
                # NaNå€¤ã®å‡¦ç†
                nan_count = df_quality[column].isna().sum()
                if nan_count > 0:
                    # å‰æ–¹ãƒ»å¾Œæ–¹è£œå®Œ
                    df_quality[column] = df_quality[column].ffill().bfill()
                    # ã¾ã NaNãŒã‚ã‚‹å ´åˆã¯é©åˆ‡ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§è£œå®Œ
                    remaining_nan = df_quality[column].isna().sum()
                    if remaining_nan > 0:
                        fill_value = self._calculate_smart_fill_value(
                            column, df_quality, 100.0
                        )
                        df_quality[column] = df_quality[column].fillna(fill_value)
                        issues_fixed += 1

                # infå€¤ã®å‡¦ç†
                inf_count = np.isinf(df_quality[column]).sum()
                if inf_count > 0:
                    # infå€¤ã‚’é©åˆ‡ãªå€¤ã§ç½®æ›
                    finite_values = df_quality[column][np.isfinite(df_quality[column])]
                    if len(finite_values) > 0:
                        replacement = finite_values.median()
                    else:
                        replacement = self._calculate_smart_fill_value(
                            column, df_quality, 100.0
                        )

                    df_quality[column] = df_quality[column].replace(
                        [np.inf, -np.inf], replacement
                    )
                    issues_fixed += 1

            except Exception as e:
                logger.warning(f"Quality check failed for {column}: {e}")
                # å•é¡Œã®ã‚ã‚‹ç‰¹å¾´é‡ã¯å®‰å…¨ãªå€¤ã§ç½®æ›
                df_quality[column] = 0.0
                issues_fixed += 1

        if issues_fixed > 0:
            logger.info(f"Fixed quality issues in {issues_fixed} features")

        return df_quality

    def _final_125_validation(self, df: pd.DataFrame) -> pd.DataFrame:
        """æœ€çµ‚çš„ãª125ç‰¹å¾´é‡æ¤œè¨¼"""
        if len(df.columns) != 125:
            logger.error(
                f"Final validation failed: {len(df.columns)} features instead of 125"
            )

            if len(df.columns) > 125:
                # ä½™åˆ†ãªç‰¹å¾´é‡ã‚’å‰Šé™¤
                df = df.iloc[:, :125]
            elif len(df.columns) < 125:
                # ä¸è¶³åˆ†ã‚’è£œå®Œ
                needed = 125 - len(df.columns)
                for i in range(needed):
                    feature_name = f"auto_generated_{i:03d}"
                    df[feature_name] = 0.0

            logger.warning(
                f"Applied emergency adjustment: now {len(df.columns)} features"
            )

        # ç‰¹å¾´é‡åã®æœ€çµ‚èª¿æ•´ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼‰
        final_columns = []
        seen_names = set()

        for col in df.columns:
            if col in seen_names:
                # é‡è¤‡ãŒã‚ã‚‹å ´åˆã¯ç•ªå·ã‚’ä»˜åŠ 
                counter = 1
                new_name = f"{col}_{counter}"
                while new_name in seen_names:
                    counter += 1
                    new_name = f"{col}_{counter}"
                final_columns.append(new_name)
                seen_names.add(new_name)
            else:
                final_columns.append(col)
                seen_names.add(col)

        df.columns = final_columns

        logger.info(f"âœ… Final validation passed: {len(df.columns)} unique features")
        return df

    def _emergency_125_fallback(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç·Šæ€¥æ™‚ã®125ç‰¹å¾´é‡ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        logger.warning("Using emergency 125-feature fallback")

        try:
            # ä½¿ç”¨å¯èƒ½ãªç‰¹å¾´é‡ã‚’æœ€å¤§é™æ´»ç”¨
            available_features = list(df.columns)

            # 125ç‰¹å¾´é‡ã®åŸºæœ¬ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½œæˆ
            result_df = pd.DataFrame(index=df.index)

            # Step 1: æ—¢å­˜ã®ç‰¹å¾´é‡ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆæœ€å¤§125ã¾ã§ï¼‰
            for i, feature in enumerate(available_features[:125]):
                result_df[f"feature_{i:03d}"] = df[feature]

            # Step 2: ä¸è¶³åˆ†ã‚’åŸºæœ¬çš„ãªç‰¹å¾´é‡ã§è£œå®Œ
            for i in range(len(available_features), 125):
                if i < 5 and "close" in df.columns:
                    # åŸºæœ¬ä¾¡æ ¼ç‰¹å¾´é‡
                    result_df[f"feature_{i:03d}"] = df["close"]
                elif i < 10 and "volume" in df.columns:
                    # å‡ºæ¥é«˜ç‰¹å¾´é‡
                    result_df[f"feature_{i:03d}"] = df["volume"]
                else:
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                    result_df[f"feature_{i:03d}"] = 0.0

            logger.warning(
                f"Emergency fallback created: {len(result_df.columns)} features"
            )
            return result_df

        except Exception as e:
            logger.error(f"Emergency fallback failed: {e}")
            # æœ€å¾Œã®æ‰‹æ®µï¼šå…¨ã¦0ã®125ç‰¹å¾´é‡DataFrame
            emergency_df = pd.DataFrame(
                0.0,
                index=df.index if not df.empty else [0],
                columns=[f"emergency_{i:03d}" for i in range(125)],
            )
            return emergency_df


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_global_feature_order_manager: Optional[FeatureOrderManager] = None


def get_feature_order_manager() -> FeatureOrderManager:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡é †åºç®¡ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _global_feature_order_manager
    if _global_feature_order_manager is None:
        _global_feature_order_manager = FeatureOrderManager()
    return _global_feature_order_manager
