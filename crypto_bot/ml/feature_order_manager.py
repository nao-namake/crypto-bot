"""
ç‰¹å¾´é‡é †åºç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
Phase H.17: å­¦ç¿’æ™‚ã¨äºˆæ¸¬æ™‚ã®ç‰¹å¾´é‡é †åºã‚’å®Œå…¨ä¸€è‡´ã•ã›ã‚‹

ç›®çš„:
- XGBoost/RandomForestã®feature_names mismatchã‚¨ãƒ©ãƒ¼è§£æ±º
- 155ç‰¹å¾´é‡ã®æ±ºå®šè«–çš„é †åºç®¡ç†
- å­¦ç¿’ãƒ»äºˆæ¸¬é–“ã®ä¸€è²«æ€§ä¿è¨¼
"""

import json
import logging
from pathlib import Path
from typing import List, Optional

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)


class FeatureOrderManager:
    """
    ç‰¹å¾´é‡é †åºã®æ±ºå®šè«–çš„ç®¡ç†ã‚¯ãƒ©ã‚¹

    æ©Ÿèƒ½:
    - 155ç‰¹å¾´é‡ã®å›ºå®šé †åºå®šç¾©
    - å­¦ç¿’æ™‚ã®ç‰¹å¾´é‡é †åºè¨˜éŒ²
    - äºˆæ¸¬æ™‚ã®ç‰¹å¾´é‡é †åºæ•´åˆ
    - ç‰¹å¾´é‡é †åºã®æ¤œè¨¼ãƒ»ãƒ­ã‚°å‡ºåŠ›
    """

    # Phase H.25: 125ç‰¹å¾´é‡ã®æ±ºå®šè«–çš„é †åºï¼ˆå¤–éƒ¨APIç‰¹å¾´é‡ã‚’é™¤å¤–ï¼‰
    FEATURE_ORDER_125 = [
        # åŸºæœ¬OHLCVç‰¹å¾´é‡
        "open",
        "high",
        "low",
        "close",
        "volume",
        # ãƒ©ã‚°ç‰¹å¾´é‡
        "close_lag_1",
        "close_lag_2",
        "close_lag_3",
        "close_lag_4",
        "close_lag_5",
        "volume_lag_1",
        "volume_lag_2",
        "volume_lag_3",
        "volume_lag_4",
        "volume_lag_5",
        # ãƒªã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡
        "returns_1",
        "returns_2",
        "returns_3",
        "returns_5",
        "returns_10",
        "log_returns_1",
        "log_returns_2",
        "log_returns_3",
        "log_returns_5",
        "log_returns_10",
        # ç§»å‹•å¹³å‡
        "sma_5",
        "sma_10",
        "sma_20",
        "sma_50",
        "sma_100",
        "sma_200",
        "ema_5",
        "ema_10",
        "ema_20",
        "ema_50",
        "ema_100",
        "ema_200",
        # ä¾¡æ ¼ä½ç½®
        "price_position_20",
        "price_position_50",
        "price_vs_sma20",
        "bb_position",
        "intraday_position",
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        "bb_upper",
        "bb_middle",
        "bb_lower",
        "bb_width",
        "bb_squeeze",
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™
        "rsi_14",
        "rsi_7",
        "rsi_21",
        "rsi_oversold",
        "rsi_overbought",
        "macd",
        "macd_signal",
        "macd_hist",
        "macd_cross_up",
        "macd_cross_down",
        "stoch_k",
        "stoch_d",
        "stoch_oversold",
        "stoch_overbought",
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        "atr_14",
        "atr_7",
        "atr_21",
        "volatility_20",
        "volatility_50",
        "high_low_ratio",
        "true_range",
        "volatility_ratio",
        # å‡ºæ¥é«˜æŒ‡æ¨™
        "volume_sma_20",
        "volume_ratio",
        "volume_trend",
        "vwap",
        "vwap_distance",
        "obv",
        "obv_sma",
        "cmf",
        "mfi",
        "ad_line",
        # ãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™
        "adx_14",
        "plus_di",
        "minus_di",
        "trend_strength",
        "trend_direction",
        "cci_20",
        "williams_r",
        "ultimate_oscillator",
        # ãƒãƒ¼ã‚±ãƒƒãƒˆæ§‹é€ 
        "support_distance",
        "resistance_distance",
        "support_strength",
        "volume_breakout",
        "price_breakout_up",
        "price_breakout_down",
        # ãƒ­ãƒ¼ã‚½ã‚¯è¶³ãƒ‘ã‚¿ãƒ¼ãƒ³
        "doji",
        "hammer",
        "engulfing",
        "pinbar",
        # çµ±è¨ˆçš„ç‰¹å¾´é‡
        "skewness_20",
        "kurtosis_20",
        "zscore",
        "mean_reversion_20",
        "mean_reversion_50",
        # æ™‚ç³»åˆ—ç‰¹å¾´é‡
        "hour",
        "day_of_week",
        "is_weekend",
        "is_asian_session",
        "is_european_session",
        "is_us_session",
        # Phase H.25: å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡ã‚’å‰Šé™¤ï¼ˆVIX, Fear&Greed, ãƒã‚¯ãƒ­, Funding, ç›¸é–¢ï¼‰
        # è¿½åŠ ã®æŠ€è¡“æŒ‡æ¨™
        "roc_10",
        "roc_20",
        "trix",
        "mass_index",
        "keltner_upper",
        "keltner_lower",
        "donchian_upper",
        "donchian_lower",
        "ichimoku_conv",
        "ichimoku_base",
        # ãã®ä»–ã®æ´¾ç”Ÿç‰¹å¾´é‡
        "price_efficiency",
        "trend_consistency",
        "volume_price_correlation",
        "volatility_regime",
        "momentum_quality",
        "market_phase",
        "momentum_14",  # Phase H.23.7: momentum_14è¿½åŠ ã§155ç‰¹å¾´é‡ã«çµ±ä¸€
    ]

    def __init__(self, feature_order_file: Optional[str] = None):
        """
        åˆæœŸåŒ–

        Args:
            feature_order_file: ç‰¹å¾´é‡é †åºã‚’ä¿å­˜/èª­è¾¼ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.feature_order_file = feature_order_file or "feature_order.json"
        self.stored_order: Optional[List[str]] = None

        # ä¿å­˜ã•ã‚ŒãŸé †åºãŒã‚ã‚Œã°èª­ã¿è¾¼ã‚€
        self._load_stored_order()

        logger.info("ğŸ”§ FeatureOrderManager initialized")
        logger.info(f"  - Default order: {len(self.FEATURE_ORDER_125)} features")
        logger.info(f"  - Storage file: {self.feature_order_file}")

    def _load_stored_order(self):
        """ä¿å­˜ã•ã‚ŒãŸç‰¹å¾´é‡é †åºã‚’èª­ã¿è¾¼ã‚€"""
        try:
            path = Path(self.feature_order_file)
            if path.exists():
                with open(path, "r") as f:
                    data = json.load(f)
                    self.stored_order = data.get("feature_order", [])
                    logger.info(
                        f"âœ… Loaded stored feature order: "
                        f"{len(self.stored_order)} features"
                    )
            else:
                logger.info("ğŸ“ No stored feature order found, using default")
        except Exception as e:
            logger.error(f"âŒ Failed to load feature order: {e}")
            self.stored_order = None

    def save_feature_order(self, features: List[str]):
        """
        ç‰¹å¾´é‡é †åºã‚’ä¿å­˜ï¼ˆå­¦ç¿’æ™‚ã«ä½¿ç”¨ï¼‰

        Args:
            features: å­¦ç¿’æ™‚ã®ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
        """
        # Phase H.29: æœ¬ç•ªç’°å¢ƒã§ã®125ç‰¹å¾´é‡ä¿è­·
        if len(features) < 100:
            logger.warning(
                f"ğŸ›¡ï¸ [PROTECTION] Rejected saving {len(features)} features "
                f"(< 100) to protect 125-feature system"
            )
            return
            
        try:
            data = {
                "feature_order": features,
                "num_features": len(features),
                "timestamp": pd.Timestamp.now().isoformat(),
            }

            with open(self.feature_order_file, "w") as f:
                json.dump(data, f, indent=2)

            self.stored_order = features
            logger.info(f"âœ… Saved feature order: {len(features)} features")

            # é †åºã®æœ€åˆã¨æœ€å¾Œã‚’è¡¨ç¤º
            if len(features) > 10:
                logger.info(f"  First 5: {features[:5]}")
                logger.info(f"  Last 5: {features[-5:]}")
            else:
                logger.info(f"  Features: {features}")

        except Exception as e:
            logger.error(f"âŒ Failed to save feature order: {e}")

    def get_consistent_order(self, current_features: List[str]) -> List[str]:
        """
        ä¸€è²«æ€§ã®ã‚ã‚‹ç‰¹å¾´é‡é †åºã‚’å–å¾—

        Args:
            current_features: ç¾åœ¨ã®ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ

        Returns:
            é †åºèª¿æ•´ã•ã‚ŒãŸç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
        """
        # ä¿å­˜ã•ã‚ŒãŸé †åºãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
        if self.stored_order:
            logger.info(
                f"ğŸ“‹ Using stored feature order ({len(self.stored_order)} features)"
            )
            return self._align_to_stored_order(current_features)

        # ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé †åºã‚’ä½¿ç”¨
        logger.info("ğŸ“‹ Using default feature order")
        return self._align_to_default_order(current_features)

    def _align_to_stored_order(self, current_features: List[str]) -> List[str]:
        """ä¿å­˜ã•ã‚ŒãŸé †åºã«åˆã‚ã›ã¦æ•´åˆ—"""
        current_set = set(current_features)
        stored_set = set(self.stored_order)

        # å…±é€šã®ç‰¹å¾´é‡ã‚’ä¿å­˜ã•ã‚ŒãŸé †åºã§ä¸¦ã¹ã‚‹
        aligned = [f for f in self.stored_order if f in current_set]

        # æ–°ã—ã„ç‰¹å¾´é‡ãŒã‚ã‚Œã°æœ€å¾Œã«è¿½åŠ 
        new_features = current_set - stored_set
        if new_features:
            logger.warning(f"âš ï¸ New features not in stored order: {new_features}")
            aligned.extend(sorted(new_features))

        # ä¸è¶³ã—ã¦ã„ã‚‹ç‰¹å¾´é‡ã‚’è­¦å‘Š
        missing_features = stored_set - current_set
        if missing_features:
            logger.warning(
                f"âš ï¸ Features in stored order but missing now: {missing_features}"
            )

        logger.info(
            f"âœ… Aligned features: {len(aligned)} (was {len(current_features)})"
        )
        return aligned

    def _align_to_default_order(self, current_features: List[str]) -> List[str]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé †åºã«åˆã‚ã›ã¦æ•´åˆ—"""
        current_set = set(current_features)

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé †åºã«å­˜åœ¨ã™ã‚‹ç‰¹å¾´é‡ã‚’æŠ½å‡º
        aligned = [f for f in self.FEATURE_ORDER_125 if f in current_set]

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ãªã„ç‰¹å¾´é‡ã‚’è¿½åŠ 
        extra_features = current_set - set(self.FEATURE_ORDER_125)
        if extra_features:
            logger.info(
                f"ğŸ“ Extra features not in default order: {len(extra_features)}"
            )
            aligned.extend(sorted(extra_features))

        logger.info(f"âœ… Aligned to default order: {len(aligned)} features")
        return aligned

    def ensure_column_order(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        DataFrameã®åˆ—é †åºã‚’ä¿è¨¼

        Args:
            df: é †åºèª¿æ•´ã™ã‚‹DataFrame

        Returns:
            åˆ—é †åºãŒèª¿æ•´ã•ã‚ŒãŸDataFrame
        """
        if df.empty:
            return df

        original_columns = list(df.columns)
        ordered_columns = self.get_consistent_order(original_columns)

        # é †åºãŒå¤‰ã‚ã£ãŸå ´åˆã®ã¿ä¸¦ã³æ›¿ãˆ
        if original_columns != ordered_columns:
            logger.info(
                f"ğŸ”„ Reordering columns: "
                f"{len(original_columns)} -> {len(ordered_columns)}"
            )
            df = df[ordered_columns]

            # å¤‰æ›´å†…å®¹ã‚’è¡¨ç¤º
            if len(ordered_columns) <= 20:
                logger.debug(f"  Original: {original_columns[:10]}...")
                logger.debug(f"  Ordered: {ordered_columns[:10]}...")
        else:
            logger.debug("âœ… Column order already consistent")

        return df

    def validate_features(
        self, train_features: List[str], predict_features: List[str]
    ) -> bool:
        """
        å­¦ç¿’æ™‚ã¨äºˆæ¸¬æ™‚ã®ç‰¹å¾´é‡ã‚’æ¤œè¨¼

        Args:
            train_features: å­¦ç¿’æ™‚ã®ç‰¹å¾´é‡
            predict_features: äºˆæ¸¬æ™‚ã®ç‰¹å¾´é‡

        Returns:
            ä¸€è‡´ã—ã¦ã„ã‚Œã°True
        """
        train_set = set(train_features)
        predict_set = set(predict_features)

        # å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
        if train_set == predict_set and train_features == predict_features:
            logger.info("âœ… Feature validation passed: perfect match")
            return True

        # å·®åˆ†åˆ†æ
        missing_in_predict = train_set - predict_set
        extra_in_predict = predict_set - train_set

        if missing_in_predict:
            logger.error(f"âŒ Features missing in prediction: {missing_in_predict}")
        if extra_in_predict:
            logger.error(f"âŒ Extra features in prediction: {extra_in_predict}")

        # é †åºã®é•ã„ã‚’ãƒã‚§ãƒƒã‚¯
        common_features = train_set & predict_set
        if common_features:
            train_order = [f for f in train_features if f in common_features]
            predict_order = [f for f in predict_features if f in common_features]

            if train_order != predict_order:
                logger.error("âŒ Feature order mismatch detected")
                # æœ€åˆã®ä¸ä¸€è‡´ã‚’è¡¨ç¤º
                for i, (t, p) in enumerate(zip(train_order, predict_order)):
                    if t != p:
                        logger.error(f"  Position {i}: '{t}' vs '{p}'")
                        break

        return False

    # Phase H.27.6: 125ç‰¹å¾´é‡å®Œå…¨æ€§ä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ å¼·åŒ–ç‰ˆ
    def ensure_125_features_completeness(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        125ç‰¹å¾´é‡ã®å®Œå…¨æ€§ã‚’ä¿è¨¼ã™ã‚‹åŒ…æ‹¬çš„ã‚·ã‚¹ãƒ†ãƒ  - Phase H.27.6å¼·åŒ–ç‰ˆ
        ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆå¾©æ´»ã®ãŸã‚ã®æ±ºå®šçš„ä¿è¨¼

        Args:
            df: å…¥åŠ›DataFrame

        Returns:
            æ­£ç¢ºã«125å€‹ã®ç‰¹å¾´é‡ã‚’æŒã¤DataFrame
        """
        logger.info(
            f"ğŸ” [Phase H.27.6] Starting enhanced 125-feature completeness: {len(df.columns)} input features"
        )

        try:
            # Phase H.27.6: Step 0 - å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡ã®ç¢ºå®Ÿãªäº‹å‰é™¤å»
            df_cleaned = self._phase_h27_external_data_cleanup(df)

            # Step 1: é‡è¤‡ç‰¹å¾´é‡ã®æ¤œå‡ºãƒ»æ’é™¤ï¼ˆæ—¢å­˜ãƒ¡ã‚½ãƒƒãƒ‰ä½¿ç”¨ï¼‰
            df_dedup = self._remove_duplicate_features(df_cleaned)

            # Step 2: 125ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã¨ã®ç…§åˆï¼ˆæ—¢å­˜ãƒ¡ã‚½ãƒƒãƒ‰ä½¿ç”¨ï¼‰
            df_aligned = self._align_to_target_features(df_dedup)

            # Step 3: ä¸è¶³ç‰¹å¾´é‡ã®è‡ªå‹•è£œå®Œï¼ˆæ—¢å­˜ãƒ¡ã‚½ãƒƒãƒ‰ä½¿ç”¨ï¼‰
            df_complete = self._fill_missing_features(df_aligned)

            # Step 4: Phase H.27.6: 125ç‰¹å¾´é‡å³å®ˆã®å¼·åŒ–ç‰ˆ
            df_trimmed = self._trim_excess_features_h27(df_complete)

            # Step 5: ç‰¹å¾´é‡å“è³ªãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£ï¼ˆæ—¢å­˜ãƒ¡ã‚½ãƒƒãƒ‰ä½¿ç”¨ï¼‰
            df_quality = self._ensure_feature_quality(df_trimmed)

            # Step 6: Phase H.27.6: æœ€çµ‚æ¤œè¨¼å³æ ¼ç‰ˆ
            df_final = self._final_125_validation_h27(df_quality)

            logger.info(
                f"âœ… [Phase H.27.6] Enhanced 125-feature completeness guaranteed: {len(df_final.columns)} features"
            )
            return df_final

        except Exception as e:
            logger.error(
                f"âŒ [Phase H.27.6] Enhanced 125-feature completeness failed: {e}"
            )
            import traceback

            traceback.print_exc()
            return self._emergency_125_fallback_enhanced(df)

    def _phase_h27_external_data_cleanup(self, df: pd.DataFrame) -> pd.DataFrame:
        """Phase H.27.6: å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡ã®ç¢ºå®Ÿãªäº‹å‰é™¤å»"""
        external_patterns = [
            "vix_",
            "fear_greed",
            "fg_",
            "dxy_",
            "treasury_",
            "us10y",
            "us2y",
            "funding_",
            "fr_",
            "oi_",
            "macro_",
            "sentiment_",
            "corr_btc_",
            "enhanced_default",
        ]

        columns_to_remove = []
        for col in df.columns:
            col_lower = col.lower()
            if any(pattern in col_lower for pattern in external_patterns):
                columns_to_remove.append(col)

        if columns_to_remove:
            logger.info(
                f"ğŸ§¹ [Phase H.27.6] Removing {len(columns_to_remove)} external data features"
            )
            df_cleaned = df.drop(columns=columns_to_remove)
        else:
            df_cleaned = df.copy()

        logger.info(
            f"ğŸ§¹ [Phase H.27.6] External cleanup: {len(df.columns)} â†’ {len(df_cleaned.columns)} features"
        )
        return df_cleaned

    def _remove_duplicate_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """é‡è¤‡ç‰¹å¾´é‡ã®æ¤œå‡ºãƒ»æ’é™¤"""
        original_count = len(df.columns)

        # å®Œå…¨ã«åŒã˜åå‰ã®é‡è¤‡ã‚’é™¤å»
        df_dedup = df.loc[:, ~df.columns.duplicated()]

        # å€¤ãŒåŒã˜ç‰¹å¾´é‡ã®æ¤œå‡ºï¼ˆç›¸é–¢1.0ã®ç‰¹å¾´é‡ï¼‰
        duplicate_pairs = []
        columns = df_dedup.columns.tolist()

        for i in range(len(columns)):
            for j in range(i + 1, len(columns)):
                col1, col2 = columns[i], columns[j]
                try:
                    # NaNã‚’é™¤å¤–ã—ã¦ç›¸é–¢è¨ˆç®—
                    clean_data1 = df_dedup[col1].dropna()
                    clean_data2 = df_dedup[col2].dropna()

                    if len(clean_data1) > 0 and len(clean_data2) > 0:
                        common_index = clean_data1.index.intersection(clean_data2.index)
                        if len(common_index) > 5:  # æœ€å°5ãƒã‚¤ãƒ³ãƒˆã§åˆ¤å®š
                            corr = clean_data1.loc[common_index].corr(
                                clean_data2.loc[common_index]
                            )
                            if abs(corr) > 0.999:  # ã»ã¼å®Œå…¨ç›¸é–¢
                                duplicate_pairs.append((col1, col2, corr))
                except Exception:
                    continue

        # é‡è¤‡ç‰¹å¾´é‡ã®å‰Šé™¤ï¼ˆã‚ˆã‚Šé‡è¦ã§ãªã„æ–¹ã‚’å‰Šé™¤ï¼‰
        features_to_remove = set()
        for col1, col2, corr in duplicate_pairs:
            # 125ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹æ–¹ã‚’å„ªå…ˆ
            if col1 in self.FEATURE_ORDER_125 and col2 not in self.FEATURE_ORDER_125:
                features_to_remove.add(col2)
            elif col2 in self.FEATURE_ORDER_125 and col1 not in self.FEATURE_ORDER_125:
                features_to_remove.add(col1)
            else:
                # ã©ã¡ã‚‰ã‚‚125ãƒªã‚¹ãƒˆã«ãªã„å ´åˆã¯å¾Œã®æ–¹ã‚’å‰Šé™¤
                features_to_remove.add(col2)

        if features_to_remove:
            df_dedup = df_dedup.drop(columns=list(features_to_remove))
            logger.info(f"Removed {len(features_to_remove)} duplicate features")

        logger.debug(
            f"Deduplication: {original_count} â†’ {len(df_dedup.columns)} features"
        )
        return df_dedup

    def _align_to_target_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """125ç‰¹å¾´é‡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒªã‚¹ãƒˆã¨ã®ç…§åˆ"""
        current_features = set(df.columns)
        target_features = set(self.FEATURE_ORDER_125)

        # ç¾åœ¨ã®ç‰¹å¾´é‡ã®åˆ†æ
        present_target = current_features.intersection(target_features)
        missing_target = target_features - current_features
        extra_features = current_features - target_features

        logger.info(
            f"Feature alignment: {len(present_target)}/125 target features present"
        )
        logger.info(f"Missing target features: {len(missing_target)}")
        logger.info(f"Extra features: {len(extra_features)}")

        if missing_target:
            logger.debug(f"Missing features sample: {list(missing_target)[:10]}")

        return df

    def _fill_missing_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¸è¶³ç‰¹å¾´é‡ã®è‡ªå‹•è£œå®Œ"""
        current_features = set(df.columns)
        target_features = set(self.FEATURE_ORDER_125)
        missing_features = target_features - current_features

        if not missing_features:
            logger.debug("No missing features to fill")
            return df

        logger.info(f"Filling {len(missing_features)} missing features")
        df_filled = df.copy()

        # çµ±è¨ˆæƒ…å ±ã®äº‹å‰è¨ˆç®—ï¼ˆåŠ¹ç‡åŒ–ï¼‰
        mean_price = df_filled.get("close", pd.Series([100.0])).mean()
        if pd.isna(mean_price) or mean_price <= 0:
            mean_price = 100.0

        # Phase H.26: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è­¦å‘Šå¯¾å¿œ - ä¸€æ‹¬ã§DataFrameä½œæˆ
        missing_data = {}
        for feature in missing_features:
            try:
                # ç‰¹å¾´é‡ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸé©åˆ‡ãªè£œå®Œå€¤ã‚’è¨ˆç®—
                fill_value = self._calculate_smart_fill_value(
                    feature, df_filled, mean_price
                )
                missing_data[feature] = fill_value
            except Exception as e:
                logger.warning(f"Failed to fill feature {feature}: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: 0ã§è£œå®Œ
                missing_data[feature] = 0.0

        # ä¸€æ‹¬ã§DataFrameã«è¿½åŠ ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼‰
        if missing_data:
            missing_df = pd.DataFrame(missing_data, index=df_filled.index)
            df_filled = pd.concat([df_filled, missing_df], axis=1)

        logger.info(f"âœ… Filled {len(missing_features)} missing features")
        return df_filled

    def _calculate_smart_fill_value(
        self, feature: str, df: pd.DataFrame, mean_price: float
    ) -> float:
        """ç‰¹å¾´é‡ã«å¿œã˜ãŸé©åˆ‡ãªè£œå®Œå€¤ã‚’è¨ˆç®—"""
        # ä¾¡æ ¼ç³»ç‰¹å¾´é‡
        if any(
            x in feature.lower()
            for x in ["close", "open", "high", "low", "price", "sma", "ema"]
        ):
            return mean_price

        # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç³»ç‰¹å¾´é‡
        elif "volume" in feature.lower():
            if "volume" in df.columns:
                return df["volume"].mean() if not df["volume"].isna().all() else 1000.0
            return 1000.0

        # RSIç³»ï¼ˆ0-100ã®ç¯„å›²ï¼‰
        elif "rsi" in feature.lower():
            return 50.0

        # ATRç³»ï¼ˆä¾¡æ ¼ã®æ•°%ï¼‰
        elif "atr" in feature.lower():
            return mean_price * 0.02

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç³»
        elif "volatility" in feature.lower() or "vol" in feature.lower():
            return 0.02

        # æ¯”ç‡ãƒ»ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆç³»
        elif any(x in feature.lower() for x in ["ratio", "pct", "change", "return"]):
            return 0.0

        # ãƒã‚¤ãƒŠãƒªæŒ‡æ¨™ï¼ˆ0/1ï¼‰
        elif any(
            x in feature.lower()
            for x in ["is_", "oversold", "overbought", "cross", "breakout"]
        ):
            return 0.0

        # æ­£è¦åŒ–ã•ã‚ŒãŸæŒ‡æ¨™
        elif any(x in feature.lower() for x in ["zscore", "normalized"]):
            return 0.0

        # ãã®ä»–ã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
        elif any(
            x in feature.lower() for x in ["macd", "stoch", "adx", "cci", "williams"]
        ):
            return 0.0

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        else:
            return 0.0

    def _trim_excess_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä½™åˆ†ç‰¹å¾´é‡ã®ç®¡ç†ï¼ˆ125ç‰¹å¾´é‡ã«èª¿æ•´ï¼‰"""
        if len(df.columns) <= 125:
            return df

        logger.info(f"Trimming excess features: {len(df.columns)} â†’ 125")

        # 125ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã®é †åºã«å¾“ã£ã¦é¸æŠ
        ordered_features = []
        available_features = set(df.columns)

        # ã¾ãš125ãƒªã‚¹ãƒˆã®ç‰¹å¾´é‡ã‚’é †åºé€šã‚Šã«è¿½åŠ 
        for feature in self.FEATURE_ORDER_125:
            if feature in available_features:
                ordered_features.append(feature)

        # ã¾ã 125ã«é”ã—ã¦ã„ãªã„å ´åˆã¯ã€æ®‹ã‚Šã®é‡è¦ãã†ãªç‰¹å¾´é‡ã‚’è¿½åŠ 
        if len(ordered_features) < 125:
            remaining_features = available_features - set(ordered_features)
            # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ã§å®‰å®šåŒ–ï¼‰
            remaining_sorted = sorted(remaining_features)
            needed = 125 - len(ordered_features)
            ordered_features.extend(remaining_sorted[:needed])

        # æ­£ç¢ºã«125ç‰¹å¾´é‡ã‚’é¸æŠ
        selected_features = ordered_features[:125]

        logger.info(f"Selected {len(selected_features)} features for final dataset")
        return df[selected_features]

    def _ensure_feature_quality(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾´é‡å“è³ªãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£"""
        df_quality = df.copy()
        issues_fixed = 0

        for column in df_quality.columns:
            try:
                # NaNå€¤ã®å‡¦ç†
                nan_count = df_quality[column].isna().sum()
                if nan_count > 0:
                    # å‰æ–¹ãƒ»å¾Œæ–¹è£œå®Œ
                    df_quality[column] = df_quality[column].ffill().bfill()
                    # ã¾ã NaNãŒã‚ã‚‹å ´åˆã¯é©åˆ‡ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§è£œå®Œ
                    remaining_nan = df_quality[column].isna().sum()
                    if remaining_nan > 0:
                        fill_value = self._calculate_smart_fill_value(
                            column, df_quality, 100.0
                        )
                        df_quality[column] = df_quality[column].fillna(fill_value)
                        issues_fixed += 1

                # infå€¤ã®å‡¦ç†
                inf_count = np.isinf(df_quality[column]).sum()
                if inf_count > 0:
                    # infå€¤ã‚’é©åˆ‡ãªå€¤ã§ç½®æ›
                    finite_values = df_quality[column][np.isfinite(df_quality[column])]
                    if len(finite_values) > 0:
                        replacement = finite_values.median()
                    else:
                        replacement = self._calculate_smart_fill_value(
                            column, df_quality, 100.0
                        )

                    df_quality[column] = df_quality[column].replace(
                        [np.inf, -np.inf], replacement
                    )
                    issues_fixed += 1

            except Exception as e:
                logger.warning(f"Quality check failed for {column}: {e}")
                # å•é¡Œã®ã‚ã‚‹ç‰¹å¾´é‡ã¯å®‰å…¨ãªå€¤ã§ç½®æ›
                df_quality[column] = 0.0
                issues_fixed += 1

        if issues_fixed > 0:
            logger.info(f"Fixed quality issues in {issues_fixed} features")

        return df_quality

    def _final_125_validation(self, df: pd.DataFrame) -> pd.DataFrame:
        """æœ€çµ‚çš„ãª125ç‰¹å¾´é‡æ¤œè¨¼"""
        if len(df.columns) != 125:
            logger.error(
                f"Final validation failed: {len(df.columns)} features instead of 125"
            )

            if len(df.columns) > 125:
                # ä½™åˆ†ãªç‰¹å¾´é‡ã‚’å‰Šé™¤
                df = df.iloc[:, :125]
            elif len(df.columns) < 125:
                # ä¸è¶³åˆ†ã‚’è£œå®Œ
                needed = 125 - len(df.columns)
                for i in range(needed):
                    feature_name = f"auto_generated_{i:03d}"
                    df[feature_name] = 0.0

            logger.warning(
                f"Applied emergency adjustment: now {len(df.columns)} features"
            )

        # ç‰¹å¾´é‡åã®æœ€çµ‚èª¿æ•´ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼‰
        final_columns = []
        seen_names = set()

        for col in df.columns:
            if col in seen_names:
                # é‡è¤‡ãŒã‚ã‚‹å ´åˆã¯ç•ªå·ã‚’ä»˜åŠ 
                counter = 1
                new_name = f"{col}_{counter}"
                while new_name in seen_names:
                    counter += 1
                    new_name = f"{col}_{counter}"
                final_columns.append(new_name)
                seen_names.add(new_name)
            else:
                final_columns.append(col)
                seen_names.add(col)

        df.columns = final_columns

        logger.info(f"âœ… Final validation passed: {len(df.columns)} unique features")
        return df

    def _trim_excess_features_h27(self, df: pd.DataFrame) -> pd.DataFrame:
        """Phase H.27.6: ä½™åˆ†ç‰¹å¾´é‡ã®ç®¡ç†å¼·åŒ–ç‰ˆï¼ˆ125ç‰¹å¾´é‡å³å®ˆï¼‰"""
        if len(df.columns) <= 125:
            logger.info(
                f"ğŸ”§ [Phase H.27.6] Features within limit: {len(df.columns)}/125"
            )
            return df

        logger.info(
            f"ğŸ”§ [Phase H.27.6] Trimming excess features: {len(df.columns)} â†’ 125"
        )

        # Phase H.27.6: å„ªå…ˆåº¦ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡é¸æŠï¼ˆFEATURE_ORDER_125æº–æ‹ ï¼‰
        ordered_features = []
        available_features = set(df.columns)

        # Step 1: 125ãƒªã‚¹ãƒˆã®ç‰¹å¾´é‡ã‚’é †åºé€šã‚Šã«æœ€å„ªå…ˆã§è¿½åŠ 
        for feature in self.FEATURE_ORDER_125:
            if feature in available_features and len(ordered_features) < 125:
                ordered_features.append(feature)

        # Step 2: ã¾ã 125ã«é”ã—ã¦ã„ãªã„å ´åˆã€é‡è¦ãã†ãªç‰¹å¾´é‡ã‚’è¿½åŠ 
        if len(ordered_features) < 125:
            remaining_features = available_features - set(ordered_features)
            # é‡è¦åº¦é †ã§ã‚½ãƒ¼ãƒˆï¼ˆãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™å„ªå…ˆï¼‰
            technical_priority = []
            other_features = []

            for feature in remaining_features:
                feature_lower = feature.lower()
                if any(
                    pattern in feature_lower
                    for pattern in [
                        "rsi",
                        "sma",
                        "ema",
                        "atr",
                        "macd",
                        "volume",
                        "price",
                        "close",
                        "returns",
                    ]
                ):
                    technical_priority.append(feature)
                else:
                    other_features.append(feature)

            # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’å„ªå…ˆçš„ã«è¿½åŠ 
            needed = 125 - len(ordered_features)
            priority_list = sorted(technical_priority) + sorted(other_features)
            ordered_features.extend(priority_list[:needed])

        # æ­£ç¢ºã«125ç‰¹å¾´é‡ã‚’é¸æŠ
        selected_features = ordered_features[:125]

        logger.info(
            f"âœ… [Phase H.27.6] Selected {len(selected_features)} features (target: 125)"
        )
        return df[selected_features]

    def _final_125_validation_h27(self, df: pd.DataFrame) -> pd.DataFrame:
        """Phase H.27.6: æœ€çµ‚çš„ãª125ç‰¹å¾´é‡æ¤œè¨¼å¼·åŒ–ç‰ˆ"""
        if len(df.columns) == 125:
            logger.info(
                "âœ… [Phase H.27.6] Final validation passed: exactly 125 features"
            )
            return df

        logger.error(
            f"âŒ [Phase H.27.6] Final validation failed: {len(df.columns)} features instead of 125"
        )

        if len(df.columns) > 125:
            # ä½™åˆ†ãªç‰¹å¾´é‡ã‚’å‰Šé™¤ï¼ˆå¾Œã‚ã‹ã‚‰ï¼‰
            df_fixed = df.iloc[:, :125]
            logger.warning(
                f"ğŸ”§ [Phase H.27.6] Trimmed to 125: removed {len(df.columns) - 125} excess features"
            )
        elif len(df.columns) < 125:
            # ä¸è¶³åˆ†ã‚’è£œå®Œï¼ˆFEATURE_ORDER_125ã‹ã‚‰ï¼‰
            df_fixed = df.copy()
            current_features = set(df_fixed.columns)
            needed = 125 - len(df_fixed.columns)

            # FEATURE_ORDER_125ã‹ã‚‰ä¸è¶³åˆ†ã‚’è£œå®Œ
            missing_candidates = [
                f for f in self.FEATURE_ORDER_125 if f not in current_features
            ]

            for i, feature_name in enumerate(missing_candidates[:needed]):
                # å®‰å…¨ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§è£œå®Œ
                df_fixed[feature_name] = 0.0

            # ã¾ã ä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯è‡ªå‹•ç”Ÿæˆ
            current_count = len(df_fixed.columns)
            for i in range(current_count, 125):
                feature_name = f"auto_h27_{i-current_count:03d}"
                df_fixed[feature_name] = 0.0

            logger.warning(
                f"ğŸ”§ [Phase H.27.6] Filled to 125: added {125 - len(df.columns)} missing features"
            )
        else:
            df_fixed = df

        # ç‰¹å¾´é‡åã®æœ€çµ‚èª¿æ•´ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯å¼·åŒ–ï¼‰
        final_columns = []
        seen_names = set()

        for col in df_fixed.columns:
            if col in seen_names:
                # é‡è¤‡ãŒã‚ã‚‹å ´åˆã¯ç•ªå·ã‚’ä»˜åŠ 
                counter = 1
                new_name = f"{col}_h27_{counter}"
                while new_name in seen_names:
                    counter += 1
                    new_name = f"{col}_h27_{counter}"
                final_columns.append(new_name)
                seen_names.add(new_name)
            else:
                final_columns.append(col)
                seen_names.add(col)

        df_fixed.columns = final_columns

        logger.info(
            f"âœ… [Phase H.27.6] Final validation completed: {len(df_fixed.columns)} unique features"
        )
        return df_fixed

    def _emergency_125_fallback_enhanced(self, df: pd.DataFrame) -> pd.DataFrame:
        """Phase H.27.6: ç·Šæ€¥æ™‚ã®125ç‰¹å¾´é‡ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¼·åŒ–ç‰ˆ"""
        logger.warning(
            "ğŸš¨ [Phase H.27.6] Using enhanced emergency 125-feature fallback"
        )

        try:
            # ä½¿ç”¨å¯èƒ½ãªç‰¹å¾´é‡ã‚’æœ€å¤§é™æ´»ç”¨
            available_features = list(df.columns)

            # 125ç‰¹å¾´é‡ã®åŸºæœ¬ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½œæˆ
            result_df = pd.DataFrame(index=df.index)

            # Step 1: FEATURE_ORDER_125ã«åŸºã¥ãå„ªå…ˆé †ä½ä»˜ã‘
            priority_features = []
            for feature in self.FEATURE_ORDER_125:
                if feature in available_features:
                    priority_features.append(feature)

            # Step 2: å„ªå…ˆç‰¹å¾´é‡ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆæœ€å¤§125ã¾ã§ï¼‰
            features_added = 0
            for feature in priority_features[:125]:
                if features_added < 125:
                    result_df[feature] = df[feature]
                    features_added += 1

            # Step 3: ä¸è¶³åˆ†ã‚’åŸºæœ¬çš„ãªç‰¹å¾´é‡ã§è£œå®Œ
            if features_added < 125:
                # åŸºæœ¬ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ç³»ã®è£œå®Œ
                basic_features = ["close", "open", "high", "low", "volume"]
                for feature in basic_features:
                    if feature in df.columns and features_added < 125:
                        for suffix in ["", "_lag_1", "_lag_2", "_sma_5", "_sma_20"]:
                            new_feature = f"{feature}{suffix}"
                            if (
                                new_feature not in result_df.columns
                                and features_added < 125
                            ):
                                if suffix == "":
                                    result_df[new_feature] = df[feature]
                                elif "_lag_" in suffix:
                                    result_df[new_feature] = df[feature].shift(
                                        int(suffix.split("_")[-1])
                                    )
                                elif "_sma_" in suffix:
                                    window = int(suffix.split("_")[-1])
                                    result_df[new_feature] = (
                                        df[feature].rolling(window=window).mean()
                                    )
                                features_added += 1

            # Step 4: ã¾ã ä¸è¶³åˆ†ãŒã‚ã‚Œã°0åŸ‹ã‚
            for i in range(features_added, 125):
                feature_name = f"emergency_h27_{i:03d}"
                result_df[feature_name] = 0.0

            # NaNå€¤ã®å‡¦ç†
            result_df = result_df.fillna(0.0)

            logger.warning(
                f"ğŸš¨ [Phase H.27.6] Enhanced emergency fallback created: {len(result_df.columns)} features"
            )
            return result_df

        except Exception as e:
            logger.error(f"âŒ [Phase H.27.6] Enhanced emergency fallback failed: {e}")
            # æœ€å¾Œã®æ‰‹æ®µï¼šFEATURE_ORDER_125ãƒ™ãƒ¼ã‚¹ã®0åŸ‹ã‚DataFrame
            emergency_df = pd.DataFrame(
                0.0,
                index=df.index if not df.empty else [0],
                columns=self.FEATURE_ORDER_125,
            )
            logger.error(
                f"ğŸš¨ [Phase H.27.6] Created zero-filled emergency DataFrame: {len(emergency_df.columns)} features"
            )
            return emergency_df

    def _emergency_125_fallback(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç·Šæ€¥æ™‚ã®125ç‰¹å¾´é‡ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæ—§ç‰ˆãƒ»äº’æ›æ€§ç¶­æŒï¼‰"""
        logger.warning("Using legacy emergency 125-feature fallback")
        return self._emergency_125_fallback_enhanced(df)


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_global_feature_order_manager: Optional[FeatureOrderManager] = None


def get_feature_order_manager() -> FeatureOrderManager:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡é †åºç®¡ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _global_feature_order_manager
    if _global_feature_order_manager is None:
        _global_feature_order_manager = FeatureOrderManager()
    return _global_feature_order_manager
