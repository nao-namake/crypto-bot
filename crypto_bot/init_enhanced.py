"""
INITæ®µéšã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ç‰ˆ
Phase 2.2: INITæ®µéšã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ãƒ»ATRè¨ˆç®—æ”¹å–„

main.pyã®INIT-5ï½INIT-8æ®µéšã‚’å¼·åŒ–ã—ãŸç‰ˆ
"""

import logging
import time
from typing import Any, Optional

import pandas as pd

from crypto_bot.utils.error_resilience import with_resilience

logger = logging.getLogger(__name__)


@with_resilience("init_system", "init_5_fetch_price_data")
def enhanced_init_5_fetch_price_data(
    fetcher, dd: dict, max_retries: int = 5, timeout: int = 120, prefetch_data=None
) -> Optional[pd.DataFrame]:
    """
    INIT-5æ®µéš: åˆæœŸä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå¼·åŒ–ç‰ˆãƒ»Phase H.13: ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ‡ãƒ¼ã‚¿çµ±åˆï¼‰

    Args:
        fetcher: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼
        dd: ãƒ‡ãƒ¼ã‚¿è¨­å®šè¾æ›¸
        max_retries: æœ€å¤§å†è©¦è¡Œå›æ•°
        timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°ï¼ˆPhase H.7: 60â†’120ç§’å»¶é•·ï¼‰
        prefetch_data: ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ‡ãƒ¼ã‚¿ï¼ˆPhase H.13: ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—å…±æœ‰ç”¨ï¼‰

    Returns:
        DataFrame: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
    """
    # Phase H.13: ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ‡ãƒ¼ã‚¿å„ªå…ˆä½¿ç”¨ï¼ˆATRè¨ˆç®—ãƒ‡ãƒ¼ã‚¿æœ€å¤§åŒ–ï¼‰
    if prefetch_data is not None and not prefetch_data.empty:
        logger.info(
            "ğŸ“Š [INIT-5] Phase H.13: Using prefetched data for optimal ATR calculation"
        )
        logger.info(
            f"âœ… [INIT-5] Prefetch data utilized: {len(prefetch_data)} records (vs previous 5 records)"
        )
        logger.info(
            f"ğŸ“ˆ [INIT-5] Data range: {prefetch_data.index.min()} to {prefetch_data.index.max()}"
        )

        # ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª
        required_columns = ["open", "high", "low", "close", "volume"]
        missing_columns = [
            col for col in required_columns if col not in prefetch_data.columns
        ]

        if missing_columns:
            logger.warning(
                f"âš ï¸ [INIT-5] Missing columns in prefetch data: {missing_columns}"
            )
            logger.info("ğŸ”„ [INIT-5] Falling back to independent fetch")
        else:
            logger.info(
                "âœ… [INIT-5] Phase H.13: All required columns present in prefetch data"
            )
            logger.info(
                f"ğŸ¯ [INIT-5] Phase H.13: ATR calculation will use {len(prefetch_data)} records (sufficient for period=14)"
            )
            return prefetch_data.copy()  # ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™

    # Phase H.8.4: ã‚¨ãƒ©ãƒ¼è€æ€§ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ çµ±åˆï¼ˆè¨˜éŒ²ã®ã¿ï¼‰
    logger.info(
        "ğŸ“ˆ [INIT-5] Fetching initial price data for ATR calculation (fallback mode)..."
    )
    logger.info(f"â° [INIT-5] Timestamp: {pd.Timestamp.now()}")
    logger.info(
        f"ğŸ”§ [INIT-5] Configuration: max_retries={max_retries}, timeout={timeout}s (Phase H.7å»¶é•·)"
    )
    logger.info("ğŸ›¡ï¸ [PHASE-H8.4] INIT-5 with enhanced error resilience")

    # Phase H.6.1: å‹•çš„sinceè¨ˆç®—ï¼ˆãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    current_time = pd.Timestamp.now(tz="UTC")

    if dd.get("since"):
        since_time = pd.Timestamp(dd["since"])
        if since_time.tz is None:
            since_time = since_time.tz_localize("UTC")
        logger.info(f"ğŸ”§ [INIT-5] Using config since: {since_time}")
    else:
        # å‹•çš„since_hoursè¨ˆç®—ï¼ˆåœŸæ—¥ã‚®ãƒ£ãƒƒãƒ—ãƒ»ç¥æ—¥å¯¾å¿œï¼‰
        base_hours = dd.get("since_hours", 120)  # Phase H.5.3: 120æ™‚é–“ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

        # æ›œæ—¥åˆ¤å®šï¼ˆæœˆæ›œæ—¥=0, æ—¥æ›œæ—¥=6ï¼‰
        current_day = current_time.dayofweek
        current_hour = current_time.hour

        # åœŸæ—¥ã‚®ãƒ£ãƒƒãƒ—å¯¾å¿œ
        if current_day == 0:  # æœˆæ›œæ—¥
            # æœˆæ›œæ—¥ã¯åœŸæ—¥ã‚®ãƒ£ãƒƒãƒ—ã‚’è€ƒæ…®ã—ã¦å»¶é•·
            extended_hours = dd.get("weekend_extension_hours", 72)  # 3æ—¥é–“è¿½åŠ 
            lookback_hours = base_hours + extended_hours
            logger.info(
                f"ğŸ”§ [INIT-5] Monday detected: extending lookback by {extended_hours}h to {lookback_hours}h"
            )
        elif current_day == 1 and current_hour < 12:  # ç«æ›œæ—¥åˆå‰
            # ç«æ›œæ—¥åˆå‰ã‚‚å°‘ã—å»¶é•·
            extended_hours = dd.get("early_week_extension_hours", 36)  # 1.5æ—¥è¿½åŠ 
            lookback_hours = base_hours + extended_hours
            logger.info(
                f"ğŸ”§ [INIT-5] Tuesday morning: extending by {extended_hours}h to {lookback_hours}h"
            )
        else:
            lookback_hours = base_hours

        since_time = current_time - pd.Timedelta(hours=lookback_hours)
        logger.info(
            f"ğŸ” [INIT-5] Dynamic since calculation - Day: {current_day}, Hour: {current_hour}, "
            f"Lookback: {lookback_hours}h, Since: {since_time}"
        )
        logger.info(
            f"   â° Time span: {lookback_hours} hours ({lookback_hours/24:.1f} days)"
        )
        logger.info(f"   ğŸ“Š Expected 1h records: ~{lookback_hours}")

    # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã®åˆæœŸåŒ–ç¢ºèª
    try:
        logger.info("ğŸ” [INIT-5] Verifying external data fetchers...")

        # yfinanceä¾å­˜é–¢ä¿‚ç¢ºèª
        import yfinance  # noqa: F401

        logger.info("âœ… [INIT-5] yfinance module verified")

        # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã®ãƒ†ã‚¹ãƒˆ
        try:
            from crypto_bot.data.vix_fetcher import VIXDataFetcher

            vix_fetcher = VIXDataFetcher()  # noqa: F841
            logger.info("âœ… [INIT-5] VIX fetcher initialized")
        except Exception as e:
            logger.warning(f"âš ï¸ [INIT-5] VIX fetcher initialization failed: {e}")

        try:
            from crypto_bot.data.macro_fetcher import MacroDataFetcher

            macro_fetcher = MacroDataFetcher()  # noqa: F841
            logger.info("âœ… [INIT-5] Macro fetcher initialized")
        except Exception as e:
            logger.warning(f"âš ï¸ [INIT-5] Macro fetcher initialization failed: {e}")

        try:
            from crypto_bot.data.fear_greed_fetcher import FearGreedDataFetcher

            fear_greed_fetcher = FearGreedDataFetcher()  # noqa: F841
            logger.info("âœ… [INIT-5] Fear&Greed fetcher initialized")
        except Exception as e:
            logger.warning(f"âš ï¸ [INIT-5] Fear&Greed fetcher initialization failed: {e}")

    except ImportError as e:
        logger.error(f"âŒ [INIT-5] External data fetcher dependency error: {e}")
        logger.error("âŒ [INIT-5] This will cause external data fetchers to fail")

    # ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒ¼ã‚¿å–å¾—å‡¦ç†
    initial_df = None

    # Phase H.8.2: API Error 10000å®Œå…¨è§£æ±ºãƒ»4hç›´æ¥å–å¾—å®Œå…¨ç¦æ­¢
    # 4hã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ¤œå‡ºã¨å¼·åˆ¶å¤‰æ›ï¼ˆè¨­å®šã«é–¢ä¿‚ãªã1hå›ºå®šï¼‰
    forced_timeframe = "1h"  # Phase H.8.2: è¨­å®šã«é–¢ä¿‚ãªã1hå›ºå®š

    # å…¨ã¦ã®4hé–¢é€£è¨­å®šã‚’æ¤œå‡ºã—ã¦è­¦å‘Šãƒ»å¼·åˆ¶å¤‰æ›
    multi_tf_base = dd.get("multi_timeframe_data", {}).get("base_timeframe", "1h")
    data_timeframe = dd.get("timeframe", "1h")

    # 4hæ¤œå‡ºã®åŒ…æ‹¬ãƒã‚§ãƒƒã‚¯
    four_hour_detected = False
    detection_sources = []

    if multi_tf_base == "4h":
        four_hour_detected = True
        detection_sources.append("multi_timeframe_data.base_timeframe")
        logger.critical(
            "ğŸš¨ [INIT-5] CRITICAL: 4h detected in multi_timeframe_data.base_timeframe"
        )

    if data_timeframe == "4h":
        four_hour_detected = True
        detection_sources.append("data.timeframe")
        logger.critical("ğŸš¨ [INIT-5] CRITICAL: 4h detected in data.timeframe")

    # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ è¨­å®šã®å¼·åˆ¶ä¿®æ­£
    if four_hour_detected:
        logger.critical(
            f"ğŸš¨ [INIT-5] Phase H.8.2: 4h timeframe BLOCKED - Sources: {', '.join(detection_sources)}"
        )
        logger.critical(
            "ğŸš¨ [INIT-5] Phase H.8.2: Forcing to 1h to prevent API Error 10000"
        )
    else:
        logger.info(
            "âœ… [INIT-5] Phase H.8.2: No 4h timeframe detected, safe to proceed"
        )

    timeframe = forced_timeframe  # å¸¸ã«1hä½¿ç”¨
    logger.info(f"ğŸ”§ [INIT-5] Phase H.8.2: Using forced timeframe: {timeframe}")

    # API Error 10000é˜²æ­¢ã®æœ€çµ‚ç¢ºèª
    if timeframe == "4h":
        logger.critical(
            "ğŸš¨ [INIT-5] Phase H.8.2: EMERGENCY: 4h still detected, forcing to 1h"
        )
        timeframe = "1h"
    # Phase H.13: INIT-5å°‚ç”¨è¨­å®šï¼ˆãƒ‡ãƒ¼ã‚¿ä¸è¶³å¯¾å¿œãƒ»ååˆ†ãªä½™è£•ç¢ºä¿ãƒ»å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³å¼·åŒ–ï¼‰
    init_limit = 200  # Phase H.13: å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³å¼·åŒ–ï¼ˆATRæœŸé–“14ã«å¯¾ã—ã¦14å€ä»¥ä¸Šã®ä½™è£•ãƒ»100ãƒ‡ãƒ¼ã‚¿æœªå–å¾—å¯¾ç­–ï¼‰
    init_paginate = True  # Phase H.13: ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æœ‰åŠ¹åŒ–ï¼ˆãƒ‡ãƒ¼ã‚¿ç¢ºä¿å„ªå…ˆï¼‰
    init_per_page = (
        100  # Phase H.13: å¤§ãã‚ã®ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºï¼ˆåŠ¹ç‡çš„ãªå¤§é‡å–å¾—ãƒ»å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³å¼·åŒ–ï¼‰
    )

    logger.info(
        f"ğŸ”§ [INIT-5] Phase H.13: timeframe={timeframe}, limit={init_limit}, paginate={init_paginate}, per_page={init_per_page}"
    )
    logger.info(
        f"ğŸ”§ [INIT-5] Phase H.13: Enhanced safety margin settings ({init_limit} records target, robust against <100 data scenarios)"
    )
    logger.info(
        "âš ï¸ [INIT-5] Phase H.13: Note - prefetch data preferred, this is safety fallback"
    )

    for attempt in range(max_retries):
        try:
            start_time = time.time()
            logger.info(
                f"ğŸ”„ [INIT-5] Attempt {attempt + 1}/{max_retries} - "
                f"Fetching initial price data..."
            )

            # Cloud Runå¯¾å¿œã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ãƒ‡ãƒ¼ã‚¿å–å¾—
            from concurrent.futures import ThreadPoolExecutor
            from concurrent.futures import TimeoutError as FutureTimeoutError

            def fetch_data():
                # Phase H.8.1: ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦ãƒã‚§ãƒƒã‚¯ä»˜ããƒ•ã‚§ãƒƒãƒä½¿ç”¨
                logger.info(
                    "ğŸš€ [PHASE-H8.1] Using freshness-aware data fetch for INIT-5"
                )
                return fetcher.fetch_with_freshness_fallback(
                    timeframe=timeframe,
                    since=since_time,  # Phase H.6.1: sinceæ™‚åˆ»ã‚’è¿½åŠ 
                    limit=init_limit,  # Phase H.13: 200ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³å¼·åŒ–ãƒ»100ãƒ‡ãƒ¼ã‚¿æœªå–å¾—å¯¾ç­–ï¼‰
                    max_age_hours=2.0,  # Phase H.8.1: 2æ™‚é–“ä»¥å†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¦æ±‚
                    paginate=init_paginate,  # Phase H.13: Trueï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æœ‰åŠ¹ï¼‰
                    per_page=init_per_page,  # Phase H.13: 100ä»¶ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºï¼ˆå¤§é‡åŠ¹ç‡å–å¾—ãƒ»å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³å¼·åŒ–ï¼‰
                    # Phase H.13: å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³å¼·åŒ–ãƒ»ååˆ†ãªå–å¾—æ©Ÿä¼šç¢ºä¿ï¼ˆAPIåˆ¶é™ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å•é¡Œå¯¾å¿œï¼‰
                    max_consecutive_empty=dd.get(
                        "max_consecutive_empty", 10
                    ),  # 5â†’10ã«æ‹¡å¤§ï¼ˆä½™è£•ç¢ºä¿ï¼‰
                    max_consecutive_no_new=dd.get(
                        "max_consecutive_no_new", 20
                    ),  # 10â†’20ã«æ‹¡å¤§ï¼ˆãƒ‡ãƒ¼ã‚¿ç¢ºä¿é‡è¦–ï¼‰
                    max_attempts=dd.get(
                        "max_attempts", 35
                    ),  # 15â†’35ã«æ‹¡å¤§ï¼ˆ200ãƒ¬ã‚³ãƒ¼ãƒ‰ç¢ºå®Ÿå–å¾—ï¼‰
                )

            try:
                with ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(fetch_data)
                    initial_df = future.result(timeout=timeout)

                fetch_time = time.time() - start_time
                logger.info(
                    f"âœ… [INIT-5] Initial price data fetched successfully: "
                    f"{len(initial_df)} records in {fetch_time:.2f}s"
                )
                logger.info(
                    "âœ… [INIT-5] Phase H.7 optimization successful - lightweight fetch completed"
                )

                # ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª
                if initial_df is not None and not initial_df.empty:
                    required_columns = ["open", "high", "low", "close", "volume"]
                    missing_columns = [
                        col for col in required_columns if col not in initial_df.columns
                    ]

                    if missing_columns:
                        logger.warning(
                            f"âš ï¸ [INIT-5] Missing required columns: {missing_columns}"
                        )
                    else:
                        logger.info("âœ… [INIT-5] All required columns present")

                    # ãƒ‡ãƒ¼ã‚¿ç¯„å›²ç¢ºèª
                    logger.info(
                        f"ğŸ“Š [INIT-5] Data range: "
                        f"{initial_df.index.min()} to {initial_df.index.max()}"
                    )
                    logger.info(
                        f"ğŸ“Š [INIT-5] Price range: "
                        f"{initial_df['close'].min():.2f} to "
                        f"{initial_df['close'].max():.2f}"
                    )

                break

            except (FutureTimeoutError, TimeoutError) as e:
                logger.error(f"â° [INIT-5] Timeout error: {e}")
                raise

        except Exception as e:
            fetch_time = time.time() - start_time
            error_str = str(e)

            # Phase F.4: APIåˆ¶é™ã‚¨ãƒ©ãƒ¼ç‰¹åˆ¥å‡¦ç†
            if "10000" in error_str or "rate limit" in error_str.lower():
                logger.error(
                    f"ğŸš¨ [INIT-5] API rate limit error detected (attempt {attempt + 1}): {e}"
                )
                # APIåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ã‚ˆã‚Šé•·ã„å¾…æ©Ÿæ™‚é–“
                wait_time = min((attempt + 1) * 20, 120)  # æœ€å¤§2åˆ†å¾…æ©Ÿ
                logger.warning(
                    f"â³ [INIT-5] API limit backoff: waiting {wait_time}s for recovery..."
                )
            else:
                logger.error(
                    f"âŒ [INIT-5] Attempt {attempt + 1} failed after {fetch_time:.2f}s: {e}"
                )
                # é€šå¸¸ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æ¨™æº–çš„ãªå¾…æ©Ÿæ™‚é–“
                wait_time = min((attempt + 1) * 10, 60)
                logger.info(
                    f"â³ [INIT-5] Standard backoff: waiting {wait_time}s before retry..."
                )

            if attempt < max_retries - 1:
                time.sleep(wait_time)
            else:
                logger.error(
                    f"âŒ [INIT-5] All {max_retries} attempts failed - "
                    "data fetch completely failed. Consider increasing timeout or reducing API load."
                )
                initial_df = None

    return initial_df


@with_resilience("init_system", "init_6_calculate_atr")
def enhanced_init_6_calculate_atr(
    initial_df: Optional[pd.DataFrame], period: int = None
) -> Optional[pd.Series]:
    """
    INIT-6æ®µéš: ATRè¨ˆç®—ï¼ˆå¼·åŒ–ç‰ˆãƒ»Phase H.22.3: è¨­å®šçµ±ä¸€ï¼‰

    Args:
        initial_df: åˆæœŸä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        period: ATRè¨ˆç®—æœŸé–“ï¼ˆNoneã®å ´åˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ï¼‰

    Returns:
        Series: ATRå€¤ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
    """
    # Phase H.22.3: ATRæœŸé–“è¨­å®šçµ±ä¸€ãƒ»production.ymlè¨­å®šå€¤ä½¿ç”¨
    if period is None:
        from crypto_bot.main import get_current_config

        try:
            config = get_current_config()
            period = config.get("risk_management", {}).get("atr_period", 20)
            logger.info(f"âœ… [INIT-6-H22.3] Using config atr_period: {period}")
        except Exception as e:
            logger.warning(
                f"âš ï¸ [INIT-6-H22.3] Config read failed, using default 20: {e}"
            )
            period = 20

    logger.info("ğŸ”¢ [INIT-6] Calculating ATR...")
    logger.info(f"â° [INIT-6] Timestamp: {pd.Timestamp.now()}")
    logger.info(f"ğŸ”§ [INIT-6-H22.3] ATR period: {period} (config-unified)")
    logger.info("ğŸ›¡ï¸ [PHASE-H8.4] INIT-6 with enhanced error resilience")

    atr_series = None

    if initial_df is not None and not initial_df.empty:
        try:
            # ãƒ‡ãƒ¼ã‚¿å“è³ªã®å†ç¢ºèª
            required_columns = ["high", "low", "close"]
            missing_columns = [
                col for col in required_columns if col not in initial_df.columns
            ]

            if missing_columns:
                logger.error(
                    f"âŒ [INIT-6] Missing required columns for ATR: {missing_columns}"
                )
                return None

            # Phase H.13: ATRè¨ˆç®—ãƒ‡ãƒ¼ã‚¿é‡è©•ä¾¡ï¼ˆä½™è£•ã‚’æŒã£ãŸåŸºæº–ãƒ»å®Ÿç”¨æ€§é‡è¦–ï¼‰
            data_count = len(initial_df)
            min_records_excellent = period * 3  # å„ªç§€: period Ã— 3ï¼ˆ42ä»¶ï¼‰
            min_records_ideal = period + 5  # ç†æƒ³: period + ä½™è£•ï¼ˆ19ä»¶ï¼‰
            min_records_good = period + 1  # è‰¯å¥½: period + 1ï¼ˆ15ä»¶ï¼‰
            min_records_acceptable = max(
                period // 2, 5
            )  # è¨±å®¹: periodåŠåˆ†ã¾ãŸã¯5ä»¶ï¼ˆ7ä»¶ï¼‰
            min_records_minimum = max(
                3, period // 3
            )  # æœ€å°: periodä¸‰åˆ†ã®ä¸€ã¾ãŸã¯3ä»¶ï¼ˆ5ä»¶ï¼‰

            # Phase H.13: ãƒ‡ãƒ¼ã‚¿é‡ã«ã‚ˆã‚‹å“è³ªè©•ä¾¡ãƒ»ä½™è£•ã‚’æŒã£ãŸåˆ¤å®š
            if data_count >= min_records_excellent:
                logger.info(
                    f"ğŸŒŸ [INIT-6] Phase H.13: EXCELLENT data for ATR calculation: {data_count} records >= {min_records_excellent} (3x period)"
                )
                logger.info(
                    "âœ¨ [INIT-6] Phase H.13: Optimal data volume for maximum ATR precision"
                )
            elif data_count >= min_records_ideal:
                logger.info(
                    f"ğŸ¯ [INIT-6] Phase H.13: VERY GOOD data for ATR calculation: {data_count} records >= {min_records_ideal} (period + 5)"
                )
                logger.info(
                    "âœ¨ [INIT-6] Phase H.13: Large data volume enables high-precision ATR calculation"
                )
            elif data_count >= min_records_good:
                logger.info(
                    f"âœ… [INIT-6] Phase H.13: GOOD data for ATR calculation: {data_count} records >= {min_records_good} (period + 1)"
                )
            elif data_count >= min_records_acceptable:
                logger.info(
                    f"ğŸ”¶ [INIT-6] Phase H.13: ACCEPTABLE data for ATR calculation: {data_count} records >= {min_records_acceptable} (adequate)"
                )
                logger.info(
                    "ğŸ“Š [INIT-6] Phase H.13: ATR calculation possible but with reduced precision"
                )
            elif data_count >= min_records_minimum:
                logger.warning(
                    f"âš ï¸ [INIT-6] Phase H.13: MINIMAL data for ATR calculation: {data_count} records >= {min_records_minimum} (minimum)"
                )
                logger.warning(
                    "âš ï¸ [INIT-6] Phase H.13: ATR calculation possible but precision may be limited"
                )
            else:
                logger.error(
                    f"âŒ [INIT-6] Phase H.13: INSUFFICIENT data for ATR calculation: "
                    f"{data_count} < {min_records_minimum} (absolute minimum)"
                )
                logger.error(
                    "âŒ [INIT-6] Phase H.13: Cannot proceed with ATR calculation"
                )
                return None

            logger.info(
                f"ğŸ“Š [INIT-6] Phase H.13: Data validation passed with {data_count} records (period={period})"
            )

            # ATRè¨ˆç®—å®Ÿè¡Œï¼ˆPhase H.13: å¼·åŒ–ç‰ˆãƒ»nanå€¤é˜²æ­¢ãƒ»å“è³ªä¿è¨¼ï¼‰
            from crypto_bot.indicator.calculator import IndicatorCalculator

            calculator = IndicatorCalculator()

            start_time = time.time()
            atr_series = calculator.calculate_atr(initial_df, period=period)
            calc_time = time.time() - start_time

            # Phase H.13: ATRè¨ˆç®—çµæœã®åŒ…æ‹¬çš„æ¤œè¨¼
            if atr_series is not None and not atr_series.empty:
                # nanå€¤ãƒã‚§ãƒƒã‚¯ï¼ˆPhase H.13: æœ€é‡è¦æ¤œè¨¼ï¼‰
                nan_count = atr_series.isna().sum()
                if nan_count > 0:
                    logger.warning(
                        f"âš ï¸ [INIT-6] Phase H.13: ATR contains {nan_count} nan values"
                    )
                    # nanå€¤ã‚’é™¤å»ã—ã¦æœ‰åŠ¹å€¤ã®ã¿ä½¿ç”¨
                    atr_series = atr_series.dropna()
                    if atr_series.empty:
                        logger.error(
                            "âŒ [INIT-6] Phase H.13: All ATR values are nan after cleaning"
                        )
                        atr_series = None
                    else:
                        logger.info(
                            f"âœ… [INIT-6] Phase H.13: ATR cleaned, {len(atr_series)} valid values remain"
                        )

                if atr_series is not None and not atr_series.empty:
                    latest_atr = atr_series.iloc[-1]
                    mean_atr = atr_series.mean()
                    std_atr = atr_series.std()

                    logger.info(
                        f"âœ… [INIT-6] Phase H.13: ATR calculated successfully: {len(atr_series)} values in {calc_time:.2f}s"
                    )
                    logger.info(
                        f"ğŸ“Š [INIT-6] Phase H.13: ATR statistics: latest={latest_atr:.6f}, mean={mean_atr:.6f}, std={std_atr:.6f}"
                    )

                    # Phase H.13: ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯å¼·åŒ–
                    if pd.isna(latest_atr):
                        logger.error(
                            "âŒ [INIT-6] Phase H.13: Latest ATR is nan - this will cause trading failure"
                        )
                        atr_series = None
                    elif latest_atr <= 0:
                        logger.error(
                            f"âŒ [INIT-6] Phase H.13: Latest ATR is zero or negative: {latest_atr}"
                        )
                        atr_series = None
                    elif latest_atr > 1.0:
                        logger.warning(
                            f"âš ï¸ [INIT-6] Phase H.13: Latest ATR unusually high: {latest_atr} (>1.0)"
                        )
                        # é«˜ã™ãã‚‹å€¤ã§ã‚‚ä½¿ç”¨å¯èƒ½ã¨ã™ã‚‹
                    else:
                        logger.info(
                            f"âœ… [INIT-6] Phase H.13: ATR value quality check passed: {latest_atr:.6f}"
                        )
            else:
                logger.error(
                    "âŒ [INIT-6] Phase H.13: ATR calculation returned empty or None series"
                )
                atr_series = None

        except Exception as e:
            logger.error(f"âŒ [INIT-6] ATR calculation failed: {e}")
            logger.error(f"âŒ [INIT-6] Error type: {type(e).__name__}")
            atr_series = None

    else:
        logger.warning("âš ï¸ [INIT-6] No initial data available for ATR calculation")

    return atr_series


def enhanced_init_6_fallback_atr(
    period: int = 14, market_context: str = "BTC/JPY"
) -> pd.Series:
    """
    INIT-6æ®µéš: ATRãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤ç”Ÿæˆï¼ˆPhase H.9.3å¼·åŒ–ç‰ˆï¼‰

    Args:
        period: ATRæœŸé–“
        market_context: å¸‚å ´ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆBTC/JPYç­‰ï¼‰

    Returns:
        Series: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ATRå€¤
    """
    logger.info(
        "ğŸ”§ [INIT-6] Phase H.9.3: Using enhanced adaptive fallback ATR calculation..."
    )
    logger.info(f"ğŸ”§ [INIT-6] Market context: {market_context}, Period: {period}")

    # ã‚ˆã‚Šç¾å®Ÿçš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤ã‚’ç”Ÿæˆ
    # æš—å·è³‡ç”£ã®å…¸å‹çš„ãªATRå€¤: 0.005-0.02 (0.5%-2%)
    base_atr = 0.01  # 1%

    # æ™‚ç³»åˆ—çš„ã«å¤‰åŒ–ã™ã‚‹ATRå€¤ã‚’ç”Ÿæˆï¼ˆã‚ˆã‚Šç¾å®Ÿçš„ï¼‰
    import numpy as np

    np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
    atr_values = []

    for _ in range(period):
        # åŸºæœ¬å€¤ã«å°ã•ãªå¤‰å‹•ã‚’åŠ ãˆã‚‹
        variation = np.random.normal(0, 0.001)  # 0.1%ã®æ¨™æº–åå·®
        atr_value = max(0.005, base_atr + variation)  # æœ€å°0.5%
        atr_values.append(atr_value)

    atr_series = pd.Series(atr_values)
    latest_atr = atr_series.iloc[-1]

    logger.info(
        f"âœ… [INIT-6] Enhanced fallback ATR generated: " f"{len(atr_series)} values"
    )
    logger.info(
        f"ğŸ“Š [INIT-6] Fallback ATR statistics: "
        f"latest={latest_atr:.6f}, mean={atr_series.mean():.6f}"
    )

    return atr_series


@with_resilience("init_system", "init_7_initialize_entry_exit")
def enhanced_init_7_initialize_entry_exit(
    strategy, risk_manager, atr_series: pd.Series
) -> Any:
    """
    INIT-7æ®µéš: Entry/Exitã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆå¼·åŒ–ç‰ˆãƒ»Phase H.8.4ï¼‰

    Args:
        strategy: å–å¼•æˆ¦ç•¥
        risk_manager: ãƒªã‚¹ã‚¯ç®¡ç†
        atr_series: ATRå€¤

    Returns:
        EntryExit: Entry/Exitã‚·ã‚¹ãƒ†ãƒ 
    """
    logger.info("ğŸ¯ [INIT-7] Initializing Entry/Exit system...")
    logger.info(f"â° [INIT-7] Timestamp: {pd.Timestamp.now()}")
    logger.info("ğŸ›¡ï¸ [PHASE-H8.4] INIT-7 with enhanced error resilience")

    try:
        # ä¾å­˜é–¢ä¿‚ã®ç¢ºèª
        from crypto_bot.execution.engine import EntryExit

        # ATRå€¤ã®æœ€çµ‚ç¢ºèª
        if atr_series is None or atr_series.empty:
            logger.error("âŒ [INIT-7] ATR series is None or empty")
            raise ValueError("ATR series is required for Entry/Exit initialization")

        latest_atr = atr_series.iloc[-1]
        logger.info(f"ğŸ“Š [INIT-7] Using ATR value: {latest_atr:.6f}")

        # Entry/Exitã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        entry_exit = EntryExit(
            strategy=strategy, risk_manager=risk_manager, atr_series=atr_series
        )

        logger.info("âœ… [INIT-7] Entry/Exit system initialized successfully")
        return entry_exit

    except Exception as e:
        logger.error(f"âŒ [INIT-7] Entry/Exit system initialization failed: {e}")
        raise


def enhanced_init_8_clear_cache() -> None:
    """
    INIT-8æ®µéš: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    """
    logger.info("ğŸ§¹ [INIT-8] Clearing old cache for fresh data...")
    logger.info(f"â° [INIT-8] Timestamp: {pd.Timestamp.now()}")

    try:
        from crypto_bot.ml.external_data_cache import (
            clear_global_cache,
            get_global_cache,
        )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ³ã®ç¢ºèª
        cache = get_global_cache()
        cache_info = cache.get_cache_info()
        logger.info(f"ğŸ“Š [INIT-8] Cache info before clear: {cache_info}")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å®Ÿè¡Œ
        clear_global_cache()

        # ã‚¯ãƒªã‚¢å¾Œã®ç¢ºèª
        cache_info_after = cache.get_cache_info()
        logger.info(f"ğŸ“Š [INIT-8] Cache info after clear: {cache_info_after}")

        logger.info("âœ… [INIT-8] Cache cleared successfully")

    except Exception as e:
        logger.error(f"âŒ [INIT-8] Cache clear failed: {e}")
        logger.warning("âš ï¸ [INIT-8] Continuing without cache clear...")


# ä½¿ç”¨ä¾‹ï¼ˆmain.pyã§ã®ç½®ãæ›ãˆç”¨ï¼‰
def enhanced_init_sequence(
    fetcher, dd: dict, strategy, risk_manager, balance: float, prefetch_data=None
):
    """
    INIT-5ï½INIT-8ã®å¼·åŒ–ç‰ˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼ˆPhase H.13: ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰

    Args:
        fetcher: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼
        dd: ãƒ‡ãƒ¼ã‚¿è¨­å®š
        strategy: å–å¼•æˆ¦ç•¥
        risk_manager: ãƒªã‚¹ã‚¯ç®¡ç†
        balance: åˆæœŸæ®‹é«˜
        prefetch_data: ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ‡ãƒ¼ã‚¿ï¼ˆPhase H.13: ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—å…±æœ‰ç”¨ï¼‰

    Returns:
        tuple: (entry_exit, position)
    """
    logger.info("ğŸš€ [INIT-ENHANCED] Starting enhanced initialization sequence...")

    # Phase H.13: ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®çŠ¶æ³ç¢ºèª
    if prefetch_data is not None and not prefetch_data.empty:
        logger.info(
            f"ğŸ“Š [INIT-ENHANCED] Phase H.13: Using prefetched data with {len(prefetch_data)} records"
        )
        logger.info(
            f"ğŸ“ˆ [INIT-ENHANCED] Prefetch data range: {prefetch_data.index.min()} to {prefetch_data.index.max()}"
        )
    else:
        logger.info(
            "ğŸ”„ [INIT-ENHANCED] Phase H.13: No prefetch data, using independent fetch"
        )

    # INIT-5: åˆæœŸä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå¼·åŒ–ç‰ˆãƒ»Phase H.13: ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ‡ãƒ¼ã‚¿çµ±åˆï¼‰
    initial_df = enhanced_init_5_fetch_price_data(
        fetcher, dd, prefetch_data=prefetch_data
    )

    # INIT-6: ATRè¨ˆç®—ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    atr_series = enhanced_init_6_calculate_atr(initial_df)

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼ˆPhase H.9.3: é©å¿œçš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    if atr_series is None or atr_series.empty:
        logger.info(
            "ğŸ”§ [INIT-6] Phase H.9.3: Using enhanced adaptive fallback ATR calculation"
        )
        symbol = dd.get("symbol", "BTC/JPY")
        atr_series = enhanced_init_6_fallback_atr(market_context=symbol)

    # INIT-7: Entry/ExitåˆæœŸåŒ–ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    entry_exit = enhanced_init_7_initialize_entry_exit(
        strategy, risk_manager, atr_series
    )
    entry_exit.current_balance = balance

    # INIT-8: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    enhanced_init_8_clear_cache()

    # INIT-9: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆPhase Hå•é¡Œè§£æ±ºï¼‰
    logger.info("ğŸ¤– [INIT-9] Training ensemble models...")
    logger.info(f"â° [INIT-9] Timestamp: {pd.Timestamp.now()}")

    if hasattr(strategy, "fit_ensemble_models"):
        try:
            # å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            if initial_df is not None and len(initial_df) >= 50:
                logger.info(
                    f"ğŸ“Š [INIT-9] Preparing training data from {len(initial_df)} records"
                )

                # ç°¡æ˜“çš„ãªãƒ©ãƒ™ãƒ«ç”Ÿæˆï¼ˆå°†æ¥ã®ä¾¡æ ¼å¤‰å‹•ã‹ã‚‰ï¼‰
                price_change = (
                    initial_df["close"].pct_change().shift(-1)
                )  # æ¬¡ã®æœŸé–“ã®ä¾¡æ ¼å¤‰å‹•
                y = (price_change > 0).astype(int)  # ä¸Šæ˜‡=1, ä¸‹é™=0
                y = y.dropna()

                # ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ç¢ºä¿
                train_df = initial_df.iloc[:-1]  # æœ€å¾Œã®è¡Œã‚’é™¤å¤–ï¼ˆãƒ©ãƒ™ãƒ«ãŒãªã„ãŸã‚ï¼‰

                if len(train_df) >= 50:
                    logger.info(
                        f"ğŸ¯ [INIT-9] Training ensemble models with {len(train_df)} samples"
                    )
                    strategy.fit_ensemble_models(train_df, y)
                    logger.info("âœ… [INIT-9] Ensemble models trained successfully")

                    # ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ã®ç¢ºèª
                    if hasattr(strategy, "timeframe_processors"):
                        for tf, processor in strategy.timeframe_processors.items():
                            if processor:
                                logger.info(
                                    f"ğŸ“Š [INIT-9] {tf} processor fitted: {processor.is_fitted}"
                                )
                else:
                    logger.warning(
                        f"âš ï¸ [INIT-9] Insufficient data for training: {len(train_df)} records (need 50+)"
                    )
            else:
                logger.warning(
                    "âš ï¸ [INIT-9] No initial data available for model training"
                )
                logger.info(
                    "ğŸ”„ [INIT-9] Models will use fallback strategies until sufficient data is collected"
                )
        except Exception as e:
            logger.error(f"âŒ [INIT-9] Ensemble model training failed: {e}")
            logger.info("ğŸ”„ [INIT-9] Continuing with untrained models (fallback mode)")
    else:
        logger.info("â„¹ï¸ [INIT-9] Strategy does not support ensemble model training")

    # PositionåˆæœŸåŒ–
    from crypto_bot.execution.engine import Position

    position = Position()

    logger.info(
        "âœ… [INIT-ENHANCED] Enhanced initialization sequence completed successfully"
    )

    return entry_exit, position
