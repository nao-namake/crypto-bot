"""
Phase 12.3: ãƒ­ãƒ¼ã‚«ãƒ«äº‹å‰è¨ˆç®—ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
14æ™‚é–“ã‚¼ãƒ­ãƒˆãƒ¬ãƒ¼ãƒ‰å•é¡Œã®æ ¹æœ¬è§£æ±ºã®ãŸã‚ã€é‡ã„è¨ˆç®—ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§äº‹å‰å®Ÿè¡Œ
"""

import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict

import pandas as pd

logger = logging.getLogger(__name__)


class PreComputedCache:
    """äº‹å‰è¨ˆç®—ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†"""

    def __init__(self, cache_dir: str = "cache"):
        """
        Args:
            cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.market_data_path = self.cache_dir / "market_data_cache.pkl"
        self.features_path = self.cache_dir / "features_97_cache.pkl"
        self.technical_path = self.cache_dir / "technical_cache.json"
        self.metadata_path = self.cache_dir / "cache_metadata.json"

    def has_valid_cache(self, max_age_hours: int = 24) -> bool:
        """æœ‰åŠ¹ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        if not self.metadata_path.exists():
            return False

        try:
            with open(self.metadata_path, "r") as f:
                metadata = json.load(f)

            created_at = datetime.fromisoformat(metadata["created_at"])
            age = datetime.now() - created_at

            if age > timedelta(hours=max_age_hours):
                logger.info(
                    f"â° Cache is too old: {age.total_seconds()/3600:.1f} hours"
                )
                return False

            # å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒå…¨ã¦å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            required_files = [
                self.market_data_path,
                self.features_path,
                self.technical_path,
            ]

            for file_path in required_files:
                if not file_path.exists():
                    logger.warning(f"âŒ Missing cache file: {file_path}")
                    return False

            return True

        except Exception as e:
            logger.error(f"âŒ Cache validation error: {e}")
            return False

    def save_market_data(self, data: pd.DataFrame) -> None:
        """å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            data.to_pickle(self.market_data_path)
            logger.info(f"âœ… Saved market data: {len(data)} records")
        except Exception as e:
            logger.error(f"âŒ Failed to save market data: {e}")

    def save_features(self, features: pd.DataFrame) -> None:
        """97ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            features.to_pickle(self.features_path)
            logger.info(f"âœ… Saved features: shape={features.shape}")
        except Exception as e:
            logger.error(f"âŒ Failed to save features: {e}")

    def save_technical(self, technical: Dict[str, Any]) -> None:
        """ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’ä¿å­˜"""
        try:
            # numpy/pandaså‹ã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
            serializable = {}
            for key, value in technical.items():
                if hasattr(value, "tolist"):
                    serializable[key] = value.tolist()
                elif isinstance(value, pd.Series):
                    serializable[key] = value.to_list()
                else:
                    serializable[key] = value

            with open(self.technical_path, "w") as f:
                json.dump(serializable, f, indent=2)
            logger.info(f"âœ… Saved technical indicators: {list(technical.keys())}")
        except Exception as e:
            logger.error(f"âŒ Failed to save technical indicators: {e}")

    def save_metadata(self) -> None:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        metadata = {
            "created_at": datetime.now().isoformat(),
            "version": "1.0",
            "phase": "12.3",
        }

        try:
            with open(self.metadata_path, "w") as f:
                json.dump(metadata, f, indent=2)
            logger.info("âœ… Saved cache metadata")
        except Exception as e:
            logger.error(f"âŒ Failed to save metadata: {e}")

    def load_all(self) -> Dict[str, Any]:
        """å…¨ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        result = {}

        try:
            # å¸‚å ´ãƒ‡ãƒ¼ã‚¿
            if self.market_data_path.exists():
                result["market_data"] = pd.read_pickle(self.market_data_path)
                logger.info(
                    f"âœ… Loaded market data: {len(result['market_data'])} records"
                )

            # ç‰¹å¾´é‡
            if self.features_path.exists():
                result["features"] = pd.read_pickle(self.features_path)
                logger.info(f"âœ… Loaded features: shape={result['features'].shape}")

            # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
            if self.technical_path.exists():
                with open(self.technical_path, "r") as f:
                    result["technical"] = json.load(f)
                logger.info(f"âœ… Loaded technical: {list(result['technical'].keys())}")

        except Exception as e:
            logger.error(f"âŒ Failed to load cache: {e}")

        return result

    def clear_cache(self) -> None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        cache_files = [
            self.market_data_path,
            self.features_path,
            self.technical_path,
            self.metadata_path,
        ]

        for file_path in cache_files:
            if file_path.exists():
                file_path.unlink()
                logger.info(f"ğŸ—‘ï¸ Deleted: {file_path}")
