# =============================================================================
# ãƒ•ã‚¡ã‚¤ãƒ«å: crypto_bot/data/multi_timeframe_fetcher.py
# èª¬æ˜:
# ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å¯¾å¿œãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼
# ãƒ»è¤‡æ•°ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆ15m/1h/4hï¼‰ã®åŠ¹ç‡çš„ãªå–å¾—ãƒ»å¤‰æ›
# ãƒ»1æ™‚é–“è¶³ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä»–ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã¸ã®è£œé–“ãƒ»é›†ç´„
# ãƒ»ãƒ‡ãƒ¼ã‚¿åŒæœŸãƒ»å“è³ªç®¡ç†ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½çµ±åˆ
# ãƒ»Phase 2.1: ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åŸºç›¤å®Ÿè£…
# =============================================================================

import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Union

import numpy as np
import pandas as pd
from scipy import interpolate

from .fetcher import MarketDataFetcher
from .timeframe_synchronizer import TimeframeSynchronizer

logger = logging.getLogger(__name__)


class MultiTimeframeDataFetcher:
    """ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å¯¾å¿œãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼"""

    def __init__(
        self,
        base_fetcher: MarketDataFetcher = None,
        config: dict = None,
        timeframes: List[str] = None,
        base_timeframe: str = "1h",
        cache_enabled: bool = True,
        data_quality_threshold: float = 0.9,
        synchronization_enabled: bool = True,
        sync_tolerance_minutes: int = 1,
    ):
        """
        åˆæœŸåŒ–

        Args:
            base_fetcher: ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹MarketDataFetcher
            config: è¨­å®šè¾æ›¸ï¼ˆbase_fetcherãŒNoneã®å ´åˆä½¿ç”¨ï¼‰
            timeframes: å¯¾è±¡ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆä¾‹: ["15m", "1h", "4h"]ï¼‰
            base_timeframe: ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆãƒ‡ãƒ¼ã‚¿å–å¾—ç”¨ï¼‰
            cache_enabled: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹åŒ–
            data_quality_threshold: ãƒ‡ãƒ¼ã‚¿å“è³ªé–¾å€¤
            synchronization_enabled: åŒæœŸæ©Ÿèƒ½æœ‰åŠ¹åŒ–
            sync_tolerance_minutes: åŒæœŸè¨±å®¹èª¤å·®ï¼ˆåˆ†ï¼‰
        """
        # configã‹ã‚‰ã®è¨­å®šèª­ã¿å–ã‚Šï¼ˆPhase H.2.2: 4hå¾©æ´»å¯¾å¿œï¼‰
        if config and "multi_timeframe" in config:
            mtf_config = config["multi_timeframe"]
            timeframes = timeframes or mtf_config.get("timeframes", ["15m", "1h", "4h"])
            synchronization_enabled = mtf_config.get(
                "data_sync_enabled", synchronization_enabled
            )
            # ãƒ‡ãƒ¼ã‚¿å“è³ªé–¾å€¤ã‚‚è¨­å®šã‹ã‚‰èª­ã¿å–ã‚‹
            data_quality_threshold = mtf_config.get(
                "data_quality_threshold", data_quality_threshold
            )
            
        # multi_timeframe_dataè¨­å®šã‚‚èª­ã¿å–ã‚Šï¼ˆPhase H.2.2: target_timeframeså¯¾å¿œï¼‰
        if config and "data" in config and "multi_timeframe_data" in config["data"]:
            mtf_data_config = config["data"]["multi_timeframe_data"]
            # base_timeframeã®ç¢ºèª
            config_base_timeframe = mtf_data_config.get("base_timeframe", "1h")
            if config_base_timeframe != base_timeframe:
                logger.info(f"Using config base_timeframe: {config_base_timeframe}")
                base_timeframe = config_base_timeframe
            
            # target_timeframesãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
            target_timeframes_config = mtf_data_config.get("target_timeframes", {})
            if target_timeframes_config:
                # è¨­å®šã•ã‚ŒãŸã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿ã‚’ä½¿ç”¨
                configured_timeframes = list(target_timeframes_config.keys())
                if configured_timeframes:
                    timeframes = configured_timeframes
                    logger.info(f"Using configured target timeframes: {timeframes}")

        self.base_fetcher = base_fetcher
        self.config = config
        self.timeframes = timeframes or ["15m", "1h", "4h"]
        self.base_timeframe = base_timeframe
        self.cache_enabled = cache_enabled
        self.data_quality_threshold = data_quality_threshold
        self.synchronization_enabled = synchronization_enabled

        # TimeframeSynchronizeråˆæœŸåŒ–ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
        if self.synchronization_enabled:
            try:
                self.synchronizer = TimeframeSynchronizer(
                    timeframes=self.timeframes,
                    base_timeframe=self.base_timeframe,
                    sync_tolerance=timedelta(minutes=sync_tolerance_minutes),
                    missing_data_threshold=min(1.0 - self.data_quality_threshold, 0.5),
                    consistency_check_enabled=True,
                )
                logger.info(
                    f"  - Synchronizer initialized: {self.synchronizer is not None}"
                )
            except Exception as e:
                logger.warning(f"âš ï¸ TimeframeSynchronizer initialization failed: {e}")
                logger.info("  - Falling back to basic synchronizer")
                self.synchronizer = TimeframeSynchronizer()
        else:
            self.synchronizer = None

        # ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.data_cache: Dict[str, pd.DataFrame] = {}
        self.cache_timestamps: Dict[str, datetime] = {}
        self.cache_ttl = timedelta(minutes=5)  # 5åˆ†é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥

        # ãƒ‡ãƒ¼ã‚¿å“è³ªçµ±è¨ˆ
        self.quality_stats = {
            "fetch_count": 0,
            "cache_hits": 0,
            "interpolation_count": 0,
            "aggregation_count": 0,
            "quality_failures": 0,
            "synchronization_count": 0,
        }

        logger.info("ğŸ”„ MultiTimeframeDataFetcher initialized")
        logger.info(f"  - Timeframes: {self.timeframes}")
        logger.info(f"  - Base timeframe: {self.base_timeframe}")
        logger.info(f"  - Cache enabled: {self.cache_enabled}")
        logger.info(f"  - Data quality threshold: {self.data_quality_threshold}")
        logger.info(f"  - Synchronization enabled: {self.synchronization_enabled}")

    def get_multi_timeframe_data(
        self,
        since: Optional[Union[str, datetime]] = None,
        limit: Optional[int] = None,
        force_refresh: bool = False,
    ) -> Dict[str, pd.DataFrame]:
        """
        è¤‡æ•°ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿ã‚’åŒæ™‚å–å¾—

        Args:
            since: é–‹å§‹æ™‚åˆ»
            limit: å–å¾—ä»¶æ•°
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¼·åˆ¶æ›´æ–°

        Returns:
            Dict[timeframe, DataFrame]: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥ãƒ‡ãƒ¼ã‚¿
        """
        try:
            logger.info(f"ğŸ”„ Fetching multi-timeframe data: {self.timeframes}")
            self.quality_stats["fetch_count"] += 1

            # ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—
            base_data = self._get_base_data(since, limit, force_refresh)
            if base_data.empty:
                logger.error("âŒ Failed to fetch base data")
                return {}

            # å„ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›
            multi_data = {}
            for timeframe in self.timeframes:
                try:
                    data = self._convert_to_timeframe(base_data, timeframe)
                    if not data.empty:
                        quality_score = self._assess_data_quality(data, timeframe)
                        if quality_score >= self.data_quality_threshold:
                            multi_data[timeframe] = data
                            logger.info(
                                f"âœ… {timeframe}: {len(data)} records "
                                f"(quality: {quality_score:.3f})"
                            )
                        else:
                            logger.warning(
                                f"âš ï¸ {timeframe}: Quality too low "
                                f"({quality_score:.3f} < {self.data_quality_threshold})"
                            )
                            self.quality_stats["quality_failures"] += 1
                    else:
                        logger.warning(f"âš ï¸ {timeframe}: No data after conversion")

                except Exception as e:
                    logger.error(f"âŒ Failed to convert to {timeframe}: {e}")
                    continue

            # Phase 2.2: ãƒ‡ãƒ¼ã‚¿åŒæœŸãƒ»çµ±åˆã‚·ã‚¹ãƒ†ãƒ é©ç”¨
            if self.synchronization_enabled and self.synchronizer and multi_data:
                logger.info("ğŸ”„ Applying timeframe synchronization")
                synchronized_data = self.synchronizer.synchronize_multi_timeframe_data(
                    multi_data
                )
                self.quality_stats["synchronization_count"] += 1

                logger.info(
                    f"âœ… Multi-timeframe fetch + synchronization complete: "
                    f"{len(synchronized_data)} timeframes"
                )
                return synchronized_data
            else:
                logger.info(
                    f"âœ… Multi-timeframe fetch complete: {len(multi_data)} timeframes"
                )
                return multi_data

        except Exception as e:
            logger.error(f"âŒ Multi-timeframe data fetch failed: {e}")
            return {}

    def _get_base_data(
        self,
        since: Optional[Union[str, datetime]],
        limit: Optional[int],
        force_refresh: bool,
    ) -> pd.DataFrame:
        """ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œãƒ»ãƒ†ã‚¹ãƒˆç”¨ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
        cache_key = f"{self.base_timeframe}_base"
        current_time = datetime.now()

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if (
            not force_refresh
            and self.cache_enabled
            and cache_key in self.data_cache
            and cache_key in self.cache_timestamps
            and current_time - self.cache_timestamps[cache_key] < self.cache_ttl
        ):
            logger.info(f"ğŸ“‹ Using cached base data: {self.base_timeframe}")
            self.quality_stats["cache_hits"] += 1
            return self.data_cache[cache_key]

        # base_fetcherãŒNoneã®å ´åˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        if self.base_fetcher is None:
            logger.info(
                f"ğŸ§ª Generating test data for {self.base_timeframe} (no base_fetcher)"
            )
            data = self._generate_test_data(limit or 100)

            if not data.empty:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
                if self.cache_enabled:
                    self.data_cache[cache_key] = data
                    self.cache_timestamps[cache_key] = current_time
                logger.info(f"âœ… Test base data generated: {len(data)} records")
                return data
            else:
                logger.error("âŒ Failed to generate test data")
                return pd.DataFrame()

        # æ–°è¦ãƒ‡ãƒ¼ã‚¿å–å¾—
        try:
            logger.info(f"ğŸ”„ Fetching base data: {self.base_timeframe}")
            data = self.base_fetcher.get_price_df(
                timeframe=self.base_timeframe,
                since=since,
                limit=limit,
                paginate=True,
            )

            if not data.empty:
                # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
                data = self._preprocess_data(data)

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
                if self.cache_enabled:
                    self.data_cache[cache_key] = data
                    self.cache_timestamps[cache_key] = current_time

                logger.info(f"âœ… Base data fetched: {len(data)} records")
                return data
            else:
                logger.error("âŒ No base data received")
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"âŒ Base data fetch failed: {e}")
            return pd.DataFrame()

    def _convert_to_timeframe(
        self, base_data: pd.DataFrame, target_timeframe: str
    ) -> pd.DataFrame:
        """ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›ï¼ˆPhase H.2.2: è¨­å®šãƒ™ãƒ¼ã‚¹å¼·åŒ–ãƒ»4hå¾©æ´»å¯¾å¿œï¼‰"""
        if target_timeframe == self.base_timeframe:
            return base_data.copy()

        try:
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¤‰æ›æ–¹æ³•ã‚’å–å¾—
            conversion_method = self._get_conversion_method(target_timeframe)
            
            logger.debug(f"Converting {self.base_timeframe} â†’ {target_timeframe} using method: {conversion_method}")

            if conversion_method == "interpolation" and target_timeframe == "15m":
                return self._interpolate_to_15m(base_data)
            elif conversion_method == "aggregation" and target_timeframe == "4h":
                return self._aggregate_to_4h(base_data)
            elif conversion_method == "direct":
                return base_data.copy()
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ–¹å¼
                if target_timeframe == "15m" and self.base_timeframe == "1h":
                    return self._interpolate_to_15m(base_data)
                elif target_timeframe == "4h" and self.base_timeframe == "1h":
                    return self._aggregate_to_4h(base_data)
                else:
                    logger.warning(
                        f"âš ï¸ Unsupported conversion: "
                        f"{self.base_timeframe} â†’ {target_timeframe}"
                    )
                    return pd.DataFrame()

        except Exception as e:
            logger.error(f"âŒ Timeframe conversion failed: {e}")
            return pd.DataFrame()
    
    def _get_conversion_method(self, target_timeframe: str) -> str:
        """è¨­å®šã‹ã‚‰å¤‰æ›æ–¹æ³•ã‚’å–å¾—ï¼ˆPhase H.2.2ï¼‰"""
        try:
            if (self.config and 
                "data" in self.config and 
                "multi_timeframe_data" in self.config["data"]):
                
                target_config = self.config["data"]["multi_timeframe_data"].get("target_timeframes", {})
                tf_config = target_config.get(target_timeframe, {})
                method = tf_config.get("method", "auto")
                
                if method != "auto":
                    return method
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ–¹å¼
            if target_timeframe == "15m":
                return "interpolation"
            elif target_timeframe == "4h":
                return "aggregation"
            else:
                return "direct"
                
        except Exception as e:
            logger.warning(f"Failed to get conversion method for {target_timeframe}: {e}")
            return "auto"

    def _interpolate_to_15m(self, hourly_data: pd.DataFrame) -> pd.DataFrame:
        """1æ™‚é–“è¶³â†’15åˆ†è¶³è£œé–“"""
        try:
            logger.debug("ğŸ”„ Interpolating 1h â†’ 15m")
            self.quality_stats["interpolation_count"] += 1

            if hourly_data.empty or len(hourly_data) < 2:
                return pd.DataFrame()

            # 15åˆ†é–“éš”ã®ã‚¿ã‚¤ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            start_time = hourly_data.index[0]
            end_time = hourly_data.index[-1]
            minute_15_index = pd.date_range(start=start_time, end=end_time, freq="15T")

            interpolated_data = pd.DataFrame(index=minute_15_index)

            # å„åˆ—ã‚’è£œé–“
            for col in ["open", "high", "low", "close", "volume"]:
                if col in hourly_data.columns:
                    if col == "volume":
                        # ãƒœãƒªãƒ¥ãƒ¼ãƒ ã¯ç·šå½¢åˆ†å‰²
                        interpolated_data[col] = (
                            hourly_data[col].reindex(minute_15_index, method="ffill")
                            / 4.0
                        )
                    elif col in ["high", "low"]:
                        # é«˜å€¤ãƒ»å®‰å€¤ã¯ç‰¹åˆ¥å‡¦ç†
                        interpolated_data[col] = self._interpolate_high_low(
                            hourly_data, col, minute_15_index
                        )
                    else:
                        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã¯ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³è£œé–“
                        if len(hourly_data) >= 4:
                            f = interpolate.interp1d(
                                hourly_data.index.astype(np.int64),
                                hourly_data[col].values,
                                kind="cubic",
                                bounds_error=False,
                                fill_value="extrapolate",
                            )
                            interpolated_data[col] = f(minute_15_index.astype(np.int64))
                        else:
                            # ãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚ã¯ç·šå½¢è£œé–“
                            interpolated_data[col] = (
                                hourly_data[col]
                                .reindex(minute_15_index)
                                .interpolate(method="linear")
                            )

            # ãƒ‡ãƒ¼ã‚¿å“è³ªæ”¹å–„
            interpolated_data = self._improve_interpolated_data(interpolated_data)

            logger.debug(f"âœ… 15m interpolation: {len(interpolated_data)} records")
            return interpolated_data

        except Exception as e:
            logger.error(f"âŒ 15m interpolation failed: {e}")
            return pd.DataFrame()

    def _interpolate_high_low(
        self, hourly_data: pd.DataFrame, col: str, target_index: pd.DatetimeIndex
    ) -> pd.Series:
        """é«˜å€¤ãƒ»å®‰å€¤ã®ç‰¹åˆ¥è£œé–“å‡¦ç†"""
        try:
            # åŸºæœ¬è£œé–“
            base_series = (
                hourly_data[col].reindex(target_index).interpolate(method="linear")
            )

            # é«˜å€¤ãƒ»å®‰å€¤ã®ç¾å®Ÿæ€§èª¿æ•´
            close_series = (
                hourly_data["close"].reindex(target_index).interpolate(method="linear")
            )

            if col == "high":
                # é«˜å€¤ã¯çµ‚å€¤ã‚ˆã‚Šä½ããªã‚‰ãªã„ã‚ˆã†èª¿æ•´
                return np.maximum(base_series, close_series * 1.001)
            else:  # low
                # å®‰å€¤ã¯çµ‚å€¤ã‚ˆã‚Šé«˜ããªã‚‰ãªã„ã‚ˆã†èª¿æ•´
                return np.minimum(base_series, close_series * 0.999)

        except Exception:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯åŸºæœ¬è£œé–“
            return hourly_data[col].reindex(target_index).interpolate(method="linear")

    def _aggregate_to_4h(self, hourly_data: pd.DataFrame) -> pd.DataFrame:
        """1æ™‚é–“è¶³â†’4æ™‚é–“è¶³é›†ç´„"""
        try:
            logger.debug("ğŸ”„ Aggregating 1h â†’ 4h")
            self.quality_stats["aggregation_count"] += 1

            if hourly_data.empty:
                return pd.DataFrame()

            # 4æ™‚é–“å˜ä½ã§ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            aggregated = hourly_data.resample("4h").agg(
                {
                    "open": "first",
                    "high": "max",
                    "low": "min",
                    "close": "last",
                    "volume": "sum",
                }
            )

            # æ¬ æãƒ‡ãƒ¼ã‚¿é™¤å»
            aggregated = aggregated.dropna()

            logger.debug(f"âœ… 4h aggregation: {len(aggregated)} records")
            return aggregated

        except Exception as e:
            logger.error(f"âŒ 4h aggregation failed: {e}")
            return pd.DataFrame()

    def _preprocess_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
        try:
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒDatetimeã§ãªã„å ´åˆã¯å¤‰æ›
            if not isinstance(data.index, pd.DatetimeIndex):
                if "timestamp" in data.columns:
                    data["timestamp"] = pd.to_datetime(data["timestamp"])
                    data = data.set_index("timestamp")
                elif "datetime" in data.columns:
                    data["datetime"] = pd.to_datetime(data["datetime"])
                    data = data.set_index("datetime")

            # é‡è¤‡å‰Šé™¤
            data = data[~data.index.duplicated(keep="last")]

            # ã‚½ãƒ¼ãƒˆ
            data = data.sort_index()

            # åŸºæœ¬çš„ãªå¤–ã‚Œå€¤é™¤å»ï¼ˆä¾¡æ ¼ã®ç•°å¸¸å€¤ï¼‰
            for col in ["open", "high", "low", "close"]:
                if col in data.columns:
                    # å‰æ—¥æ¯”Â±50%ã‚’è¶…ãˆã‚‹å¤‰åŒ–ã‚’ç•°å¸¸å€¤ã¨ã—ã¦é™¤å»
                    pct_change = data[col].pct_change().abs()
                    outliers = pct_change > 0.5
                    if outliers.any():
                        logger.warning(
                            f"âš ï¸ Removed {outliers.sum()} outliers from {col}"
                        )
                        data.loc[outliers, col] = np.nan
                        data[col] = data[col].interpolate(method="linear")

            return data

        except Exception as e:
            logger.error(f"âŒ Data preprocessing failed: {e}")
            return data

    def _improve_interpolated_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """è£œé–“ãƒ‡ãƒ¼ã‚¿ã®å“è³ªæ”¹å–„"""
        try:
            # OHLCé–¢ä¿‚ã®æ•´åˆæ€§ç¢ºä¿
            if all(col in data.columns for col in ["open", "high", "low", "close"]):
                # High >= max(Open, Close), Low <= min(Open, Close)
                data["high"] = np.maximum(
                    data["high"], np.maximum(data["open"], data["close"])
                )
                data["low"] = np.minimum(
                    data["low"], np.minimum(data["open"], data["close"])
                )

            # ãƒœãƒªãƒ¥ãƒ¼ãƒ ã®è² å€¤é™¤å»
            if "volume" in data.columns:
                data["volume"] = np.maximum(data["volume"], 0)

            return data

        except Exception as e:
            logger.error(f"âŒ Data improvement failed: {e}")
            return data

    def _assess_data_quality(self, data: pd.DataFrame, timeframe: str) -> float:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡"""
        if data.empty:
            return 0.0

        try:
            quality_factors = []

            # 1. ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ï¼ˆæ¬ æç‡ï¼‰
            total_cells = len(data) * len(data.columns)
            missing_cells = data.isnull().sum().sum()
            completeness = 1.0 - (missing_cells / total_cells)
            quality_factors.append(completeness * 0.3)

            # 2. ãƒ‡ãƒ¼ã‚¿é‡å……è¶³æ€§
            min_required = {"15m": 50, "1h": 100, "4h": 25}
            min_req = min_required.get(timeframe, 50)
            data_sufficiency = min(len(data) / min_req, 1.0)
            quality_factors.append(data_sufficiency * 0.3)

            # 3. OHLCæ•´åˆæ€§
            if all(col in data.columns for col in ["open", "high", "low", "close"]):
                ohlc_valid = (
                    (data["high"] >= data["open"])
                    & (data["high"] >= data["close"])
                    & (data["low"] <= data["open"])
                    & (data["low"] <= data["close"])
                    & (data["high"] >= data["low"])
                ).mean()
                quality_factors.append(ohlc_valid * 0.2)

            # 4. ä¾¡æ ¼å¤‰å‹•ã®å¦¥å½“æ€§
            if "close" in data.columns and len(data) > 1:
                price_changes = data["close"].pct_change().abs()
                reasonable_changes = (price_changes <= 0.1).mean()  # 10%ä»¥ä¸‹ã®å¤‰åŒ–ç‡
                quality_factors.append(reasonable_changes * 0.2)

            overall_quality = sum(quality_factors)
            return min(1.0, max(0.0, overall_quality))

        except Exception as e:
            logger.error(f"âŒ Quality assessment failed: {e}")
            return 0.5

    def get_cache_info(self) -> Dict:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»çµ±è¨ˆæƒ…å ±å–å¾—"""
        info = {
            "cache_enabled": self.cache_enabled,
            "cached_timeframes": list(self.data_cache.keys()),
            "cache_timestamps": {
                tf: ts.isoformat() for tf, ts in self.cache_timestamps.items()
            },
            "quality_stats": self.quality_stats.copy(),
            "synchronization_enabled": self.synchronization_enabled,
        }

        # åŒæœŸçµ±è¨ˆæƒ…å ±è¿½åŠ 
        if self.synchronizer:
            info["synchronization_stats"] = (
                self.synchronizer.get_synchronization_stats()
            )

        return info

    def clear_cache(self, timeframe: Optional[str] = None) -> None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        if timeframe:
            self.data_cache.pop(timeframe, None)
            self.cache_timestamps.pop(timeframe, None)
            logger.info(f"ğŸ—‘ï¸ Cleared cache for {timeframe}")
        else:
            self.data_cache.clear()
            self.cache_timestamps.clear()
            logger.info("ğŸ—‘ï¸ Cleared all cache")

    def _generate_test_data(self, n_records: int = 100) -> pd.DataFrame:
        """ãƒ†ã‚¹ãƒˆç”¨ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        try:
            import random
            from datetime import timedelta

            # æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”Ÿæˆ
            end_time = datetime.now().replace(minute=0, second=0, microsecond=0)
            timestamps = [end_time - timedelta(hours=i) for i in range(n_records)]
            timestamps.reverse()

            # ãƒ©ãƒ³ãƒ€ãƒ ãªOHLCVãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            base_price = 5000000  # BTC/JPYæƒ³å®š
            ohlcv_data = []

            for _ts in timestamps:
                # ä¾¡æ ¼å¤‰å‹•ç”Ÿæˆ
                price_change = random.gauss(0, 0.01) * 0.02
                base_price *= 1 + price_change

                # OHLCç”Ÿæˆ
                volatility = abs(random.gauss(0.005, 0.002))
                high = base_price * (1 + volatility)
                low = base_price * (1 - volatility)
                open_price = base_price + random.gauss(0, volatility * 0.5) * base_price
                close_price = base_price
                volume = abs(random.gauss(50, 20))

                ohlcv_data.append(
                    {
                        "open": open_price,
                        "high": high,
                        "low": low,
                        "close": close_price,
                        "volume": volume,
                    }
                )

            df = pd.DataFrame(ohlcv_data, index=timestamps)
            logger.info(f"ğŸ§ª Generated test data: {len(df)} records")
            return df

        except Exception as e:
            logger.error(f"âŒ Test data generation failed: {e}")
            return pd.DataFrame()

    def get_data_for_timeframe(
        self,
        timeframe: str,
        since: Optional[Union[str, datetime]] = None,
        limit: Optional[int] = None,
        force_refresh: bool = False,
    ) -> pd.DataFrame:
        """å˜ä¸€ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        multi_data = self.get_multi_timeframe_data(since, limit, force_refresh)
        return multi_data.get(timeframe, pd.DataFrame())
