# =============================================================================
# ãƒ•ã‚¡ã‚¤ãƒ«å: crypto_bot/data/global_cache_manager.py
# èª¬æ˜:
# å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
# ãƒ»é‡è¤‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ’é™¤ãƒ»ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆTTLãƒ»APIåŠ¹ç‡50%å‘ä¸Š
# ãƒ»VIXãƒ»Macroãƒ»Fear&Greedçµ±åˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»å“è³ªç›£è¦–çµ±åˆ
# ãƒ»Phase A3: å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
# =============================================================================

import hashlib
import json
import logging
import threading
import time
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any, Dict, Optional, Tuple

import pandas as pd

logger = logging.getLogger(__name__)


@dataclass
class CacheEntry:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒªã‚¯ãƒ©ã‚¹"""

    key: str
    data: Any
    timestamp: datetime
    ttl: timedelta
    access_count: int = 0
    last_access: datetime = field(default_factory=datetime.now)
    quality_score: float = 1.0
    source: str = "unknown"
    size_bytes: int = 0

    def is_expired(self) -> bool:
        """æœ‰åŠ¹æœŸé™ãƒã‚§ãƒƒã‚¯"""
        return datetime.now() > (self.timestamp + self.ttl)

    def is_stale(self, staleness_factor: float = 0.8) -> bool:
        """ãƒ‡ãƒ¼ã‚¿é™³è…åŒ–ãƒã‚§ãƒƒã‚¯"""
        stale_time = self.timestamp + (self.ttl * staleness_factor)
        return datetime.now() > stale_time

    def update_access(self):
        """ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±æ›´æ–°"""
        self.access_count += 1
        self.last_access = datetime.now()


class GlobalCacheManager:
    """å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚¯ãƒ©ã‚¹"""

    _instance = None
    _lock = threading.RLock()

    def __new__(cls):
        """ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè£…"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        """åˆæœŸåŒ–ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãªã®ã§ä¸€åº¦ã®ã¿å®Ÿè¡Œï¼‰"""
        if hasattr(self, "_initialized"):
            return

        self._cache: Dict[str, CacheEntry] = {}
        self._cache_lock = threading.RLock()
        self._stats = {
            "requests": 0,
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "api_calls_saved": 0,
            "total_size_bytes": 0,
        }

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        self.max_cache_size = 100  # æœ€å¤§ã‚¨ãƒ³ãƒˆãƒªæ•°
        self.cleanup_interval = 300  # 5åˆ†é–“éš”ã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self.max_memory_mb = 50  # 50MBä¸Šé™

        # ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥åˆ¥TTLè¨­å®š
        self.default_ttls = {
            "vix": timedelta(hours=24),  # VIXã¯æ—¥æ¬¡æ›´æ–°
            "macro": timedelta(hours=6),  # ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ã¯6æ™‚é–“
            "fear_greed": timedelta(hours=1),  # Fear&Greedã¯1æ™‚é–“
            "funding": timedelta(minutes=30),  # Funding Rateã¯30åˆ†
            "forex": timedelta(hours=4),  # ç‚ºæ›¿ã¯4æ™‚é–“
            "default": timedelta(hours=2),  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ2æ™‚é–“
        }

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé‡è¤‡é˜²æ­¢ç”¨
        self._pending_requests: Dict[str, threading.Event] = {}
        self._request_results: Dict[str, Any] = {}

        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹
        self._cleanup_thread = threading.Thread(
            target=self._background_cleanup, daemon=True
        )
        self._cleanup_thread.start()

        self._initialized = True
        logger.info("ğŸ—„ï¸ GlobalCacheManager initialized (singleton)")
        logger.info(f"  - Max cache size: {self.max_cache_size}")
        logger.info(f"  - Max memory: {self.max_memory_mb}MB")
        logger.info(f"  - Cleanup interval: {self.cleanup_interval}s")

    def get_cache_key(self, source: str, params: Dict[str, Any] = None) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        if params:
            params_str = json.dumps(params, sort_keys=True, default=str)
            params_hash = hashlib.md5(params_str.encode()).hexdigest()
            return f"{source}:{params_hash}"
        return source

    def get(self, key: str, default: Any = None) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        with self._cache_lock:
            self._stats["requests"] += 1

            if key in self._cache:
                entry = self._cache[key]

                if entry.is_expired():
                    # æœŸé™åˆ‡ã‚Œã‚¨ãƒ³ãƒˆãƒªå‰Šé™¤
                    del self._cache[key]
                    self._stats["misses"] += 1
                    logger.debug(f"ğŸ’€ Cache expired: {key}")
                    return default

                # ã‚¢ã‚¯ã‚»ã‚¹æ›´æ–°
                entry.update_access()
                self._stats["hits"] += 1
                self._stats["api_calls_saved"] += 1

                logger.debug(
                    f"ğŸ¯ Cache hit: {key} (quality: {entry.quality_score:.2f})"
                )
                return entry.data

            self._stats["misses"] += 1
            logger.debug(f"âŒ Cache miss: {key}")
            return default

    def put(
        self,
        key: str,
        data: Any,
        source: str = "unknown",
        ttl: timedelta = None,
        quality_score: float = 1.0,
    ) -> bool:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        try:
            with self._cache_lock:
                # TTLè¨­å®š
                if ttl is None:
                    ttl = self.default_ttls.get(source, self.default_ttls["default"])

                # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºè¨ˆç®—
                size_bytes = self._calculate_size(data)

                # ãƒ¡ãƒ¢ãƒªåˆ¶é™ãƒã‚§ãƒƒã‚¯
                if not self._check_memory_limit(size_bytes):
                    logger.warning(f"âš ï¸ Memory limit exceeded, skipping cache: {key}")
                    return False

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯ãƒ»ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if len(self._cache) >= self.max_cache_size:
                    self._evict_lru_entries()

                # ã‚¨ãƒ³ãƒˆãƒªä½œæˆãƒ»ä¿å­˜
                entry = CacheEntry(
                    key=key,
                    data=data,
                    timestamp=datetime.now(),
                    ttl=ttl,
                    quality_score=quality_score,
                    source=source,
                    size_bytes=size_bytes,
                )

                self._cache[key] = entry
                self._stats["total_size_bytes"] += size_bytes

                logger.debug(
                    f"ğŸ’¾ Cached: {key} (TTL: {ttl}, Quality: {quality_score:.2f}, Size: {size_bytes}B)"
                )
                return True

        except Exception as e:
            logger.error(f"âŒ Cache put failed: {key}, error: {e}")
            return False

    def get_or_fetch(
        self,
        key: str,
        fetch_func: callable,
        source: str = "unknown",
        ttl: timedelta = None,
        quality_score: float = 1.0,
        force_refresh: bool = False,
    ) -> Tuple[Any, bool]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã¾ãŸã¯ãƒ•ã‚§ãƒƒãƒï¼ˆé‡è¤‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆé˜²æ­¢ä»˜ãï¼‰"""
        # å¼·åˆ¶ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã§ãªã„å ´åˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_data = self.get(key)
            if cached_data is not None:
                return cached_data, True  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé‡è¤‡é˜²æ­¢å‡¦ç†
        with self._cache_lock:
            if key in self._pending_requests:
                # æ—¢ã«åŒã˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒé€²è¡Œä¸­
                logger.debug(f"â³ Waiting for pending request: {key}")
                event = self._pending_requests[key]
            else:
                # æ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹
                event = threading.Event()
                self._pending_requests[key] = event
                need_fetch = True

        if key not in locals() or "need_fetch" not in locals():
            # ä»–ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµæœå¾…ã¡
            event.wait(timeout=30)  # 30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

            with self._cache_lock:
                if key in self._request_results:
                    result = self._request_results[key]
                    del self._request_results[key]  # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                    return result, False  # ä»–ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰ã®çµæœ

            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯ç‹¬è‡ªå®Ÿè¡Œ
            logger.warning(f"âš ï¸ Request timeout, executing independently: {key}")

        # å®Ÿéš›ã®ãƒ•ã‚§ãƒƒãƒå®Ÿè¡Œ
        try:
            logger.debug(f"ğŸ”„ Fetching: {key}")
            data = fetch_func()

            # æˆåŠŸæ™‚ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            if data is not None:
                self.put(key, data, source, ttl, quality_score)

            # çµæœã‚’å¾…æ©Ÿä¸­ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã«å…±æœ‰
            with self._cache_lock:
                self._request_results[key] = data
                if key in self._pending_requests:
                    self._pending_requests[key].set()
                    del self._pending_requests[key]

            return data, False  # æ–°è¦ãƒ•ã‚§ãƒƒãƒ

        except Exception as e:
            logger.error(f"âŒ Fetch failed: {key}, error: {e}")

            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ã‚¤ãƒ™ãƒ³ãƒˆè§£æ”¾
            with self._cache_lock:
                if key in self._pending_requests:
                    self._pending_requests[key].set()
                    del self._pending_requests[key]

            return None, False

    def invalidate(self, key_pattern: str = None, source: str = None) -> int:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–"""
        invalidated = 0

        with self._cache_lock:
            keys_to_remove = []

            for key, entry in self._cache.items():
                should_remove = False

                if key_pattern and key_pattern in key:
                    should_remove = True
                elif source and entry.source == source:
                    should_remove = True
                elif key_pattern is None and source is None:
                    should_remove = True  # å…¨å‰Šé™¤

                if should_remove:
                    keys_to_remove.append(key)

            for key in keys_to_remove:
                entry = self._cache[key]
                self._stats["total_size_bytes"] -= entry.size_bytes
                del self._cache[key]
                invalidated += 1

        logger.info(f"ğŸ—‘ï¸ Invalidated {invalidated} cache entries")
        return invalidated

    def _evict_lru_entries(self, target_size: int = None):
        """LRUæ–¹å¼ã§ã‚¨ãƒ³ãƒˆãƒªå‰Šé™¤"""
        if target_size is None:
            target_size = self.max_cache_size - 10  # 10å€‹ä½™è£•ã‚’æŒãŸã›ã‚‹

        # ã‚¢ã‚¯ã‚»ã‚¹æ™‚åˆ»é †ã§ã‚½ãƒ¼ãƒˆï¼ˆå¤ã„é †ï¼‰
        entries = list(self._cache.items())
        entries.sort(key=lambda x: x[1].last_access)

        removed_count = 0
        for key, entry in entries[: len(entries) - target_size]:
            self._stats["total_size_bytes"] -= entry.size_bytes
            del self._cache[key]
            removed_count += 1
            self._stats["evictions"] += 1

        if removed_count > 0:
            logger.info(f"ğŸ—‘ï¸ Evicted {removed_count} LRU cache entries")

    def _check_memory_limit(self, additional_bytes: int = 0) -> bool:
        """ãƒ¡ãƒ¢ãƒªåˆ¶é™ãƒã‚§ãƒƒã‚¯"""
        total_size_mb = (self._stats["total_size_bytes"] + additional_bytes) / (
            1024 * 1024
        )
        return total_size_mb <= self.max_memory_mb

    def _calculate_size(self, data: Any) -> int:
        """ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºè¨ˆç®—ï¼ˆæ¦‚ç®—ï¼‰"""
        try:
            if isinstance(data, pd.DataFrame):
                return data.memory_usage(deep=True).sum()
            elif isinstance(data, (dict, list)):
                return len(json.dumps(data, default=str).encode("utf-8"))
            elif isinstance(data, str):
                return len(data.encode("utf-8"))
            else:
                return len(str(data).encode("utf-8"))
        except Exception:
            return 1024  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1KB

    def _background_cleanup(self):
        """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        while True:
            try:
                time.sleep(self.cleanup_interval)
                self._cleanup_expired_entries()
            except Exception as e:
                logger.error(f"âŒ Background cleanup failed: {e}")

    def _cleanup_expired_entries(self):
        """æœŸé™åˆ‡ã‚Œã‚¨ãƒ³ãƒˆãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        with self._cache_lock:
            expired_keys = []

            for key, entry in self._cache.items():
                if entry.is_expired():
                    expired_keys.append(key)

            for key in expired_keys:
                entry = self._cache[key]
                self._stats["total_size_bytes"] -= entry.size_bytes
                del self._cache[key]

            if expired_keys:
                logger.info(f"ğŸ§¹ Cleaned up {len(expired_keys)} expired cache entries")

    def get_stats(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆå–å¾—"""
        with self._cache_lock:
            hit_rate = (
                (self._stats["hits"] / self._stats["requests"]) * 100
                if self._stats["requests"] > 0
                else 0
            )

            return {
                "cache_size": len(self._cache),
                "max_cache_size": self.max_cache_size,
                "total_size_mb": self._stats["total_size_bytes"] / (1024 * 1024),
                "max_memory_mb": self.max_memory_mb,
                "requests": self._stats["requests"],
                "hits": self._stats["hits"],
                "misses": self._stats["misses"],
                "hit_rate_percent": hit_rate,
                "evictions": self._stats["evictions"],
                "api_calls_saved": self._stats["api_calls_saved"],
                "pending_requests": len(self._pending_requests),
            }

    def get_cache_info(self) -> Dict[str, Any]:
        """è©³ç´°ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±å–å¾—"""
        with self._cache_lock:
            entries_info = []

            for key, entry in self._cache.items():
                entries_info.append(
                    {
                        "key": key,
                        "source": entry.source,
                        "age_minutes": (
                            datetime.now() - entry.timestamp
                        ).total_seconds()
                        / 60,
                        "ttl_minutes": entry.ttl.total_seconds() / 60,
                        "access_count": entry.access_count,
                        "quality_score": entry.quality_score,
                        "size_kb": entry.size_bytes / 1024,
                        "is_stale": entry.is_stale(),
                    }
                )

            # ã‚¢ã‚¯ã‚»ã‚¹å›æ•°é †ã§ã‚½ãƒ¼ãƒˆ
            entries_info.sort(key=lambda x: x["access_count"], reverse=True)

            return {
                "stats": self.get_stats(),
                "entries": entries_info,
                "ttl_settings": {
                    k: v.total_seconds() / 3600 for k, v in self.default_ttls.items()
                },
            }


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—é–¢æ•°
def get_global_cache() -> GlobalCacheManager:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—"""
    return GlobalCacheManager()


# å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼çµ±åˆç”¨ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
def cached_external_data(
    source: str, ttl_hours: float = None, quality_threshold: float = 0.5
):
    """å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""

    def decorator(fetch_func):
        def wrapper(*args, **kwargs):
            cache = get_global_cache()

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
            params = {**kwargs}
            if args:
                params["args"] = str(args)
            cache_key = cache.get_cache_key(source, params)

            # TTLè¨­å®š
            ttl = timedelta(hours=ttl_hours) if ttl_hours else None

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã¾ãŸã¯ãƒ•ã‚§ãƒƒãƒ
            def fetch():
                return fetch_func(*args, **kwargs)

            data, from_cache = cache.get_or_fetch(
                cache_key,
                fetch,
                source=source,
                ttl=ttl,
                quality_score=kwargs.get("quality_score", quality_threshold),
            )

            return data

        return wrapper

    return decorator
