# =============================================================================
# ãƒ•ã‚¡ã‚¤ãƒ«å: crypto_bot/data/fetcher.py
# èª¬æ˜:
# ãƒ»MarketDataFetcherï¼šå–å¼•æ‰€ï¼ˆBybitç­‰ï¼‰ã‹ã‚‰OHLCVãƒ‡ãƒ¼ã‚¿ã‚’DataFrameå½¢å¼ã§å–å¾—ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ãƒ»DataPreprocessorï¼šå–å¾—ã—ãŸä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆOHLCVï¼‰ã®é‡è¤‡é™¤å»ã€æ¬ æè£œå®Œã€å¤–ã‚Œå€¤é™¤å»ãªã©ã®å‰å‡¦ç†ã‚’ä¸€æ‹¬ã§è¡Œã†
# ãƒ».envãƒ•ã‚¡ã‚¤ãƒ«ã‚„APIã‚­ãƒ¼è‡ªå‹•èª­è¾¼ã€Bybitå°‚ç”¨ã®ç´°ã‹ã„å·¥å¤«ã‚‚ã‚ã‚Š
# ãƒ»ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚„å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”¨ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ•´å½¢ã®ä¸­å¿ƒçš„ãªå½¹å‰²
# =============================================================================

import logging
import numbers
import os
from datetime import datetime
from typing import List, Optional, Union

import numpy as np
import pandas as pd
from dotenv import load_dotenv
from pandas.tseries.frequencies import to_offset

from crypto_bot.execution.factory import create_exchange_client
from crypto_bot.utils.error_resilience import get_resilience_manager, with_resilience

load_dotenv()

logger = logging.getLogger(__name__)


class MarketDataFetcher:
    def __init__(
        self,
        exchange_id: str = "bitbank",
        api_key: Optional[str] = None,
        api_secret: Optional[str] = None,
        symbol: str = "BTC/JPY",
        testnet: bool = False,
        ccxt_options: Optional[dict] = None,
        csv_path: Optional[str] = None,
    ):
        self.exchange_id = exchange_id
        self.symbol = symbol
        self.csv_path = csv_path
        self.testnet = testnet

        # Phase H.8.3: ã‚¨ãƒ©ãƒ¼è€æ€§ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
        self.resilience = get_resilience_manager()

        # CSV ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯APIæ¥ç¶šã‚’ã‚¹ã‚­ãƒƒãƒ—
        if csv_path:
            self.client = None
            self.exchange = None
            logger.info(f"ğŸ—‚ï¸ [RESILIENCE] CSV mode initialized: {csv_path}")
        else:
            try:
                api_key = api_key or os.getenv(f"{exchange_id.upper()}_API_KEY")
                api_secret = api_secret or os.getenv(
                    f"{exchange_id.upper()}_API_SECRET"
                )
                env_test_key = os.getenv(f"{exchange_id.upper()}_TESTNET_API_KEY")
                testnet = testnet or bool(env_test_key)
                self.client = create_exchange_client(
                    exchange_id=exchange_id,
                    api_key=api_key,
                    api_secret=api_secret,
                    testnet=testnet,
                    ccxt_options=ccxt_options or {},
                )
                # Bitbankå›ºæœ‰ã®è¨­å®šãŒã‚ã‚Œã°è¿½åŠ 
                if exchange_id == "bitbank":
                    # Bitbankç‰¹æœ‰ã®è¨­å®šã‚’è¿½åŠ ã™ã‚‹å ´åˆã¯ã“ã“ã§å®Ÿè£…
                    pass
                self.exchange = getattr(self.client, "_exchange", self.client)

                # Phase H.8.3: åˆæœŸåŒ–æˆåŠŸã‚’è¨˜éŒ²
                self.resilience.record_success("market_data_fetcher")
                logger.info(
                    f"âœ… [RESILIENCE] Market data fetcher initialized: {exchange_id}"
                )

            except Exception as e:
                # Phase H.8.3: åˆæœŸåŒ–å¤±æ•—ã‚’è¨˜éŒ²
                self.resilience.record_error(
                    component="market_data_fetcher",
                    error_type="InitializationError",
                    error_message=f"Failed to initialize {exchange_id}: {str(e)}",
                    severity="CRITICAL",
                )
                logger.error(
                    f"âŒ [RESILIENCE] Market data fetcher initialization failed: {e}"
                )
                raise

    @with_resilience("market_data_fetcher", "fetch_balance")
    def fetch_balance(self) -> dict:
        """
        æ®‹é«˜æƒ…å ±ã‚’å–å¾—

        Returns:
            dict: æ®‹é«˜æƒ…å ±
        """
        if not self.client:
            raise RuntimeError("Client not initialized (CSV mode)")

        logger.info("ğŸ’° [RESILIENCE] Fetching balance with error resilience")
        return self.client.fetch_balance()

    @with_resilience("market_data_fetcher", "get_price_df")
    def get_price_df(
        self,
        timeframe: str = "1m",
        since: Optional[Union[int, float, str, datetime]] = None,
        limit: Optional[int] = None,
        paginate: bool = False,
        sleep: bool = True,
        per_page: int = 500,
        max_consecutive_empty: Optional[int] = None,
        max_consecutive_no_new: Optional[int] = None,
        max_attempts: Optional[int] = None,
    ) -> pd.DataFrame:
        # CSV ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ CSV ã‹ã‚‰èª­ã¿è¾¼ã¿
        if self.csv_path:
            return self._get_price_from_csv(since, limit)

        import time

        since_ms: Optional[int] = None
        if since is not None:
            # Phase H.22 fix: pd.Timestampå‹ã®å‡¦ç†ã‚’è¿½åŠ 
            if hasattr(since, "value"):  # pd.Timestampã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
                # pd.Timestamp.valueã¯ãƒŠãƒç§’ãªã®ã§ã€ãƒŸãƒªç§’ã«å¤‰æ›
                since_ms = int(since.value // 1_000_000)
            elif isinstance(since, str):
                dt = datetime.fromisoformat(since.replace("Z", "+00:00"))
                since_ms = int(dt.timestamp() * 1000)
            elif isinstance(since, datetime):
                since_ms = int(since.timestamp() * 1000)
            elif isinstance(since, numbers.Real):
                ts = int(since)
                since_ms = ts if ts > 1e12 else int(ts * 1000)
            else:
                raise TypeError(f"Unsupported type for since: {type(since)}")

        max_records = limit if limit is not None else float("inf")

        if paginate and limit:
            # Phase H.23.6: ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ€é©åŒ–ï¼ˆ400ãƒ¬ã‚³ãƒ¼ãƒ‰ç¢ºå®Ÿé”æˆï¼‰
            MAX_ATTEMPTS = (
                max_attempts if max_attempts is not None else 25
            )  # 20â†’25ã«å¢—åŠ ï¼ˆper_page=200å¯¾å¿œï¼‰
            MAX_CONSECUTIVE_EMPTY = (
                max_consecutive_empty
                if max_consecutive_empty is not None
                else 8  # 5â†’8ã«å¢—åŠ ï¼ˆå®‰å®šå–å¾—å¼·åŒ–ï¼‰
            )
            MAX_CONSECUTIVE_NO_NEW = (
                max_consecutive_no_new
                if max_consecutive_no_new is not None
                else 15  # 8â†’15ã«å¢—åŠ ï¼ˆper_pageå¤§å¹…å¢—åŠ å¯¾å¿œï¼‰
            )
            logger.info(f"ğŸ”„ Paginated fetch: limit={limit}, per_page={per_page}")
            logger.info(
                f"ğŸ”§ [PHASE-H4] Pagination config: MAX_ATTEMPTS={MAX_ATTEMPTS}, MAX_CONSECUTIVE_EMPTY={MAX_CONSECUTIVE_EMPTY}, MAX_CONSECUTIVE_NO_NEW={MAX_CONSECUTIVE_NO_NEW}"
            )
            records: List = []
            seen_ts = set()
            last_since = since_ms
            attempt = 0
            consecutive_empty = 0
            consecutive_no_new = 0

            while len(records) < max_records and attempt < MAX_ATTEMPTS:
                logger.info(
                    f"ğŸ”„ Attempt {attempt + 1}/{MAX_ATTEMPTS}: fetching from {last_since}, "
                    f"current={len(records)}/{max_records}"
                )

                try:
                    # Phase H.6.3: APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è©³ç´°ãƒ­ã‚°
                    logger.info(
                        f"ğŸ” [PHASE-H6] Calling API: symbol={self.symbol}, timeframe={timeframe}, "
                        f"since={last_since}, limit={per_page}"
                    )

                    batch = self.client.fetch_ohlcv(
                        self.symbol, timeframe, last_since, per_page
                    )

                    # Phase H.6.3: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã¨å†…å®¹ã®è©³ç´°ãƒ­ã‚°
                    logger.info(
                        f"ğŸ” [PHASE-H6] API response type: {type(batch).__name__}"
                    )

                    if batch and isinstance(batch, list) and len(batch) > 0:
                        logger.info(f"ğŸ” [PHASE-H6] First record sample: {batch[0]}")

                    if isinstance(batch, pd.DataFrame):
                        logger.info(
                            f"âœ… Received DataFrame directly: {len(batch)} records"
                        )
                        return batch

                    if not batch:
                        consecutive_empty += 1
                        logger.warning(
                            f"âš ï¸ Empty batch {consecutive_empty}/{MAX_CONSECUTIVE_EMPTY}"
                        )

                        # Phase H.4: ç©ºãƒãƒƒãƒã®è©³ç´°æƒ…å ±
                        logger.warning(
                            f"ğŸ” [PHASE-H4] Empty batch at timestamp: {last_since}, attempt {attempt + 1}"
                        )

                        if consecutive_empty >= MAX_CONSECUTIVE_EMPTY:
                            logger.warning(
                                f"âŒ [PHASE-H4] EARLY TERMINATION: Too many consecutive empty batches ({consecutive_empty}/{MAX_CONSECUTIVE_EMPTY}), stopping pagination"
                            )
                            logger.warning(
                                f"ğŸ“Š [PHASE-H4] Final stats: {len(records)} records collected in {attempt + 1} attempts"
                            )
                            break

                        # Phase H.20.1.3: æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒƒã‚¯ã‚ªãƒ•æˆ¦ç•¥
                        # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªå–å¾—ã®ãŸã‚å¾…æ©Ÿæ™‚é–“çŸ­ç¸®ï¼ˆ10ç§’â†’6ç§’ä¸Šé™ï¼‰
                        backoff_delay = min(
                            consecutive_empty * 1.5, 6
                        )  # 2â†’1.5, 10â†’6ã«çŸ­ç¸®
                        logger.debug(
                            f"ğŸ”„ [PHASE-H20.1.3] Backoff delay: {backoff_delay}ç§’"
                        )
                        time.sleep(backoff_delay)
                        attempt += 1
                        continue

                    # ç©ºãƒãƒƒãƒã‚«ã‚¦ãƒ³ã‚¿ãƒªã‚»ãƒƒãƒˆ
                    consecutive_empty = 0

                    logger.info(f"ğŸ“Š Batch received: {len(batch)} records")

                    # Phase H.4: ãƒãƒƒãƒå†…å®¹ã®è©³ç´°åˆ†æãƒ­ã‚°
                    if batch:
                        first_ts = batch[0][0]
                        last_ts = batch[-1][0]
                        first_time = pd.Timestamp(first_ts, unit="ms")
                        last_time = pd.Timestamp(last_ts, unit="ms")
                        logger.info(
                            f"ğŸ” [PHASE-H4] Batch time range: {first_time} to {last_time}"
                        )
                        logger.info(
                            f"ğŸ” [PHASE-H4] Batch time span: {(last_ts - first_ts) / (1000 * 3600):.2f} hours"
                        )

                    added = False
                    new_records_count = 0
                    duplicate_count = 0
                    for row in batch:
                        ts = row[0]
                        if ts not in seen_ts:
                            seen_ts.add(ts)
                            records.append(row)
                            new_records_count += 1
                            # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¿œã˜ãŸé©åˆ‡ãªæ™‚åˆ»é€²è¡Œ
                            # 1h = 3600ç§’, 4h = 14400ç§’, 15m = 900ç§’
                            timeframe_ms = {
                                "1m": 60 * 1000,
                                "5m": 5 * 60 * 1000,
                                "15m": 15 * 60 * 1000,
                                "1h": 60 * 60 * 1000,
                                "4h": 4 * 60 * 60 * 1000,
                                "1d": 24 * 60 * 60 * 1000,
                            }.get(
                                timeframe, 60 * 60 * 1000
                            )  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1æ™‚é–“
                            last_since = ts + timeframe_ms
                            added = True
                        else:
                            duplicate_count += 1

                    logger.info(
                        f"âœ… [PHASE-H4] Added {new_records_count} new records, {duplicate_count} duplicates, total={len(records)}"
                    )
                    logger.info(
                        f"ğŸ“ˆ [PHASE-H4] Progress: {len(records)}/{max_records} ({len(records)/max_records*100:.1f}%)"
                    )

                    if not added:
                        consecutive_no_new += 1
                        logger.warning(
                            f"âš ï¸ No new records {consecutive_no_new}/{MAX_CONSECUTIVE_NO_NEW}"
                        )

                        # Phase H.4: æ—©æœŸçµ‚äº†ã®è©³ç´°ç†ç”±ãƒ­ã‚°
                        logger.warning(
                            f"ğŸ” [PHASE-H4] No new records reason: {len(batch)} total received, {duplicate_count} were duplicates"
                        )

                        if consecutive_no_new >= MAX_CONSECUTIVE_NO_NEW:
                            logger.warning(
                                f"âŒ [PHASE-H4] EARLY TERMINATION: Too many attempts with no new records ({consecutive_no_new}/{MAX_CONSECUTIVE_NO_NEW}), stopping pagination"
                            )
                            logger.warning(
                                f"ğŸ“Š [PHASE-H4] Final stats: {len(records)} records collected in {attempt + 1} attempts"
                            )
                            break

                        # Phase H.20.1.3: æ”¹å–„ã•ã‚ŒãŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é€²è¡Œæˆ¦ç•¥
                        if batch:
                            # ã‚ˆã‚Šå°ã•ãªå˜ä½ã§ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’é€²ã‚ã¦è¦‹é€ƒã—ã‚’æ¸›å°‘
                            timeframe_ms = {
                                "1m": 60 * 1000,
                                "5m": 5 * 60 * 1000,
                                "15m": 15 * 60 * 1000,
                                "1h": 60 * 60 * 1000,
                                "4h": 4 * 60 * 60 * 1000,
                                "1d": 24 * 60 * 60 * 1000,
                            }.get(timeframe, 60 * 60 * 1000)

                            # å°åˆ»ã¿ã«é€²è¡Œï¼ˆå¾“æ¥ã®åŠåˆ†ã®å¹…ã§é€²ã‚€ï¼‰
                            step_ms = timeframe_ms // 2  # åŠåˆ†ã®æ™‚é–“é–“éš”ã§é€²è¡Œ
                            last_since = batch[-1][0] + step_ms

                            logger.debug(
                                f"ğŸ”„ [PHASE-H20.1.3] Timestamp advance: +{step_ms}ms "
                                f"(half of {timeframe} interval)"
                            )
                    else:
                        # æ–°ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã£ãŸå ´åˆã¯ã‚«ã‚¦ãƒ³ã‚¿ãƒªã‚»ãƒƒãƒˆ
                        consecutive_no_new = 0
                        logger.info(
                            f"âœ… Added {sum(1 for row in batch if row[0] in seen_ts and row[0] >= last_since - len(batch))} records, total={len(records)}"
                        )

                    # Phase H.20.1.3: æœ€é©åŒ–ã•ã‚ŒãŸãƒ¬ãƒ¼ãƒˆåˆ¶é™
                    if (
                        sleep
                        and hasattr(self.exchange, "rateLimit")
                        and self.exchange.rateLimit
                    ):
                        base_delay = self.exchange.rateLimit / 1000.0

                        # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªå–å¾—ã®ãŸã‚åŸºæœ¬å¾…æ©Ÿæ™‚é–“ã‚’çŸ­ç¸®
                        base_delay *= 0.8  # 20%çŸ­ç¸®ã§ã‚ˆã‚Šç©æ¥µçš„å–å¾—

                        # é€£ç¶šå•é¡Œç™ºç”Ÿæ™‚ã®å»¶é•·ã‚‚æŠ‘åˆ¶ï¼ˆ1.5â†’1.3ï¼‰
                        if consecutive_empty > 0 or consecutive_no_new > 0:
                            base_delay *= 1.3

                        logger.debug(
                            f"ğŸ”„ [PHASE-H20.1.3] Rate limit delay: {base_delay:.3f}ç§’"
                        )
                        time.sleep(base_delay)

                except Exception as e:
                    logger.error(f"âŒ Batch fetch error on attempt {attempt + 1}: {e}")
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å°‘ã—å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤
                    error_delay = min((attempt + 1) * 1.5, 8)
                    time.sleep(error_delay)

                attempt += 1

            # Phase H.4: ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ã®è©³ç´°ã‚µãƒãƒªãƒ¼
            logger.info(
                f"âœ… [PHASE-H4] Pagination complete: {len(records)} total records collected in {attempt} attempts"
            )
            if records:
                first_record_time = pd.Timestamp(records[0][0], unit="ms")
                last_record_time = pd.Timestamp(records[-1][0], unit="ms")
                time_span = (records[-1][0] - records[0][0]) / (1000 * 3600)  # hours
                logger.info(
                    f"ğŸ“Š [PHASE-H4] Data time range: {first_record_time} to {last_record_time} ({time_span:.2f} hours)"
                )
            logger.info(
                f"ğŸ”§ [PHASE-H4] Termination reason: MAX_RECORDS_REACHED={len(records) >= max_records}, MAX_ATTEMPTS_REACHED={attempt >= MAX_ATTEMPTS}"
            )
            data = records if limit is None else records[:limit]

        else:
            # Phase H.6.3: éãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§ã‚‚ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ 
            logger.info(
                f"ğŸ” [PHASE-H6] Non-paginated fetch: timeframe={timeframe}, "
                f"since_ms={since_ms}, limit={limit}"
            )

            raw = self.client.fetch_ohlcv(self.symbol, timeframe, since_ms, limit)

            # Phase H.6.3: ãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°ãƒ­ã‚°
            logger.info(
                f"ğŸ” [PHASE-H6] Response type: {type(raw).__name__}, "
                f"content: {len(raw) if raw else 0} records"
            )
            if (
                sleep
                and hasattr(self.exchange, "rateLimit")
                and self.exchange.rateLimit
            ):
                time.sleep(self.exchange.rateLimit / 1000.0)
            if isinstance(raw, pd.DataFrame):
                return raw
            data = raw or []

            # Bitbankå›ºæœ‰ã®å†è©¦è¡Œãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå¿…è¦ã«å¿œã˜ã¦å®Ÿè£…ï¼‰
            if not data and self.exchange_id == "bitbank":
                logger.warning(
                    f"âš ï¸ [PHASE-H6] Bitbank returned no data for since_ms={since_ms}"
                )
                # Phase H.6.3: æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’è©¦ã¿ã‚‹
                logger.info(
                    "ğŸ”„ [PHASE-H6] Trying to fetch latest data without since parameter"
                )
                raw_latest = self.client.fetch_ohlcv(self.symbol, timeframe, None, 10)
                if raw_latest:
                    logger.info(f"âœ… [PHASE-H6] Got {len(raw_latest)} latest records")
                    data = raw_latest

        if not data:
            return pd.DataFrame()

        df = pd.DataFrame(
            data, columns=["timestamp", "open", "high", "low", "close", "volume"]
        )
        df["datetime"] = pd.to_datetime(df["timestamp"], unit="ms", utc=True)
        df = df.set_index("datetime")

        if isinstance(since, datetime) or (isinstance(since, str) and since):
            df = df.iloc[1:]

        if limit is not None:
            df = df.head(limit)

        return df[["open", "high", "low", "close", "volume"]]

    def _is_data_too_old(self, data: pd.DataFrame, max_age_hours: float = 2.0) -> bool:
        """
        ãƒ‡ãƒ¼ã‚¿ãŒå¤ã™ãã‚‹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯

        Args:
            data: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®DataFrame
            max_age_hours: è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§æ™‚é–“ï¼ˆæ™‚é–“ï¼‰

        Returns:
            bool: ãƒ‡ãƒ¼ã‚¿ãŒå¤ã™ãã‚‹å ´åˆTrue
        """
        if data is None or data.empty:
            return True

        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
        latest_timestamp = data.index.max()
        current_time = pd.Timestamp.now(tz="UTC")

        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ã‚’çµ±ä¸€
        if latest_timestamp.tz is None:
            latest_timestamp = latest_timestamp.tz_localize("UTC")

        # ãƒ‡ãƒ¼ã‚¿ã®å¹´é½¢ã‚’è¨ˆç®—
        data_age = current_time - latest_timestamp
        data_age_hours = data_age.total_seconds() / 3600

        logger.info(
            f"ğŸ” [DATA-FRESHNESS] Latest data: {latest_timestamp}, Age: {data_age_hours:.1f}h"
        )

        if data_age_hours > max_age_hours:
            logger.warning(
                f"âš ï¸ [DATA-FRESHNESS] Data too old: {data_age_hours:.1f}h > {max_age_hours}h"
            )
            return True

        logger.info(
            f"âœ… [DATA-FRESHNESS] Data is fresh: {data_age_hours:.1f}h <= {max_age_hours}h"
        )
        return False

    def _select_freshest_data(
        self, data1: pd.DataFrame, data2: pd.DataFrame
    ) -> pd.DataFrame:
        """
        2ã¤ã®DataFrameã‹ã‚‰æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ

        Args:
            data1: æ¯”è¼ƒå¯¾è±¡ãƒ‡ãƒ¼ã‚¿1
            data2: æ¯”è¼ƒå¯¾è±¡ãƒ‡ãƒ¼ã‚¿2

        Returns:
            pd.DataFrame: ã‚ˆã‚Šæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿
        """
        if data1 is None or data1.empty:
            return data2 if data2 is not None else pd.DataFrame()

        if data2 is None or data2.empty:
            return data1

        # æœ€æ–°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ¯”è¼ƒ
        latest1 = data1.index.max()
        latest2 = data2.index.max()

        if latest1.tz is None:
            latest1 = latest1.tz_localize("UTC")
        if latest2.tz is None:
            latest2 = latest2.tz_localize("UTC")

        if latest2 > latest1:
            logger.info(
                f"âœ… [DATA-SELECT] Selected data2 (newer): {latest2} vs {latest1}"
            )
            return data2
        else:
            logger.info(
                f"âœ… [DATA-SELECT] Selected data1 (newer/equal): {latest1} vs {latest2}"
            )
            return data1

    def fetch_with_freshness_fallback(
        self,
        timeframe: str = "1h",
        since: Optional[Union[int, float, str, datetime]] = None,
        limit: Optional[int] = None,
        max_age_hours: float = 2.0,
        **kwargs,
    ) -> pd.DataFrame:
        """
        ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦ãƒã‚§ãƒƒã‚¯ä»˜ããƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆPhase H.8.1 + H.9.4åœŸæ—¥å¯¾å¿œå¼·åŒ–ï¼‰

        Args:
            timeframe: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ 
            since: é–‹å§‹æ™‚åˆ»
            limit: å–å¾—ä»¶æ•°ä¸Šé™
            max_age_hours: è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§ãƒ‡ãƒ¼ã‚¿å¹´é½¢ï¼ˆæ™‚é–“ï¼‰
            **kwargs: get_price_dfã¸ã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            pd.DataFrame: æ–°é®®ãªãƒ‡ãƒ¼ã‚¿
        """
        logger.info(
            "ğŸš€ [PHASE-H9.4] Starting enhanced freshness data fetch with fallback optimization"
        )

        # Phase H.9.4: sinceè¨ˆç®—å•é¡Œè§£æ±ºãƒ»å¼·åˆ¶æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—æˆ¦ç•¥
        current_time = pd.Timestamp.now(tz="UTC")

        # since ãŒå¤ã™ãã‚‹å ´åˆã®è‡ªå‹•èª¿æ•´ï¼ˆPhase H.9.4æ ¹æœ¬è§£æ±ºï¼‰
        if since is not None:
            if isinstance(since, str):
                since_dt = pd.to_datetime(since, utc=True)
            elif isinstance(since, datetime):
                since_dt = (
                    since
                    if since.tzinfo
                    else since.replace(tzinfo=pd.Timestamp.now().tz)
                )
            else:
                since_dt = pd.to_datetime(
                    since, unit="ms" if since > 1e12 else "s", utc=True
                )

            age_hours = (current_time - since_dt).total_seconds() / 3600
            if age_hours > 24:  # 24æ™‚é–“ä»¥ä¸Šå¤ã„å ´åˆã¯å¼·åˆ¶èª¿æ•´
                new_since = current_time - pd.Timedelta(hours=6)  # 6æ™‚é–“å‰ã«èª¿æ•´
                logger.warning(
                    f"ğŸ”§ [PHASE-H9.4] since too old ({age_hours:.1f}h), adjusting: {since_dt} â†’ {new_since}"
                )
                since = new_since

        logger.info(
            f"ğŸ”§ [PHASE-H9.4] Config: timeframe={timeframe}, since={since}, limit={limit}, max_age_hours={max_age_hours}"
        )

        try:
            # é€šå¸¸ã®sinceæŒ‡å®šå–å¾—
            logger.info("ğŸ“¡ [PHASE-H8.1] Attempting since-based fetch...")
            data = self.get_price_df(
                timeframe=timeframe, since=since, limit=limit, **kwargs
            )

            # ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆPhase H.9.4: èª¿æ•´ã•ã‚ŒãŸmax_age_hoursä½¿ç”¨ï¼‰
            if not self._is_data_too_old(data, max_age_hours):
                logger.info("âœ… [PHASE-H9.4] Since-based data is fresh, using it")
                return data

            # ãƒ‡ãƒ¼ã‚¿ãŒå¤ã„å ´åˆï¼šsince=Noneã§æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—
            logger.warning(
                "ğŸ”„ [PHASE-H8.1] Data too old, falling back to latest data fetch"
            )
            latest_data = self.get_price_df(
                timeframe=timeframe,
                since=None,
                limit=min(limit or 100, 100),  # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã¯100ä»¶ä»¥ä¸‹ã«åˆ¶é™
                paginate=False,  # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã¯é«˜é€Ÿå–å¾—
                **{k: v for k, v in kwargs.items() if k != "paginate"},
            )

            if not latest_data.empty:
                logger.info(
                    f"âœ… [PHASE-H8.1] Latest data fallback successful: {len(latest_data)} records"
                )
                return latest_data
            else:
                logger.warning(
                    "âš ï¸ [PHASE-H8.1] Latest data fallback also empty, returning original"
                )
                return data

        except Exception as e:
            logger.error(f"âŒ [PHASE-H8.1] Freshness fallback failed: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯é€šå¸¸ã®å–å¾—ã‚’è©¦è¡Œ
            try:
                return self.get_price_df(
                    timeframe=timeframe, since=since, limit=limit, **kwargs
                )
            except Exception as e2:
                logger.error(
                    f"âŒ [PHASE-H8.1] Fallback to normal fetch also failed: {e2}"
                )
                return pd.DataFrame()

    def parallel_data_fetch(
        self,
        timeframe: str = "1h",
        since: Optional[Union[int, float, str, datetime]] = None,
        limit: Optional[int] = None,
        **kwargs,
    ) -> pd.DataFrame:
        """
        ä¸¦è¡Œãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆsinceæŒ‡å®š vs since=Noneï¼‰ï¼ˆPhase H.8.1ï¼‰

        Args:
            timeframe: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ 
            since: é–‹å§‹æ™‚åˆ»
            limit: å–å¾—ä»¶æ•°ä¸Šé™
            **kwargs: get_price_dfã¸ã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            pd.DataFrame: ã‚ˆã‚Šæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿
        """
        logger.info("ğŸš€ [PHASE-H8.1] Starting parallel data fetch")

        import concurrent.futures

        def fetch_since_data():
            try:
                logger.info("ğŸ“¡ [PARALLEL-SINCE] Fetching since-based data...")
                return self.get_price_df(
                    timeframe=timeframe, since=since, limit=limit, **kwargs
                )
            except Exception as e:
                logger.warning(f"âš ï¸ [PARALLEL-SINCE] Failed: {e}")
                return pd.DataFrame()

        def fetch_latest_data():
            try:
                logger.info("ğŸ“¡ [PARALLEL-LATEST] Fetching latest data...")
                # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã¯é«˜é€Ÿè¨­å®šã§å–å¾—
                return self.get_price_df(
                    timeframe=timeframe,
                    since=None,
                    limit=min(limit or 50, 50),
                    paginate=False,
                    **{
                        k: v
                        for k, v in kwargs.items()
                        if k not in ["paginate", "per_page"]
                    },
                )
            except Exception as e:
                logger.warning(f"âš ï¸ [PARALLEL-LATEST] Failed: {e}")
                return pd.DataFrame()

        try:
            # ä¸¦è¡Œå®Ÿè¡Œï¼ˆæœ€å¤§60ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                future_since = executor.submit(fetch_since_data)
                future_latest = executor.submit(fetch_latest_data)

                # çµæœå–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
                try:
                    data_since = future_since.result(timeout=60)
                    data_latest = future_latest.result(timeout=60)
                except concurrent.futures.TimeoutError:
                    logger.warning(
                        "âš ï¸ [PHASE-H8.1] Parallel fetch timeout, canceling futures"
                    )
                    future_since.cancel()
                    future_latest.cancel()
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯é€šå¸¸ã®å–å¾—ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    return self.get_price_df(
                        timeframe=timeframe, since=since, limit=limit, **kwargs
                    )

            # ã‚ˆã‚Šæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ
            best_data = self._select_freshest_data(data_since, data_latest)

            if not best_data.empty:
                logger.info(
                    f"âœ… [PHASE-H8.1] Parallel fetch successful: {len(best_data)} records"
                )
            else:
                logger.warning(
                    "âš ï¸ [PHASE-H8.1] Both parallel fetches returned empty data"
                )

            return best_data

        except Exception as e:
            logger.error(f"âŒ [PHASE-H8.1] Parallel fetch failed: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯é€šå¸¸ã®å–å¾—ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self.get_price_df(
                timeframe=timeframe, since=since, limit=limit, **kwargs
            )

    def _get_price_from_csv(
        self,
        since: Optional[Union[int, float, str, datetime]] = None,
        limit: Optional[int] = None,
    ) -> pd.DataFrame:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿

        Parameters
        ----------
        since : Optional[Union[int, float, str, datetime]]
            é–‹å§‹æ™‚åˆ»
        limit : Optional[int]
            å–å¾—ä»¶æ•°åˆ¶é™

        Returns
        -------
        pd.DataFrame
            ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        """
        if not os.path.exists(self.csv_path):
            raise FileNotFoundError(f"CSV file not found: {self.csv_path}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        df = pd.read_csv(self.csv_path, parse_dates=True, index_col=0)

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒdatetimeã§ãªã„å ´åˆã¯å¤‰æ›
        if not isinstance(df.index, pd.DatetimeIndex):
            df.index = pd.to_datetime(df.index)

        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’ UTC ã«è¨­å®š
        if df.index.tz is None:
            df.index = df.index.tz_localize("UTC")

        # since ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if since is not None:
            if isinstance(since, str):
                since_dt = pd.to_datetime(since, utc=True)
            elif isinstance(since, datetime):
                since_dt = since
                if since_dt.tzinfo is None:
                    since_dt = since_dt.replace(tzinfo=pd.Timestamp.now().tz)
            else:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å ´åˆ
                since_dt = pd.to_datetime(since, unit="ms", utc=True)

            df = df[df.index >= since_dt]

        # limit ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä»¶æ•°åˆ¶é™
        if limit is not None:
            df = df.head(limit)

        # å¿…è¦ãªåˆ—ã®ã¿è¿”ã™
        required_columns = ["open", "high", "low", "close", "volume"]
        available_columns = [col for col in required_columns if col in df.columns]

        if not available_columns:
            raise ValueError(
                f"Required columns {required_columns} not found in CSV file"
            )

        return df[available_columns]


class DataPreprocessor:
    @staticmethod
    def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:
        return df[~df.index.duplicated(keep="first")]

    @staticmethod
    def fill_missing_bars(df: pd.DataFrame, timeframe: str = "1h") -> pd.DataFrame:
        freq_offset = to_offset(timeframe)
        idx = pd.date_range(
            start=df.index[0],
            end=df.index[-1],
            freq=freq_offset,
            tz=df.index.tz,
        )
        df2 = df.reindex(idx)
        for col in ["open", "high", "low", "close", "volume"]:
            df2[col] = df2[col].ffill()
        return df2

    @staticmethod
    def remove_outliers(
        df: pd.DataFrame, thresh: float = 3.5, window: int = 20
    ) -> pd.DataFrame:
        df = df.copy()
        price_cols = ["open", "high", "low", "close"]
        for col in [c for c in price_cols if c in df.columns]:
            median = (
                df[col]
                .rolling(
                    window=window,
                    center=True,
                    min_periods=1,
                )
                .median()
            )
            deviation = (df[col] - median).abs()
            mad = deviation.rolling(
                window=window,
                center=True,
                min_periods=1,
            ).median()
            mad = mad + 1e-8
            modified_zscore = 0.6745 * deviation / mad
            is_outlier = modified_zscore > thresh
            temp = df[col].copy()
            temp[is_outlier] = np.nan
            filled = temp.rolling(
                window=window,
                center=True,
                min_periods=1,
            ).mean()
            filled = filled.fillna(method="ffill").fillna(method="bfill")
            df[col] = np.where(is_outlier, filled, df[col])
        return df

    @staticmethod
    def clean(
        df: pd.DataFrame,
        timeframe: str = "1h",
        thresh: float = 3.5,
        window: int = 20,
    ) -> pd.DataFrame:
        df = DataPreprocessor.remove_duplicates(df)
        df = DataPreprocessor.fill_missing_bars(df, timeframe)
        df = DataPreprocessor.remove_outliers(df, thresh, window)
        for col in ["open", "high", "low", "close", "volume"]:
            df[col] = df[col].ffill().bfill()
        return df


class OIDataFetcher:
    """OIï¼ˆæœªæ±ºæ¸ˆå»ºç‰ï¼‰ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¹"""

    def __init__(self, market_fetcher: MarketDataFetcher):
        self.market_fetcher = market_fetcher
        self.exchange = market_fetcher.exchange
        self.symbol = market_fetcher.symbol

    def get_oi_data(
        self,
        timeframe: str = "1h",
        since: Optional[Union[str, datetime]] = None,
        limit: Optional[int] = None,
    ) -> pd.DataFrame:
        """
        OIï¼ˆæœªæ±ºæ¸ˆå»ºç‰ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Returns
        -------
        pd.DataFrame
            OIãƒ‡ãƒ¼ã‚¿ï¼ˆindex: datetime, columns: oi_amount, oi_valueï¼‰
        """
        try:
            # Bitbankã®å ´åˆã®OIå–å¾—ï¼ˆç¾ç‰©ãƒ»ä¿¡ç”¨å–å¼•å¯¾å¿œï¼‰
            if self.market_fetcher.exchange_id == "bitbank":
                # Bitbankç¾ç‰©å–å¼•ã«ã¯OIãŒãªã„ãŸã‚ã€ä»£æ›¿æŒ‡æ¨™ã‚’ä½¿ç”¨
                # å–å¼•é‡Ã—ä¾¡æ ¼ã§ãƒã‚¸ã‚·ãƒ§ãƒ³è¦æ¨¡ã‚’æ¨å®š
                logger.info("Generating OI approximation for Bitbank")

                # æœ€æ–°ã®ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                price_data = self.market_fetcher.get_price_df(
                    timeframe="1h", limit=168  # 1é€±é–“åˆ†
                )

                if not price_data.empty:
                    # ãƒã‚¸ã‚·ãƒ§ãƒ³è¦æ¨¡ã®æ¨å®šï¼ˆå‡ºæ¥é«˜Ã—ä¾¡æ ¼ï¼‰
                    position_size = price_data["volume"] * price_data["close"]

                    result = pd.DataFrame(
                        {
                            "oi_amount": position_size,
                            "oi_value": position_size * price_data["close"],
                        }
                    )

                    return result

                # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ç©ºã®DataFrame
                return pd.DataFrame(columns=["oi_amount", "oi_value"])

        except Exception as e:
            logger.warning(f"Could not fetch OI approximation: {e}")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç©ºã®DataFrameã‚’è¿”ã™
        return pd.DataFrame(columns=["oi_amount", "oi_value"])

    def calculate_oi_features(self, df: pd.DataFrame, window: int = 24) -> pd.DataFrame:
        """
        OIé–¢é€£ã®ç‰¹å¾´é‡ã‚’è¨ˆç®—

        Parameters
        ----------
        df : pd.DataFrame
            OIãƒ‡ãƒ¼ã‚¿
        window : int
            ç§»å‹•å¹³å‡ã®æœŸé–“

        Returns
        -------
        pd.DataFrame
            OIç‰¹å¾´é‡ä»˜ãDataFrame
        """
        if df.empty or "oi_amount" not in df.columns:
            return df

        # OIå¤‰åŒ–ç‡
        df["oi_pct_change"] = df["oi_amount"].pct_change()

        # OIç§»å‹•å¹³å‡
        df[f"oi_ma_{window}"] = df["oi_amount"].rolling(window=window).mean()

        # OIæ¨™æº–åŒ–ï¼ˆZ-scoreï¼‰
        df["oi_zscore"] = (
            df["oi_amount"] - df["oi_amount"].rolling(window=window).mean()
        ) / df["oi_amount"].rolling(window=window).std()

        # OIå‹¢ã„ï¼ˆmomentumï¼‰
        df["oi_momentum"] = df["oi_amount"] / df["oi_amount"].shift(window) - 1

        # OIæ€¥æ¿€ãªå¤‰åŒ–æ¤œçŸ¥
        df["oi_spike"] = (
            df["oi_pct_change"].abs()
            > df["oi_pct_change"].rolling(window=window).std() * 2
        ).astype(int)

        return df
