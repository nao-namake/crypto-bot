# =============================================================================
# ãƒ•ã‚¡ã‚¤ãƒ«å: crypto_bot/data/fetcher.py
# èª¬æ˜:
# ãƒ»MarketDataFetcherï¼šå–å¼•æ‰€ï¼ˆBybitç­‰ï¼‰ã‹ã‚‰OHLCVãƒ‡ãƒ¼ã‚¿ã‚’DataFrameå½¢å¼ã§å–å¾—ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ãƒ»DataPreprocessorï¼šå–å¾—ã—ãŸä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆOHLCVï¼‰ã®é‡è¤‡é™¤å»ã€æ¬ æè£œå®Œã€å¤–ã‚Œå€¤é™¤å»ãªã©ã®å‰å‡¦ç†ã‚’ä¸€æ‹¬ã§è¡Œã†
# ãƒ».envãƒ•ã‚¡ã‚¤ãƒ«ã‚„APIã‚­ãƒ¼è‡ªå‹•èª­è¾¼ã€Bybitå°‚ç”¨ã®ç´°ã‹ã„å·¥å¤«ã‚‚ã‚ã‚Š
# ãƒ»ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚„å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”¨ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ•´å½¢ã®ä¸­å¿ƒçš„ãªå½¹å‰²
# =============================================================================

import logging
import numbers
import os
import time
from datetime import datetime
from typing import List, Optional, Union

import numpy as np
import pandas as pd
from dotenv import load_dotenv
from pandas.tseries.frequencies import to_offset

from crypto_bot.execution.factory import create_exchange_client
from crypto_bot.utils.error_resilience import get_resilience_manager, with_resilience

load_dotenv()

logger = logging.getLogger(__name__)


class MarketDataFetcher:
    def __init__(
        self,
        exchange_id: str = "bitbank",
        api_key: Optional[str] = None,
        api_secret: Optional[str] = None,
        symbol: str = "BTC/JPY",
        testnet: bool = False,
        ccxt_options: Optional[dict] = None,
        csv_path: Optional[str] = None,
    ):
        self.exchange_id = exchange_id
        self.symbol = symbol
        self.csv_path = csv_path
        self.testnet = testnet

        # Phase H.8.3: ã‚¨ãƒ©ãƒ¼è€æ€§ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
        self.resilience = get_resilience_manager()

        # CSV ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯APIæ¥ç¶šã‚’ã‚¹ã‚­ãƒƒãƒ—
        if csv_path:
            self.client = None
            self.exchange = None
            logger.info(f"ğŸ—‚ï¸ [RESILIENCE] CSV mode initialized: {csv_path}")
        else:
            try:
                api_key = api_key or os.getenv(f"{exchange_id.upper()}_API_KEY")
                api_secret = api_secret or os.getenv(
                    f"{exchange_id.upper()}_API_SECRET"
                )
                env_test_key = os.getenv(f"{exchange_id.upper()}_TESTNET_API_KEY")
                testnet = testnet or bool(env_test_key)
                self.client = create_exchange_client(
                    exchange_id=exchange_id,
                    api_key=api_key,
                    api_secret=api_secret,
                    testnet=testnet,
                    ccxt_options=ccxt_options or {},
                )
                # Bitbankå›ºæœ‰ã®è¨­å®šãŒã‚ã‚Œã°è¿½åŠ 
                if exchange_id == "bitbank":
                    # Bitbankç‰¹æœ‰ã®è¨­å®šã‚’è¿½åŠ ã™ã‚‹å ´åˆã¯ã“ã“ã§å®Ÿè£…
                    pass
                self.exchange = getattr(self.client, "_exchange", self.client)

                # Phase H.8.3: åˆæœŸåŒ–æˆåŠŸã‚’è¨˜éŒ²
                self.resilience.record_success("market_data_fetcher")
                logger.info(
                    f"âœ… [RESILIENCE] Market data fetcher initialized: {exchange_id}"
                )

            except Exception as e:
                # Phase H.8.3: åˆæœŸåŒ–å¤±æ•—ã‚’è¨˜éŒ²
                self.resilience.record_error(
                    component="market_data_fetcher",
                    error_type="InitializationError",
                    error_message=f"Failed to initialize {exchange_id}: {str(e)}",
                    severity="CRITICAL",
                )
                logger.error(
                    f"âŒ [RESILIENCE] Market data fetcher initialization failed: {e}"
                )
                raise

    @with_resilience("market_data_fetcher", "fetch_balance")
    def fetch_balance(self) -> dict:
        """
        æ®‹é«˜æƒ…å ±ã‚’å–å¾—

        Returns:
            dict: æ®‹é«˜æƒ…å ±
        """
        if not self.client:
            raise RuntimeError("Client not initialized (CSV mode)")

        logger.info("ğŸ’° [RESILIENCE] Fetching balance with error resilience")
        return self.client.fetch_balance()

    def _validate_timestamp_h28(
        self, timestamp: Optional[int], context: str = "unknown"
    ) -> Optional[int]:
        """
        Phase H.28.1: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å …ç‰¢æ€§ã‚·ã‚¹ãƒ†ãƒ  - 5æ®µéšæ¤œè¨¼

        Args:
            timestamp: æ¤œè¨¼å¯¾è±¡ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆãƒŸãƒªç§’ï¼‰
            context: å‘¼ã³å‡ºã—å…ƒã®æ–‡è„ˆæƒ…å ±

        Returns:
            Optional[int]: æ¤œè¨¼æ¸ˆã¿ãƒ»ä¿®æ­£æ¸ˆã¿ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã€ã¾ãŸã¯ Noneï¼ˆç•°å¸¸å€¤ã®å ´åˆï¼‰
        """
        if timestamp is None:
            return None

        try:
            # H.28.1-Stage1: å‹å®‰å…¨æ€§ä¿è¨¼
            if (
                not isinstance(timestamp, (int, float))
                or np.isnan(timestamp)
                or np.isinf(timestamp)
            ):
                logger.error(
                    f"ğŸš¨ [H.28.1-Stage1] Invalid timestamp type/value: {timestamp} (context: {context})"
                )
                return None

            ts = int(timestamp)

            # H.28.1-Stage2: ãƒŸãƒªç§’æ¡æ•°æ¤œè¨¼ï¼ˆ10æ¡=ç§’ã€13æ¡=ãƒŸãƒªç§’ï¼‰
            if ts < 1e12:  # 10æ¡ä»¥ä¸‹ï¼ˆç§’å˜ä½ã®å¯èƒ½æ€§ï¼‰
                ts = ts * 1000  # ãƒŸãƒªç§’ã«å¤‰æ›
                logger.info(
                    f"ğŸ”„ [H.28.1-Stage2] Converted seconds to milliseconds: {timestamp} -> {ts}"
                )
            elif ts > 1e16:  # 16æ¡ä»¥ä¸Šï¼ˆç•°å¸¸å€¤ï¼‰
                logger.error(
                    f"ğŸš¨ [H.28.1-Stage2] Timestamp too large: {ts} (context: {context})"
                )
                return None

            # H.28.1-Stage3: ç¾åœ¨æ™‚åˆ»æ¯”è¼ƒãƒ»åˆç†çš„ç¯„å›²ãƒã‚§ãƒƒã‚¯
            current_time_ms = int(time.time() * 1000)
            one_year_ago_ms = current_time_ms - (365 * 24 * 60 * 60 * 1000)
            # Phase H.29: æœªæ¥æ™‚åˆ»è¨±å®¹ã‚’24æ™‚é–“ã‹ã‚‰1æ™‚é–“ã«å³æ ¼åŒ–
            one_hour_future_ms = current_time_ms + (60 * 60 * 1000)  # 1æ™‚é–“å¾Œã¾ã§

            if ts < one_year_ago_ms:
                logger.error(
                    f"ğŸš¨ [H.28.1-Stage3] Timestamp too old: {ts} < {one_year_ago_ms} (context: {context})"
                )
                return None
            elif ts > one_hour_future_ms:
                # Phase H.29: æœªæ¥æ™‚åˆ»æ¤œå‡ºæ™‚ã®è©³ç´°ãƒ­ã‚°
                time_diff_hours = (ts - current_time_ms) / (60 * 60 * 1000)
                logger.error(
                    f"ğŸš¨ [H.29-Stage3] Future timestamp detected: {ts} is {time_diff_hours:.2f} hours ahead (context: {context})"
                )
                # æœªæ¥æ™‚åˆ»ã®å ´åˆã¯ç¾åœ¨æ™‚åˆ»ã«ä¿®æ­£
                corrected_ts = current_time_ms
                logger.warning(
                    f"ğŸ”§ [H.29-Stage3] Corrected future timestamp: {ts} -> {corrected_ts} (context: {context})"
                )
                return corrected_ts

            # H.28.1-Stage4: APIä»•æ§˜æº–æ‹ ç¢ºèªï¼ˆBitbankç”¨ï¼‰
            # Bitbank APIã¯é€šå¸¸ã€ç¾åœ¨æ™‚åˆ»ã‹ã‚‰éå»72æ™‚é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’æä¾›
            bitbank_limit_ms = current_time_ms - (72 * 60 * 60 * 1000)
            if ts < bitbank_limit_ms:
                logger.warning(
                    f"âš ï¸ [H.28.1-Stage4] Timestamp beyond Bitbank limit: {ts} < {bitbank_limit_ms} (context: {context})"
                )
                # APIåˆ¶é™ã‚’è¶…ãˆã¦ã„ã‚‹ãŒã€ã‚¨ãƒ©ãƒ¼ã§ã¯ãªãè­¦å‘Šã¨ã—ã¦å‡¦ç†

            # H.28.1-Stage5: æœ€çµ‚æ¤œè¨¼ãƒ»ãƒ­ã‚°å‡ºåŠ›
            ts_datetime = datetime.fromtimestamp(ts / 1000)
            logger.debug(
                f"âœ… [H.28.1-Stage5] Timestamp validated: {ts} ({ts_datetime}) (context: {context})"
            )

            return ts

        except Exception as e:
            logger.error(
                f"ğŸš¨ [H.28.1] Timestamp validation error: {e} (timestamp: {timestamp}, context: {context})"
            )
            return None

    def _calculate_safe_since_h28(self, base_timestamp: int, timeframe: str) -> int:
        """
        Phase H.28.1: å®‰å…¨ãªsinceå€¤è¨ˆç®—

        Args:
            base_timestamp: ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆãƒŸãƒªç§’ï¼‰
            timeframe: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆ"1h", "4h", "15m"ç­‰ï¼‰

        Returns:
            int: æ¤œè¨¼æ¸ˆã¿ã®æ¬¡å›å–å¾—ç”¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
        """
        try:
            # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ â†’ãƒŸãƒªç§’å¤‰æ›
            timeframe_ms = {
                "1m": 60 * 1000,
                "5m": 5 * 60 * 1000,
                "15m": 15 * 60 * 1000,
                "1h": 60 * 60 * 1000,
                "4h": 4 * 60 * 60 * 1000,
                "1d": 24 * 60 * 60 * 1000,
            }.get(
                timeframe, 60 * 60 * 1000
            )  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1æ™‚é–“

            # å®‰å…¨ãªæ¬¡å›ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¨ˆç®—
            next_timestamp = base_timestamp + timeframe_ms

            # H.28.1 æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã§æ¤œè¨¼
            validated_timestamp = self._validate_timestamp_h28(
                next_timestamp, f"calculate_since_{timeframe}"
            )

            if validated_timestamp is None:
                # ç•°å¸¸å€¤ã®å ´åˆã¯ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ç”¨
                current_time_ms = int(time.time() * 1000)
                logger.error(
                    f"ğŸš¨ [H.28.1] Safe since calculation failed, using current time: {current_time_ms}"
                )
                return current_time_ms

            return validated_timestamp

        except Exception as e:
            # è¨ˆç®—å¤±æ•—æ™‚ã¯ç¾åœ¨æ™‚åˆ»ã‚’å®‰å…¨ãªå€¤ã¨ã—ã¦è¿”ã™
            current_time_ms = int(time.time() * 1000)
            logger.error(
                f"ğŸš¨ [H.28.1] Since calculation error: {e}, using current time: {current_time_ms}"
            )
            return current_time_ms

    def _should_abort_retry_h28(
        self, error_context: dict, attempt: int, max_attempts: int
    ) -> tuple[bool, str]:
        """
        Phase H.28.2: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒªãƒˆãƒ©ã‚¤åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ 

        Args:
            error_context: ã‚¨ãƒ©ãƒ¼æ–‡è„ˆæƒ…å ±
            attempt: ç¾åœ¨ã®è©¦è¡Œå›æ•°
            max_attempts: æœ€å¤§è©¦è¡Œå›æ•°

        Returns:
            tuple[bool, str]: (åœæ­¢ã™ã¹ãã‹, ç†ç”±)
        """
        # H.28.2-Rule1: ç•°å¸¸ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ¤œå‡ºæ™‚ã¯å³åº§åœæ­¢
        if error_context.get("timestamp_anomaly", False):
            return True, "Timestamp anomaly detected - no point in retrying"

        # H.28.2-Rule2: æ§‹é€ çš„å•é¡Œæ¤œå‡ºæ™‚ã¯å³åº§åœæ­¢
        structural_issues = [
            "Invalid API credentials",
            "Symbol not found",
            "Timeframe not supported",
            "Permission denied",
        ]
        error_msg = error_context.get("error_message", "")
        for issue in structural_issues:
            if issue.lower() in error_msg.lower():
                return True, f"Structural issue detected: {issue}"

        # H.28.2-Rule3: é€£ç¶šç©ºãƒ¬ã‚¹ãƒãƒ³ã‚¹æ•°ã«åŸºã¥ãåˆ¤å®š
        consecutive_empty = error_context.get("consecutive_empty", 0)
        if consecutive_empty >= 8:  # 12â†’8ã«å³æ ¼åŒ–
            return True, f"Too many consecutive empty responses: {consecutive_empty}"

        # H.28.2-Rule4: ã‚¿ã‚¤ãƒ ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ¶å¾¡
        time_span_hours = error_context.get("time_span_hours", 0)
        if time_span_hours > 96:  # 96æ™‚é–“ï¼ˆ4æ—¥ï¼‰ã‚’è¶…ãˆã‚‹å ´åˆã¯åœæ­¢
            return True, f"Time window exceeded: {time_span_hours}h > 96h limit"

        # H.28.2-Rule5: é€šå¸¸ã®è©¦è¡Œå›æ•°åˆ¶é™
        if attempt >= max_attempts:
            return True, f"Max attempts reached: {attempt}/{max_attempts}"

        return False, "Continue retrying"

    def _calculate_smart_backoff_h28(
        self, attempt: int, consecutive_empty: int, error_type: str
    ) -> float:
        """
        Phase H.28.2: ã‚¹ãƒãƒ¼ãƒˆãƒãƒƒã‚¯ã‚ªãƒ•æˆ¦ç•¥

        Args:
            attempt: è©¦è¡Œå›æ•°
            consecutive_empty: é€£ç¶šç©ºãƒ¬ã‚¹ãƒãƒ³ã‚¹æ•°
            error_type: ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—

        Returns:
            float: å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
        """
        # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥ã®åŸºæœ¬å¾…æ©Ÿæ™‚é–“
        base_delays = {
            "rate_limit": 30,  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            "server_error": 10,  # ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
            "network_error": 5,  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼
            "empty_response": 2,  # ç©ºãƒ¬ã‚¹ãƒãƒ³ã‚¹
            "default": 3,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        }

        base_delay = base_delays.get(error_type, base_delays["default"])

        # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼ˆä¸Šé™ä»˜ãï¼‰
        exponential_factor = min(2 ** (attempt - 1), 8)  # æœ€å¤§8å€

        # é€£ç¶šç©ºãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒšãƒŠãƒ«ãƒ†ã‚£
        empty_penalty = consecutive_empty * 0.5

        # æœ€çµ‚è¨ˆç®—ï¼ˆä¸Šé™15ç§’ï¼‰
        total_delay = min(base_delay * exponential_factor + empty_penalty, 15)

        logger.debug(
            f"ğŸ”„ [H.28.2] Smart backoff: attempt={attempt}, empty={consecutive_empty}, type={error_type} -> {total_delay}s"
        )

        return total_delay

    @with_resilience("market_data_fetcher", "get_price_df")
    def get_price_df(
        self,
        timeframe: str = "1m",
        since: Optional[Union[int, float, str, datetime]] = None,
        limit: Optional[int] = None,
        paginate: bool = False,
        sleep: bool = True,
        per_page: int = 500,
        max_consecutive_empty: Optional[int] = None,
        max_consecutive_no_new: Optional[int] = None,
        max_attempts: Optional[int] = None,
    ) -> pd.DataFrame:
        # CSV ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ CSV ã‹ã‚‰èª­ã¿è¾¼ã¿
        if self.csv_path:
            return self._get_price_from_csv(since, limit)

        import time

        since_ms: Optional[int] = None
        if since is not None:
            # Phase H.28.1: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å …ç‰¢æ€§ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
            raw_since_ms: Optional[int] = None

            try:
                if hasattr(since, "value"):  # pd.Timestampã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
                    # pd.Timestamp.valueã¯ãƒŠãƒç§’ãªã®ã§ã€ãƒŸãƒªç§’ã«å¤‰æ›
                    raw_since_ms = int(since.value // 1_000_000)
                elif isinstance(since, str):
                    dt = datetime.fromisoformat(since.replace("Z", "+00:00"))
                    raw_since_ms = int(dt.timestamp() * 1000)
                elif isinstance(since, datetime):
                    raw_since_ms = int(since.timestamp() * 1000)
                elif isinstance(since, numbers.Real):
                    ts = int(since)
                    raw_since_ms = ts if ts > 1e12 else int(ts * 1000)
                else:
                    raise TypeError(f"Unsupported type for since: {type(since)}")

                # H.28.1: 5æ®µéšæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã§æ¤œè¨¼
                since_ms = self._validate_timestamp_h28(
                    raw_since_ms, f"since_calculation_{type(since).__name__}"
                )

                if since_ms is None:
                    # æ¤œè¨¼å¤±æ•—æ™‚ã¯ç¾åœ¨æ™‚åˆ»ã‹ã‚‰72æ™‚é–“å‰ã‚’å®‰å…¨ãªé–‹å§‹ç‚¹ã¨ã—ã¦ä½¿ç”¨
                    current_time_ms = int(time.time() * 1000)
                    since_ms = current_time_ms - (72 * 60 * 60 * 1000)  # 72æ™‚é–“å‰
                    logger.warning(
                        f"ğŸ”§ [H.28.1] Invalid since value, using 72h ago: {since_ms}"
                    )

            except Exception as e:
                logger.error(f"ğŸš¨ [H.28.1] Since calculation error: {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç¾åœ¨æ™‚åˆ»ã‹ã‚‰72æ™‚é–“å‰ã‚’ä½¿ç”¨
                current_time_ms = int(time.time() * 1000)
                since_ms = current_time_ms - (72 * 60 * 60 * 1000)
                logger.warning(f"ğŸ”§ [H.28.1] Error fallback, using 72h ago: {since_ms}")

        max_records = limit if limit is not None else float("inf")

        if paginate and limit:
            # Phase H.23.6: ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ€é©åŒ–ï¼ˆ400ãƒ¬ã‚³ãƒ¼ãƒ‰ç¢ºå®Ÿé”æˆï¼‰
            MAX_ATTEMPTS = (
                max_attempts if max_attempts is not None else 25
            )  # 20â†’25ã«å¢—åŠ ï¼ˆper_page=200å¯¾å¿œï¼‰
            MAX_CONSECUTIVE_EMPTY = (
                max_consecutive_empty
                if max_consecutive_empty is not None
                else 12  # Phase H.26: 200ãƒ¬ã‚³ãƒ¼ãƒ‰ç¢ºå®Ÿå–å¾—ã®ãŸã‚å¢—åŠ 
            )
            MAX_CONSECUTIVE_NO_NEW = (
                max_consecutive_no_new
                if max_consecutive_no_new is not None
                else 20  # Phase H.26: å°ãƒãƒƒãƒå¯¾å¿œãƒ»ç¶™ç¶šå–å¾—å¼·åŒ–
            )
            logger.info(f"ğŸ”„ Paginated fetch: limit={limit}, per_page={per_page}")
            logger.info(
                f"ğŸ”§ [PHASE-H4] Pagination config: MAX_ATTEMPTS={MAX_ATTEMPTS}, MAX_CONSECUTIVE_EMPTY={MAX_CONSECUTIVE_EMPTY}, MAX_CONSECUTIVE_NO_NEW={MAX_CONSECUTIVE_NO_NEW}"
            )
            records: List = []
            seen_ts = set()
            last_since = since_ms
            attempt = 0
            consecutive_empty = 0
            consecutive_no_new = 0

            while len(records) < max_records and attempt < MAX_ATTEMPTS:
                logger.info(
                    f"ğŸ”„ Attempt {attempt + 1}/{MAX_ATTEMPTS}: fetching from {last_since}, "
                    f"current={len(records)}/{max_records}"
                )

                try:
                    # Phase H.6.3: APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è©³ç´°ãƒ­ã‚°
                    logger.info(
                        f"ğŸ” [PHASE-H6] Calling API: symbol={self.symbol}, timeframe={timeframe}, "
                        f"since={last_since}, limit={per_page}"
                    )

                    batch = self.client.fetch_ohlcv(
                        self.symbol, timeframe, last_since, per_page
                    )

                    # Phase H.6.3: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã¨å†…å®¹ã®è©³ç´°ãƒ­ã‚°
                    logger.info(
                        f"ğŸ” [PHASE-H6] API response type: {type(batch).__name__}"
                    )

                    if batch and isinstance(batch, list) and len(batch) > 0:
                        logger.info(f"ğŸ” [PHASE-H6] First record sample: {batch[0]}")

                    if isinstance(batch, pd.DataFrame):
                        logger.info(
                            f"âœ… Received DataFrame directly: {len(batch)} records"
                        )
                        return batch

                    if not batch:
                        consecutive_empty += 1
                        logger.warning(
                            f"âš ï¸ Empty batch {consecutive_empty}/{MAX_CONSECUTIVE_EMPTY}"
                        )

                        # Phase H.28.2: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒªãƒˆãƒ©ã‚¤ã‚·ã‚¹ãƒ†ãƒ é©ç”¨
                        # ã‚¨ãƒ©ãƒ¼æ–‡è„ˆæ§‹ç¯‰
                        error_context = {
                            "consecutive_empty": consecutive_empty,
                            "timestamp_anomaly": last_since
                            and (
                                last_since
                                > int(time.time() * 1000) + 24 * 60 * 60 * 1000
                            ),
                            "error_message": "Empty batch response",
                            "time_span_hours": (
                                (int(time.time() * 1000) - (since_ms or 0))
                                / (1000 * 3600)
                                if since_ms
                                else 0
                            ),
                        }

                        # ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœæ­¢åˆ¤å®š
                        should_abort, abort_reason = self._should_abort_retry_h28(
                            error_context, attempt + 1, MAX_ATTEMPTS
                        )

                        if should_abort:
                            logger.warning(
                                f"ğŸš¨ [H.28.2] INTELLIGENT TERMINATION: {abort_reason}"
                            )
                            logger.warning(
                                f"ğŸ“Š [H.28.2] Final stats: {len(records)} records collected in {attempt + 1} attempts"
                            )
                            break

                        # Phase H.28.2: ã‚¹ãƒãƒ¼ãƒˆãƒãƒƒã‚¯ã‚ªãƒ•æˆ¦ç•¥
                        backoff_delay = self._calculate_smart_backoff_h28(
                            attempt + 1, consecutive_empty, "empty_response"
                        )

                        logger.info(
                            f"ğŸ”„ [H.28.2] Smart backoff: {backoff_delay}s (attempt={attempt + 1}, empty={consecutive_empty})"
                        )
                        time.sleep(backoff_delay)
                        attempt += 1
                        continue

                    # ç©ºãƒãƒƒãƒã‚«ã‚¦ãƒ³ã‚¿ãƒªã‚»ãƒƒãƒˆ
                    consecutive_empty = 0

                    logger.info(f"ğŸ“Š Batch received: {len(batch)} records")

                    # Phase H.4: ãƒãƒƒãƒå†…å®¹ã®è©³ç´°åˆ†æãƒ­ã‚°
                    if batch:
                        first_ts = batch[0][0]
                        last_ts = batch[-1][0]
                        first_time = pd.Timestamp(first_ts, unit="ms")
                        last_time = pd.Timestamp(last_ts, unit="ms")
                        logger.info(
                            f"ğŸ” [PHASE-H4] Batch time range: {first_time} to {last_time}"
                        )
                        logger.info(
                            f"ğŸ” [PHASE-H4] Batch time span: {(last_ts - first_ts) / (1000 * 3600):.2f} hours"
                        )

                    added = False
                    new_records_count = 0
                    duplicate_count = 0
                    for row in batch:
                        ts = row[0]
                        if ts not in seen_ts:
                            seen_ts.add(ts)
                            records.append(row)
                            new_records_count += 1
                            # Phase H.28.1: å®‰å…¨ãªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¨ˆç®—
                            # ã¾ãšã€ç¾åœ¨ã®tsã‚’æ¤œè¨¼
                            validated_ts = self._validate_timestamp_h28(
                                ts, f"batch_record_{new_records_count}"
                            )
                            if validated_ts is not None:
                                # æ¤œè¨¼æ¸ˆã¿tsã‹ã‚‰å®‰å…¨ãªæ¬¡å›ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨ˆç®—
                                last_since = self._calculate_safe_since_h28(
                                    validated_ts, timeframe
                                )
                                logger.debug(
                                    f"ğŸ”§ [H.28.1] Safe since calculated: {validated_ts} + {timeframe} -> {last_since}"
                                )
                            else:
                                # tsãŒç•°å¸¸å€¤ã®å ´åˆã¯ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ç”¨
                                current_time_ms = int(time.time() * 1000)
                                last_since = current_time_ms
                                logger.warning(
                                    f"ğŸš¨ [H.28.1] Invalid batch timestamp {ts}, using current time: {last_since}"
                                )
                            added = True
                        else:
                            duplicate_count += 1

                    logger.info(
                        f"âœ… [PHASE-H4] Added {new_records_count} new records, {duplicate_count} duplicates, total={len(records)}"
                    )
                    logger.info(
                        f"ğŸ“ˆ [PHASE-H4] Progress: {len(records)}/{max_records} ({len(records)/max_records*100:.1f}%)"
                    )

                    if not added:
                        consecutive_no_new += 1
                        logger.warning(
                            f"âš ï¸ No new records {consecutive_no_new}/{MAX_CONSECUTIVE_NO_NEW}"
                        )

                        # Phase H.4: æ—©æœŸçµ‚äº†ã®è©³ç´°ç†ç”±ãƒ­ã‚°
                        logger.warning(
                            f"ğŸ” [PHASE-H4] No new records reason: {len(batch)} total received, {duplicate_count} were duplicates"
                        )

                        if consecutive_no_new >= MAX_CONSECUTIVE_NO_NEW:
                            logger.warning(
                                f"âŒ [PHASE-H4] EARLY TERMINATION: Too many attempts with no new records ({consecutive_no_new}/{MAX_CONSECUTIVE_NO_NEW}), stopping pagination"
                            )
                            logger.warning(
                                f"ğŸ“Š [PHASE-H4] Final stats: {len(records)} records collected in {attempt + 1} attempts"
                            )
                            break

                        # Phase H.20.1.3: æ”¹å–„ã•ã‚ŒãŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é€²è¡Œæˆ¦ç•¥
                        if batch:
                            # ã‚ˆã‚Šå°ã•ãªå˜ä½ã§ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’é€²ã‚ã¦è¦‹é€ƒã—ã‚’æ¸›å°‘
                            timeframe_ms = {
                                "1m": 60 * 1000,
                                "5m": 5 * 60 * 1000,
                                "15m": 15 * 60 * 1000,
                                "1h": 60 * 60 * 1000,
                                "4h": 4 * 60 * 60 * 1000,
                                "1d": 24 * 60 * 60 * 1000,
                            }.get(timeframe, 60 * 60 * 1000)

                            # å°åˆ»ã¿ã«é€²è¡Œï¼ˆå¾“æ¥ã®åŠåˆ†ã®å¹…ã§é€²ã‚€ï¼‰
                            step_ms = timeframe_ms // 2  # åŠåˆ†ã®æ™‚é–“é–“éš”ã§é€²è¡Œ
                            last_since = batch[-1][0] + step_ms

                            logger.debug(
                                f"ğŸ”„ [PHASE-H20.1.3] Timestamp advance: +{step_ms}ms "
                                f"(half of {timeframe} interval)"
                            )
                    else:
                        # æ–°ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã£ãŸå ´åˆã¯ã‚«ã‚¦ãƒ³ã‚¿ãƒªã‚»ãƒƒãƒˆ
                        consecutive_no_new = 0
                        logger.info(
                            f"âœ… Added {sum(1 for row in batch if row[0] in seen_ts and row[0] >= last_since - len(batch))} records, total={len(records)}"
                        )

                    # Phase H.20.1.3: æœ€é©åŒ–ã•ã‚ŒãŸãƒ¬ãƒ¼ãƒˆåˆ¶é™
                    if (
                        sleep
                        and hasattr(self.exchange, "rateLimit")
                        and self.exchange.rateLimit
                    ):
                        base_delay = self.exchange.rateLimit / 1000.0

                        # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªå–å¾—ã®ãŸã‚åŸºæœ¬å¾…æ©Ÿæ™‚é–“ã‚’çŸ­ç¸®
                        base_delay *= 0.8  # 20%çŸ­ç¸®ã§ã‚ˆã‚Šç©æ¥µçš„å–å¾—

                        # é€£ç¶šå•é¡Œç™ºç”Ÿæ™‚ã®å»¶é•·ã‚‚æŠ‘åˆ¶ï¼ˆ1.5â†’1.3ï¼‰
                        if consecutive_empty > 0 or consecutive_no_new > 0:
                            base_delay *= 1.3

                        logger.debug(
                            f"ğŸ”„ [PHASE-H20.1.3] Rate limit delay: {base_delay:.3f}ç§’"
                        )
                        time.sleep(base_delay)

                except Exception as e:
                    error_str = str(e).lower()
                    logger.error(
                        f"âŒ [H.28.2] Batch fetch error on attempt {attempt + 1}: {e}"
                    )

                    # Phase H.28.2: ã‚¨ãƒ©ãƒ¼åˆ†é¡ã¨ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆå¯¾å¿œ
                    error_type = "default"
                    if "rate limit" in error_str or "too many requests" in error_str:
                        error_type = "rate_limit"
                    elif "timeout" in error_str or "connection" in error_str:
                        error_type = "network_error"
                    elif (
                        "server error" in error_str
                        or "500" in error_str
                        or "502" in error_str
                    ):
                        error_type = "server_error"
                    elif "permission" in error_str or "unauthorized" in error_str:
                        error_type = "structural"
                    elif "symbol" in error_str or "market" in error_str:
                        error_type = "structural"

                    # ã‚¨ãƒ©ãƒ¼æ–‡è„ˆæ§‹ç¯‰
                    error_context = {
                        "consecutive_empty": consecutive_empty,
                        "timestamp_anomaly": last_since
                        and (
                            last_since > int(time.time() * 1000) + 24 * 60 * 60 * 1000
                        ),
                        "error_message": str(e),
                        "time_span_hours": (
                            (int(time.time() * 1000) - (since_ms or 0)) / (1000 * 3600)
                            if since_ms
                            else 0
                        ),
                    }

                    # ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœæ­¢åˆ¤å®š
                    should_abort, abort_reason = self._should_abort_retry_h28(
                        error_context, attempt + 1, MAX_ATTEMPTS
                    )

                    if should_abort:
                        logger.warning(
                            f"ğŸš¨ [H.28.2] INTELLIGENT TERMINATION after error: {abort_reason}"
                        )
                        break

                    # Phase H.28.2: ã‚¹ãƒãƒ¼ãƒˆãƒãƒƒã‚¯ã‚ªãƒ•æˆ¦ç•¥
                    error_delay = self._calculate_smart_backoff_h28(
                        attempt + 1, consecutive_empty, error_type
                    )
                    logger.info(
                        f"ğŸ”„ [H.28.2] Error recovery backoff: {error_delay}s (type={error_type})"
                    )
                    time.sleep(error_delay)

                attempt += 1

            # Phase H.4: ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ã®è©³ç´°ã‚µãƒãƒªãƒ¼
            logger.info(
                f"âœ… [PHASE-H4] Pagination complete: {len(records)} total records collected in {attempt} attempts"
            )
            if records:
                first_record_time = pd.Timestamp(records[0][0], unit="ms")
                last_record_time = pd.Timestamp(records[-1][0], unit="ms")
                time_span = (records[-1][0] - records[0][0]) / (1000 * 3600)  # hours
                logger.info(
                    f"ğŸ“Š [PHASE-H4] Data time range: {first_record_time} to {last_record_time} ({time_span:.2f} hours)"
                )
            logger.info(
                f"ğŸ”§ [PHASE-H4] Termination reason: MAX_RECORDS_REACHED={len(records) >= max_records}, MAX_ATTEMPTS_REACHED={attempt >= MAX_ATTEMPTS}"
            )
            data = records if limit is None else records[:limit]

        else:
            # Phase H.6.3: éãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§ã‚‚ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ 
            logger.info(
                f"ğŸ” [PHASE-H6] Non-paginated fetch: timeframe={timeframe}, "
                f"since_ms={since_ms}, limit={limit}"
            )

            raw = self.client.fetch_ohlcv(self.symbol, timeframe, since_ms, limit)

            # Phase H.6.3: ãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°ãƒ­ã‚°
            logger.info(
                f"ğŸ” [PHASE-H6] Response type: {type(raw).__name__}, "
                f"content: {len(raw) if raw else 0} records"
            )

            # Phase 6.3: APIå¿œç­”ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ¤œè¨¼æ©Ÿèƒ½è¿½åŠ 
            if raw and isinstance(raw, list) and len(raw) > 0:
                current_time = pd.Timestamp.now(tz="UTC")
                current_ts_ms = int(current_time.timestamp() * 1000)

                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç•°å¸¸æ¤œå‡º
                anomalous_timestamps = []
                valid_records = []

                for i, record in enumerate(raw):
                    if len(record) >= 6:  # OHLCV + timestamp
                        timestamp_ms = record[0]
                        timestamp_dt = pd.to_datetime(timestamp_ms, unit="ms", utc=True)

                        # ç•°å¸¸ãƒã‚§ãƒƒã‚¯: æœªæ¥ãƒ‡ãƒ¼ã‚¿
                        if timestamp_ms > current_ts_ms:
                            anomalous_timestamps.append(
                                {
                                    "index": i,
                                    "timestamp_ms": timestamp_ms,
                                    "timestamp_dt": timestamp_dt,
                                    "issue": "future_data",
                                }
                            )

                        # ç•°å¸¸ãƒã‚§ãƒƒã‚¯: æ¥µç«¯ã«å¤ã„ãƒ‡ãƒ¼ã‚¿ï¼ˆ2å¹´ä»¥ä¸Šå‰ï¼‰
                        elif timestamp_ms < current_ts_ms - (
                            2 * 365 * 24 * 60 * 60 * 1000
                        ):
                            anomalous_timestamps.append(
                                {
                                    "index": i,
                                    "timestamp_ms": timestamp_ms,
                                    "timestamp_dt": timestamp_dt,
                                    "issue": "too_old",
                                }
                            )

                        # ç•°å¸¸ãƒã‚§ãƒƒã‚¯: ç„¡åŠ¹ãªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆè² ã®å€¤ãªã©ï¼‰
                        elif timestamp_ms <= 0:
                            anomalous_timestamps.append(
                                {
                                    "index": i,
                                    "timestamp_ms": timestamp_ms,
                                    "timestamp_dt": "invalid",
                                    "issue": "invalid_timestamp",
                                }
                            )
                        else:
                            valid_records.append(record)

                # ç•°å¸¸ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®ãƒ­ã‚°å‡ºåŠ›
                if anomalous_timestamps:
                    logger.warning(
                        "ğŸš¨ [PHASE-6.3] Anomalous timestamps detected in API response:"
                    )
                    logger.warning(
                        f"   Total records: {len(raw)}, Anomalous: {len(anomalous_timestamps)}, Valid: {len(valid_records)}"
                    )

                    for anomaly in anomalous_timestamps[:3]:  # æœ€å¤§3ä»¶ã¾ã§è©³ç´°è¡¨ç¤º
                        logger.warning(
                            f"   [{anomaly['index']}] {anomaly['issue']}: "
                            f"{anomaly['timestamp_ms']} -> {anomaly['timestamp_dt']}"
                        )

                    if len(anomalous_timestamps) > 3:
                        logger.warning(
                            f"   ... and {len(anomalous_timestamps) - 3} more anomalous records"
                        )

                    # æœ‰åŠ¹ãªãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ã‚’ä½¿ç”¨
                    raw = valid_records
                    logger.info(
                        f"âœ… [PHASE-6.3] Using {len(valid_records)} valid records after timestamp filtering"
                    )
                else:
                    logger.debug(
                        f"âœ… [PHASE-6.3] All {len(raw)} API response timestamps are valid"
                    )

                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †åºæ€§æ¤œè¨¼
                if len(raw) > 1:
                    timestamps = [record[0] for record in raw]
                    is_sorted = all(
                        timestamps[i] <= timestamps[i + 1]
                        for i in range(len(timestamps) - 1)
                    )
                    if not is_sorted:
                        logger.warning(
                            "âš ï¸ [PHASE-6.3] API response timestamps are not in chronological order"
                        )
                        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆ
                        raw.sort(key=lambda x: x[0])
                        logger.info("âœ… [PHASE-6.3] Records sorted by timestamp")
                    else:
                        logger.debug(
                            "âœ… [PHASE-6.3] API response timestamps are properly ordered"
                        )
            else:
                logger.debug(
                    "ğŸ” [PHASE-6.3] No data to validate or invalid response format"
                )
            if (
                sleep
                and hasattr(self.exchange, "rateLimit")
                and self.exchange.rateLimit
            ):
                time.sleep(self.exchange.rateLimit / 1000.0)
            if isinstance(raw, pd.DataFrame):
                return raw
            data = raw or []

            # Bitbankå›ºæœ‰ã®å†è©¦è¡Œãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå¿…è¦ã«å¿œã˜ã¦å®Ÿè£…ï¼‰
            if not data and self.exchange_id == "bitbank":
                logger.warning(
                    f"âš ï¸ [PHASE-H6] Bitbank returned no data for since_ms={since_ms}"
                )
                # Phase H.6.3: æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’è©¦ã¿ã‚‹
                logger.info(
                    "ğŸ”„ [PHASE-H6] Trying to fetch latest data without since parameter"
                )
                raw_latest = self.client.fetch_ohlcv(self.symbol, timeframe, None, 10)
                if raw_latest:
                    logger.info(f"âœ… [PHASE-H6] Got {len(raw_latest)} latest records")
                    data = raw_latest

        if not data:
            return pd.DataFrame()

        df = pd.DataFrame(
            data, columns=["timestamp", "open", "high", "low", "close", "volume"]
        )
        df["datetime"] = pd.to_datetime(df["timestamp"], unit="ms", utc=True)
        df = df.set_index("datetime")

        # Phase 6.1: æœªæ¥ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½è¿½åŠ ï¼ˆç·©å’Œç‰ˆï¼‰
        # 24æ™‚é–“ã®ä½™è£•ã‚’æŒãŸã›ã¦ç•°å¸¸ãªæœªæ¥ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        current_time = pd.Timestamp.now(tz="UTC")
        tolerance_hours = 24  # 24æ™‚é–“ã®ä½™è£•
        future_threshold = current_time + pd.Timedelta(hours=tolerance_hours)
        future_data_mask = df.index > future_threshold
        if future_data_mask.any():
            future_count = future_data_mask.sum()
            logger.warning(
                f"ğŸš« [PHASE-6.1] Future data detected and filtered: {future_count} records"
            )
            logger.warning(
                f"   Current time: {current_time.strftime('%Y-%m-%d %H:%M:%S UTC')}"
            )
            if future_count > 0:
                future_samples = df[future_data_mask].head(3)
                for idx, row in future_samples.iterrows():
                    logger.warning(
                        f"   Future timestamp: {idx.strftime('%Y-%m-%d %H:%M:%S UTC')}"
                    )
            # æœªæ¥ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»
            df = df[~future_data_mask]
            logger.info(
                f"âœ… [PHASE-6.1] Remaining records after future data removal: {len(df)}"
            )
        else:
            logger.debug(f"âœ… [PHASE-6.1] No future data detected in {len(df)} records")

        if isinstance(since, datetime) or (isinstance(since, str) and since):
            df = df.iloc[1:]

        if limit is not None:
            df = df.head(limit)

        return df[["open", "high", "low", "close", "volume"]]

    def _is_data_too_old(self, data: pd.DataFrame, max_age_hours: float = 2.0) -> bool:
        """
        ãƒ‡ãƒ¼ã‚¿ãŒå¤ã™ãã‚‹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯

        Args:
            data: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®DataFrame
            max_age_hours: è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§æ™‚é–“ï¼ˆæ™‚é–“ï¼‰

        Returns:
            bool: ãƒ‡ãƒ¼ã‚¿ãŒå¤ã™ãã‚‹å ´åˆTrue
        """
        if data is None or data.empty:
            return True

        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
        latest_timestamp = data.index.max()
        current_time = pd.Timestamp.now(tz="UTC")

        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ã‚’çµ±ä¸€
        if latest_timestamp.tz is None:
            latest_timestamp = latest_timestamp.tz_localize("UTC")

        # ãƒ‡ãƒ¼ã‚¿ã®å¹´é½¢ã‚’è¨ˆç®—
        data_age = current_time - latest_timestamp
        data_age_hours = data_age.total_seconds() / 3600

        logger.info(
            f"ğŸ” [DATA-FRESHNESS] Latest data: {latest_timestamp}, Age: {data_age_hours:.1f}h"
        )

        if data_age_hours > max_age_hours:
            logger.warning(
                f"âš ï¸ [DATA-FRESHNESS] Data too old: {data_age_hours:.1f}h > {max_age_hours}h"
            )
            return True

        logger.info(
            f"âœ… [DATA-FRESHNESS] Data is fresh: {data_age_hours:.1f}h <= {max_age_hours}h"
        )
        return False

    def _select_freshest_data(
        self, data1: pd.DataFrame, data2: pd.DataFrame
    ) -> pd.DataFrame:
        """
        2ã¤ã®DataFrameã‹ã‚‰æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ

        Args:
            data1: æ¯”è¼ƒå¯¾è±¡ãƒ‡ãƒ¼ã‚¿1
            data2: æ¯”è¼ƒå¯¾è±¡ãƒ‡ãƒ¼ã‚¿2

        Returns:
            pd.DataFrame: ã‚ˆã‚Šæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿
        """
        if data1 is None or data1.empty:
            return data2 if data2 is not None else pd.DataFrame()

        if data2 is None or data2.empty:
            return data1

        # æœ€æ–°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ¯”è¼ƒ
        latest1 = data1.index.max()
        latest2 = data2.index.max()

        if latest1.tz is None:
            latest1 = latest1.tz_localize("UTC")
        if latest2.tz is None:
            latest2 = latest2.tz_localize("UTC")

        if latest2 > latest1:
            logger.info(
                f"âœ… [DATA-SELECT] Selected data2 (newer): {latest2} vs {latest1}"
            )
            return data2
        else:
            logger.info(
                f"âœ… [DATA-SELECT] Selected data1 (newer/equal): {latest1} vs {latest2}"
            )
            return data1

    def fetch_with_freshness_fallback(
        self,
        timeframe: str = "1h",
        since: Optional[Union[int, float, str, datetime]] = None,
        limit: Optional[int] = None,
        max_age_hours: float = 2.0,
        **kwargs,
    ) -> pd.DataFrame:
        """
        ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦ãƒã‚§ãƒƒã‚¯ä»˜ããƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆPhase H.8.1 + H.9.4åœŸæ—¥å¯¾å¿œå¼·åŒ–ï¼‰

        Args:
            timeframe: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ 
            since: é–‹å§‹æ™‚åˆ»
            limit: å–å¾—ä»¶æ•°ä¸Šé™
            max_age_hours: è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§ãƒ‡ãƒ¼ã‚¿å¹´é½¢ï¼ˆæ™‚é–“ï¼‰
            **kwargs: get_price_dfã¸ã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            pd.DataFrame: æ–°é®®ãªãƒ‡ãƒ¼ã‚¿
        """
        logger.info(
            "ğŸš€ [PHASE-H9.4] Starting enhanced freshness data fetch with fallback optimization"
        )

        # Phase H.9.4: sinceè¨ˆç®—å•é¡Œè§£æ±ºãƒ»å¼·åˆ¶æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—æˆ¦ç•¥
        current_time = pd.Timestamp.now(tz="UTC")

        # since ãŒå¤ã™ãã‚‹å ´åˆã®è‡ªå‹•èª¿æ•´ï¼ˆPhase H.9.4æ ¹æœ¬è§£æ±ºï¼‰
        if since is not None:
            if isinstance(since, str):
                since_dt = pd.to_datetime(since, utc=True)
            elif isinstance(since, datetime):
                since_dt = (
                    since
                    if since.tzinfo
                    else since.replace(tzinfo=pd.Timestamp.now().tz)
                )
            else:
                since_dt = pd.to_datetime(
                    since, unit="ms" if since > 1e12 else "s", utc=True
                )

            age_hours = (current_time - since_dt).total_seconds() / 3600
            if age_hours > 24:  # 24æ™‚é–“ä»¥ä¸Šå¤ã„å ´åˆã¯å¼·åˆ¶èª¿æ•´
                new_since = current_time - pd.Timedelta(hours=6)  # 6æ™‚é–“å‰ã«èª¿æ•´
                logger.warning(
                    f"ğŸ”§ [PHASE-H9.4] since too old ({age_hours:.1f}h), adjusting: {since_dt} â†’ {new_since}"
                )
                since = new_since

        logger.info(
            f"ğŸ”§ [PHASE-H9.4] Config: timeframe={timeframe}, since={since}, limit={limit}, max_age_hours={max_age_hours}"
        )

        try:
            # é€šå¸¸ã®sinceæŒ‡å®šå–å¾—
            logger.info("ğŸ“¡ [PHASE-H8.1] Attempting since-based fetch...")
            data = self.get_price_df(
                timeframe=timeframe, since=since, limit=limit, **kwargs
            )

            # ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆPhase H.9.4: èª¿æ•´ã•ã‚ŒãŸmax_age_hoursä½¿ç”¨ï¼‰
            if not self._is_data_too_old(data, max_age_hours):
                logger.info("âœ… [PHASE-H9.4] Since-based data is fresh, using it")
                return data

            # ãƒ‡ãƒ¼ã‚¿ãŒå¤ã„å ´åˆï¼šsince=Noneã§æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—
            logger.warning(
                "ğŸ”„ [PHASE-H8.1] Data too old, falling back to latest data fetch"
            )
            latest_data = self.get_price_df(
                timeframe=timeframe,
                since=None,
                limit=min(limit or 100, 100),  # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã¯100ä»¶ä»¥ä¸‹ã«åˆ¶é™
                paginate=False,  # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã¯é«˜é€Ÿå–å¾—
                **{k: v for k, v in kwargs.items() if k != "paginate"},
            )

            if not latest_data.empty:
                logger.info(
                    f"âœ… [PHASE-H8.1] Latest data fallback successful: {len(latest_data)} records"
                )
                return latest_data
            else:
                logger.warning(
                    "âš ï¸ [PHASE-H8.1] Latest data fallback also empty, returning original"
                )
                return data

        except Exception as e:
            logger.error(f"âŒ [PHASE-H8.1] Freshness fallback failed: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯é€šå¸¸ã®å–å¾—ã‚’è©¦è¡Œ
            try:
                return self.get_price_df(
                    timeframe=timeframe, since=since, limit=limit, **kwargs
                )
            except Exception as e2:
                logger.error(
                    f"âŒ [PHASE-H8.1] Fallback to normal fetch also failed: {e2}"
                )
                return pd.DataFrame()

    def parallel_data_fetch(
        self,
        timeframe: str = "1h",
        since: Optional[Union[int, float, str, datetime]] = None,
        limit: Optional[int] = None,
        **kwargs,
    ) -> pd.DataFrame:
        """
        ä¸¦è¡Œãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆsinceæŒ‡å®š vs since=Noneï¼‰ï¼ˆPhase H.8.1ï¼‰

        Args:
            timeframe: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ 
            since: é–‹å§‹æ™‚åˆ»
            limit: å–å¾—ä»¶æ•°ä¸Šé™
            **kwargs: get_price_dfã¸ã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            pd.DataFrame: ã‚ˆã‚Šæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿
        """
        logger.info("ğŸš€ [PHASE-H8.1] Starting parallel data fetch")

        import concurrent.futures

        def fetch_since_data():
            try:
                logger.info("ğŸ“¡ [PARALLEL-SINCE] Fetching since-based data...")
                return self.get_price_df(
                    timeframe=timeframe, since=since, limit=limit, **kwargs
                )
            except Exception as e:
                logger.warning(f"âš ï¸ [PARALLEL-SINCE] Failed: {e}")
                return pd.DataFrame()

        def fetch_latest_data():
            try:
                logger.info("ğŸ“¡ [PARALLEL-LATEST] Fetching latest data...")
                # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã¯é«˜é€Ÿè¨­å®šã§å–å¾—
                return self.get_price_df(
                    timeframe=timeframe,
                    since=None,
                    limit=min(limit or 50, 50),
                    paginate=False,
                    **{
                        k: v
                        for k, v in kwargs.items()
                        if k not in ["paginate", "per_page"]
                    },
                )
            except Exception as e:
                logger.warning(f"âš ï¸ [PARALLEL-LATEST] Failed: {e}")
                return pd.DataFrame()

        try:
            # ä¸¦è¡Œå®Ÿè¡Œï¼ˆæœ€å¤§60ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                future_since = executor.submit(fetch_since_data)
                future_latest = executor.submit(fetch_latest_data)

                # çµæœå–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
                try:
                    data_since = future_since.result(timeout=60)
                    data_latest = future_latest.result(timeout=60)
                except concurrent.futures.TimeoutError:
                    logger.warning(
                        "âš ï¸ [PHASE-H8.1] Parallel fetch timeout, canceling futures"
                    )
                    future_since.cancel()
                    future_latest.cancel()
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯é€šå¸¸ã®å–å¾—ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    return self.get_price_df(
                        timeframe=timeframe, since=since, limit=limit, **kwargs
                    )

            # ã‚ˆã‚Šæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ
            best_data = self._select_freshest_data(data_since, data_latest)

            if not best_data.empty:
                logger.info(
                    f"âœ… [PHASE-H8.1] Parallel fetch successful: {len(best_data)} records"
                )
            else:
                logger.warning(
                    "âš ï¸ [PHASE-H8.1] Both parallel fetches returned empty data"
                )

            return best_data

        except Exception as e:
            logger.error(f"âŒ [PHASE-H8.1] Parallel fetch failed: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯é€šå¸¸ã®å–å¾—ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self.get_price_df(
                timeframe=timeframe, since=since, limit=limit, **kwargs
            )

    def _get_price_from_csv(
        self,
        since: Optional[Union[int, float, str, datetime]] = None,
        limit: Optional[int] = None,
    ) -> pd.DataFrame:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿

        Parameters
        ----------
        since : Optional[Union[int, float, str, datetime]]
            é–‹å§‹æ™‚åˆ»
        limit : Optional[int]
            å–å¾—ä»¶æ•°åˆ¶é™

        Returns
        -------
        pd.DataFrame
            ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        """
        if not os.path.exists(self.csv_path):
            raise FileNotFoundError(f"CSV file not found: {self.csv_path}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        df = pd.read_csv(self.csv_path, parse_dates=True, index_col=0)

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒdatetimeã§ãªã„å ´åˆã¯å¤‰æ›
        if not isinstance(df.index, pd.DatetimeIndex):
            df.index = pd.to_datetime(df.index)

        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’ UTC ã«è¨­å®š
        if df.index.tz is None:
            df.index = df.index.tz_localize("UTC")

        # since ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if since is not None:
            if isinstance(since, str):
                since_dt = pd.to_datetime(since, utc=True)
            elif isinstance(since, datetime):
                since_dt = since
                if since_dt.tzinfo is None:
                    since_dt = since_dt.replace(tzinfo=pd.Timestamp.now().tz)
            else:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å ´åˆ
                since_dt = pd.to_datetime(since, unit="ms", utc=True)

            df = df[df.index >= since_dt]

        # limit ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä»¶æ•°åˆ¶é™
        if limit is not None:
            df = df.head(limit)

        # å¿…è¦ãªåˆ—ã®ã¿è¿”ã™
        required_columns = ["open", "high", "low", "close", "volume"]
        available_columns = [col for col in required_columns if col in df.columns]

        if not available_columns:
            raise ValueError(
                f"Required columns {required_columns} not found in CSV file"
            )

        return df[available_columns]


class DataPreprocessor:
    @staticmethod
    def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:
        return df[~df.index.duplicated(keep="first")]

    @staticmethod
    def fill_missing_bars(df: pd.DataFrame, timeframe: str = "1h") -> pd.DataFrame:
        freq_offset = to_offset(timeframe)
        idx = pd.date_range(
            start=df.index[0],
            end=df.index[-1],
            freq=freq_offset,
            tz=df.index.tz,
        )
        df2 = df.reindex(idx)
        for col in ["open", "high", "low", "close", "volume"]:
            df2[col] = df2[col].ffill()
        return df2

    @staticmethod
    def remove_outliers(
        df: pd.DataFrame, thresh: float = 3.5, window: int = 20
    ) -> pd.DataFrame:
        df = df.copy()
        price_cols = ["open", "high", "low", "close"]
        for col in [c for c in price_cols if c in df.columns]:
            median = (
                df[col]
                .rolling(
                    window=window,
                    center=True,
                    min_periods=1,
                )
                .median()
            )
            deviation = (df[col] - median).abs()
            mad = deviation.rolling(
                window=window,
                center=True,
                min_periods=1,
            ).median()
            mad = mad + 1e-8
            modified_zscore = 0.6745 * deviation / mad
            is_outlier = modified_zscore > thresh
            temp = df[col].copy()
            temp[is_outlier] = np.nan
            filled = temp.rolling(
                window=window,
                center=True,
                min_periods=1,
            ).mean()
            filled = filled.fillna(method="ffill").fillna(method="bfill")
            df[col] = np.where(is_outlier, filled, df[col])
        return df

    @staticmethod
    def clean(
        df: pd.DataFrame,
        timeframe: str = "1h",
        thresh: float = 3.5,
        window: int = 20,
    ) -> pd.DataFrame:
        df = DataPreprocessor.remove_duplicates(df)
        df = DataPreprocessor.fill_missing_bars(df, timeframe)
        df = DataPreprocessor.remove_outliers(df, thresh, window)
        for col in ["open", "high", "low", "close", "volume"]:
            df[col] = df[col].ffill().bfill()
        return df


class OIDataFetcher:
    """OIï¼ˆæœªæ±ºæ¸ˆå»ºç‰ï¼‰ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¹"""

    def __init__(self, market_fetcher: MarketDataFetcher):
        self.market_fetcher = market_fetcher
        self.exchange = market_fetcher.exchange
        self.symbol = market_fetcher.symbol

    def get_oi_data(
        self,
        timeframe: str = "1h",
        since: Optional[Union[str, datetime]] = None,
        limit: Optional[int] = None,
    ) -> pd.DataFrame:
        """
        OIï¼ˆæœªæ±ºæ¸ˆå»ºç‰ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Returns
        -------
        pd.DataFrame
            OIãƒ‡ãƒ¼ã‚¿ï¼ˆindex: datetime, columns: oi_amount, oi_valueï¼‰
        """
        try:
            # Bitbankã®å ´åˆã®OIå–å¾—ï¼ˆç¾ç‰©ãƒ»ä¿¡ç”¨å–å¼•å¯¾å¿œï¼‰
            if self.market_fetcher.exchange_id == "bitbank":
                # Bitbankç¾ç‰©å–å¼•ã«ã¯OIãŒãªã„ãŸã‚ã€ä»£æ›¿æŒ‡æ¨™ã‚’ä½¿ç”¨
                # å–å¼•é‡Ã—ä¾¡æ ¼ã§ãƒã‚¸ã‚·ãƒ§ãƒ³è¦æ¨¡ã‚’æ¨å®š
                logger.info("Generating OI approximation for Bitbank")

                # æœ€æ–°ã®ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                price_data = self.market_fetcher.get_price_df(
                    timeframe="1h", limit=168  # 1é€±é–“åˆ†
                )

                if not price_data.empty:
                    # ãƒã‚¸ã‚·ãƒ§ãƒ³è¦æ¨¡ã®æ¨å®šï¼ˆå‡ºæ¥é«˜Ã—ä¾¡æ ¼ï¼‰
                    position_size = price_data["volume"] * price_data["close"]

                    result = pd.DataFrame(
                        {
                            "oi_amount": position_size,
                            "oi_value": position_size * price_data["close"],
                        }
                    )

                    return result

                # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ç©ºã®DataFrame
                return pd.DataFrame(columns=["oi_amount", "oi_value"])

        except Exception as e:
            logger.warning(f"Could not fetch OI approximation: {e}")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç©ºã®DataFrameã‚’è¿”ã™
        return pd.DataFrame(columns=["oi_amount", "oi_value"])

    def calculate_oi_features(self, df: pd.DataFrame, window: int = 24) -> pd.DataFrame:
        """
        OIé–¢é€£ã®ç‰¹å¾´é‡ã‚’è¨ˆç®—

        Parameters
        ----------
        df : pd.DataFrame
            OIãƒ‡ãƒ¼ã‚¿
        window : int
            ç§»å‹•å¹³å‡ã®æœŸé–“

        Returns
        -------
        pd.DataFrame
            OIç‰¹å¾´é‡ä»˜ãDataFrame
        """
        if df.empty or "oi_amount" not in df.columns:
            return df

        # OIå¤‰åŒ–ç‡
        df["oi_pct_change"] = df["oi_amount"].pct_change()

        # OIç§»å‹•å¹³å‡
        df[f"oi_ma_{window}"] = df["oi_amount"].rolling(window=window).mean()

        # OIæ¨™æº–åŒ–ï¼ˆZ-scoreï¼‰
        df["oi_zscore"] = (
            df["oi_amount"] - df["oi_amount"].rolling(window=window).mean()
        ) / df["oi_amount"].rolling(window=window).std()

        # OIå‹¢ã„ï¼ˆmomentumï¼‰
        df["oi_momentum"] = df["oi_amount"] / df["oi_amount"].shift(window) - 1

        # OIæ€¥æ¿€ãªå¤‰åŒ–æ¤œçŸ¥
        df["oi_spike"] = (
            df["oi_pct_change"].abs()
            > df["oi_pct_change"].rolling(window=window).std() * 2
        ).astype(int)

        return df
