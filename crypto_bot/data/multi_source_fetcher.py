"""
MultiSourceDataFetcher - è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çµ±åˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
ChatGPTã‚¢ãƒ‰ãƒã‚¤ã‚¹åæ˜ ç‰ˆï¼šæ®µéšçš„ãƒ»å“è³ªé‡è¦–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®çµ±åˆç®¡ç†ãƒ»è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ»ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼
Phase A3: å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ãƒ»ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±åˆ
"""

import logging
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd

from .global_cache_manager import get_global_cache

logger = logging.getLogger(__name__)


class DataSourceStatus(Enum):
    """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çŠ¶æ…‹ç®¡ç†"""

    ACTIVE = "active"
    FAILING = "failing"
    DISABLED = "disabled"
    CIRCUIT_BREAKER = "circuit_breaker"


@dataclass
class DataSourceConfig:
    """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š"""

    name: str
    enabled: bool = True
    priority: int = 1  # 1ãŒæœ€é«˜å„ªå…ˆåº¦
    timeout: int = 10
    max_retries: int = 3
    circuit_breaker_threshold: int = 5
    circuit_breaker_timeout: int = 300  # 5åˆ†
    quality_threshold: float = 0.7


@dataclass
class DataSourceResult:
    """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å–å¾—çµæœ"""

    source_name: str
    data: Optional[pd.DataFrame]
    quality_score: float
    error: Optional[str] = None
    fetch_time: float = 0.0
    success: bool = False


class MultiSourceDataFetcher(ABC):
    """
    è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çµ±åˆç®¡ç†ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹

    æ©Ÿèƒ½ï¼š
    - è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®é †æ¬¡è©¦è¡Œ
    - è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½
    - ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ãƒ»é–¾å€¤ç®¡ç†
    - 24æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
    - ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼æ©Ÿèƒ½
    - è¨­å®šãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
    """

    def __init__(
        self, config: Optional[Dict[str, Any]] = None, data_type: str = "unknown"
    ):
        self.config = config or {}
        self.data_type = data_type
        self.data_config = self.config.get("external_data", {}).get(data_type, {})

        # åŸºæœ¬è¨­å®š
        self.cache_hours = self.data_config.get("cache_hours", 24)
        self.quality_threshold = self.data_config.get("quality_threshold", 0.7)
        self.enabled = self.data_config.get("enabled", True)

        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼çµ±åˆï¼ˆPhase A3ï¼‰
        self.global_cache = get_global_cache()

        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çŠ¶æ…‹ç®¡ç†
        self.source_states: Dict[str, DataSourceStatus] = {}
        self.source_failure_counts: Dict[str, int] = {}
        self.source_circuit_breaker_times: Dict[str, datetime] = {}

        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®šåˆæœŸåŒ–
        self._initialize_data_sources()

        logger.info(f"ğŸ”§ MultiSourceDataFetcher initialized for {data_type}")
        logger.info(f"  - Sources: {[ds.name for ds in self.data_sources]}")
        logger.info(f"  - Cache hours: {self.cache_hours}")
        logger.info(f"  - Quality threshold: {self.quality_threshold}")
        logger.info(f"  - Global cache integrated: {self.global_cache is not None}")

    def _initialize_data_sources(self) -> None:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®šåˆæœŸåŒ–"""
        sources_config = self.data_config.get("sources", [])
        self.data_sources: List[DataSourceConfig] = []

        for i, source_name in enumerate(sources_config):
            source_config = DataSourceConfig(
                name=source_name,
                priority=i + 1,
                enabled=True,
                timeout=self.data_config.get("timeout", 10),
                max_retries=self.data_config.get("max_retries", 3),
                circuit_breaker_threshold=self.data_config.get(
                    "circuit_breaker_threshold", 5
                ),
                circuit_breaker_timeout=self.data_config.get(
                    "circuit_breaker_timeout", 300
                ),
                quality_threshold=self.quality_threshold,
            )

            self.data_sources.append(source_config)
            self.source_states[source_name] = DataSourceStatus.ACTIVE
            self.source_failure_counts[source_name] = 0

        # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
        self.data_sources.sort(key=lambda x: x.priority)

    def _get_cache_key(self, **kwargs) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        return self.global_cache.get_cache_key(self.data_type, kwargs)

    def _is_cache_valid(self, **kwargs) -> Tuple[Optional[pd.DataFrame], bool]:
        """ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆPhase A3å¯¾å¿œï¼‰"""
        cache_key = self._get_cache_key(**kwargs)
        cached_data = self.global_cache.get(cache_key)
        
        if cached_data is not None:
            logger.debug(f"ğŸ“‹ Global cache hit for {self.data_type}: {cache_key}")
            return cached_data, True
        
        logger.debug(f"âŒ Global cache miss for {self.data_type}: {cache_key}")
        return None, False

    def _update_cache(self, data: pd.DataFrame, quality_score: float, **kwargs) -> None:
        """ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°ï¼ˆPhase A3å¯¾å¿œï¼‰"""
        cache_key = self._get_cache_key(**kwargs)
        ttl = timedelta(hours=self.cache_hours)
        
        success = self.global_cache.put(
            cache_key,
            data,
            source=self.data_type,
            ttl=ttl,
            quality_score=quality_score
        )
        
        if success:
            logger.info(
                f"âœ… {self.data_type} global cache updated: {len(data)} records, "
                f"quality={quality_score:.3f}, ttl={self.cache_hours}h"
            )
        else:
            logger.warning(f"âš ï¸ Failed to update global cache for {self.data_type}")

    def _check_circuit_breaker(self, source_name: str) -> bool:
        """ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯"""
        if source_name not in self.source_circuit_breaker_times:
            return False

        circuit_time = self.source_circuit_breaker_times[source_name]
        timeout_seconds = next(
            (
                ds.circuit_breaker_timeout
                for ds in self.data_sources
                if ds.name == source_name
            ),
            300,
        )

        if datetime.now() - circuit_time > timedelta(seconds=timeout_seconds):
            # ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼è§£é™¤
            self.source_states[source_name] = DataSourceStatus.ACTIVE
            self.source_failure_counts[source_name] = 0
            del self.source_circuit_breaker_times[source_name]
            logger.info(f"ğŸ”„ Circuit breaker reset for {source_name}")
            return False

        return True

    def _update_source_state(self, source_name: str, success: bool) -> None:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çŠ¶æ…‹æ›´æ–°"""
        if success:
            self.source_failure_counts[source_name] = 0
            self.source_states[source_name] = DataSourceStatus.ACTIVE
        else:
            self.source_failure_counts[source_name] += 1

            # ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼åˆ¤å®š
            threshold = next(
                (
                    ds.circuit_breaker_threshold
                    for ds in self.data_sources
                    if ds.name == source_name
                ),
                5,
            )

            if self.source_failure_counts[source_name] >= threshold:
                self.source_states[source_name] = DataSourceStatus.CIRCUIT_BREAKER
                self.source_circuit_breaker_times[source_name] = datetime.now()
                logger.warning(f"ğŸš¨ Circuit breaker activated for {source_name}")
            else:
                self.source_states[source_name] = DataSourceStatus.FAILING

    def get_data(self, **kwargs) -> Optional[pd.DataFrame]:
        """
        è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå“è³ªç›£è¦–çµ±åˆç‰ˆï¼‰

        Args:
            **kwargs: å„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            å–å¾—ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®DataFrame
        """
        start_time = time.time()

        try:
            # æ©Ÿèƒ½ç„¡åŠ¹ãƒã‚§ãƒƒã‚¯
            if not self.enabled:
                logger.info(f"âš ï¸ {self.data_type} fetcher is disabled")
                return None

            # å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ å–å¾—
            try:
                from ..monitoring.data_quality_monitor import get_quality_monitor

                quality_monitor = get_quality_monitor(self.config)
            except ImportError:
                quality_monitor = None

            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆPhase A3å¯¾å¿œï¼‰
            cached_data, cache_hit = self._is_cache_valid(**kwargs)
            if cache_hit and cached_data is not None:
                logger.info(
                    f"âœ… Using global cached {self.data_type} data "
                    f"({len(cached_data)} records)"
                )

                # å“è³ªç›£è¦–è¨˜éŒ²ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼‰
                if quality_monitor:
                    quality_monitor.record_quality_metrics(
                        source_type=self.data_type,
                        source_name="global_cache",
                        quality_score=1.0,  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã¯å“è³ªä¿è¨¼æ¸ˆã¿
                        default_ratio=0.0,  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãªã—
                        success=True,
                        latency_ms=(time.time() - start_time) * 1000,
                    )

                return cached_data

            logger.info(f"ğŸ” Fetching {self.data_type} data from multiple sources")

            # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é †æ¬¡è©¦è¡Œ
            for source_config in self.data_sources:
                if not source_config.enabled:
                    continue

                # ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ãƒã‚§ãƒƒã‚¯
                if self._check_circuit_breaker(source_config.name):
                    logger.warning(
                        f"ğŸš¨ {source_config.name} circuit breaker active, skipping"
                    )
                    continue

                # ãƒ‡ãƒ¼ã‚¿å–å¾—è©¦è¡Œ
                result = self._fetch_from_source(source_config, **kwargs)

                # å“è³ªç›£è¦–è¨˜éŒ²
                if quality_monitor:
                    quality_monitor.record_quality_metrics(
                        source_type=self.data_type,
                        source_name=source_config.name,
                        quality_score=result.quality_score,
                        default_ratio=self._calculate_default_ratio(result.data),
                        success=result.success,
                        latency_ms=result.fetch_time * 1000,
                        error_count=1 if not result.success else 0,
                    )

                # çµæœè©•ä¾¡
                if result.success and result.data is not None:
                    logger.info(
                        f"âœ… {self.data_type} data from {source_config.name}: "
                        f"{len(result.data)} records, "
                        f"quality={result.quality_score:.3f}"
                    )

                    # å“è³ªé–¾å€¤ãƒã‚§ãƒƒã‚¯
                    if result.quality_score >= source_config.quality_threshold:
                        # æˆåŠŸå‡¦ç†
                        self._update_source_state(source_config.name, True)
                        self._update_cache(result.data, result.quality_score, **kwargs)
                        return result.data
                    else:
                        logger.warning(
                            f"âš ï¸ Low quality {self.data_type} data from "
                            f"{source_config.name}: {result.quality_score:.3f} < "
                            f"{source_config.quality_threshold}"
                        )
                else:
                    logger.warning(
                        f"âŒ Failed to fetch {self.data_type} data from "
                        f"{source_config.name}: {result.error}"
                    )

                # å¤±æ•—å‡¦ç†
                self._update_source_state(source_config.name, False)

            # ã™ã¹ã¦ã®ã‚½ãƒ¼ã‚¹ãŒå¤±æ•—
            logger.warning(f"âŒ All {self.data_type} data sources failed")

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
            fallback_data = self._generate_fallback_data(**kwargs)
            if fallback_data is not None and not fallback_data.empty:
                logger.info(
                    f"âœ… Using {self.data_type} fallback data: "
                    f"{len(fallback_data)} records"
                )

                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å“è³ªç›£è¦–è¨˜éŒ²
                if quality_monitor:
                    quality_monitor.record_quality_metrics(
                        source_type=self.data_type,
                        source_name="fallback",
                        quality_score=0.3,  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ä½å“è³ª
                        default_ratio=1.0,  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯100%ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                        success=True,
                        latency_ms=(time.time() - start_time) * 1000,
                    )

                # ä½å“è³ªã§ã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
                self._update_cache(fallback_data, 0.3)
                return fallback_data

            logger.error(f"âŒ All {self.data_type} data sources and fallback failed")

            # å®Œå…¨å¤±æ•—ã®å“è³ªç›£è¦–è¨˜éŒ²
            if quality_monitor:
                quality_monitor.record_quality_metrics(
                    source_type=self.data_type,
                    source_name="all_sources",
                    quality_score=0.0,
                    default_ratio=1.0,
                    success=False,
                    latency_ms=(time.time() - start_time) * 1000,
                    error_count=len(self.data_sources),
                )

            return None

        except Exception as e:
            logger.error(f"âŒ {self.data_type} data fetch failed: {e}")

            # ä¾‹å¤–æ™‚ã®å“è³ªç›£è¦–è¨˜éŒ²
            if quality_monitor:
                quality_monitor.record_quality_metrics(
                    source_type=self.data_type,
                    source_name="exception",
                    quality_score=0.0,
                    default_ratio=1.0,
                    success=False,
                    latency_ms=(time.time() - start_time) * 1000,
                    error_count=1,
                )

            return None

    def _fetch_from_source(
        self, source_config: DataSourceConfig, **kwargs
    ) -> DataSourceResult:
        """ç‰¹å®šãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        start_time = time.time()

        try:
            # æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—
            data = self._fetch_data_from_source(source_config.name, **kwargs)
            fetch_time = time.time() - start_time

            if data is None or data.empty:
                return DataSourceResult(
                    source_name=source_config.name,
                    data=None,
                    quality_score=0.0,
                    error="No data returned",
                    fetch_time=fetch_time,
                    success=False,
                )

            # ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼
            quality_score = self._validate_data_quality(data)

            return DataSourceResult(
                source_name=source_config.name,
                data=data,
                quality_score=quality_score,
                error=None,
                fetch_time=fetch_time,
                success=True,
            )

        except Exception as e:
            fetch_time = time.time() - start_time
            return DataSourceResult(
                source_name=source_config.name,
                data=None,
                quality_score=0.0,
                error=str(e),
                fetch_time=fetch_time,
                success=False,
            )

    def get_source_status(self) -> Dict[str, Dict[str, Any]]:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çŠ¶æ…‹å–å¾—"""
        status = {}
        for source_config in self.data_sources:
            status[source_config.name] = {
                "status": self.source_states.get(
                    source_config.name, DataSourceStatus.ACTIVE
                ).value,
                "failure_count": self.source_failure_counts.get(source_config.name, 0),
                "enabled": source_config.enabled,
                "priority": source_config.priority,
                "circuit_breaker_active": self._check_circuit_breaker(
                    source_config.name
                ),
            }
        return status

    def get_cache_info(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±å–å¾—"""
        return {
            "cache_valid": self._is_cache_valid(),
            "cache_age_hours": (
                (datetime.now() - self.cache_timestamp).total_seconds() / 3600
                if self.cache_timestamp
                else None
            ),
            "cache_records": (
                len(self.cache_data) if self.cache_data is not None else 0
            ),
            "cache_quality_score": self.cache_quality_score,
            "cache_hours_limit": self.cache_hours,
        }

    def _calculate_default_ratio(self, data: Optional[pd.DataFrame]) -> float:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤æ¯”ç‡è¨ˆç®—ï¼ˆ30%ãƒ«ãƒ¼ãƒ«ç”¨ï¼‰"""
        if data is None or data.empty:
            return 1.0  # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯100%ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ç‰¹æœ‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
            # åŸºæœ¬çš„ã«ã¯0å€¤ã€NaNå€¤ã€ç‰¹å®šã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ãƒã‚§ãƒƒã‚¯
            total_values = data.size
            if total_values == 0:
                return 1.0

            # NaNå€¤ã®ã‚«ã‚¦ãƒ³ãƒˆ
            nan_count = data.isnull().sum().sum()

            # 0å€¤ã®ã‚«ã‚¦ãƒ³ãƒˆï¼ˆæ•°å€¤ã‚«ãƒ©ãƒ ã®ã¿ï¼‰
            zero_count = 0
            for col in data.select_dtypes(include=["number"]).columns:
                zero_count += (data[col] == 0).sum()

            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ç‰¹æœ‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤åˆ¤å®š
            default_count = self._count_default_values(data)

            # ç·ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆNaN + 0 + ç‰¹æœ‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            total_default = nan_count + zero_count + default_count

            # æ¯”ç‡è¨ˆç®—
            default_ratio = min(total_default / total_values, 1.0)

            return default_ratio

        except Exception as e:
            logger.error(f"âŒ Failed to calculate default ratio: {e}")
            return 1.0  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯100%ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ‰±ã„

    def _count_default_values(self, data: pd.DataFrame) -> int:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ç‰¹æœ‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚«ã‚¦ãƒ³ãƒˆï¼ˆç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰å¯èƒ½ï¼‰"""
        # åŸºæœ¬å®Ÿè£…ã§ã¯0ã‚’è¿”ã™
        # ç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§ç‰¹æœ‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤åˆ¤å®šã‚’å®Ÿè£…
        return 0

    # æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰

    @abstractmethod
    def _fetch_data_from_source(
        self, source_name: str, **kwargs
    ) -> Optional[pd.DataFrame]:
        """ç‰¹å®šãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        pass

    @abstractmethod
    def _validate_data_quality(self, data: pd.DataFrame) -> float:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ï¼ˆç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        pass

    @abstractmethod
    def _generate_fallback_data(self, **kwargs) -> Optional[pd.DataFrame]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        pass
