"""
Data Processor - Phase 16.3-C Split

çµ±åˆå‰: crypto_bot/data/fetcher.pyï¼ˆ1,456è¡Œï¼‰
åˆ†å‰²å¾Œ: crypto_bot/data/fetching/data_processor.py

æ©Ÿèƒ½:
- è¤‡é›‘ãªget_price_dfæ ¸å¿ƒå‡¦ç†ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ãƒªãƒˆãƒ©ã‚¤ãƒ»æ¤œè¨¼ï¼‰
- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ¤œè¨¼ãƒ»ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ 
- DataPreprocessorçµ±åˆï¼ˆé‡è¤‡é™¤å»ãƒ»æ¬ æè£œå®Œãƒ»å¤–ã‚Œå€¤é™¤å»ï¼‰
- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æœ€é©åŒ–
- æ€§èƒ½æœ€é©åŒ–ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿å–å¾—åŠ¹ç‡å‘ä¸Š

Phase 16.3-Cå®Ÿè£…æ—¥: 2025å¹´8æœˆ8æ—¥
"""

import logging
import numbers
import time
from datetime import datetime
from typing import List, Optional, Union

import numpy as np
import pandas as pd
from pandas.tseries.frequencies import to_offset

logger = logging.getLogger(__name__)


class DataProcessor:
    """
    ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»å“è³ªä¿è¨¼çµ±åˆã‚¯ãƒ©ã‚¹

    - è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿å–å¾—å‡¦ç†ï¼ˆget_price_dfï¼‰
    - ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ¤œè¨¼ãƒ»ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼
    - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    - ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†çµ±åˆï¼ˆDataPreprocessoræ©Ÿèƒ½çµ±åˆï¼‰
    """

    def __init__(self, market_client):
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–

        Parameters
        ----------
        market_client : MarketDataFetcher
            å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        """
        self.client_obj = market_client
        self.client = market_client.client
        self.exchange = market_client.exchange
        self.symbol = market_client.symbol
        self.exchange_id = market_client.exchange_id
        self.csv_path = market_client.csv_path
        self.resilience = market_client.resilience

    def _validate_timestamp_h28(
        self, timestamp: Optional[int], context: str = "unknown"
    ) -> Optional[int]:
        """
        Phase H.28.1: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å …ç‰¢æ€§ã‚·ã‚¹ãƒ†ãƒ  - 5æ®µéšæ¤œè¨¼

        Args:
            timestamp: æ¤œè¨¼å¯¾è±¡ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆãƒŸãƒªç§’ï¼‰
            context: å‘¼ã³å‡ºã—å…ƒã®æ–‡è„ˆæƒ…å ±

        Returns:
            Optional[int]: æ¤œè¨¼æ¸ˆã¿ãƒ»ä¿®æ­£æ¸ˆã¿ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã€ã¾ãŸã¯ Noneï¼ˆç•°å¸¸å€¤ã®å ´åˆï¼‰
        """
        if timestamp is None:
            return None

        try:
            # H.28.1-Stage1: å‹å®‰å…¨æ€§ä¿è¨¼
            if (
                not isinstance(timestamp, (int, float))
                or np.isnan(timestamp)
                or np.isinf(timestamp)
            ):
                logger.error(
                    f"ğŸš¨ [H.28.1-Stage1] Invalid timestamp type/value: {timestamp} (context: {context})"
                )
                return None

            ts = int(timestamp)

            # H.28.1-Stage2: ãƒŸãƒªç§’æ¡æ•°æ¤œè¨¼ï¼ˆ10æ¡=ç§’ã€13æ¡=ãƒŸãƒªç§’ï¼‰
            if ts < 1e12:  # 10æ¡ä»¥ä¸‹ï¼ˆç§’å˜ä½ã®å¯èƒ½æ€§ï¼‰
                ts = ts * 1000  # ãƒŸãƒªç§’ã«å¤‰æ›
                logger.info(
                    f"ğŸ”„ [H.28.1-Stage2] Converted seconds to milliseconds: {timestamp} -> {ts}"
                )

            # H.28.1-Stage3: ç¾å®Ÿçš„ãªæ™‚é–“ç¯„å›²æ¤œè¨¼
            current_time_ms = int(time.time() * 1000)
            # 2020å¹´1æœˆ1æ—¥ã‹ã‚‰æœªæ¥100å¹´ã¾ã§ã‚’æœ‰åŠ¹ç¯„å›²ã¨ã™ã‚‹
            min_timestamp = int(datetime(2020, 1, 1).timestamp() * 1000)
            max_timestamp = current_time_ms + (
                100 * 365 * 24 * 60 * 60 * 1000
            )  # 100å¹´å¾Œ

            if ts < min_timestamp or ts > max_timestamp:
                logger.error(
                    f"ğŸš¨ [H.28.1-Stage3] Timestamp out of realistic range: {ts} (context: {context})"
                )
                logger.error(f"   Valid range: {min_timestamp} - {max_timestamp}")
                return None

            # H.28.1-Stage4: å–å¼•æ‰€APIåˆ¶é™è€ƒæ…®ï¼ˆBitbank: 168æ™‚é–“åˆ¶é™ï¼‰
            if self.exchange_id == "bitbank":
                max_lookback_ms = 168 * 60 * 60 * 1000  # 168æ™‚é–“ï¼ˆ1é€±é–“ï¼‰
                oldest_allowed = current_time_ms - max_lookback_ms

                if ts < oldest_allowed:
                    logger.warning(
                        f"âš ï¸ [H.28.1-Stage4] Timestamp beyond Bitbank limit, adjusting: {ts} -> {oldest_allowed}"
                    )
                    ts = oldest_allowed

            # H.28.1-Stage5: æœªæ¥ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ï¼ˆ24æ™‚é–“ã®ä½™è£•ã‚’æŒãŸã›ã‚‹ï¼‰
            future_threshold = current_time_ms + (24 * 60 * 60 * 1000)
            if ts > future_threshold:
                logger.warning(
                    f"âš ï¸ [H.28.1-Stage5] Future timestamp detected, adjusting: {ts} -> {current_time_ms}"
                )
                ts = current_time_ms

            # æœ€çµ‚æ¤œè¨¼: æ­£å¸¸ç¯„å›²å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
            if oldest_allowed <= ts <= future_threshold:
                logger.debug(f"âœ… [H.28.1] Timestamp validated successfully: {ts}")
                return ts
            else:
                logger.error(
                    f"ğŸš¨ [H.28.1] Final validation failed: {ts} (context: {context})"
                )
                return None

        except Exception as e:
            logger.error(
                f"âŒ [H.28.1] Timestamp validation error: {e} (context: {context})"
            )
            return None

    def _calculate_safe_since_h28(self, base_timestamp: int, timeframe: str) -> int:
        """
        Phase H.28.1: å®‰å…¨ãªsinceå€¤è¨ˆç®—ï¼ˆå–å¼•æ‰€åˆ¶é™ãƒ»ãƒ‡ãƒ¼ã‚¿è¦æ±‚æœ€é©åŒ–ï¼‰

        Args:
            base_timestamp: åŸºæº–ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆãƒŸãƒªç§’ï¼‰
            timeframe: æ™‚é–“æ ï¼ˆ"1m", "1h"ç­‰ï¼‰

        Returns:
            int: æœ€é©åŒ–ã•ã‚ŒãŸsinceå€¤ï¼ˆãƒŸãƒªç§’ï¼‰
        """
        current_time_ms = int(time.time() * 1000)

        # Phase 16.1-C: Bitbank 168æ™‚é–“åˆ¶é™ã¸ã®å¯¾å¿œ
        if self.exchange_id == "bitbank":
            # 167æ™‚é–“å‰ã‚’ä¸Šé™ã¨ã™ã‚‹ï¼ˆ1æ™‚é–“ã®å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ï¼‰
            max_lookback_hours = 167
            max_lookback_ms = max_lookback_hours * 60 * 60 * 1000
            earliest_allowed = current_time_ms - max_lookback_ms

            if base_timestamp < earliest_allowed:
                logger.info(
                    f"ğŸ”§ [Phase 16.1-C] Adjusting since for Bitbank 168h limit: "
                    f"{base_timestamp} -> {earliest_allowed} ({max_lookback_hours}h ago)"
                )
                return earliest_allowed

        # æ™‚é–“æ ã«å¿œã˜ãŸæœ€å°å¿…è¦æœŸé–“ã‚’ç¢ºä¿
        timeframe_multipliers = {
            "1m": 1,
            "5m": 5,
            "15m": 15,
            "1h": 60,
            "4h": 240,
            "1d": 1440,
        }
        multiplier = timeframe_multipliers.get(timeframe, 60)

        # æœ€ä½é™å¿…è¦ãªãƒ‡ãƒ¼ã‚¿æœŸé–“ï¼ˆ400ãƒãƒ¼ Ã— timeframeï¼‰ã‚’ç¢ºä¿
        required_bars = 400
        required_duration_ms = required_bars * multiplier * 60 * 1000

        # å¿…è¦æœŸé–“ã‚’ç¢ºä¿ã§ãã‚‹ç¯„å›²ã§æœ€é©ãªsinceå€¤ã‚’è¨ˆç®—
        optimal_since = max(base_timestamp, current_time_ms - required_duration_ms)

        logger.debug(
            f"ğŸ”§ [H.28.1] Calculated safe since: {optimal_since} "
            f"(timeframe: {timeframe}, required_bars: {required_bars})"
        )

        return optimal_since

    def _should_abort_retry_h28(
        self, error_context: dict, current_attempt: int, max_attempts: int
    ) -> tuple[bool, str]:
        """
        Phase H.28.2: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœæ­¢åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ 

        Args:
            error_context: ã‚¨ãƒ©ãƒ¼æ–‡è„ˆæƒ…å ±
            current_attempt: ç¾åœ¨ã®è©¦è¡Œå›æ•°
            max_attempts: æœ€å¤§è©¦è¡Œå›æ•°

        Returns:
            tuple[bool, str]: (åœæ­¢ã™ã¹ãã‹, åœæ­¢ç†ç”±)
        """
        # æœ€å¤§è©¦è¡Œå›æ•°ã«é”ã—ãŸå ´åˆ
        if current_attempt >= max_attempts:
            return True, f"Max attempts reached ({max_attempts})"

        # é€£ç¶šã—ã¦é•·æ™‚é–“ç©ºãƒ‡ãƒ¼ã‚¿ãŒç¶šãå ´åˆ
        consecutive_empty = error_context.get("consecutive_empty", 0)
        if consecutive_empty >= 15:  # 15å›é€£ç¶šç©ºãƒ‡ãƒ¼ã‚¿
            return True, f"Too many consecutive empty responses ({consecutive_empty})"

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«æ˜ã‚‰ã‹ãªç•°å¸¸ãŒã‚ã‚‹å ´åˆ
        if error_context.get("timestamp_anomaly", False):
            return True, "Timestamp anomaly detected"

        # è¦æ±‚æœŸé–“ãŒç¾å®Ÿçš„ã§ãªã„å ´åˆï¼ˆ30æ—¥ä»¥ä¸Šï¼‰
        time_span_hours = error_context.get("time_span_hours", 0)
        if time_span_hours > 30 * 24:  # 30æ—¥ä»¥ä¸Š
            return True, f"Time span too large ({time_span_hours:.1f}h > 720h limit)"

        return False, ""

    def _calculate_smart_backoff_h28(
        self, attempt_num: int, consecutive_failures: int, error_type: str
    ) -> float:
        """
        Phase H.28.2: ã‚¹ãƒãƒ¼ãƒˆãƒãƒƒã‚¯ã‚ªãƒ•æˆ¦ç•¥

        Args:
            attempt_num: è©¦è¡Œå›æ•°
            consecutive_failures: é€£ç¶šå¤±æ•—å›æ•°
            error_type: ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—

        Returns:
            float: ãƒãƒƒã‚¯ã‚ªãƒ•é…å»¶æ™‚é–“ï¼ˆç§’ï¼‰
        """
        base_delay = 1.0  # åŸºæœ¬é…å»¶1ç§’

        # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥èª¿æ•´
        error_multipliers = {
            "empty_response": 2.0,  # ç©ºãƒ¬ã‚¹ãƒãƒ³ã‚¹
            "rate_limit": 5.0,  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            "timeout": 3.0,  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            "api_error": 2.5,  # API ã‚¨ãƒ©ãƒ¼
        }

        multiplier = error_multipliers.get(error_type, 2.0)

        # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼ˆä¸Šé™10ç§’ï¼‰
        exponential_delay = min(base_delay * (2 ** (attempt_num - 1)), 10.0)

        # é€£ç¶šå¤±æ•—ã«ã‚ˆã‚‹è¿½åŠ é…å»¶
        failure_penalty = consecutive_failures * 0.5

        total_delay = (exponential_delay * multiplier) + failure_penalty

        # æœ€å°0.5ç§’ã€æœ€å¤§15ç§’ã«åˆ¶é™
        final_delay = max(0.5, min(total_delay, 15.0))

        logger.debug(
            f"ğŸ”§ [H.28.2] Smart backoff: attempt={attempt_num}, "
            f"consecutive_failures={consecutive_failures}, type={error_type}, "
            f"delay={final_delay:.2f}s"
        )

        return final_delay

    def get_price_df(
        self,
        timeframe: str = "1m",
        since: Optional[Union[int, float, str, datetime]] = None,
        limit: Optional[int] = None,
        paginate: bool = False,
        sleep: bool = True,
        per_page: int = 500,
        max_consecutive_empty: Optional[int] = None,
        max_consecutive_no_new: Optional[int] = None,
        max_attempts: Optional[int] = None,
    ) -> pd.DataFrame:
        """
        ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆæœ€é©åŒ–ç‰ˆãƒ»Phase 16.3-Cæ€§èƒ½å‘ä¸Šï¼‰

        Parameters
        ----------
        timeframe : str
            æ™‚é–“æ ï¼ˆ"1m", "1h"ç­‰ï¼‰
        since : Optional[Union[int, float, str, datetime]]
            é–‹å§‹æ™‚åˆ»
        limit : Optional[int]
            å–å¾—ä»¶æ•°åˆ¶é™
        paginate : bool
            ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ä½¿ç”¨
        sleep : bool
            ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¾…æ©Ÿ
        per_page : int
            1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®å–å¾—ä»¶æ•°
        max_consecutive_empty : Optional[int]
            æœ€å¤§é€£ç¶šç©ºãƒ¬ã‚¹ãƒãƒ³ã‚¹å›æ•°
        max_consecutive_no_new : Optional[int]
            æœ€å¤§é€£ç¶šæ–°è¦ãƒ‡ãƒ¼ã‚¿ãªã—å›æ•°
        max_attempts : Optional[int]
            æœ€å¤§è©¦è¡Œå›æ•°

        Returns
        -------
        pd.DataFrame
            ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        """
        # CSV ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ CSV ã‹ã‚‰èª­ã¿è¾¼ã¿
        if self.csv_path:
            return self.client_obj._get_price_from_csv(since, limit)

        since_ms: Optional[int] = None
        if since is not None:
            # Phase H.28.1: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å …ç‰¢æ€§ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
            raw_since_ms: Optional[int] = None

            try:
                if hasattr(since, "value"):  # pd.Timestampã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
                    # pd.Timestamp.valueã¯ãƒŠãƒç§’ãªã®ã§ã€ãƒŸãƒªç§’ã«å¤‰æ›
                    raw_since_ms = int(since.value // 1_000_000)
                elif isinstance(since, str):
                    dt = datetime.fromisoformat(since.replace("Z", "+00:00"))
                    raw_since_ms = int(dt.timestamp() * 1000)
                elif isinstance(since, datetime):
                    raw_since_ms = int(since.timestamp() * 1000)
                elif isinstance(since, numbers.Real):
                    ts = int(since)
                    raw_since_ms = ts if ts > 1e12 else int(ts * 1000)
                else:
                    raise TypeError(f"Unsupported type for since: {type(since)}")

                # H.28.1: 5æ®µéšæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã§æ¤œè¨¼
                since_ms = self._validate_timestamp_h28(
                    raw_since_ms, f"since_calculation_{type(since).__name__}"
                )

                if since_ms is None:
                    # æ¤œè¨¼å¤±æ•—æ™‚ã¯å®‰å…¨ãªé–‹å§‹ç‚¹ã‚’ä½¿ç”¨
                    since_ms = self._calculate_safe_since_h28(
                        raw_since_ms or (int(time.time() * 1000) - 167 * 60 * 60 * 1000),
                        timeframe,
                    )
                    logger.warning(
                        f"ğŸ”§ [Phase 16.1-C] Invalid since value, using safe fallback: {since_ms}"
                    )

            except Exception as e:
                logger.error(f"ğŸš¨ [H.28.1] Since calculation error: {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨ãªé–‹å§‹ç‚¹ã‚’ä½¿ç”¨
                since_ms = self._calculate_safe_since_h28(
                    int(time.time() * 1000) - 167 * 60 * 60 * 1000, timeframe
                )
                logger.warning(
                    f"ğŸ”§ [H.28.1] Error fallback, using safe since: {since_ms}"
                )
        if paginate and limit:
            return self._paginated_fetch(
                timeframe,
                since_ms,
                limit,
                per_page,
                sleep,
                max_consecutive_empty,
                max_consecutive_no_new,
                max_attempts,
            )
        else:
            return self._simple_fetch(timeframe, since_ms, limit, sleep)

    def _paginated_fetch(
        self,
        timeframe: str,
        since_ms: Optional[int],
        limit: int,
        per_page: int,
        sleep: bool,
        max_consecutive_empty: Optional[int],
        max_consecutive_no_new: Optional[int],
        max_attempts: Optional[int],
    ) -> pd.DataFrame:
        """
        ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å–å¾—ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
        """
        # Phase H.23.6: ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ€é©åŒ–ï¼ˆ400ãƒ¬ã‚³ãƒ¼ãƒ‰ç¢ºå®Ÿé”æˆï¼‰
        MAX_ATTEMPTS = max_attempts if max_attempts is not None else 25
        MAX_CONSECUTIVE_EMPTY = (
            max_consecutive_empty if max_consecutive_empty is not None else 12
        )
        MAX_CONSECUTIVE_NO_NEW = (
            max_consecutive_no_new if max_consecutive_no_new is not None else 20
        )

        logger.info(f"ğŸ”„ Paginated fetch: limit={limit}, per_page={per_page}")
        logger.info(
            f"ğŸ”§ [PHASE-H4] Pagination config: MAX_ATTEMPTS={MAX_ATTEMPTS}, "
            f"MAX_CONSECUTIVE_EMPTY={MAX_CONSECUTIVE_EMPTY}, MAX_CONSECUTIVE_NO_NEW={MAX_CONSECUTIVE_NO_NEW}"
        )

        records: List = []
        seen_ts = set()
        last_since = since_ms
        attempt = 0
        consecutive_empty = 0
        consecutive_no_new = 0

        while len(records) < limit and attempt < MAX_ATTEMPTS:
            logger.info(
                f"ğŸ”„ Attempt {attempt + 1}/{MAX_ATTEMPTS}: fetching from {last_since}, "
                f"current={len(records)}/{limit}"
            )

            try:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ¤œè¨¼ã¨èª¿æ•´ï¼ˆä¿®æ­£ç‰ˆï¼‰
                current_ms = int(time.time() * 1000)

                # åˆå›ã®å ´åˆã€å®‰å…¨ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                if last_since is None:
                    # 24æ™‚é–“å‰ã‹ã‚‰é–‹å§‹ï¼ˆå®‰å…¨ãªç¯„å›²ï¼‰
                    last_since = current_ms - (24 * 60 * 60 * 1000)
                    logger.info(
                        f"ğŸ”§ [TIMESTAMP] Initial timestamp set to 24h ago: {last_since}"
                    )

                # æœªæ¥ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒã‚§ãƒƒã‚¯
                elif last_since > current_ms:
                    logger.warning(
                        f"âš ï¸ [TIMESTAMP] Future timestamp detected: {last_since} > {current_ms}"
                    )
                    # 24æ™‚é–“å‰ã«å®‰å…¨ã«ãƒªã‚»ãƒƒãƒˆ
                    last_since = current_ms - (24 * 60 * 60 * 1000)
                    logger.info(f"ğŸ”§ [TIMESTAMP] Reset to 24h ago: {last_since}")

                # Bitbank APIåˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆ168æ™‚é–“ä»¥å†…ã«çŸ­ç¸®ï¼‰
                else:
                    max_age_ms = 168 * 60 * 60 * 1000  # 168æ™‚é–“ï¼ˆ1é€±é–“ã€è¨­å®šå€¤ã¨ä¸€è‡´ï¼‰
                    min_since = current_ms - max_age_ms
                    if last_since < min_since:
                        logger.warning(
                            f"âš ï¸ [TIMESTAMP] Too old timestamp: {last_since} < {min_since}"
                        )
                        last_since = min_since
                        logger.info(f"ğŸ”§ [TIMESTAMP] Adjusted to 168h ago: {last_since}")

                batch = self.client.fetch_ohlcv(
                    self.symbol, timeframe, last_since, per_page
                )

                if isinstance(batch, pd.DataFrame):
                    logger.info(f"âœ… Received DataFrame directly: {len(batch)} records")
                    return batch

                if not batch:
                    consecutive_empty += 1
                    logger.warning(
                        f"âš ï¸ Empty batch {consecutive_empty}/{MAX_CONSECUTIVE_EMPTY}"
                    )

                    # Phase H.28.2: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒªãƒˆãƒ©ã‚¤ã‚·ã‚¹ãƒ†ãƒ é©ç”¨
                    error_context = {
                        "consecutive_empty": consecutive_empty,
                        "timestamp_anomaly": (
                            last_since
                            and last_since
                            > int(time.time() * 1000) + 24 * 60 * 60 * 1000
                        ),
                        "error_message": "Empty batch response",
                        "time_span_hours": (
                            (int(time.time() * 1000) - (since_ms or 0)) / (1000 * 3600)
                            if since_ms
                            else 0
                        ),
                    }

                    should_abort, abort_reason = self._should_abort_retry_h28(
                        error_context, attempt + 1, MAX_ATTEMPTS
                    )

                    if should_abort:
                        logger.warning(
                            f"ğŸš¨ [H.28.2] INTELLIGENT TERMINATION: {abort_reason}"
                        )
                        logger.warning(
                            f"ğŸ“Š [H.28.2] Final stats: {len(records)} records collected in {attempt + 1} attempts"
                        )
                        break

                    backoff_delay = self._calculate_smart_backoff_h28(
                        attempt + 1, consecutive_empty, "empty_response"
                    )
                    logger.info(f"â³ [H.28.2] Smart backoff: {backoff_delay:.2f}s")
                    time.sleep(backoff_delay)

                    attempt += 1
                    continue

                # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»å“è³ªæ¤œè¨¼
                batch = self._process_batch_data(batch, seen_ts, timeframe)

                if not batch:
                    consecutive_no_new += 1
                    if consecutive_no_new >= MAX_CONSECUTIVE_NO_NEW:
                        logger.info(
                            f"ğŸ”š [PAGINATION] No new data for {consecutive_no_new} attempts, stopping"
                        )
                        break
                else:
                    consecutive_empty = 0
                    consecutive_no_new = 0
                    records.extend(batch)

                    # æ¬¡ã®å–å¾—é–‹å§‹ç‚¹ã‚’æ›´æ–°ï¼ˆtimeframeã«å¿œã˜ã¦èª¿æ•´ï¼‰
                    if batch:
                        # timeframeã«å¿œã˜ãŸé–“éš”ã‚’è¨­å®š
                        if timeframe == "15m":
                            interval_ms = 15 * 60 * 1000  # 15åˆ†
                        elif timeframe == "1h":
                            interval_ms = 60 * 60 * 1000  # 1æ™‚é–“
                        elif timeframe == "4h":
                            interval_ms = 4 * 60 * 60 * 1000  # 4æ™‚é–“
                        else:
                            interval_ms = 60 * 60 * 1000  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1æ™‚é–“

                        # æ¬¡ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨ˆç®—ï¼ˆå®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
                        next_ts = int(batch[-1][0] + interval_ms)

                        # æœªæ¥ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’é˜²ã
                        current_ms = int(time.time() * 1000)
                        if next_ts > current_ms:
                            logger.warning(
                                "âš ï¸ [TIMESTAMP] Next timestamp would be in future, using current time"
                            )
                            last_since = current_ms
                        else:
                            last_since = next_ts

                # Phase 12.2: éƒ¨åˆ†ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®æ•‘æ¸ˆç”¨ï¼‰
                self.client_obj._last_partial_records = records.copy()

                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
                if (
                    sleep
                    and hasattr(self.exchange, "rateLimit")
                    and self.exchange.rateLimit
                ):
                    time.sleep(self.exchange.rateLimit / 1000.0)

            except Exception as e:
                logger.error(f"âŒ [PAGINATION] API call failed: {e}")
                backoff_delay = self._calculate_smart_backoff_h28(
                    attempt + 1, consecutive_empty, "api_error"
                )
                time.sleep(backoff_delay)

            attempt += 1

        # çµæœã‚’DataFrameã«å¤‰æ›
        if records:
            df = self._convert_records_to_dataframe(records)
            logger.info(f"âœ… [PAGINATION] Completed: {len(df)} records collected")
            return df
        else:
            logger.warning("âš ï¸ [PAGINATION] No data collected")
            return pd.DataFrame()

    def _simple_fetch(
        self, timeframe: str, since_ms: Optional[int], limit: Optional[int], sleep: bool
    ) -> pd.DataFrame:
        """
        ã‚·ãƒ³ãƒ—ãƒ«ãªä¸€æ‹¬å–å¾—
        """
        try:
            raw = self.client.fetch_ohlcv(self.symbol, timeframe, since_ms, limit)

            if isinstance(raw, pd.DataFrame):
                return raw

            if not raw:
                return pd.DataFrame()

            # ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            raw = self._validate_and_filter_data(raw, timeframe)

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
            if (
                sleep
                and hasattr(self.exchange, "rateLimit")
                and self.exchange.rateLimit
            ):
                time.sleep(self.exchange.rateLimit / 1000.0)

            return self._convert_records_to_dataframe(raw)

        except Exception as e:
            logger.error(f"âŒ [SIMPLE_FETCH] Failed: {e}")
            return pd.DataFrame()

    def _process_batch_data(self, batch: List, seen_ts: set, timeframe: str) -> List:
        """
        ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
        """
        if not batch:
            return []

        # ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼
        batch = self._validate_and_filter_data(batch, timeframe)

        # é‡è¤‡å‰Šé™¤ï¼ˆæ€§èƒ½æœ€é©åŒ–ï¼šsetæ“ä½œã‚’ä½¿ç”¨ï¼‰
        new_records = []
        for record in batch:
            if record[0] not in seen_ts:
                seen_ts.add(record[0])
                new_records.append(record)

        return new_records

    def _validate_and_filter_data(self, data: List, timeframe: str) -> List:
        """
        ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
        """
        if not data:
            return []

        valid_records = []
        anomalous_timestamps = []

        current_time_ms = int(time.time() * 1000)
        future_threshold = current_time_ms + (24 * 60 * 60 * 1000)  # 24æ™‚é–“å¾Œ

        for record in data:
            try:
                timestamp_ms = int(record[0])

                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ¤œè¨¼ï¼ˆæœ€é©åŒ–ï¼šç¯„å›²ãƒã‚§ãƒƒã‚¯ã®ã¿ï¼‰
                if timestamp_ms <= future_threshold and timestamp_ms > 0:
                    valid_records.append(record)
                else:
                    anomalous_timestamps.append(
                        {
                            "timestamp_ms": timestamp_ms,
                            "timestamp_dt": pd.to_datetime(
                                timestamp_ms, unit="ms", utc=True
                            ),
                        }
                    )
            except (ValueError, IndexError, TypeError):
                continue

        # ç•°å¸¸ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®è­¦å‘Šï¼ˆæœ€åˆã®3ä»¶ã®ã¿ï¼‰
        if anomalous_timestamps:
            logger.warning(
                f"âš ï¸ [DATA_VALIDATION] Found {len(anomalous_timestamps)} anomalous timestamps"
            )
            for i, anomaly in enumerate(anomalous_timestamps[:3]):
                logger.warning(
                    f"   Anomaly {i+1}: {anomaly['timestamp_ms']} -> {anomaly['timestamp_dt']}"
                )

        return valid_records

    def _convert_records_to_dataframe(self, records: List) -> pd.DataFrame:
        """
        ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’DataFrameã«å¤‰æ›ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
        """
        if not records:
            return pd.DataFrame()

        df = pd.DataFrame(
            records, columns=["timestamp", "open", "high", "low", "close", "volume"]
        )
        df["datetime"] = pd.to_datetime(df["timestamp"], unit="ms", utc=True)
        df = df.set_index("datetime")

        # æœªæ¥ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆ24æ™‚é–“ã®ä½™è£•ï¼‰
        current_time = pd.Timestamp.now(tz="UTC")
        future_threshold = current_time + pd.Timedelta(hours=24)
        future_data_mask = df.index > future_threshold

        if future_data_mask.any():
            future_count = future_data_mask.sum()
            logger.warning(
                f"ğŸš« [DATA_FILTER] Future data filtered: {future_count} records"
            )
            df = df[~future_data_mask]

        return df[["open", "high", "low", "close", "volume"]]

    def fetch_with_freshness_fallback(
        self,
        timeframe: str = "1h",
        since: Optional[Union[int, float, str, datetime]] = None,
        limit: Optional[int] = None,
        max_age_hours: float = 2.0,
        **kwargs,
    ) -> pd.DataFrame:
        """
        ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦ãƒã‚§ãƒƒã‚¯ä»˜ããƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰

        Args:
            timeframe: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ 
            since: é–‹å§‹æ™‚åˆ»
            limit: å–å¾—ä»¶æ•°ä¸Šé™
            max_age_hours: è¨±å®¹ã•ã‚Œã‚‹æœ€å¤§ãƒ‡ãƒ¼ã‚¿å¹´é½¢ï¼ˆæ™‚é–“ï¼‰
            **kwargs: get_price_dfã¸ã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            pd.DataFrame: æ–°é®®ãªãƒ‡ãƒ¼ã‚¿
        """
        logger.info("ğŸ” [FRESHNESS] Starting freshness-aware data fetch")
        logger.info(f"ğŸ” [FRESHNESS] Max age: {max_age_hours}h, timeframe: {timeframe}")

        try:
            # é€šå¸¸ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’è©¦è¡Œ
            primary_data = self.get_price_df(
                timeframe=timeframe, since=since, limit=limit, **kwargs
            )

            # ãƒ‡ãƒ¼ã‚¿ã®æ–°é®®åº¦ãƒã‚§ãƒƒã‚¯
            if not primary_data.empty and not self.client_obj._is_data_too_old(
                primary_data, max_age_hours
            ):
                logger.info(
                    f"âœ… [FRESHNESS] Primary data is fresh: {len(primary_data)} records"
                )
                return primary_data

            logger.warning(
                "âš ï¸ [FRESHNESS] Primary data is stale, trying fallback strategies"
            )

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥1: æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—
            fallback_data = self.get_price_df(
                timeframe=timeframe,
                since=None,
                limit=min(limit or 100, 100),
                paginate=False,
                **kwargs,
            )

            if not fallback_data.empty and not self.client_obj._is_data_too_old(
                fallback_data, max_age_hours
            ):
                logger.info(
                    f"âœ… [FRESHNESS] Fallback data is fresh: {len(fallback_data)} records"
                )
                return fallback_data

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥2: éƒ¨åˆ†ãƒ‡ãƒ¼ã‚¿æ•‘æ¸ˆ
            partial_data = self.client_obj.get_last_partial_data()
            if partial_data is not None and not partial_data.empty:
                logger.info(
                    f"âœ… [FRESHNESS] Using partial data rescue: {len(partial_data)} records"
                )
                return partial_data

            logger.warning("âš ï¸ [FRESHNESS] All fallback strategies failed")
            return primary_data  # å¤ãã¦ã‚‚ä½•ã‹ã—ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™

        except Exception as e:
            logger.error(f"âŒ [FRESHNESS] Freshness fallback failed: {e}")
            return pd.DataFrame()


class DataPreprocessor:
    """
    ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¯ãƒ©ã‚¹ï¼ˆPhase 16.3-Cçµ±åˆç‰ˆï¼‰

    - é‡è¤‡é™¤å»ãƒ»æ¬ æè£œå®Œãƒ»å¤–ã‚Œå€¤é™¤å»
    - é™çš„ãƒ¡ã‚½ãƒƒãƒ‰ã«ã‚ˆã‚‹ç‹¬ç«‹æ€§ä¿è¨¼
    - æ€§èƒ½æœ€é©åŒ–ã«ã‚ˆã‚‹å‡¦ç†æ™‚é–“çŸ­ç¸®
    """

    @staticmethod
    def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:
        """é‡è¤‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é™¤å»"""
        return df[~df.index.duplicated(keep="first")]

    @staticmethod
    def fill_missing_bars(df: pd.DataFrame, timeframe: str = "1h") -> pd.DataFrame:
        """æ¬ æãƒãƒ¼ã‚’è£œå®Œ"""
        if df.empty:
            return df

        freq_offset = to_offset(timeframe)
        idx = pd.date_range(
            start=df.index[0],
            end=df.index[-1],
            freq=freq_offset,
            tz=df.index.tz,
        )
        df2 = df.reindex(idx)

        # é«˜é€Ÿå‰æ–¹è£œå®Œ
        for col in ["open", "high", "low", "close", "volume"]:
            if col in df2.columns:
                df2[col] = df2[col].ffill()

        return df2

    @staticmethod
    def remove_outliers(
        df: pd.DataFrame, thresh: float = 3.5, window: int = 20
    ) -> pd.DataFrame:
        """å¤–ã‚Œå€¤é™¤å»ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        if df.empty:
            return df

        df = df.copy()
        price_cols = ["open", "high", "low", "close"]

        for col in [c for c in price_cols if c in df.columns]:
            # ä¸­å¤®å€¤ãƒ™ãƒ¼ã‚¹ã®å¤–ã‚Œå€¤æ¤œå‡ºï¼ˆMADæ³•ï¼‰
            rolling_median = (
                df[col].rolling(window=window, center=True, min_periods=1).median()
            )
            deviation = (df[col] - rolling_median).abs()
            mad = deviation.rolling(window=window, center=True, min_periods=1).median()
            mad = mad + 1e-8  # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢

            modified_zscore = 0.6745 * deviation / mad
            is_outlier = modified_zscore > thresh

            # å¤–ã‚Œå€¤ã‚’ç§»å‹•å¹³å‡ã§ç½®æ›
            temp = df[col].copy()
            temp[is_outlier] = np.nan
            filled = temp.rolling(window=window, center=True, min_periods=1).mean()
            filled = filled.fillna(method="ffill").fillna(method="bfill")
            df[col] = np.where(is_outlier, filled, df[col])

        return df

    @staticmethod
    def clean(
        df: pd.DataFrame,
        timeframe: str = "1h",
        thresh: float = 3.5,
        window: int = 20,
    ) -> pd.DataFrame:
        """
        çµ±åˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
        """
        if df.empty:
            return df

        # é‡è¤‡é™¤å» â†’ æ¬ æè£œå®Œ â†’ å¤–ã‚Œå€¤é™¤å»ã®é †åºã§å‡¦ç†
        df = DataPreprocessor.remove_duplicates(df)
        df = DataPreprocessor.fill_missing_bars(df, timeframe)
        df = DataPreprocessor.remove_outliers(df, thresh, window)

        # æœ€çµ‚çš„ãªå‰æ–¹ãƒ»å¾Œæ–¹è£œå®Œ
        for col in ["open", "high", "low", "close", "volume"]:
            if col in df.columns:
                df[col] = df[col].ffill().bfill()

        return df
