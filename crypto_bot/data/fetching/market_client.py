"""
Market Client - Phase 16.3-C Split

çµ±åˆå‰: crypto_bot/data/fetcher.pyï¼ˆ1,456è¡Œï¼‰
åˆ†å‰²å¾Œ: crypto_bot/data/fetching/market_client.py

æ©Ÿèƒ½:
- MarketDataFetcheråŸºæœ¬ã‚¯ãƒ©ã‚¹ï¼ˆAPIæ¥ç¶šãƒ»èªè¨¼ãƒ»è¨­å®šï¼‰
- ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»CSVèª­ã¿è¾¼ã¿æ©Ÿèƒ½
- æ®‹é«˜å–å¾—ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»resilienceçµ±åˆ
- åŸºæœ¬çš„ãªã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ©Ÿèƒ½ï¼ˆè¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¯é™¤ãï¼‰

Phase 16.3-Cå®Ÿè£…æ—¥: 2025å¹´8æœˆ8æ—¥
"""

import concurrent.futures
import logging
import os
from datetime import datetime
from typing import Optional, Union

import pandas as pd
from dotenv import load_dotenv

from crypto_bot.execution.factory import create_exchange_client
from crypto_bot.utils.error_resilience import get_resilience_manager, with_resilience

load_dotenv()

logger = logging.getLogger(__name__)


class MarketDataFetcher:
    """
    å¸‚å ´ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

    - å–å¼•æ‰€APIæ¥ç¶šãƒ»èªè¨¼
    - åŸºæœ¬ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ®‹é«˜å–å¾—
    - ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»CSVèª­ã¿è¾¼ã¿
    - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»resilienceçµ±åˆ
    """

    def __init__(
        self,
        exchange_id: str = "bitbank",
        api_key: Optional[str] = None,
        api_secret: Optional[str] = None,
        symbol: str = "BTC/JPY",
        testnet: bool = False,
        ccxt_options: Optional[dict] = None,
        csv_path: Optional[str] = None,
    ):
        self.exchange_id = exchange_id
        self.symbol = symbol
        self.csv_path = csv_path
        self.testnet = testnet

        # Phase H.8.3: ã‚¨ãƒ©ãƒ¼è€æ€§ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
        self.resilience = get_resilience_manager()

        # CSV ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯APIæ¥ç¶šã‚’ã‚¹ã‚­ãƒƒãƒ—
        if csv_path:
            self.client = None
            self.exchange = None
            logger.info(f"ğŸ—‚ï¸ [RESILIENCE] CSV mode initialized: {csv_path}")
        else:
            try:
                api_key = api_key or os.getenv(f"{exchange_id.upper()}_API_KEY")
                api_secret = api_secret or os.getenv(
                    f"{exchange_id.upper()}_API_SECRET"
                )
                env_test_key = os.getenv(f"{exchange_id.upper()}_TESTNET_API_KEY")
                testnet = testnet or bool(env_test_key)
                self.client = create_exchange_client(
                    exchange_id=exchange_id,
                    api_key=api_key,
                    api_secret=api_secret,
                    testnet=testnet,
                    ccxt_options=ccxt_options or {},
                )
                # Bitbankå›ºæœ‰ã®è¨­å®šãŒã‚ã‚Œã°è¿½åŠ 
                if exchange_id == "bitbank":
                    # Bitbankç‰¹æœ‰ã®è¨­å®šã‚’è¿½åŠ ã™ã‚‹å ´åˆã¯ã“ã“ã§å®Ÿè£…
                    pass
                self.exchange = getattr(self.client, "_exchange", self.client)

                # Phase H.8.3: åˆæœŸåŒ–æˆåŠŸã‚’è¨˜éŒ²
                self.resilience.record_success("market_data_fetcher")
                logger.info(
                    f"âœ… [RESILIENCE] Market data fetcher initialized: {exchange_id}"
                )

            except Exception as e:
                # Phase H.8.3: åˆæœŸåŒ–å¤±æ•—ã‚’è¨˜éŒ²
                self.resilience.record_error(
                    component="market_data_fetcher",
                    error_type="InitializationError",
                    error_message=f"Failed to initialize {exchange_id}: {str(e)}",
                    severity="CRITICAL",
                )
                logger.error(
                    f"âŒ [RESILIENCE] Market data fetcher initialization failed: {e}"
                )
                raise

        # Phase 12.2: éƒ¨åˆ†ãƒ‡ãƒ¼ã‚¿ä¿å­˜ç”¨ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®æ•‘æ¸ˆç”¨ï¼‰
        self._last_partial_records = []

    @with_resilience("market_data_fetcher", "fetch_balance")
    def fetch_balance(self) -> dict:
        """
        æ®‹é«˜æƒ…å ±ã‚’å–å¾—

        Returns:
            dict: æ®‹é«˜æƒ…å ±
        """
        if not self.client:
            raise RuntimeError("Client not initialized (CSV mode)")

        logger.info("ğŸ’° [RESILIENCE] Fetching balance with error resilience")
        return self.client.fetch_balance()

    def get_last_partial_data(self) -> Optional[pd.DataFrame]:
        """
        Phase 12.2: æœ€å¾Œã«å–å¾—ã—ãŸéƒ¨åˆ†ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®æ•‘æ¸ˆç”¨ï¼‰

        Returns:
            Optional[pd.DataFrame]: éƒ¨åˆ†ãƒ‡ãƒ¼ã‚¿ã€ã¾ãŸã¯None
        """
        if not self._last_partial_records:
            logger.debug("ğŸ” [PARTIAL-DATA] No partial data available")
            return None

        logger.info(
            f"âœ… [PARTIAL-DATA] Rescued {len(self._last_partial_records)} records from partial data"
        )

        df = self._convert_to_dataframe(self._last_partial_records)
        return df

    def _convert_to_dataframe(self, data) -> pd.DataFrame:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›

        Parameters
        ----------
        data : list or pd.DataFrame
            å¤‰æ›å¯¾è±¡ãƒ‡ãƒ¼ã‚¿

        Returns
        -------
        pd.DataFrame
            å¤‰æ›æ¸ˆã¿DataFrame
        """
        if isinstance(data, pd.DataFrame):
            return data

        if not data:
            return pd.DataFrame()

        df = pd.DataFrame(
            data, columns=["timestamp", "open", "high", "low", "close", "volume"]
        )
        df["datetime"] = pd.to_datetime(df["timestamp"], unit="ms", utc=True)
        df = df.set_index("datetime")
        return df

    def _is_data_too_old(self, data: pd.DataFrame, max_age_hours: float = 2.0) -> bool:
        """
        ãƒ‡ãƒ¼ã‚¿ãŒå¤ã™ãã‚‹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆæ–°é®®ã•åˆ¤å®šï¼‰

        Parameters
        ----------
        data : pd.DataFrame
            åˆ¤å®šå¯¾è±¡ãƒ‡ãƒ¼ã‚¿
        max_age_hours : float
            æœ€å¤§è¨±å®¹æ™‚é–“ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ2æ™‚é–“ï¼‰

        Returns
        -------
        bool
            True: ãƒ‡ãƒ¼ã‚¿ãŒå¤ã™ãã‚‹, False: è¨±å®¹ç¯„å›²å†…
        """
        if data.empty:
            return True

        # æœ€æ–°ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
        latest_timestamp = data.index.max()
        current_time = pd.Timestamp.now(tz="UTC")
        age_hours = (current_time - latest_timestamp).total_seconds() / 3600

        is_too_old = age_hours > max_age_hours

        if is_too_old:
            logger.warning(
                f"âš ï¸ [FRESHNESS] Data is too old: {age_hours:.1f}h > {max_age_hours}h limit"
            )
            logger.warning(
                f"   Latest data: {latest_timestamp.strftime('%Y-%m-%d %H:%M:%S UTC')}"
            )
            logger.warning(
                f"   Current time: {current_time.strftime('%Y-%m-%d %H:%M:%S UTC')}"
            )

        return is_too_old

    def _select_freshest_data(
        self, data1: pd.DataFrame, data2: pd.DataFrame
    ) -> pd.DataFrame:
        """
        2ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã†ã¡ã€ã‚ˆã‚Šæ–°ã—ã„ã‚‚ã®ã‚’é¸æŠ

        Parameters
        ----------
        data1 : pd.DataFrame
            ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ1
        data2 : pd.DataFrame
            ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ2

        Returns
        -------
        pd.DataFrame
            ã‚ˆã‚Šæ–°ã—ã„ï¼ˆã¾ãŸã¯æœ‰åŠ¹ãªï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        """
        # ç©ºã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        if data1.empty and data2.empty:
            logger.warning("âš ï¸ [FRESHNESS] Both datasets are empty")
            return pd.DataFrame()
        elif data1.empty:
            logger.info("ğŸ“Š [FRESHNESS] Data1 is empty, selecting data2")
            return data2
        elif data2.empty:
            logger.info("ğŸ“Š [FRESHNESS] Data2 is empty, selecting data1")
            return data1

        # æœ€æ–°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§æ¯”è¼ƒ
        latest1 = data1.index.max()
        latest2 = data2.index.max()

        if latest1 >= latest2:
            logger.info(
                f"ğŸ“Š [FRESHNESS] Selected data1: {latest1.strftime('%Y-%m-%d %H:%M:%S UTC')} >= {latest2.strftime('%Y-%m-%d %H:%M:%S UTC')}"
            )
            return data1
        else:
            logger.info(
                f"ğŸ“Š [FRESHNESS] Selected data2: {latest2.strftime('%Y-%m-%d %H:%M:%S UTC')} > {latest1.strftime('%Y-%m-%d %H:%M:%S UTC')}"
            )
            return data2

    def parallel_data_fetch(
        self,
        timeframe: str = "1h",
        since: Optional[Union[int, float, str, datetime]] = None,
        limit: Optional[int] = None,
        **kwargs,
    ) -> pd.DataFrame:
        """
        ä¸¦è¡Œãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆsinceæŒ‡å®š vs since=Noneï¼‰ï¼ˆPhase H.8.1ï¼‰

        Args:
            timeframe: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ 
            since: é–‹å§‹æ™‚åˆ»
            limit: å–å¾—ä»¶æ•°ä¸Šé™
            **kwargs: get_price_dfã¸ã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            pd.DataFrame: ã‚ˆã‚Šæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿
        """
        logger.info("ğŸš€ [PHASE-H8.1] Starting parallel data fetch")

        def fetch_since_data():
            try:
                logger.info("ğŸ“¡ [PARALLEL-SINCE] Fetching since-based data...")
                # ã“ã“ã§data_processorã®get_price_dfã‚’å‘¼ã³å‡ºã™ã“ã¨ã«ãªã‚‹
                # å¾ªç’°importã‚’é¿ã‘ã‚‹ãŸã‚ã€å®Ÿéš›ã®å®Ÿè£…ã§ã¯é…å»¶importã¾ãŸã¯ä¾å­˜æ³¨å…¥ã‚’ä½¿ç”¨
                from .data_processor import DataProcessor

                processor = DataProcessor(self)
                return processor.get_price_df(
                    timeframe=timeframe, since=since, limit=limit, **kwargs
                )
            except Exception as e:
                logger.warning(f"âš ï¸ [PARALLEL-SINCE] Failed: {e}")
                return pd.DataFrame()

        def fetch_latest_data():
            try:
                logger.info("ğŸ“¡ [PARALLEL-LATEST] Fetching latest data...")
                # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã¯é«˜é€Ÿè¨­å®šã§å–å¾—
                from .data_processor import DataProcessor

                processor = DataProcessor(self)
                return processor.get_price_df(
                    timeframe=timeframe,
                    since=None,
                    limit=min(limit or 50, 50),
                    paginate=False,
                    **{
                        k: v
                        for k, v in kwargs.items()
                        if k not in ["paginate", "per_page"]
                    },
                )
            except Exception as e:
                logger.warning(f"âš ï¸ [PARALLEL-LATEST] Failed: {e}")
                return pd.DataFrame()

        try:
            # ä¸¦è¡Œå®Ÿè¡Œï¼ˆæœ€å¤§60ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                future_since = executor.submit(fetch_since_data)
                future_latest = executor.submit(fetch_latest_data)

                # çµæœå–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ããƒ»Phase 12.2: 90ç§’çµ±ä¸€ï¼‰
                try:
                    data_since = future_since.result(timeout=90)
                    data_latest = future_latest.result(timeout=90)
                except concurrent.futures.TimeoutError:
                    logger.warning(
                        "âš ï¸ [PHASE-H8.1] Parallel fetch timeout, canceling futures"
                    )
                    future_since.cancel()
                    future_latest.cancel()
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯é€šå¸¸ã®å–å¾—ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    from .data_processor import DataProcessor

                    processor = DataProcessor(self)
                    return processor.get_price_df(
                        timeframe=timeframe, since=since, limit=limit, **kwargs
                    )

            # ã‚ˆã‚Šæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ
            best_data = self._select_freshest_data(data_since, data_latest)

            if not best_data.empty:
                logger.info(
                    f"âœ… [PHASE-H8.1] Parallel fetch successful: {len(best_data)} records"
                )
            else:
                logger.warning(
                    "âš ï¸ [PHASE-H8.1] Both parallel fetches returned empty data"
                )

            return best_data

        except Exception as e:
            logger.error(f"âŒ [PHASE-H8.1] Parallel fetch failed: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯é€šå¸¸ã®å–å¾—ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            from .data_processor import DataProcessor

            processor = DataProcessor(self)
            return processor.get_price_df(
                timeframe=timeframe, since=since, limit=limit, **kwargs
            )

    def _get_price_from_csv(
        self,
        since: Optional[Union[int, float, str, datetime]] = None,
        limit: Optional[int] = None,
    ) -> pd.DataFrame:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿

        Parameters
        ----------
        since : Optional[Union[int, float, str, datetime]]
            é–‹å§‹æ™‚åˆ»
        limit : Optional[int]
            å–å¾—ä»¶æ•°åˆ¶é™

        Returns
        -------
        pd.DataFrame
            ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        """
        if not os.path.exists(self.csv_path):
            raise FileNotFoundError(f"CSV file not found: {self.csv_path}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        df = pd.read_csv(self.csv_path, parse_dates=True, index_col=0)

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒdatetimeã§ãªã„å ´åˆã¯å¤‰æ›
        if not isinstance(df.index, pd.DatetimeIndex):
            df.index = pd.to_datetime(df.index)

        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’ UTC ã«è¨­å®š
        if df.index.tz is None:
            df.index = df.index.tz_localize("UTC")

        # since ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if since is not None:
            if isinstance(since, str):
                since_dt = pd.to_datetime(since, utc=True)
            elif isinstance(since, datetime):
                since_dt = since
                if since_dt.tzinfo is None:
                    since_dt = since_dt.replace(tzinfo=pd.Timestamp.now().tz)
            else:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å ´åˆ
                since_dt = pd.to_datetime(since, unit="ms", utc=True)

            df = df[df.index >= since_dt]

        # limit ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä»¶æ•°åˆ¶é™
        if limit is not None:
            df = df.head(limit)

        # å¿…è¦ãªåˆ—ã®ã¿è¿”ã™
        required_columns = ["open", "high", "low", "close", "volume"]
        available_columns = [col for col in required_columns if col in df.columns]

        if not available_columns:
            raise ValueError(
                f"Required columns {required_columns} not found in CSV file"
            )

        return df[available_columns]
