"""
ãƒã‚¯ãƒ­çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼
Yahoo Financeã‹ã‚‰ç±³ãƒ‰ãƒ«æŒ‡æ•°(DXY)ãƒ»é‡‘åˆ©ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€101ç‰¹å¾´é‡ã‚·ã‚¹ãƒ†ãƒ ã«çµ±åˆ
Phase A3: å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ãƒ»ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±åˆ
"""

import logging
from datetime import datetime, timedelta
from typing import Any, Dict, Optional

import pandas as pd
import yfinance as yf

from ..utils.api_retry import api_retry
from .multi_source_fetcher import MultiSourceDataFetcher

logger = logging.getLogger(__name__)


class MacroDataFetcher(MultiSourceDataFetcher):
    """ãƒã‚¯ãƒ­çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¹ï¼ˆMultiSourceDataFetcherç¶™æ‰¿ãƒ»Phase B1å¯¾å¿œï¼‰"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        # è¦ªã‚¯ãƒ©ã‚¹åˆæœŸåŒ–
        super().__init__(config, data_type="macro")

        self.symbols = {
            "dxy": "DX-Y.NYB",  # ãƒ‰ãƒ«æŒ‡æ•°
            "us10y": "^TNX",  # ç±³10å¹´å‚µåˆ©å›ã‚Š
            "us2y": "^IRX",  # ç±³2å¹´å‚µåˆ©å›ã‚Š
            "usdjpy": "USDJPY=X",  # USD/JPYç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆ
        }

        logger.info("ğŸ”§ MacroDataFetcher initialized with MultiSourceDataFetcher base")

    def _validate_data_quality(self, data: pd.DataFrame) -> float:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ï¼ˆMultiSourceDataFetcheræŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè£…ï¼‰"""
        if data is None or data.empty:
            return 0.0

        # å“è³ªè©•ä¾¡æŒ‡æ¨™
        total_points = len(data)
        valid_points = len(data.dropna())

        if total_points == 0:
            return 0.0

        # ãƒã‚¯ãƒ­çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®å“è³ªæ¤œè¨¼
        range_quality_score = 0.0
        numeric_cols = data.select_dtypes(include=["number"]).columns

        if len(numeric_cols) > 0:
            # å¦¥å½“ãªæ•°å€¤ç¯„å›²ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆæ¥µç«¯ãªå€¤ã‚’é™¤å¤–ï¼‰
            valid_ranges = 0
            for col in numeric_cols:
                # å„æŒ‡æ¨™ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                if "dxy" in col.lower():
                    # DXYã¯90-120ã®ç¯„å›²ãŒå¦¥å½“
                    valid_ranges += ((data[col] >= 80) & (data[col] <= 130)).sum()
                elif (
                    "treasury" in col.lower()
                    or "us10y" in col.lower()
                    or "us2y" in col.lower()
                ):
                    # é‡‘åˆ©ã¯0-15%ã®ç¯„å›²ãŒå¦¥å½“
                    valid_ranges += ((data[col] >= 0) & (data[col] <= 15)).sum()
                elif "usdjpy" in col.lower():
                    # USD/JPYã¯100-180ã®ç¯„å›²ãŒå¦¥å½“
                    valid_ranges += ((data[col] >= 80) & (data[col] <= 200)).sum()
                else:
                    # ãã®ä»–ã¯ç„¡é™å¤§å€¤ã®ã¿ã‚’ãƒã‚§ãƒƒã‚¯
                    valid_ranges += (~(data[col].isinf() | data[col].isna())).sum()

            range_quality_score = valid_ranges / (len(numeric_cols) * total_points)

        # ç·åˆå“è³ªã‚¹ã‚³ã‚¢
        quality_score = (valid_points / total_points) * 0.6 + range_quality_score * 0.4
        logger.debug(
            f"ğŸ“Š Macro data quality: {quality_score:.3f} "
            f"(valid: {valid_points}/{total_points}, range: {range_quality_score:.3f})"
        )

        return quality_score

    def _fetch_data_from_source(
        self, source_name: str, **kwargs
    ) -> Optional[pd.DataFrame]:
        """ç‰¹å®šãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆMultiSourceDataFetcheræŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè£…ï¼‰"""
        start_date = kwargs.get(
            "start_date", (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")
        )
        end_date = kwargs.get("end_date", datetime.now().strftime("%Y-%m-%d"))

        if source_name == "yahoo":
            return self._fetch_yahoo_macro_data(start_date, end_date)
        elif source_name == "alpha_vantage":
            return self._fetch_alpha_vantage_macro_data(start_date, end_date)
        elif source_name == "fred":
            return self._fetch_fred_macro_data(start_date, end_date)
        else:
            logger.warning(f"Unknown macro data source: {source_name}")
            return None

    def _generate_fallback_data(self, **kwargs) -> Optional[pd.DataFrame]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆMultiSourceDataFetcheræŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè£…ï¼‰"""
        try:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœŸé–“è¨­å®š
            start_date = kwargs.get(
                "start_date", (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
            )
            end_date = kwargs.get("end_date", datetime.now().strftime("%Y-%m-%d"))

            # éå»30æ—¥åˆ†ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
            try:
                dates = pd.date_range(start=start_date, end=end_date, freq="D")
            except ValueError:
                # æ—¥ä»˜ç¯„å›²æŒ‡å®šã«å•é¡ŒãŒã‚ã‚‹å ´åˆã¯30æ—¥é–“ã§ç”Ÿæˆ
                dates = pd.date_range(end=datetime.now(), periods=30, freq="D")

            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆç¾åœ¨ã®æ¦‚ç®—å€¤ï¼‰
            fallback_data = pd.DataFrame(index=dates)
            fallback_data["dxy_close"] = 103.0  # DXYé€šå¸¸ãƒ¬ãƒ™ãƒ«
            fallback_data["us10y_close"] = 4.5  # 10å¹´å‚µåˆ©å›ã‚Š
            fallback_data["us2y_close"] = 4.8  # 2å¹´å‚µåˆ©å›ã‚Š
            fallback_data["usdjpy_close"] = 150.0  # USD/JPY

            logger.info("ğŸ“ˆ Generated macro fallback data: %d days", len(fallback_data))
            return fallback_data

        except Exception as e:
            logger.error(f"âŒ Failed to generate macro fallback data: {e}")
            return None

    @api_retry(max_retries=3, base_delay=2.0, circuit_breaker=True)
    def _fetch_yahoo_macro_data(
        self, start_date: str, end_date: str
    ) -> Optional[pd.DataFrame]:
        """Yahoo Financeã‹ã‚‰ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ã‚’ã¾ã¨ã‚ã¦å–å¾—ï¼ˆCloud Runå¯¾å¿œï¼‰"""
        try:
            # Phase H.19: HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæœ€é©åŒ–
            import os

            from ..utils.http_client_optimizer import get_optimized_client

            is_cloud_run = os.getenv("K_SERVICE") is not None

            if is_cloud_run:
                logger.info("ğŸŒ Cloud Run environment detected for macro data")
                yf.set_tz_cache_location("/tmp")  # Cloud Runç”¨ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

                # Phase H.19: æœ€é©åŒ–ã•ã‚ŒãŸHTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
                http_client = get_optimized_client("yahoo")
                # yfinanceã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æ³¨å…¥
                try:
                    import yfinance.utils as yf_utils

                    yf_utils.requests = http_client.session
                except (ImportError, AttributeError):
                    pass

            combined_data = pd.DataFrame()

            for name, symbol in self.symbols.items():
                try:
                    ticker = yf.Ticker(symbol)

                    # Phase H.17: è¤‡æ•°ã®æ–¹æ³•ã§å–å¾—ã‚’è©¦ã¿ã‚‹
                    data = None

                    # æ–¹æ³•1: history()ãƒ¡ã‚½ãƒƒãƒ‰
                    try:
                        data = ticker.history(start=start_date, end=end_date)
                        if not data.empty:
                            logger.debug(f"âœ… {name} fetched via history()")
                    except Exception as e:
                        logger.debug(f"history() failed for {name}: {e}")

                    # æ–¹æ³•2: download()ã‚’ç›´æ¥ä½¿ç”¨
                    if data is None or data.empty:
                        try:
                            data = yf.download(
                                symbol,
                                start=start_date,
                                end=end_date,
                                progress=False,
                                timeout=30 if is_cloud_run else 10,
                            )
                            if not data.empty:
                                logger.debug(f"âœ… {name} fetched via download()")
                        except Exception as e:
                            logger.debug(f"download() failed for {name}: {e}")

                    if data.empty:
                        logger.warning(f"âš ï¸ No data for {name} ({symbol})")
                        continue

                    # ã‚«ãƒ©ãƒ åã‚’çµ±ä¸€ï¼ˆã‚·ãƒ³ãƒœãƒ«åã‚’ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã«è¿½åŠ ï¼‰
                    data.columns = [f"{name}_{col.lower()}" for col in data.columns]

                    if combined_data.empty:
                        combined_data = data.copy()
                    else:
                        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆæ—¥ä»˜ï¼‰ã§çµåˆ
                        combined_data = combined_data.join(data, how="outer")

                    logger.info(f"âœ… {name} data: {len(data)} records")

                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to fetch {name} data: {e}")
                    continue

            if combined_data.empty:
                raise ValueError("All macro symbols failed to fetch")

            # å‰æ–¹åŸ‹ã‚ã§æ¬ æå€¤ã‚’è£œå®Œ
            combined_data = combined_data.fillna(method="ffill")

            return combined_data

        except Exception as e:
            logger.error(f"Yahoo Finance macro fetch failed: {e}")
            raise

    @api_retry(max_retries=3, base_delay=2.0, circuit_breaker=True)
    def _fetch_alpha_vantage_macro_data(
        self, start_date: str, end_date: str
    ) -> Optional[pd.DataFrame]:
        """Alpha Vantageã‹ã‚‰ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆä»£æ›¿å®Ÿè£…ï¼‰"""
        try:
            # Alpha Vantage APIå®Ÿè£…ï¼ˆç°¡ç•¥ç‰ˆï¼‰
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€Alpha Vantage APIã‚­ãƒ¼ãŒå¿…è¦
            # ç¾åœ¨ã¯ Yahoo Finance ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦å®Ÿè£…
            logger.info("ğŸ“¡ Using Yahoo Finance as Alpha Vantage macro alternative")

            return self._fetch_yahoo_macro_data(start_date, end_date)

        except Exception as e:
            logger.error(f"Alpha Vantage macro fetch failed: {e}")
            raise

    @api_retry(max_retries=3, base_delay=2.0, circuit_breaker=True)
    def _fetch_fred_macro_data(
        self, start_date: str, end_date: str
    ) -> Optional[pd.DataFrame]:
        """FREDï¼ˆFederal Reserve Economic Dataï¼‰ã‹ã‚‰ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆPhase H.22.1: å®ŸAPIã‚­ãƒ¼çµ±åˆï¼‰"""
        try:
            import os

            # Phase H.23.4: FREDå®ŸAPIã‚­ãƒ¼å–å¾—ãƒ»ç›´æ¥åŸ‹ã‚è¾¼ã¿æ–¹å¼
            from ..config.external_api_keys import get_api_key
            from ..utils.http_client_optimizer import OptimizedHTTPClient

            api_key = get_api_key("FRED_API_KEY")
            if not api_key:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚‚ç¢ºèª
                api_key = os.getenv("FRED_API_KEY")
                if not api_key:
                    logger.warning(
                        "âš ï¸ FRED APIã‚­ãƒ¼æœªè¨­å®šã€Yahoo Financeãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨"
                    )
                    return self._fetch_yahoo_macro_data(start_date, end_date)
                else:
                    logger.info("âœ… Phase H.23.4: FREDç’°å¢ƒå¤‰æ•°APIã‚­ãƒ¼ä½¿ç”¨")
            else:
                logger.info("âœ… Phase H.23.4: FREDç›´æ¥åŸ‹ã‚è¾¼ã¿APIã‚­ãƒ¼ä½¿ç”¨")

            logger.info("ğŸ“¡ Phase H.22.1: FREDçµŒæ¸ˆæŒ‡æ¨™å–å¾—é–‹å§‹")

            # FRED APIå®Ÿè£…ï¼ˆå®Ÿã‚­ãƒ¼ä½¿ç”¨ï¼‰
            http_client = OptimizedHTTPClient.get_instance("fred")

            # FREDä¸»è¦çµŒæ¸ˆæŒ‡æ¨™å–å¾—
            fred_indicators = {
                "DGS10": "us10y_close",  # 10å¹´å‚µåˆ©å›ã‚Š
                "DGS2": "us2y_close",  # 2å¹´å‚µåˆ©å›ã‚Š
                "DEXUSEU": "dxy_close",  # ç±³ãƒ‰ãƒ«æŒ‡æ•°ï¼ˆä»£æ›¿ï¼‰
                "DEXJPUS": "usdjpy_close",  # USD/JPY
            }

            combined_data = pd.DataFrame()

            for fred_id, column_name in fred_indicators.items():
                try:
                    url = "https://api.stlouisfed.org/fred/series/observations"
                    params = {
                        "series_id": fred_id,
                        "api_key": api_key,
                        "file_type": "json",
                        "observation_start": start_date,
                        "observation_end": end_date,
                        "frequency": "d",  # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿
                        "aggregation_method": "avg",
                    }

                    response = http_client.get_with_api_optimization(
                        url, "fred", params=params
                    )
                    response.raise_for_status()
                    data = response.json()

                    if "observations" in data:
                        observations = data["observations"]
                        dates = []
                        values = []

                        for obs in observations:
                            if obs["value"] != ".":  # FRED missing value indicator
                                dates.append(pd.Timestamp(obs["date"]))
                                values.append(float(obs["value"]))

                        if dates and values:
                            indicator_df = pd.DataFrame(
                                {"date": dates, column_name: values}
                            ).set_index("date")

                            if combined_data.empty:
                                combined_data = indicator_df
                            else:
                                combined_data = combined_data.join(
                                    indicator_df, how="outer"
                                )

                            logger.info(f"âœ… FRED {fred_id}: {len(indicator_df)}ä»¶å–å¾—")

                except Exception as e:
                    logger.warning(f"âš ï¸ FRED {fred_id}å–å¾—å¤±æ•—: {e}")
                    continue

            if not combined_data.empty:
                logger.info(
                    f"âœ… Phase H.22.1: FREDçµ±åˆãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(combined_data)}ä»¶"
                )
                return combined_data.sort_index()
            else:
                logger.warning("âŒ FREDå…¨æŒ‡æ¨™å–å¾—å¤±æ•—ã€Yahoo Financeãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                return self._fetch_yahoo_macro_data(start_date, end_date)

        except Exception as e:
            logger.error(f"FRED macro fetch failed: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯Yahoo Financeãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._fetch_yahoo_macro_data(start_date, end_date)

    def get_macro_data(
        self,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        limit: Optional[int] = None,
    ) -> Dict[str, pd.DataFrame]:
        """
        ãƒã‚¯ãƒ­çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆMultiSourceDataFetcherçµ±åˆç‰ˆï¼‰

        Args:
            start_date: é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDï¼‰
            end_date: çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDï¼‰

        Returns:
            ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸
        """
        # è¦ªã‚¯ãƒ©ã‚¹ã®get_dataãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—
        unified_data = self.get_data(
            start_date=start_date, end_date=end_date, limit=limit
        )

        if unified_data is None or unified_data.empty:
            logger.warning("âŒ No macro data retrieved from MultiSourceDataFetcher")
            return {}

        # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
        macro_data = {}
        for symbol_name in self.symbols.keys():
            # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„ã‚·ãƒ³ãƒœãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            symbol_columns = [
                col for col in unified_data.columns if symbol_name in col.lower()
            ]
            if symbol_columns:
                macro_data[symbol_name] = unified_data[symbol_columns].copy()
                # 'close'åˆ—ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆç‰¹å¾´é‡è¨ˆç®—ã§ä½¿ç”¨ï¼‰
                if "close" not in macro_data[symbol_name].columns and symbol_columns:
                    # æœ€åˆã®æ•°å€¤åˆ—ã‚’closeã¨ã—ã¦ä½¿ç”¨
                    numeric_cols = (
                        macro_data[symbol_name]
                        .select_dtypes(include=["number"])
                        .columns
                    )
                    if len(numeric_cols) > 0:
                        macro_data[symbol_name]["close"] = macro_data[symbol_name][
                            numeric_cols[0]
                        ]

        return macro_data

    def calculate_macro_features(
        self, macro_data: Dict[str, pd.DataFrame]
    ) -> pd.DataFrame:
        """
        ãƒã‚¯ãƒ­çµŒæ¸ˆç‰¹å¾´é‡è¨ˆç®—ï¼ˆ10ç‰¹å¾´é‡ï¼‰

        Args:
            macro_data: ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿è¾æ›¸

        Returns:
            ãƒã‚¯ãƒ­ç‰¹å¾´é‡DataFrame
        """
        try:
            if not macro_data or all(df.empty for df in macro_data.values()):
                # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§DataFrameã‚’ä½œæˆ
                default_features = self._get_default_macro_features()
                return pd.DataFrame(
                    [default_features], index=[pd.Timestamp.now(tz="UTC")]
                )

            # å…±é€šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®šï¼ˆæœ€ã‚‚é•·ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åŸºæº–ï¼‰
            max_data = max(macro_data.values(), key=len, default=pd.DataFrame())
            if max_data.empty:
                default_features = self._get_default_macro_features()
                return pd.DataFrame(
                    [default_features], index=[pd.Timestamp.now(tz="UTC")]
                )

            features_df = pd.DataFrame(index=max_data.index)

            # DXYï¼ˆãƒ‰ãƒ«æŒ‡æ•°ï¼‰ç‰¹å¾´é‡ï¼ˆpreprocessoræœŸå¾…å½¢å¼ï¼‰
            if "dxy" in macro_data and not macro_data["dxy"].empty:
                dxy_data = macro_data["dxy"]
                # Phase H.18ä¿®æ­£: å‹å®‰å…¨ãªreindexå‡¦ç†
                try:
                    dxy_aligned = dxy_data.reindex(features_df.index, method="ffill")
                except TypeError:
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‹ä¸ä¸€è‡´æ™‚ã®å‡¦ç†
                    if hasattr(dxy_data.index, "to_timestamp"):
                        dxy_data.index = dxy_data.index.to_timestamp()
                    dxy_aligned = dxy_data.reindex(features_df.index, method="ffill")

                dxy_ma20 = dxy_aligned["close"].rolling(20).mean()
                dxy_std = dxy_aligned["close"].rolling(20).std()

                features_df["dxy_level"] = dxy_aligned["close"]
                features_df["dxy_change"] = dxy_aligned["close"].pct_change()
                features_df["dxy_zscore"] = (dxy_aligned["close"] - dxy_ma20) / dxy_std
                features_df["dxy_strength"] = (dxy_aligned["close"] > dxy_ma20).astype(
                    int
                )
            else:
                defaults = self._get_default_dxy_features()
                for key, value in defaults.items():
                    features_df[key] = value

            # ç±³10å¹´å‚µåˆ©å›ã‚Šç‰¹å¾´é‡ï¼ˆpreprocessoræœŸå¾…å½¢å¼ï¼‰
            if "us10y" in macro_data and not macro_data["us10y"].empty:
                us10y_data = macro_data["us10y"]
                # Phase H.18ä¿®æ­£: å‹å®‰å…¨ãªreindexå‡¦ç†
                try:
                    us10y_aligned = us10y_data.reindex(
                        features_df.index, method="ffill"
                    )
                except TypeError:
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‹ä¸ä¸€è‡´æ™‚ã®å‡¦ç†
                    if hasattr(us10y_data.index, "to_timestamp"):
                        us10y_data.index = us10y_data.index.to_timestamp()
                    us10y_aligned = us10y_data.reindex(
                        features_df.index, method="ffill"
                    )

                us10y_ma20 = us10y_aligned["close"].rolling(20).mean()
                us10y_std = us10y_aligned["close"].rolling(20).std()

                features_df["treasury_10y_level"] = us10y_aligned["close"]
                features_df["treasury_10y_change"] = us10y_aligned["close"].diff()
                features_df["treasury_10y_zscore"] = (
                    us10y_aligned["close"] - us10y_ma20
                ) / us10y_std
                features_df["treasury_regime"] = (
                    us10y_aligned["close"] > us10y_ma20
                ).astype(int)
            else:
                defaults = self._get_default_10y_features()
                for key, value in defaults.items():
                    features_df[key] = value

            # ç±³2å¹´å‚µåˆ©å›ã‚Šç‰¹å¾´é‡ã¨ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–
            if "us2y" in macro_data and not macro_data["us2y"].empty:
                us2y_data = macro_data["us2y"]
                # Phase H.18ä¿®æ­£: å‹å®‰å…¨ãªreindexå‡¦ç†
                try:
                    us2y_aligned = us2y_data.reindex(features_df.index, method="ffill")
                except TypeError:
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‹ä¸ä¸€è‡´æ™‚ã®å‡¦ç†
                    if hasattr(us2y_data.index, "to_timestamp"):
                        us2y_data.index = us2y_data.index.to_timestamp()
                    us2y_aligned = us2y_data.reindex(features_df.index, method="ffill")

                # ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–ã¨ãƒªã‚¹ã‚¯ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ
                treasury_10y = features_df.get("treasury_10y_level", 4.0)
                if isinstance(treasury_10y, pd.Series):
                    yield_spread = treasury_10y - us2y_aligned["close"]
                    features_df["yield_curve_spread"] = yield_spread
                    features_df["risk_sentiment"] = (yield_spread > 0).astype(int)
                else:
                    features_df["yield_curve_spread"] = (
                        treasury_10y - us2y_aligned["close"]
                    )
                    features_df["risk_sentiment"] = (
                        features_df["yield_curve_spread"] > 0
                    ).astype(int)
            else:
                defaults = self._get_default_2y_features()
                for key, value in defaults.items():
                    features_df[key] = value

            # USD/JPYç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆç‰¹å¾´é‡ï¼ˆBTC/JPYäºˆæ¸¬ç²¾åº¦å‘ä¸Šï¼‰
            if "usdjpy" in macro_data and not macro_data["usdjpy"].empty:
                usdjpy_data = macro_data["usdjpy"]
                # Phase H.18ä¿®æ­£: å‹å®‰å…¨ãªreindexå‡¦ç†
                try:
                    usdjpy_aligned = usdjpy_data.reindex(
                        features_df.index, method="ffill"
                    )
                except TypeError:
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‹ä¸ä¸€è‡´æ™‚ã®å‡¦ç†
                    if hasattr(usdjpy_data.index, "to_timestamp"):
                        usdjpy_data.index = usdjpy_data.index.to_timestamp()
                    usdjpy_aligned = usdjpy_data.reindex(
                        features_df.index, method="ffill"
                    )

                usdjpy_ma20 = usdjpy_aligned["close"].rolling(20).mean()
                usdjpy_std = usdjpy_aligned["close"].rolling(20).std()

                features_df["usdjpy_level"] = usdjpy_aligned["close"]
                features_df["usdjpy_change"] = usdjpy_aligned["close"].pct_change()
                features_df["usdjpy_volatility"] = (
                    usdjpy_aligned["close"].rolling(24).std()
                )
                features_df["usdjpy_zscore"] = (
                    usdjpy_aligned["close"] - usdjpy_ma20
                ) / usdjpy_std
                features_df["usdjpy_trend"] = (
                    usdjpy_aligned["close"] > usdjpy_ma20
                ).astype(int)
                features_df["usdjpy_strength"] = (
                    usdjpy_aligned["close"].pct_change() > 0
                ).astype(int)
            else:
                defaults = self._get_default_usdjpy_features()
                for key, value in defaults.items():
                    features_df[key] = value

            # NaNå€¤ã‚’é©åˆ‡ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§è£œå®Œ
            default_values = self._get_default_macro_features()
            for col in features_df.columns:
                features_df[col] = features_df[col].fillna(default_values.get(col, 0))
                # ç„¡é™å¤§å€¤ã‚‚è£œå®Œ
                features_df[col] = features_df[col].replace(
                    [float("inf"), float("-inf")], default_values.get(col, 0)
                )

            logger.info(
                f"Macro features calculated: {len(features_df)} rows, "
                f"{len(features_df.columns)} columns"
            )
            return features_df

        except Exception as e:
            logger.error(f"Failed to calculate macro features: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§DataFrameã‚’è¿”ã™
            default_features = self._get_default_macro_features()
            return pd.DataFrame([default_features], index=[pd.Timestamp.now(tz="UTC")])

    def _get_default_dxy_features(self) -> Dict[str, Any]:
        """DXYç‰¹å¾´é‡ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤"""
        return {
            "dxy_level": 103.0,  # å…¸å‹çš„ãªDXYãƒ¬ãƒ™ãƒ«
            "dxy_change": 0.0,
            "dxy_zscore": 0.0,
            "dxy_strength": 0,
        }

    def _get_default_10y_features(self) -> Dict[str, Any]:
        """10å¹´å‚µç‰¹å¾´é‡ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤"""
        return {
            "treasury_10y_level": 4.0,  # å…¸å‹çš„ãª10å¹´å‚µåˆ©å›ã‚Š
            "treasury_10y_change": 0.0,
            "treasury_10y_zscore": 0.0,
            "treasury_regime": 0,
        }

    def _get_default_2y_features(self) -> Dict[str, Any]:
        """2å¹´å‚µç‰¹å¾´é‡ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤"""
        return {
            "yield_curve_spread": -0.5,  # é€†ã‚¤ãƒ¼ãƒ«ãƒ‰çŠ¶æ…‹
            "risk_sentiment": 0,
        }

    def _get_default_usdjpy_features(self) -> Dict[str, Any]:
        """USD/JPYç‚ºæ›¿ç‰¹å¾´é‡ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤"""
        return {
            "usdjpy_level": 150.0,  # å…¸å‹çš„ãªUSD/JPYãƒ¬ãƒ™ãƒ«
            "usdjpy_change": 0.0,  # å¤‰å‹•ç‡
            "usdjpy_volatility": 0.005,  # ç‚ºæ›¿ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            "usdjpy_zscore": 0.0,  # Z-score
            "usdjpy_trend": 0,  # ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘
            "usdjpy_strength": 0,  # å¼·åº¦
        }

    def _get_default_macro_features(self) -> Dict[str, Any]:
        """ãƒã‚¯ãƒ­ç‰¹å¾´é‡ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆ16ç‰¹å¾´é‡ï¼‰"""
        features = {}
        features.update(self._get_default_dxy_features())
        features.update(self._get_default_10y_features())
        features.update(self._get_default_2y_features())
        features.update(self._get_default_usdjpy_features())
        return features

    def _count_default_values(self, data: pd.DataFrame) -> int:
        """ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå“è³ªç›£è¦–ç”¨ï¼‰"""
        try:
            default_count = 0

            # DXYç‰¹æœ‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãƒã‚§ãƒƒã‚¯
            for col in data.columns:
                if "dxy" in col.lower() and "close" in col.lower():
                    # DXY=103.0ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                    default_count += (data[col] == 103.0).sum()
                elif (
                    "treasury" in col.lower() or "us10y" in col.lower()
                ) and "close" in col.lower():
                    # 10å¹´å‚µ=4.5ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                    default_count += (data[col] == 4.5).sum()
                elif "us2y" in col.lower() and "close" in col.lower():
                    # 2å¹´å‚µ=4.8ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                    default_count += (data[col] == 4.8).sum()
                elif "usdjpy" in col.lower() and "close" in col.lower():
                    # USD/JPY=150.0ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                    default_count += (data[col] == 150.0).sum()

            return default_count

        except Exception as e:
            logger.error(f"âŒ Failed to count macro default values: {e}")
            return 0
